// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: store_types.proto

package org.apache.spark.status.protobuf;

public final class StoreTypes {
  private StoreTypes() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * Protobuf enum {@code org.apache.spark.status.protobuf.JobExecutionStatus}
   */
  public enum JobExecutionStatus
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>JOB_EXECUTION_STATUS_UNSPECIFIED = 0;</code>
     */
    JOB_EXECUTION_STATUS_UNSPECIFIED(0),
    /**
     * <code>JOB_EXECUTION_STATUS_RUNNING = 1;</code>
     */
    JOB_EXECUTION_STATUS_RUNNING(1),
    /**
     * <code>JOB_EXECUTION_STATUS_SUCCEEDED = 2;</code>
     */
    JOB_EXECUTION_STATUS_SUCCEEDED(2),
    /**
     * <code>JOB_EXECUTION_STATUS_FAILED = 3;</code>
     */
    JOB_EXECUTION_STATUS_FAILED(3),
    /**
     * <code>JOB_EXECUTION_STATUS_UNKNOWN = 4;</code>
     */
    JOB_EXECUTION_STATUS_UNKNOWN(4),
    UNRECOGNIZED(-1),
    ;

    /**
     * <code>JOB_EXECUTION_STATUS_UNSPECIFIED = 0;</code>
     */
    public static final int JOB_EXECUTION_STATUS_UNSPECIFIED_VALUE = 0;
    /**
     * <code>JOB_EXECUTION_STATUS_RUNNING = 1;</code>
     */
    public static final int JOB_EXECUTION_STATUS_RUNNING_VALUE = 1;
    /**
     * <code>JOB_EXECUTION_STATUS_SUCCEEDED = 2;</code>
     */
    public static final int JOB_EXECUTION_STATUS_SUCCEEDED_VALUE = 2;
    /**
     * <code>JOB_EXECUTION_STATUS_FAILED = 3;</code>
     */
    public static final int JOB_EXECUTION_STATUS_FAILED_VALUE = 3;
    /**
     * <code>JOB_EXECUTION_STATUS_UNKNOWN = 4;</code>
     */
    public static final int JOB_EXECUTION_STATUS_UNKNOWN_VALUE = 4;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static JobExecutionStatus valueOf(int value) {
      return forNumber(value);
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     */
    public static JobExecutionStatus forNumber(int value) {
      switch (value) {
        case 0: return JOB_EXECUTION_STATUS_UNSPECIFIED;
        case 1: return JOB_EXECUTION_STATUS_RUNNING;
        case 2: return JOB_EXECUTION_STATUS_SUCCEEDED;
        case 3: return JOB_EXECUTION_STATUS_FAILED;
        case 4: return JOB_EXECUTION_STATUS_UNKNOWN;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<JobExecutionStatus>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        JobExecutionStatus> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<JobExecutionStatus>() {
            public JobExecutionStatus findValueByNumber(int number) {
              return JobExecutionStatus.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalStateException(
            "Can't get the descriptor of an unrecognized enum value.");
      }
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.getDescriptor().getEnumTypes().get(0);
    }

    private static final JobExecutionStatus[] VALUES = values();

    public static JobExecutionStatus valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private JobExecutionStatus(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:org.apache.spark.status.protobuf.JobExecutionStatus)
  }

  /**
   * Protobuf enum {@code org.apache.spark.status.protobuf.DeterministicLevel}
   */
  public enum DeterministicLevel
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>DETERMINISTIC_LEVEL_UNSPECIFIED = 0;</code>
     */
    DETERMINISTIC_LEVEL_UNSPECIFIED(0),
    /**
     * <code>DETERMINISTIC_LEVEL_DETERMINATE = 1;</code>
     */
    DETERMINISTIC_LEVEL_DETERMINATE(1),
    /**
     * <code>DETERMINISTIC_LEVEL_UNORDERED = 2;</code>
     */
    DETERMINISTIC_LEVEL_UNORDERED(2),
    /**
     * <code>DETERMINISTIC_LEVEL_INDETERMINATE = 3;</code>
     */
    DETERMINISTIC_LEVEL_INDETERMINATE(3),
    UNRECOGNIZED(-1),
    ;

    /**
     * <code>DETERMINISTIC_LEVEL_UNSPECIFIED = 0;</code>
     */
    public static final int DETERMINISTIC_LEVEL_UNSPECIFIED_VALUE = 0;
    /**
     * <code>DETERMINISTIC_LEVEL_DETERMINATE = 1;</code>
     */
    public static final int DETERMINISTIC_LEVEL_DETERMINATE_VALUE = 1;
    /**
     * <code>DETERMINISTIC_LEVEL_UNORDERED = 2;</code>
     */
    public static final int DETERMINISTIC_LEVEL_UNORDERED_VALUE = 2;
    /**
     * <code>DETERMINISTIC_LEVEL_INDETERMINATE = 3;</code>
     */
    public static final int DETERMINISTIC_LEVEL_INDETERMINATE_VALUE = 3;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static DeterministicLevel valueOf(int value) {
      return forNumber(value);
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     */
    public static DeterministicLevel forNumber(int value) {
      switch (value) {
        case 0: return DETERMINISTIC_LEVEL_UNSPECIFIED;
        case 1: return DETERMINISTIC_LEVEL_DETERMINATE;
        case 2: return DETERMINISTIC_LEVEL_UNORDERED;
        case 3: return DETERMINISTIC_LEVEL_INDETERMINATE;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<DeterministicLevel>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        DeterministicLevel> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<DeterministicLevel>() {
            public DeterministicLevel findValueByNumber(int number) {
              return DeterministicLevel.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalStateException(
            "Can't get the descriptor of an unrecognized enum value.");
      }
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.getDescriptor().getEnumTypes().get(1);
    }

    private static final DeterministicLevel[] VALUES = values();

    public static DeterministicLevel valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private DeterministicLevel(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:org.apache.spark.status.protobuf.DeterministicLevel)
  }

  /**
   * Protobuf enum {@code org.apache.spark.status.protobuf.StageStatus}
   */
  public enum StageStatus
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>STAGE_STATUS_UNSPECIFIED = 0;</code>
     */
    STAGE_STATUS_UNSPECIFIED(0),
    /**
     * <code>STAGE_STATUS_ACTIVE = 1;</code>
     */
    STAGE_STATUS_ACTIVE(1),
    /**
     * <code>STAGE_STATUS_COMPLETE = 2;</code>
     */
    STAGE_STATUS_COMPLETE(2),
    /**
     * <code>STAGE_STATUS_FAILED = 3;</code>
     */
    STAGE_STATUS_FAILED(3),
    /**
     * <code>STAGE_STATUS_PENDING = 4;</code>
     */
    STAGE_STATUS_PENDING(4),
    /**
     * <code>STAGE_STATUS_SKIPPED = 5;</code>
     */
    STAGE_STATUS_SKIPPED(5),
    UNRECOGNIZED(-1),
    ;

    /**
     * <code>STAGE_STATUS_UNSPECIFIED = 0;</code>
     */
    public static final int STAGE_STATUS_UNSPECIFIED_VALUE = 0;
    /**
     * <code>STAGE_STATUS_ACTIVE = 1;</code>
     */
    public static final int STAGE_STATUS_ACTIVE_VALUE = 1;
    /**
     * <code>STAGE_STATUS_COMPLETE = 2;</code>
     */
    public static final int STAGE_STATUS_COMPLETE_VALUE = 2;
    /**
     * <code>STAGE_STATUS_FAILED = 3;</code>
     */
    public static final int STAGE_STATUS_FAILED_VALUE = 3;
    /**
     * <code>STAGE_STATUS_PENDING = 4;</code>
     */
    public static final int STAGE_STATUS_PENDING_VALUE = 4;
    /**
     * <code>STAGE_STATUS_SKIPPED = 5;</code>
     */
    public static final int STAGE_STATUS_SKIPPED_VALUE = 5;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static StageStatus valueOf(int value) {
      return forNumber(value);
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     */
    public static StageStatus forNumber(int value) {
      switch (value) {
        case 0: return STAGE_STATUS_UNSPECIFIED;
        case 1: return STAGE_STATUS_ACTIVE;
        case 2: return STAGE_STATUS_COMPLETE;
        case 3: return STAGE_STATUS_FAILED;
        case 4: return STAGE_STATUS_PENDING;
        case 5: return STAGE_STATUS_SKIPPED;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<StageStatus>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        StageStatus> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<StageStatus>() {
            public StageStatus findValueByNumber(int number) {
              return StageStatus.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalStateException(
            "Can't get the descriptor of an unrecognized enum value.");
      }
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.getDescriptor().getEnumTypes().get(2);
    }

    private static final StageStatus[] VALUES = values();

    public static StageStatus valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private StageStatus(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:org.apache.spark.status.protobuf.StageStatus)
  }

  public interface JobDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.JobData)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * All IDs are int64 for extendability, even when they are currently int32 in Spark.
     * </pre>
     *
     * <code>int64 job_id = 1;</code>
     * @return The jobId.
     */
    long getJobId();

    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>string description = 3;</code>
     * @return Whether the description field is set.
     */
    boolean hasDescription();
    /**
     * <code>string description = 3;</code>
     * @return The description.
     */
    java.lang.String getDescription();
    /**
     * <code>string description = 3;</code>
     * @return The bytes for description.
     */
    com.google.protobuf.ByteString
        getDescriptionBytes();

    /**
     * <code>int64 submission_time = 4;</code>
     * @return Whether the submissionTime field is set.
     */
    boolean hasSubmissionTime();
    /**
     * <code>int64 submission_time = 4;</code>
     * @return The submissionTime.
     */
    long getSubmissionTime();

    /**
     * <code>int64 completion_time = 5;</code>
     * @return Whether the completionTime field is set.
     */
    boolean hasCompletionTime();
    /**
     * <code>int64 completion_time = 5;</code>
     * @return The completionTime.
     */
    long getCompletionTime();

    /**
     * <code>repeated int64 stage_ids = 6;</code>
     * @return A list containing the stageIds.
     */
    java.util.List<java.lang.Long> getStageIdsList();
    /**
     * <code>repeated int64 stage_ids = 6;</code>
     * @return The count of stageIds.
     */
    int getStageIdsCount();
    /**
     * <code>repeated int64 stage_ids = 6;</code>
     * @param index The index of the element to return.
     * @return The stageIds at the given index.
     */
    long getStageIds(int index);

    /**
     * <code>string job_group = 7;</code>
     * @return Whether the jobGroup field is set.
     */
    boolean hasJobGroup();
    /**
     * <code>string job_group = 7;</code>
     * @return The jobGroup.
     */
    java.lang.String getJobGroup();
    /**
     * <code>string job_group = 7;</code>
     * @return The bytes for jobGroup.
     */
    com.google.protobuf.ByteString
        getJobGroupBytes();

    /**
     * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
     * @return The enum numeric value on the wire for status.
     */
    int getStatusValue();
    /**
     * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
     * @return The status.
     */
    org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getStatus();

    /**
     * <code>int32 num_tasks = 9;</code>
     * @return The numTasks.
     */
    int getNumTasks();

    /**
     * <code>int32 num_active_tasks = 10;</code>
     * @return The numActiveTasks.
     */
    int getNumActiveTasks();

    /**
     * <code>int32 num_completed_tasks = 11;</code>
     * @return The numCompletedTasks.
     */
    int getNumCompletedTasks();

    /**
     * <code>int32 num_skipped_tasks = 12;</code>
     * @return The numSkippedTasks.
     */
    int getNumSkippedTasks();

    /**
     * <code>int32 num_failed_tasks = 13;</code>
     * @return The numFailedTasks.
     */
    int getNumFailedTasks();

    /**
     * <code>int32 num_killed_tasks = 14;</code>
     * @return The numKilledTasks.
     */
    int getNumKilledTasks();

    /**
     * <code>int32 num_completed_indices = 15;</code>
     * @return The numCompletedIndices.
     */
    int getNumCompletedIndices();

    /**
     * <code>int32 num_active_stages = 16;</code>
     * @return The numActiveStages.
     */
    int getNumActiveStages();

    /**
     * <code>int32 num_completed_stages = 17;</code>
     * @return The numCompletedStages.
     */
    int getNumCompletedStages();

    /**
     * <code>int32 num_skipped_stages = 18;</code>
     * @return The numSkippedStages.
     */
    int getNumSkippedStages();

    /**
     * <code>int32 num_failed_stages = 19;</code>
     * @return The numFailedStages.
     */
    int getNumFailedStages();

    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */
    int getKillTasksSummaryCount();
    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */
    boolean containsKillTasksSummary(
        java.lang.String key);
    /**
     * Use {@link #getKillTasksSummaryMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.Integer>
    getKillTasksSummary();
    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */
    java.util.Map<java.lang.String, java.lang.Integer>
    getKillTasksSummaryMap();
    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */

    int getKillTasksSummaryOrDefault(
        java.lang.String key,
        int defaultValue);
    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */

    int getKillTasksSummaryOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.JobData}
   */
  public static final class JobData extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.JobData)
      JobDataOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use JobData.newBuilder() to construct.
    private JobData(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private JobData() {
      name_ = "";
      description_ = "";
      stageIds_ = emptyLongList();
      jobGroup_ = "";
      status_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new JobData();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private JobData(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              jobId_ = input.readInt64();
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              description_ = s;
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              submissionTime_ = input.readInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000008;
              completionTime_ = input.readInt64();
              break;
            }
            case 48: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                stageIds_ = newLongList();
                mutable_bitField0_ |= 0x00000010;
              }
              stageIds_.addLong(input.readInt64());
              break;
            }
            case 50: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000010) != 0) && input.getBytesUntilLimit() > 0) {
                stageIds_ = newLongList();
                mutable_bitField0_ |= 0x00000010;
              }
              while (input.getBytesUntilLimit() > 0) {
                stageIds_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 58: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000010;
              jobGroup_ = s;
              break;
            }
            case 64: {
              int rawValue = input.readEnum();

              status_ = rawValue;
              break;
            }
            case 72: {

              numTasks_ = input.readInt32();
              break;
            }
            case 80: {

              numActiveTasks_ = input.readInt32();
              break;
            }
            case 88: {

              numCompletedTasks_ = input.readInt32();
              break;
            }
            case 96: {

              numSkippedTasks_ = input.readInt32();
              break;
            }
            case 104: {

              numFailedTasks_ = input.readInt32();
              break;
            }
            case 112: {

              numKilledTasks_ = input.readInt32();
              break;
            }
            case 120: {

              numCompletedIndices_ = input.readInt32();
              break;
            }
            case 128: {

              numActiveStages_ = input.readInt32();
              break;
            }
            case 136: {

              numCompletedStages_ = input.readInt32();
              break;
            }
            case 144: {

              numSkippedStages_ = input.readInt32();
              break;
            }
            case 152: {

              numFailedStages_ = input.readInt32();
              break;
            }
            case 162: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                killTasksSummary_ = com.google.protobuf.MapField.newMapField(
                    KillTasksSummaryDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000040;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.Integer>
              killTasksSummary__ = input.readMessage(
                  KillTasksSummaryDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              killTasksSummary_.getMutableMap().put(
                  killTasksSummary__.getKey(), killTasksSummary__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          stageIds_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobData_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 20:
          return internalGetKillTasksSummary();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.JobData.class, org.apache.spark.status.protobuf.StoreTypes.JobData.Builder.class);
    }

    private int bitField0_;
    public static final int JOB_ID_FIELD_NUMBER = 1;
    private long jobId_;
    /**
     * <pre>
     * All IDs are int64 for extendability, even when they are currently int32 in Spark.
     * </pre>
     *
     * <code>int64 job_id = 1;</code>
     * @return The jobId.
     */
    @java.lang.Override
    public long getJobId() {
      return jobId_;
    }

    public static final int NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DESCRIPTION_FIELD_NUMBER = 3;
    private volatile java.lang.Object description_;
    /**
     * <code>string description = 3;</code>
     * @return Whether the description field is set.
     */
    @java.lang.Override
    public boolean hasDescription() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string description = 3;</code>
     * @return The description.
     */
    @java.lang.Override
    public java.lang.String getDescription() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        description_ = s;
        return s;
      }
    }
    /**
     * <code>string description = 3;</code>
     * @return The bytes for description.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDescriptionBytes() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        description_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int SUBMISSION_TIME_FIELD_NUMBER = 4;
    private long submissionTime_;
    /**
     * <code>int64 submission_time = 4;</code>
     * @return Whether the submissionTime field is set.
     */
    @java.lang.Override
    public boolean hasSubmissionTime() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>int64 submission_time = 4;</code>
     * @return The submissionTime.
     */
    @java.lang.Override
    public long getSubmissionTime() {
      return submissionTime_;
    }

    public static final int COMPLETION_TIME_FIELD_NUMBER = 5;
    private long completionTime_;
    /**
     * <code>int64 completion_time = 5;</code>
     * @return Whether the completionTime field is set.
     */
    @java.lang.Override
    public boolean hasCompletionTime() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>int64 completion_time = 5;</code>
     * @return The completionTime.
     */
    @java.lang.Override
    public long getCompletionTime() {
      return completionTime_;
    }

    public static final int STAGE_IDS_FIELD_NUMBER = 6;
    private com.google.protobuf.Internal.LongList stageIds_;
    /**
     * <code>repeated int64 stage_ids = 6;</code>
     * @return A list containing the stageIds.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getStageIdsList() {
      return stageIds_;
    }
    /**
     * <code>repeated int64 stage_ids = 6;</code>
     * @return The count of stageIds.
     */
    public int getStageIdsCount() {
      return stageIds_.size();
    }
    /**
     * <code>repeated int64 stage_ids = 6;</code>
     * @param index The index of the element to return.
     * @return The stageIds at the given index.
     */
    public long getStageIds(int index) {
      return stageIds_.getLong(index);
    }
    private int stageIdsMemoizedSerializedSize = -1;

    public static final int JOB_GROUP_FIELD_NUMBER = 7;
    private volatile java.lang.Object jobGroup_;
    /**
     * <code>string job_group = 7;</code>
     * @return Whether the jobGroup field is set.
     */
    @java.lang.Override
    public boolean hasJobGroup() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>string job_group = 7;</code>
     * @return The jobGroup.
     */
    @java.lang.Override
    public java.lang.String getJobGroup() {
      java.lang.Object ref = jobGroup_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        jobGroup_ = s;
        return s;
      }
    }
    /**
     * <code>string job_group = 7;</code>
     * @return The bytes for jobGroup.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getJobGroupBytes() {
      java.lang.Object ref = jobGroup_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        jobGroup_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int STATUS_FIELD_NUMBER = 8;
    private int status_;
    /**
     * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
     * @return The enum numeric value on the wire for status.
     */
    @java.lang.Override public int getStatusValue() {
      return status_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
     * @return The status.
     */
    @java.lang.Override public org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getStatus() {
      @SuppressWarnings("deprecation")
      org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus result = org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.valueOf(status_);
      return result == null ? org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.UNRECOGNIZED : result;
    }

    public static final int NUM_TASKS_FIELD_NUMBER = 9;
    private int numTasks_;
    /**
     * <code>int32 num_tasks = 9;</code>
     * @return The numTasks.
     */
    @java.lang.Override
    public int getNumTasks() {
      return numTasks_;
    }

    public static final int NUM_ACTIVE_TASKS_FIELD_NUMBER = 10;
    private int numActiveTasks_;
    /**
     * <code>int32 num_active_tasks = 10;</code>
     * @return The numActiveTasks.
     */
    @java.lang.Override
    public int getNumActiveTasks() {
      return numActiveTasks_;
    }

    public static final int NUM_COMPLETED_TASKS_FIELD_NUMBER = 11;
    private int numCompletedTasks_;
    /**
     * <code>int32 num_completed_tasks = 11;</code>
     * @return The numCompletedTasks.
     */
    @java.lang.Override
    public int getNumCompletedTasks() {
      return numCompletedTasks_;
    }

    public static final int NUM_SKIPPED_TASKS_FIELD_NUMBER = 12;
    private int numSkippedTasks_;
    /**
     * <code>int32 num_skipped_tasks = 12;</code>
     * @return The numSkippedTasks.
     */
    @java.lang.Override
    public int getNumSkippedTasks() {
      return numSkippedTasks_;
    }

    public static final int NUM_FAILED_TASKS_FIELD_NUMBER = 13;
    private int numFailedTasks_;
    /**
     * <code>int32 num_failed_tasks = 13;</code>
     * @return The numFailedTasks.
     */
    @java.lang.Override
    public int getNumFailedTasks() {
      return numFailedTasks_;
    }

    public static final int NUM_KILLED_TASKS_FIELD_NUMBER = 14;
    private int numKilledTasks_;
    /**
     * <code>int32 num_killed_tasks = 14;</code>
     * @return The numKilledTasks.
     */
    @java.lang.Override
    public int getNumKilledTasks() {
      return numKilledTasks_;
    }

    public static final int NUM_COMPLETED_INDICES_FIELD_NUMBER = 15;
    private int numCompletedIndices_;
    /**
     * <code>int32 num_completed_indices = 15;</code>
     * @return The numCompletedIndices.
     */
    @java.lang.Override
    public int getNumCompletedIndices() {
      return numCompletedIndices_;
    }

    public static final int NUM_ACTIVE_STAGES_FIELD_NUMBER = 16;
    private int numActiveStages_;
    /**
     * <code>int32 num_active_stages = 16;</code>
     * @return The numActiveStages.
     */
    @java.lang.Override
    public int getNumActiveStages() {
      return numActiveStages_;
    }

    public static final int NUM_COMPLETED_STAGES_FIELD_NUMBER = 17;
    private int numCompletedStages_;
    /**
     * <code>int32 num_completed_stages = 17;</code>
     * @return The numCompletedStages.
     */
    @java.lang.Override
    public int getNumCompletedStages() {
      return numCompletedStages_;
    }

    public static final int NUM_SKIPPED_STAGES_FIELD_NUMBER = 18;
    private int numSkippedStages_;
    /**
     * <code>int32 num_skipped_stages = 18;</code>
     * @return The numSkippedStages.
     */
    @java.lang.Override
    public int getNumSkippedStages() {
      return numSkippedStages_;
    }

    public static final int NUM_FAILED_STAGES_FIELD_NUMBER = 19;
    private int numFailedStages_;
    /**
     * <code>int32 num_failed_stages = 19;</code>
     * @return The numFailedStages.
     */
    @java.lang.Override
    public int getNumFailedStages() {
      return numFailedStages_;
    }

    public static final int KILL_TASKS_SUMMARY_FIELD_NUMBER = 20;
    private static final class KillTasksSummaryDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.Integer> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.Integer>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobData_KillTasksSummaryEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.INT32,
                  0);
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.Integer> killTasksSummary_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
    internalGetKillTasksSummary() {
      if (killTasksSummary_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            KillTasksSummaryDefaultEntryHolder.defaultEntry);
      }
      return killTasksSummary_;
    }

    public int getKillTasksSummaryCount() {
      return internalGetKillTasksSummary().getMap().size();
    }
    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */

    @java.lang.Override
    public boolean containsKillTasksSummary(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetKillTasksSummary().getMap().containsKey(key);
    }
    /**
     * Use {@link #getKillTasksSummaryMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.Integer> getKillTasksSummary() {
      return getKillTasksSummaryMap();
    }
    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.Integer> getKillTasksSummaryMap() {
      return internalGetKillTasksSummary().getMap();
    }
    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */
    @java.lang.Override

    public int getKillTasksSummaryOrDefault(
        java.lang.String key,
        int defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Integer> map =
          internalGetKillTasksSummary().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
     */
    @java.lang.Override

    public int getKillTasksSummaryOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Integer> map =
          internalGetKillTasksSummary().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (jobId_ != 0L) {
        output.writeInt64(1, jobId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, description_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt64(4, submissionTime_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(5, completionTime_);
      }
      if (getStageIdsList().size() > 0) {
        output.writeUInt32NoTag(50);
        output.writeUInt32NoTag(stageIdsMemoizedSerializedSize);
      }
      for (int i = 0; i < stageIds_.size(); i++) {
        output.writeInt64NoTag(stageIds_.getLong(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 7, jobGroup_);
      }
      if (status_ != org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.JOB_EXECUTION_STATUS_UNSPECIFIED.getNumber()) {
        output.writeEnum(8, status_);
      }
      if (numTasks_ != 0) {
        output.writeInt32(9, numTasks_);
      }
      if (numActiveTasks_ != 0) {
        output.writeInt32(10, numActiveTasks_);
      }
      if (numCompletedTasks_ != 0) {
        output.writeInt32(11, numCompletedTasks_);
      }
      if (numSkippedTasks_ != 0) {
        output.writeInt32(12, numSkippedTasks_);
      }
      if (numFailedTasks_ != 0) {
        output.writeInt32(13, numFailedTasks_);
      }
      if (numKilledTasks_ != 0) {
        output.writeInt32(14, numKilledTasks_);
      }
      if (numCompletedIndices_ != 0) {
        output.writeInt32(15, numCompletedIndices_);
      }
      if (numActiveStages_ != 0) {
        output.writeInt32(16, numActiveStages_);
      }
      if (numCompletedStages_ != 0) {
        output.writeInt32(17, numCompletedStages_);
      }
      if (numSkippedStages_ != 0) {
        output.writeInt32(18, numSkippedStages_);
      }
      if (numFailedStages_ != 0) {
        output.writeInt32(19, numFailedStages_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetKillTasksSummary(),
          KillTasksSummaryDefaultEntryHolder.defaultEntry,
          20);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (jobId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, jobId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, description_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, submissionTime_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, completionTime_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < stageIds_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(stageIds_.getLong(i));
        }
        size += dataSize;
        if (!getStageIdsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        stageIdsMemoizedSerializedSize = dataSize;
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(7, jobGroup_);
      }
      if (status_ != org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.JOB_EXECUTION_STATUS_UNSPECIFIED.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(8, status_);
      }
      if (numTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(9, numTasks_);
      }
      if (numActiveTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(10, numActiveTasks_);
      }
      if (numCompletedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(11, numCompletedTasks_);
      }
      if (numSkippedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(12, numSkippedTasks_);
      }
      if (numFailedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(13, numFailedTasks_);
      }
      if (numKilledTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(14, numKilledTasks_);
      }
      if (numCompletedIndices_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(15, numCompletedIndices_);
      }
      if (numActiveStages_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(16, numActiveStages_);
      }
      if (numCompletedStages_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(17, numCompletedStages_);
      }
      if (numSkippedStages_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(18, numSkippedStages_);
      }
      if (numFailedStages_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(19, numFailedStages_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.Integer> entry
           : internalGetKillTasksSummary().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.Integer>
        killTasksSummary__ = KillTasksSummaryDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(20, killTasksSummary__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.JobData)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.JobData other = (org.apache.spark.status.protobuf.StoreTypes.JobData) obj;

      if (getJobId()
          != other.getJobId()) return false;
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasDescription() != other.hasDescription()) return false;
      if (hasDescription()) {
        if (!getDescription()
            .equals(other.getDescription())) return false;
      }
      if (hasSubmissionTime() != other.hasSubmissionTime()) return false;
      if (hasSubmissionTime()) {
        if (getSubmissionTime()
            != other.getSubmissionTime()) return false;
      }
      if (hasCompletionTime() != other.hasCompletionTime()) return false;
      if (hasCompletionTime()) {
        if (getCompletionTime()
            != other.getCompletionTime()) return false;
      }
      if (!getStageIdsList()
          .equals(other.getStageIdsList())) return false;
      if (hasJobGroup() != other.hasJobGroup()) return false;
      if (hasJobGroup()) {
        if (!getJobGroup()
            .equals(other.getJobGroup())) return false;
      }
      if (status_ != other.status_) return false;
      if (getNumTasks()
          != other.getNumTasks()) return false;
      if (getNumActiveTasks()
          != other.getNumActiveTasks()) return false;
      if (getNumCompletedTasks()
          != other.getNumCompletedTasks()) return false;
      if (getNumSkippedTasks()
          != other.getNumSkippedTasks()) return false;
      if (getNumFailedTasks()
          != other.getNumFailedTasks()) return false;
      if (getNumKilledTasks()
          != other.getNumKilledTasks()) return false;
      if (getNumCompletedIndices()
          != other.getNumCompletedIndices()) return false;
      if (getNumActiveStages()
          != other.getNumActiveStages()) return false;
      if (getNumCompletedStages()
          != other.getNumCompletedStages()) return false;
      if (getNumSkippedStages()
          != other.getNumSkippedStages()) return false;
      if (getNumFailedStages()
          != other.getNumFailedStages()) return false;
      if (!internalGetKillTasksSummary().equals(
          other.internalGetKillTasksSummary())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + JOB_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getJobId());
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasDescription()) {
        hash = (37 * hash) + DESCRIPTION_FIELD_NUMBER;
        hash = (53 * hash) + getDescription().hashCode();
      }
      if (hasSubmissionTime()) {
        hash = (37 * hash) + SUBMISSION_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getSubmissionTime());
      }
      if (hasCompletionTime()) {
        hash = (37 * hash) + COMPLETION_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getCompletionTime());
      }
      if (getStageIdsCount() > 0) {
        hash = (37 * hash) + STAGE_IDS_FIELD_NUMBER;
        hash = (53 * hash) + getStageIdsList().hashCode();
      }
      if (hasJobGroup()) {
        hash = (37 * hash) + JOB_GROUP_FIELD_NUMBER;
        hash = (53 * hash) + getJobGroup().hashCode();
      }
      hash = (37 * hash) + STATUS_FIELD_NUMBER;
      hash = (53 * hash) + status_;
      hash = (37 * hash) + NUM_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumTasks();
      hash = (37 * hash) + NUM_ACTIVE_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumActiveTasks();
      hash = (37 * hash) + NUM_COMPLETED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumCompletedTasks();
      hash = (37 * hash) + NUM_SKIPPED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumSkippedTasks();
      hash = (37 * hash) + NUM_FAILED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumFailedTasks();
      hash = (37 * hash) + NUM_KILLED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumKilledTasks();
      hash = (37 * hash) + NUM_COMPLETED_INDICES_FIELD_NUMBER;
      hash = (53 * hash) + getNumCompletedIndices();
      hash = (37 * hash) + NUM_ACTIVE_STAGES_FIELD_NUMBER;
      hash = (53 * hash) + getNumActiveStages();
      hash = (37 * hash) + NUM_COMPLETED_STAGES_FIELD_NUMBER;
      hash = (53 * hash) + getNumCompletedStages();
      hash = (37 * hash) + NUM_SKIPPED_STAGES_FIELD_NUMBER;
      hash = (53 * hash) + getNumSkippedStages();
      hash = (37 * hash) + NUM_FAILED_STAGES_FIELD_NUMBER;
      hash = (53 * hash) + getNumFailedStages();
      if (!internalGetKillTasksSummary().getMap().isEmpty()) {
        hash = (37 * hash) + KILL_TASKS_SUMMARY_FIELD_NUMBER;
        hash = (53 * hash) + internalGetKillTasksSummary().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.JobData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.JobData}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.JobData)
        org.apache.spark.status.protobuf.StoreTypes.JobDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobData_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 20:
            return internalGetKillTasksSummary();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 20:
            return internalGetMutableKillTasksSummary();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.JobData.class, org.apache.spark.status.protobuf.StoreTypes.JobData.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.JobData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        jobId_ = 0L;

        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        description_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        submissionTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        completionTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        stageIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000010);
        jobGroup_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        status_ = 0;

        numTasks_ = 0;

        numActiveTasks_ = 0;

        numCompletedTasks_ = 0;

        numSkippedTasks_ = 0;

        numFailedTasks_ = 0;

        numKilledTasks_ = 0;

        numCompletedIndices_ = 0;

        numActiveStages_ = 0;

        numCompletedStages_ = 0;

        numSkippedStages_ = 0;

        numFailedStages_ = 0;

        internalGetMutableKillTasksSummary().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobData_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.JobData getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.JobData.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.JobData build() {
        org.apache.spark.status.protobuf.StoreTypes.JobData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.JobData buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.JobData result = new org.apache.spark.status.protobuf.StoreTypes.JobData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.jobId_ = jobId_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.description_ = description_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.submissionTime_ = submissionTime_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.completionTime_ = completionTime_;
          to_bitField0_ |= 0x00000008;
        }
        if (((bitField0_ & 0x00000010) != 0)) {
          stageIds_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.stageIds_ = stageIds_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.jobGroup_ = jobGroup_;
        result.status_ = status_;
        result.numTasks_ = numTasks_;
        result.numActiveTasks_ = numActiveTasks_;
        result.numCompletedTasks_ = numCompletedTasks_;
        result.numSkippedTasks_ = numSkippedTasks_;
        result.numFailedTasks_ = numFailedTasks_;
        result.numKilledTasks_ = numKilledTasks_;
        result.numCompletedIndices_ = numCompletedIndices_;
        result.numActiveStages_ = numActiveStages_;
        result.numCompletedStages_ = numCompletedStages_;
        result.numSkippedStages_ = numSkippedStages_;
        result.numFailedStages_ = numFailedStages_;
        result.killTasksSummary_ = internalGetKillTasksSummary();
        result.killTasksSummary_.makeImmutable();
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.JobData) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.JobData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.JobData other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.JobData.getDefaultInstance()) return this;
        if (other.getJobId() != 0L) {
          setJobId(other.getJobId());
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasDescription()) {
          bitField0_ |= 0x00000002;
          description_ = other.description_;
          onChanged();
        }
        if (other.hasSubmissionTime()) {
          setSubmissionTime(other.getSubmissionTime());
        }
        if (other.hasCompletionTime()) {
          setCompletionTime(other.getCompletionTime());
        }
        if (!other.stageIds_.isEmpty()) {
          if (stageIds_.isEmpty()) {
            stageIds_ = other.stageIds_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureStageIdsIsMutable();
            stageIds_.addAll(other.stageIds_);
          }
          onChanged();
        }
        if (other.hasJobGroup()) {
          bitField0_ |= 0x00000020;
          jobGroup_ = other.jobGroup_;
          onChanged();
        }
        if (other.status_ != 0) {
          setStatusValue(other.getStatusValue());
        }
        if (other.getNumTasks() != 0) {
          setNumTasks(other.getNumTasks());
        }
        if (other.getNumActiveTasks() != 0) {
          setNumActiveTasks(other.getNumActiveTasks());
        }
        if (other.getNumCompletedTasks() != 0) {
          setNumCompletedTasks(other.getNumCompletedTasks());
        }
        if (other.getNumSkippedTasks() != 0) {
          setNumSkippedTasks(other.getNumSkippedTasks());
        }
        if (other.getNumFailedTasks() != 0) {
          setNumFailedTasks(other.getNumFailedTasks());
        }
        if (other.getNumKilledTasks() != 0) {
          setNumKilledTasks(other.getNumKilledTasks());
        }
        if (other.getNumCompletedIndices() != 0) {
          setNumCompletedIndices(other.getNumCompletedIndices());
        }
        if (other.getNumActiveStages() != 0) {
          setNumActiveStages(other.getNumActiveStages());
        }
        if (other.getNumCompletedStages() != 0) {
          setNumCompletedStages(other.getNumCompletedStages());
        }
        if (other.getNumSkippedStages() != 0) {
          setNumSkippedStages(other.getNumSkippedStages());
        }
        if (other.getNumFailedStages() != 0) {
          setNumFailedStages(other.getNumFailedStages());
        }
        internalGetMutableKillTasksSummary().mergeFrom(
            other.internalGetKillTasksSummary());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.JobData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.JobData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long jobId_ ;
      /**
       * <pre>
       * All IDs are int64 for extendability, even when they are currently int32 in Spark.
       * </pre>
       *
       * <code>int64 job_id = 1;</code>
       * @return The jobId.
       */
      @java.lang.Override
      public long getJobId() {
        return jobId_;
      }
      /**
       * <pre>
       * All IDs are int64 for extendability, even when they are currently int32 in Spark.
       * </pre>
       *
       * <code>int64 job_id = 1;</code>
       * @param value The jobId to set.
       * @return This builder for chaining.
       */
      public Builder setJobId(long value) {
        
        jobId_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * All IDs are int64 for extendability, even when they are currently int32 in Spark.
       * </pre>
       *
       * <code>int64 job_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearJobId() {
        
        jobId_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 2;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 2;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object description_ = "";
      /**
       * <code>string description = 3;</code>
       * @return Whether the description field is set.
       */
      public boolean hasDescription() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string description = 3;</code>
       * @return The description.
       */
      public java.lang.String getDescription() {
        java.lang.Object ref = description_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          description_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string description = 3;</code>
       * @return The bytes for description.
       */
      public com.google.protobuf.ByteString
          getDescriptionBytes() {
        java.lang.Object ref = description_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          description_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string description = 3;</code>
       * @param value The description to set.
       * @return This builder for chaining.
       */
      public Builder setDescription(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        description_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string description = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDescription() {
        bitField0_ = (bitField0_ & ~0x00000002);
        description_ = getDefaultInstance().getDescription();
        onChanged();
        return this;
      }
      /**
       * <code>string description = 3;</code>
       * @param value The bytes for description to set.
       * @return This builder for chaining.
       */
      public Builder setDescriptionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        description_ = value;
        onChanged();
        return this;
      }

      private long submissionTime_ ;
      /**
       * <code>int64 submission_time = 4;</code>
       * @return Whether the submissionTime field is set.
       */
      @java.lang.Override
      public boolean hasSubmissionTime() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>int64 submission_time = 4;</code>
       * @return The submissionTime.
       */
      @java.lang.Override
      public long getSubmissionTime() {
        return submissionTime_;
      }
      /**
       * <code>int64 submission_time = 4;</code>
       * @param value The submissionTime to set.
       * @return This builder for chaining.
       */
      public Builder setSubmissionTime(long value) {
        bitField0_ |= 0x00000004;
        submissionTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 submission_time = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearSubmissionTime() {
        bitField0_ = (bitField0_ & ~0x00000004);
        submissionTime_ = 0L;
        onChanged();
        return this;
      }

      private long completionTime_ ;
      /**
       * <code>int64 completion_time = 5;</code>
       * @return Whether the completionTime field is set.
       */
      @java.lang.Override
      public boolean hasCompletionTime() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>int64 completion_time = 5;</code>
       * @return The completionTime.
       */
      @java.lang.Override
      public long getCompletionTime() {
        return completionTime_;
      }
      /**
       * <code>int64 completion_time = 5;</code>
       * @param value The completionTime to set.
       * @return This builder for chaining.
       */
      public Builder setCompletionTime(long value) {
        bitField0_ |= 0x00000008;
        completionTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 completion_time = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearCompletionTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        completionTime_ = 0L;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList stageIds_ = emptyLongList();
      private void ensureStageIdsIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          stageIds_ = mutableCopy(stageIds_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <code>repeated int64 stage_ids = 6;</code>
       * @return A list containing the stageIds.
       */
      public java.util.List<java.lang.Long>
          getStageIdsList() {
        return ((bitField0_ & 0x00000010) != 0) ?
                 java.util.Collections.unmodifiableList(stageIds_) : stageIds_;
      }
      /**
       * <code>repeated int64 stage_ids = 6;</code>
       * @return The count of stageIds.
       */
      public int getStageIdsCount() {
        return stageIds_.size();
      }
      /**
       * <code>repeated int64 stage_ids = 6;</code>
       * @param index The index of the element to return.
       * @return The stageIds at the given index.
       */
      public long getStageIds(int index) {
        return stageIds_.getLong(index);
      }
      /**
       * <code>repeated int64 stage_ids = 6;</code>
       * @param index The index to set the value at.
       * @param value The stageIds to set.
       * @return This builder for chaining.
       */
      public Builder setStageIds(
          int index, long value) {
        ensureStageIdsIsMutable();
        stageIds_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stage_ids = 6;</code>
       * @param value The stageIds to add.
       * @return This builder for chaining.
       */
      public Builder addStageIds(long value) {
        ensureStageIdsIsMutable();
        stageIds_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stage_ids = 6;</code>
       * @param values The stageIds to add.
       * @return This builder for chaining.
       */
      public Builder addAllStageIds(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureStageIdsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, stageIds_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stage_ids = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageIds() {
        stageIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }

      private java.lang.Object jobGroup_ = "";
      /**
       * <code>string job_group = 7;</code>
       * @return Whether the jobGroup field is set.
       */
      public boolean hasJobGroup() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>string job_group = 7;</code>
       * @return The jobGroup.
       */
      public java.lang.String getJobGroup() {
        java.lang.Object ref = jobGroup_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          jobGroup_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string job_group = 7;</code>
       * @return The bytes for jobGroup.
       */
      public com.google.protobuf.ByteString
          getJobGroupBytes() {
        java.lang.Object ref = jobGroup_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          jobGroup_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string job_group = 7;</code>
       * @param value The jobGroup to set.
       * @return This builder for chaining.
       */
      public Builder setJobGroup(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        jobGroup_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string job_group = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearJobGroup() {
        bitField0_ = (bitField0_ & ~0x00000020);
        jobGroup_ = getDefaultInstance().getJobGroup();
        onChanged();
        return this;
      }
      /**
       * <code>string job_group = 7;</code>
       * @param value The bytes for jobGroup to set.
       * @return This builder for chaining.
       */
      public Builder setJobGroupBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000020;
        jobGroup_ = value;
        onChanged();
        return this;
      }

      private int status_ = 0;
      /**
       * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
       * @return The enum numeric value on the wire for status.
       */
      @java.lang.Override public int getStatusValue() {
        return status_;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
       * @param value The enum numeric value on the wire for status to set.
       * @return This builder for chaining.
       */
      public Builder setStatusValue(int value) {
        
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
       * @return The status.
       */
      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getStatus() {
        @SuppressWarnings("deprecation")
        org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus result = org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.valueOf(status_);
        return result == null ? org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.UNRECOGNIZED : result;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
       * @param value The status to set.
       * @return This builder for chaining.
       */
      public Builder setStatus(org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        status_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobExecutionStatus status = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearStatus() {
        
        status_ = 0;
        onChanged();
        return this;
      }

      private int numTasks_ ;
      /**
       * <code>int32 num_tasks = 9;</code>
       * @return The numTasks.
       */
      @java.lang.Override
      public int getNumTasks() {
        return numTasks_;
      }
      /**
       * <code>int32 num_tasks = 9;</code>
       * @param value The numTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumTasks(int value) {
        
        numTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_tasks = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumTasks() {
        
        numTasks_ = 0;
        onChanged();
        return this;
      }

      private int numActiveTasks_ ;
      /**
       * <code>int32 num_active_tasks = 10;</code>
       * @return The numActiveTasks.
       */
      @java.lang.Override
      public int getNumActiveTasks() {
        return numActiveTasks_;
      }
      /**
       * <code>int32 num_active_tasks = 10;</code>
       * @param value The numActiveTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumActiveTasks(int value) {
        
        numActiveTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_active_tasks = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumActiveTasks() {
        
        numActiveTasks_ = 0;
        onChanged();
        return this;
      }

      private int numCompletedTasks_ ;
      /**
       * <code>int32 num_completed_tasks = 11;</code>
       * @return The numCompletedTasks.
       */
      @java.lang.Override
      public int getNumCompletedTasks() {
        return numCompletedTasks_;
      }
      /**
       * <code>int32 num_completed_tasks = 11;</code>
       * @param value The numCompletedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumCompletedTasks(int value) {
        
        numCompletedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_completed_tasks = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCompletedTasks() {
        
        numCompletedTasks_ = 0;
        onChanged();
        return this;
      }

      private int numSkippedTasks_ ;
      /**
       * <code>int32 num_skipped_tasks = 12;</code>
       * @return The numSkippedTasks.
       */
      @java.lang.Override
      public int getNumSkippedTasks() {
        return numSkippedTasks_;
      }
      /**
       * <code>int32 num_skipped_tasks = 12;</code>
       * @param value The numSkippedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumSkippedTasks(int value) {
        
        numSkippedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_skipped_tasks = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumSkippedTasks() {
        
        numSkippedTasks_ = 0;
        onChanged();
        return this;
      }

      private int numFailedTasks_ ;
      /**
       * <code>int32 num_failed_tasks = 13;</code>
       * @return The numFailedTasks.
       */
      @java.lang.Override
      public int getNumFailedTasks() {
        return numFailedTasks_;
      }
      /**
       * <code>int32 num_failed_tasks = 13;</code>
       * @param value The numFailedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumFailedTasks(int value) {
        
        numFailedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_failed_tasks = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumFailedTasks() {
        
        numFailedTasks_ = 0;
        onChanged();
        return this;
      }

      private int numKilledTasks_ ;
      /**
       * <code>int32 num_killed_tasks = 14;</code>
       * @return The numKilledTasks.
       */
      @java.lang.Override
      public int getNumKilledTasks() {
        return numKilledTasks_;
      }
      /**
       * <code>int32 num_killed_tasks = 14;</code>
       * @param value The numKilledTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumKilledTasks(int value) {
        
        numKilledTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_killed_tasks = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumKilledTasks() {
        
        numKilledTasks_ = 0;
        onChanged();
        return this;
      }

      private int numCompletedIndices_ ;
      /**
       * <code>int32 num_completed_indices = 15;</code>
       * @return The numCompletedIndices.
       */
      @java.lang.Override
      public int getNumCompletedIndices() {
        return numCompletedIndices_;
      }
      /**
       * <code>int32 num_completed_indices = 15;</code>
       * @param value The numCompletedIndices to set.
       * @return This builder for chaining.
       */
      public Builder setNumCompletedIndices(int value) {
        
        numCompletedIndices_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_completed_indices = 15;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCompletedIndices() {
        
        numCompletedIndices_ = 0;
        onChanged();
        return this;
      }

      private int numActiveStages_ ;
      /**
       * <code>int32 num_active_stages = 16;</code>
       * @return The numActiveStages.
       */
      @java.lang.Override
      public int getNumActiveStages() {
        return numActiveStages_;
      }
      /**
       * <code>int32 num_active_stages = 16;</code>
       * @param value The numActiveStages to set.
       * @return This builder for chaining.
       */
      public Builder setNumActiveStages(int value) {
        
        numActiveStages_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_active_stages = 16;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumActiveStages() {
        
        numActiveStages_ = 0;
        onChanged();
        return this;
      }

      private int numCompletedStages_ ;
      /**
       * <code>int32 num_completed_stages = 17;</code>
       * @return The numCompletedStages.
       */
      @java.lang.Override
      public int getNumCompletedStages() {
        return numCompletedStages_;
      }
      /**
       * <code>int32 num_completed_stages = 17;</code>
       * @param value The numCompletedStages to set.
       * @return This builder for chaining.
       */
      public Builder setNumCompletedStages(int value) {
        
        numCompletedStages_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_completed_stages = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCompletedStages() {
        
        numCompletedStages_ = 0;
        onChanged();
        return this;
      }

      private int numSkippedStages_ ;
      /**
       * <code>int32 num_skipped_stages = 18;</code>
       * @return The numSkippedStages.
       */
      @java.lang.Override
      public int getNumSkippedStages() {
        return numSkippedStages_;
      }
      /**
       * <code>int32 num_skipped_stages = 18;</code>
       * @param value The numSkippedStages to set.
       * @return This builder for chaining.
       */
      public Builder setNumSkippedStages(int value) {
        
        numSkippedStages_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_skipped_stages = 18;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumSkippedStages() {
        
        numSkippedStages_ = 0;
        onChanged();
        return this;
      }

      private int numFailedStages_ ;
      /**
       * <code>int32 num_failed_stages = 19;</code>
       * @return The numFailedStages.
       */
      @java.lang.Override
      public int getNumFailedStages() {
        return numFailedStages_;
      }
      /**
       * <code>int32 num_failed_stages = 19;</code>
       * @param value The numFailedStages to set.
       * @return This builder for chaining.
       */
      public Builder setNumFailedStages(int value) {
        
        numFailedStages_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_failed_stages = 19;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumFailedStages() {
        
        numFailedStages_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.Integer> killTasksSummary_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
      internalGetKillTasksSummary() {
        if (killTasksSummary_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              KillTasksSummaryDefaultEntryHolder.defaultEntry);
        }
        return killTasksSummary_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
      internalGetMutableKillTasksSummary() {
        onChanged();;
        if (killTasksSummary_ == null) {
          killTasksSummary_ = com.google.protobuf.MapField.newMapField(
              KillTasksSummaryDefaultEntryHolder.defaultEntry);
        }
        if (!killTasksSummary_.isMutable()) {
          killTasksSummary_ = killTasksSummary_.copy();
        }
        return killTasksSummary_;
      }

      public int getKillTasksSummaryCount() {
        return internalGetKillTasksSummary().getMap().size();
      }
      /**
       * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
       */

      @java.lang.Override
      public boolean containsKillTasksSummary(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetKillTasksSummary().getMap().containsKey(key);
      }
      /**
       * Use {@link #getKillTasksSummaryMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Integer> getKillTasksSummary() {
        return getKillTasksSummaryMap();
      }
      /**
       * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.Integer> getKillTasksSummaryMap() {
        return internalGetKillTasksSummary().getMap();
      }
      /**
       * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
       */
      @java.lang.Override

      public int getKillTasksSummaryOrDefault(
          java.lang.String key,
          int defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Integer> map =
            internalGetKillTasksSummary().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
       */
      @java.lang.Override

      public int getKillTasksSummaryOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Integer> map =
            internalGetKillTasksSummary().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearKillTasksSummary() {
        internalGetMutableKillTasksSummary().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
       */

      public Builder removeKillTasksSummary(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableKillTasksSummary().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Integer>
      getMutableKillTasksSummary() {
        return internalGetMutableKillTasksSummary().getMutableMap();
      }
      /**
       * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
       */
      public Builder putKillTasksSummary(
          java.lang.String key,
          int value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        
        internalGetMutableKillTasksSummary().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, int32&gt; kill_tasks_summary = 20;</code>
       */

      public Builder putAllKillTasksSummary(
          java.util.Map<java.lang.String, java.lang.Integer> values) {
        internalGetMutableKillTasksSummary().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.JobData)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.JobData)
    private static final org.apache.spark.status.protobuf.StoreTypes.JobData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.JobData();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.JobData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<JobData>
        PARSER = new com.google.protobuf.AbstractParser<JobData>() {
      @java.lang.Override
      public JobData parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new JobData(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<JobData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<JobData> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.JobData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface JobDataWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.JobDataWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.JobData getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.JobDataOrBuilder getInfoOrBuilder();

    /**
     * <code>repeated int32 skipped_stages = 2;</code>
     * @return A list containing the skippedStages.
     */
    java.util.List<java.lang.Integer> getSkippedStagesList();
    /**
     * <code>repeated int32 skipped_stages = 2;</code>
     * @return The count of skippedStages.
     */
    int getSkippedStagesCount();
    /**
     * <code>repeated int32 skipped_stages = 2;</code>
     * @param index The index of the element to return.
     * @return The skippedStages at the given index.
     */
    int getSkippedStages(int index);

    /**
     * <code>int64 sql_execution_id = 3;</code>
     * @return Whether the sqlExecutionId field is set.
     */
    boolean hasSqlExecutionId();
    /**
     * <code>int64 sql_execution_id = 3;</code>
     * @return The sqlExecutionId.
     */
    long getSqlExecutionId();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.JobDataWrapper}
   */
  public static final class JobDataWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.JobDataWrapper)
      JobDataWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use JobDataWrapper.newBuilder() to construct.
    private JobDataWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private JobDataWrapper() {
      skippedStages_ = emptyIntList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new JobDataWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private JobDataWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.JobData.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.JobData.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            case 16: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                skippedStages_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              skippedStages_.addInt(input.readInt32());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                skippedStages_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                skippedStages_.addInt(input.readInt32());
              }
              input.popLimit(limit);
              break;
            }
            case 24: {
              bitField0_ |= 0x00000001;
              sqlExecutionId_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          skippedStages_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobDataWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobDataWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper.class, org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper.Builder.class);
    }

    private int bitField0_;
    public static final int INFO_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.JobData info_;
    /**
     * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.JobData getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.JobData.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.JobDataOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    public static final int SKIPPED_STAGES_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.IntList skippedStages_;
    /**
     * <code>repeated int32 skipped_stages = 2;</code>
     * @return A list containing the skippedStages.
     */
    @java.lang.Override
    public java.util.List<java.lang.Integer>
        getSkippedStagesList() {
      return skippedStages_;
    }
    /**
     * <code>repeated int32 skipped_stages = 2;</code>
     * @return The count of skippedStages.
     */
    public int getSkippedStagesCount() {
      return skippedStages_.size();
    }
    /**
     * <code>repeated int32 skipped_stages = 2;</code>
     * @param index The index of the element to return.
     * @return The skippedStages at the given index.
     */
    public int getSkippedStages(int index) {
      return skippedStages_.getInt(index);
    }
    private int skippedStagesMemoizedSerializedSize = -1;

    public static final int SQL_EXECUTION_ID_FIELD_NUMBER = 3;
    private long sqlExecutionId_;
    /**
     * <code>int64 sql_execution_id = 3;</code>
     * @return Whether the sqlExecutionId field is set.
     */
    @java.lang.Override
    public boolean hasSqlExecutionId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>int64 sql_execution_id = 3;</code>
     * @return The sqlExecutionId.
     */
    @java.lang.Override
    public long getSqlExecutionId() {
      return sqlExecutionId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (info_ != null) {
        output.writeMessage(1, getInfo());
      }
      if (getSkippedStagesList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(skippedStagesMemoizedSerializedSize);
      }
      for (int i = 0; i < skippedStages_.size(); i++) {
        output.writeInt32NoTag(skippedStages_.getInt(i));
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(3, sqlExecutionId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getInfo());
      }
      {
        int dataSize = 0;
        for (int i = 0; i < skippedStages_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt32SizeNoTag(skippedStages_.getInt(i));
        }
        size += dataSize;
        if (!getSkippedStagesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        skippedStagesMemoizedSerializedSize = dataSize;
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, sqlExecutionId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper other = (org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper) obj;

      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!getSkippedStagesList()
          .equals(other.getSkippedStagesList())) return false;
      if (hasSqlExecutionId() != other.hasSqlExecutionId()) return false;
      if (hasSqlExecutionId()) {
        if (getSqlExecutionId()
            != other.getSqlExecutionId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      if (getSkippedStagesCount() > 0) {
        hash = (37 * hash) + SKIPPED_STAGES_FIELD_NUMBER;
        hash = (53 * hash) + getSkippedStagesList().hashCode();
      }
      if (hasSqlExecutionId()) {
        hash = (37 * hash) + SQL_EXECUTION_ID_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getSqlExecutionId());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.JobDataWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.JobDataWrapper)
        org.apache.spark.status.protobuf.StoreTypes.JobDataWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobDataWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobDataWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper.class, org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        skippedStages_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        sqlExecutionId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_JobDataWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper result = new org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        if (((bitField0_ & 0x00000001) != 0)) {
          skippedStages_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.skippedStages_ = skippedStages_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.sqlExecutionId_ = sqlExecutionId_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper.getDefaultInstance()) return this;
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        if (!other.skippedStages_.isEmpty()) {
          if (skippedStages_.isEmpty()) {
            skippedStages_ = other.skippedStages_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureSkippedStagesIsMutable();
            skippedStages_.addAll(other.skippedStages_);
          }
          onChanged();
        }
        if (other.hasSqlExecutionId()) {
          setSqlExecutionId(other.getSqlExecutionId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.spark.status.protobuf.StoreTypes.JobData info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.JobData, org.apache.spark.status.protobuf.StoreTypes.JobData.Builder, org.apache.spark.status.protobuf.StoreTypes.JobDataOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.JobData getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.JobData.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.JobData value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.JobData.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.JobData value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.JobData.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.JobData.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.JobDataOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.JobData.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.JobData info = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.JobData, org.apache.spark.status.protobuf.StoreTypes.JobData.Builder, org.apache.spark.status.protobuf.StoreTypes.JobDataOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.JobData, org.apache.spark.status.protobuf.StoreTypes.JobData.Builder, org.apache.spark.status.protobuf.StoreTypes.JobDataOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }

      private com.google.protobuf.Internal.IntList skippedStages_ = emptyIntList();
      private void ensureSkippedStagesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          skippedStages_ = mutableCopy(skippedStages_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated int32 skipped_stages = 2;</code>
       * @return A list containing the skippedStages.
       */
      public java.util.List<java.lang.Integer>
          getSkippedStagesList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(skippedStages_) : skippedStages_;
      }
      /**
       * <code>repeated int32 skipped_stages = 2;</code>
       * @return The count of skippedStages.
       */
      public int getSkippedStagesCount() {
        return skippedStages_.size();
      }
      /**
       * <code>repeated int32 skipped_stages = 2;</code>
       * @param index The index of the element to return.
       * @return The skippedStages at the given index.
       */
      public int getSkippedStages(int index) {
        return skippedStages_.getInt(index);
      }
      /**
       * <code>repeated int32 skipped_stages = 2;</code>
       * @param index The index to set the value at.
       * @param value The skippedStages to set.
       * @return This builder for chaining.
       */
      public Builder setSkippedStages(
          int index, int value) {
        ensureSkippedStagesIsMutable();
        skippedStages_.setInt(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int32 skipped_stages = 2;</code>
       * @param value The skippedStages to add.
       * @return This builder for chaining.
       */
      public Builder addSkippedStages(int value) {
        ensureSkippedStagesIsMutable();
        skippedStages_.addInt(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int32 skipped_stages = 2;</code>
       * @param values The skippedStages to add.
       * @return This builder for chaining.
       */
      public Builder addAllSkippedStages(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensureSkippedStagesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, skippedStages_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int32 skipped_stages = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearSkippedStages() {
        skippedStages_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private long sqlExecutionId_ ;
      /**
       * <code>int64 sql_execution_id = 3;</code>
       * @return Whether the sqlExecutionId field is set.
       */
      @java.lang.Override
      public boolean hasSqlExecutionId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>int64 sql_execution_id = 3;</code>
       * @return The sqlExecutionId.
       */
      @java.lang.Override
      public long getSqlExecutionId() {
        return sqlExecutionId_;
      }
      /**
       * <code>int64 sql_execution_id = 3;</code>
       * @param value The sqlExecutionId to set.
       * @return This builder for chaining.
       */
      public Builder setSqlExecutionId(long value) {
        bitField0_ |= 0x00000002;
        sqlExecutionId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 sql_execution_id = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearSqlExecutionId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        sqlExecutionId_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.JobDataWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.JobDataWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<JobDataWrapper>
        PARSER = new com.google.protobuf.AbstractParser<JobDataWrapper>() {
      @java.lang.Override
      public JobDataWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new JobDataWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<JobDataWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<JobDataWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.JobDataWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface AccumulableInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.AccumulableInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 id = 1;</code>
     * @return The id.
     */
    long getId();

    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>string update = 3;</code>
     * @return Whether the update field is set.
     */
    boolean hasUpdate();
    /**
     * <code>string update = 3;</code>
     * @return The update.
     */
    java.lang.String getUpdate();
    /**
     * <code>string update = 3;</code>
     * @return The bytes for update.
     */
    com.google.protobuf.ByteString
        getUpdateBytes();

    /**
     * <code>string value = 4;</code>
     * @return Whether the value field is set.
     */
    boolean hasValue();
    /**
     * <code>string value = 4;</code>
     * @return The value.
     */
    java.lang.String getValue();
    /**
     * <code>string value = 4;</code>
     * @return The bytes for value.
     */
    com.google.protobuf.ByteString
        getValueBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.AccumulableInfo}
   */
  public static final class AccumulableInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.AccumulableInfo)
      AccumulableInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use AccumulableInfo.newBuilder() to construct.
    private AccumulableInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private AccumulableInfo() {
      name_ = "";
      update_ = "";
      value_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new AccumulableInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private AccumulableInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              id_ = input.readInt64();
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              update_ = s;
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              value_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AccumulableInfo_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AccumulableInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.class, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private long id_;
    /**
     * <code>int64 id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public long getId() {
      return id_;
    }

    public static final int NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int UPDATE_FIELD_NUMBER = 3;
    private volatile java.lang.Object update_;
    /**
     * <code>string update = 3;</code>
     * @return Whether the update field is set.
     */
    @java.lang.Override
    public boolean hasUpdate() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string update = 3;</code>
     * @return The update.
     */
    @java.lang.Override
    public java.lang.String getUpdate() {
      java.lang.Object ref = update_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        update_ = s;
        return s;
      }
    }
    /**
     * <code>string update = 3;</code>
     * @return The bytes for update.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getUpdateBytes() {
      java.lang.Object ref = update_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        update_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VALUE_FIELD_NUMBER = 4;
    private volatile java.lang.Object value_;
    /**
     * <code>string value = 4;</code>
     * @return Whether the value field is set.
     */
    @java.lang.Override
    public boolean hasValue() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string value = 4;</code>
     * @return The value.
     */
    @java.lang.Override
    public java.lang.String getValue() {
      java.lang.Object ref = value_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        value_ = s;
        return s;
      }
    }
    /**
     * <code>string value = 4;</code>
     * @return The bytes for value.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getValueBytes() {
      java.lang.Object ref = value_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        value_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (id_ != 0L) {
        output.writeInt64(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, update_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, value_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (id_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, update_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, value_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo other = (org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo) obj;

      if (getId()
          != other.getId()) return false;
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasUpdate() != other.hasUpdate()) return false;
      if (hasUpdate()) {
        if (!getUpdate()
            .equals(other.getUpdate())) return false;
      }
      if (hasValue() != other.hasValue()) return false;
      if (hasValue()) {
        if (!getValue()
            .equals(other.getValue())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getId());
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasUpdate()) {
        hash = (37 * hash) + UPDATE_FIELD_NUMBER;
        hash = (53 * hash) + getUpdate().hashCode();
      }
      if (hasValue()) {
        hash = (37 * hash) + VALUE_FIELD_NUMBER;
        hash = (53 * hash) + getValue().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.AccumulableInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.AccumulableInfo)
        org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AccumulableInfo_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AccumulableInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.class, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0L;

        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        update_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        value_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AccumulableInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo build() {
        org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo result = new org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.update_ = update_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.value_ = value_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.getDefaultInstance()) return this;
        if (other.getId() != 0L) {
          setId(other.getId());
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasUpdate()) {
          bitField0_ |= 0x00000002;
          update_ = other.update_;
          onChanged();
        }
        if (other.hasValue()) {
          bitField0_ |= 0x00000004;
          value_ = other.value_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long id_ ;
      /**
       * <code>int64 id = 1;</code>
       * @return The id.
       */
      @java.lang.Override
      public long getId() {
        return id_;
      }
      /**
       * <code>int64 id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(long value) {
        
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        
        id_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 2;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 2;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object update_ = "";
      /**
       * <code>string update = 3;</code>
       * @return Whether the update field is set.
       */
      public boolean hasUpdate() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string update = 3;</code>
       * @return The update.
       */
      public java.lang.String getUpdate() {
        java.lang.Object ref = update_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          update_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string update = 3;</code>
       * @return The bytes for update.
       */
      public com.google.protobuf.ByteString
          getUpdateBytes() {
        java.lang.Object ref = update_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          update_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string update = 3;</code>
       * @param value The update to set.
       * @return This builder for chaining.
       */
      public Builder setUpdate(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        update_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string update = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearUpdate() {
        bitField0_ = (bitField0_ & ~0x00000002);
        update_ = getDefaultInstance().getUpdate();
        onChanged();
        return this;
      }
      /**
       * <code>string update = 3;</code>
       * @param value The bytes for update to set.
       * @return This builder for chaining.
       */
      public Builder setUpdateBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        update_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object value_ = "";
      /**
       * <code>string value = 4;</code>
       * @return Whether the value field is set.
       */
      public boolean hasValue() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string value = 4;</code>
       * @return The value.
       */
      public java.lang.String getValue() {
        java.lang.Object ref = value_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          value_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string value = 4;</code>
       * @return The bytes for value.
       */
      public com.google.protobuf.ByteString
          getValueBytes() {
        java.lang.Object ref = value_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          value_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string value = 4;</code>
       * @param value The value to set.
       * @return This builder for chaining.
       */
      public Builder setValue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        value_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string value = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearValue() {
        bitField0_ = (bitField0_ & ~0x00000004);
        value_ = getDefaultInstance().getValue();
        onChanged();
        return this;
      }
      /**
       * <code>string value = 4;</code>
       * @param value The bytes for value to set.
       * @return This builder for chaining.
       */
      public Builder setValueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        value_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.AccumulableInfo)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.AccumulableInfo)
    private static final org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<AccumulableInfo>
        PARSER = new com.google.protobuf.AbstractParser<AccumulableInfo>() {
      @java.lang.Override
      public AccumulableInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AccumulableInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<AccumulableInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<AccumulableInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskDataWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.TaskDataWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 task_id = 1;</code>
     * @return The taskId.
     */
    long getTaskId();

    /**
     * <code>int32 index = 2;</code>
     * @return The index.
     */
    int getIndex();

    /**
     * <code>int32 attempt = 3;</code>
     * @return The attempt.
     */
    int getAttempt();

    /**
     * <code>int32 partition_id = 4;</code>
     * @return The partitionId.
     */
    int getPartitionId();

    /**
     * <code>int64 launch_time = 5;</code>
     * @return The launchTime.
     */
    long getLaunchTime();

    /**
     * <code>int64 result_fetch_start = 6;</code>
     * @return The resultFetchStart.
     */
    long getResultFetchStart();

    /**
     * <code>int64 duration = 7;</code>
     * @return The duration.
     */
    long getDuration();

    /**
     * <code>string executor_id = 8;</code>
     * @return Whether the executorId field is set.
     */
    boolean hasExecutorId();
    /**
     * <code>string executor_id = 8;</code>
     * @return The executorId.
     */
    java.lang.String getExecutorId();
    /**
     * <code>string executor_id = 8;</code>
     * @return The bytes for executorId.
     */
    com.google.protobuf.ByteString
        getExecutorIdBytes();

    /**
     * <code>string host = 9;</code>
     * @return Whether the host field is set.
     */
    boolean hasHost();
    /**
     * <code>string host = 9;</code>
     * @return The host.
     */
    java.lang.String getHost();
    /**
     * <code>string host = 9;</code>
     * @return The bytes for host.
     */
    com.google.protobuf.ByteString
        getHostBytes();

    /**
     * <code>string status = 10;</code>
     * @return Whether the status field is set.
     */
    boolean hasStatus();
    /**
     * <code>string status = 10;</code>
     * @return The status.
     */
    java.lang.String getStatus();
    /**
     * <code>string status = 10;</code>
     * @return The bytes for status.
     */
    com.google.protobuf.ByteString
        getStatusBytes();

    /**
     * <code>string task_locality = 11;</code>
     * @return Whether the taskLocality field is set.
     */
    boolean hasTaskLocality();
    /**
     * <code>string task_locality = 11;</code>
     * @return The taskLocality.
     */
    java.lang.String getTaskLocality();
    /**
     * <code>string task_locality = 11;</code>
     * @return The bytes for taskLocality.
     */
    com.google.protobuf.ByteString
        getTaskLocalityBytes();

    /**
     * <code>bool speculative = 12;</code>
     * @return The speculative.
     */
    boolean getSpeculative();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> 
        getAccumulatorUpdatesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    int getAccumulatorUpdatesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
        getAccumulatorUpdatesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
        int index);

    /**
     * <code>string error_message = 14;</code>
     * @return Whether the errorMessage field is set.
     */
    boolean hasErrorMessage();
    /**
     * <code>string error_message = 14;</code>
     * @return The errorMessage.
     */
    java.lang.String getErrorMessage();
    /**
     * <code>string error_message = 14;</code>
     * @return The bytes for errorMessage.
     */
    com.google.protobuf.ByteString
        getErrorMessageBytes();

    /**
     * <code>bool has_metrics = 15;</code>
     * @return The hasMetrics.
     */
    boolean getHasMetrics();

    /**
     * <code>int64 executor_deserialize_time = 16;</code>
     * @return The executorDeserializeTime.
     */
    long getExecutorDeserializeTime();

    /**
     * <code>int64 executor_deserialize_cpu_time = 17;</code>
     * @return The executorDeserializeCpuTime.
     */
    long getExecutorDeserializeCpuTime();

    /**
     * <code>int64 executor_run_time = 18;</code>
     * @return The executorRunTime.
     */
    long getExecutorRunTime();

    /**
     * <code>int64 executor_cpu_time = 19;</code>
     * @return The executorCpuTime.
     */
    long getExecutorCpuTime();

    /**
     * <code>int64 result_size = 20;</code>
     * @return The resultSize.
     */
    long getResultSize();

    /**
     * <code>int64 jvm_gc_time = 21;</code>
     * @return The jvmGcTime.
     */
    long getJvmGcTime();

    /**
     * <code>int64 result_serialization_time = 22;</code>
     * @return The resultSerializationTime.
     */
    long getResultSerializationTime();

    /**
     * <code>int64 memory_bytes_spilled = 23;</code>
     * @return The memoryBytesSpilled.
     */
    long getMemoryBytesSpilled();

    /**
     * <code>int64 disk_bytes_spilled = 24;</code>
     * @return The diskBytesSpilled.
     */
    long getDiskBytesSpilled();

    /**
     * <code>int64 peak_execution_memory = 25;</code>
     * @return The peakExecutionMemory.
     */
    long getPeakExecutionMemory();

    /**
     * <code>int64 input_bytes_read = 26;</code>
     * @return The inputBytesRead.
     */
    long getInputBytesRead();

    /**
     * <code>int64 input_records_read = 27;</code>
     * @return The inputRecordsRead.
     */
    long getInputRecordsRead();

    /**
     * <code>int64 output_bytes_written = 28;</code>
     * @return The outputBytesWritten.
     */
    long getOutputBytesWritten();

    /**
     * <code>int64 output_records_written = 29;</code>
     * @return The outputRecordsWritten.
     */
    long getOutputRecordsWritten();

    /**
     * <code>int64 shuffle_remote_blocks_fetched = 30;</code>
     * @return The shuffleRemoteBlocksFetched.
     */
    long getShuffleRemoteBlocksFetched();

    /**
     * <code>int64 shuffle_local_blocks_fetched = 31;</code>
     * @return The shuffleLocalBlocksFetched.
     */
    long getShuffleLocalBlocksFetched();

    /**
     * <code>int64 shuffle_fetch_wait_time = 32;</code>
     * @return The shuffleFetchWaitTime.
     */
    long getShuffleFetchWaitTime();

    /**
     * <code>int64 shuffle_remote_bytes_read = 33;</code>
     * @return The shuffleRemoteBytesRead.
     */
    long getShuffleRemoteBytesRead();

    /**
     * <code>int64 shuffle_remote_bytes_read_to_disk = 34;</code>
     * @return The shuffleRemoteBytesReadToDisk.
     */
    long getShuffleRemoteBytesReadToDisk();

    /**
     * <code>int64 shuffle_local_bytes_read = 35;</code>
     * @return The shuffleLocalBytesRead.
     */
    long getShuffleLocalBytesRead();

    /**
     * <code>int64 shuffle_records_read = 36;</code>
     * @return The shuffleRecordsRead.
     */
    long getShuffleRecordsRead();

    /**
     * <code>int64 shuffle_bytes_written = 37;</code>
     * @return The shuffleBytesWritten.
     */
    long getShuffleBytesWritten();

    /**
     * <code>int64 shuffle_write_time = 38;</code>
     * @return The shuffleWriteTime.
     */
    long getShuffleWriteTime();

    /**
     * <code>int64 shuffle_records_written = 39;</code>
     * @return The shuffleRecordsWritten.
     */
    long getShuffleRecordsWritten();

    /**
     * <code>int64 stage_id = 40;</code>
     * @return The stageId.
     */
    long getStageId();

    /**
     * <code>int32 stage_attempt_id = 41;</code>
     * @return The stageAttemptId.
     */
    int getStageAttemptId();

    /**
     * <code>int64 shuffle_corrupt_merged_block_chunks = 42;</code>
     * @return The shuffleCorruptMergedBlockChunks.
     */
    long getShuffleCorruptMergedBlockChunks();

    /**
     * <code>int64 shuffle_merged_fetch_fallback_count = 43;</code>
     * @return The shuffleMergedFetchFallbackCount.
     */
    long getShuffleMergedFetchFallbackCount();

    /**
     * <code>int64 shuffle_merged_remote_blocks_fetched = 44;</code>
     * @return The shuffleMergedRemoteBlocksFetched.
     */
    long getShuffleMergedRemoteBlocksFetched();

    /**
     * <code>int64 shuffle_merged_local_blocks_fetched = 45;</code>
     * @return The shuffleMergedLocalBlocksFetched.
     */
    long getShuffleMergedLocalBlocksFetched();

    /**
     * <code>int64 shuffle_merged_remote_chunks_fetched = 46;</code>
     * @return The shuffleMergedRemoteChunksFetched.
     */
    long getShuffleMergedRemoteChunksFetched();

    /**
     * <code>int64 shuffle_merged_local_chunks_fetched = 47;</code>
     * @return The shuffleMergedLocalChunksFetched.
     */
    long getShuffleMergedLocalChunksFetched();

    /**
     * <code>int64 shuffle_merged_remote_bytes_read = 48;</code>
     * @return The shuffleMergedRemoteBytesRead.
     */
    long getShuffleMergedRemoteBytesRead();

    /**
     * <code>int64 shuffle_merged_local_bytes_read = 49;</code>
     * @return The shuffleMergedLocalBytesRead.
     */
    long getShuffleMergedLocalBytesRead();

    /**
     * <code>int64 shuffle_remote_reqs_duration = 50;</code>
     * @return The shuffleRemoteReqsDuration.
     */
    long getShuffleRemoteReqsDuration();

    /**
     * <code>int64 shuffle_merged_remote_req_duration = 51;</code>
     * @return The shuffleMergedRemoteReqDuration.
     */
    long getShuffleMergedRemoteReqDuration();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.TaskDataWrapper}
   */
  public static final class TaskDataWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.TaskDataWrapper)
      TaskDataWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskDataWrapper.newBuilder() to construct.
    private TaskDataWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskDataWrapper() {
      executorId_ = "";
      host_ = "";
      status_ = "";
      taskLocality_ = "";
      accumulatorUpdates_ = java.util.Collections.emptyList();
      errorMessage_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new TaskDataWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskDataWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              taskId_ = input.readInt64();
              break;
            }
            case 16: {

              index_ = input.readInt32();
              break;
            }
            case 24: {

              attempt_ = input.readInt32();
              break;
            }
            case 32: {

              partitionId_ = input.readInt32();
              break;
            }
            case 40: {

              launchTime_ = input.readInt64();
              break;
            }
            case 48: {

              resultFetchStart_ = input.readInt64();
              break;
            }
            case 56: {

              duration_ = input.readInt64();
              break;
            }
            case 66: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              executorId_ = s;
              break;
            }
            case 74: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              host_ = s;
              break;
            }
            case 82: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              status_ = s;
              break;
            }
            case 90: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000008;
              taskLocality_ = s;
              break;
            }
            case 96: {

              speculative_ = input.readBool();
              break;
            }
            case 106: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                accumulatorUpdates_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo>();
                mutable_bitField0_ |= 0x00000010;
              }
              accumulatorUpdates_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.parser(), extensionRegistry));
              break;
            }
            case 114: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000010;
              errorMessage_ = s;
              break;
            }
            case 120: {

              hasMetrics_ = input.readBool();
              break;
            }
            case 128: {

              executorDeserializeTime_ = input.readInt64();
              break;
            }
            case 136: {

              executorDeserializeCpuTime_ = input.readInt64();
              break;
            }
            case 144: {

              executorRunTime_ = input.readInt64();
              break;
            }
            case 152: {

              executorCpuTime_ = input.readInt64();
              break;
            }
            case 160: {

              resultSize_ = input.readInt64();
              break;
            }
            case 168: {

              jvmGcTime_ = input.readInt64();
              break;
            }
            case 176: {

              resultSerializationTime_ = input.readInt64();
              break;
            }
            case 184: {

              memoryBytesSpilled_ = input.readInt64();
              break;
            }
            case 192: {

              diskBytesSpilled_ = input.readInt64();
              break;
            }
            case 200: {

              peakExecutionMemory_ = input.readInt64();
              break;
            }
            case 208: {

              inputBytesRead_ = input.readInt64();
              break;
            }
            case 216: {

              inputRecordsRead_ = input.readInt64();
              break;
            }
            case 224: {

              outputBytesWritten_ = input.readInt64();
              break;
            }
            case 232: {

              outputRecordsWritten_ = input.readInt64();
              break;
            }
            case 240: {

              shuffleRemoteBlocksFetched_ = input.readInt64();
              break;
            }
            case 248: {

              shuffleLocalBlocksFetched_ = input.readInt64();
              break;
            }
            case 256: {

              shuffleFetchWaitTime_ = input.readInt64();
              break;
            }
            case 264: {

              shuffleRemoteBytesRead_ = input.readInt64();
              break;
            }
            case 272: {

              shuffleRemoteBytesReadToDisk_ = input.readInt64();
              break;
            }
            case 280: {

              shuffleLocalBytesRead_ = input.readInt64();
              break;
            }
            case 288: {

              shuffleRecordsRead_ = input.readInt64();
              break;
            }
            case 296: {

              shuffleBytesWritten_ = input.readInt64();
              break;
            }
            case 304: {

              shuffleWriteTime_ = input.readInt64();
              break;
            }
            case 312: {

              shuffleRecordsWritten_ = input.readInt64();
              break;
            }
            case 320: {

              stageId_ = input.readInt64();
              break;
            }
            case 328: {

              stageAttemptId_ = input.readInt32();
              break;
            }
            case 336: {

              shuffleCorruptMergedBlockChunks_ = input.readInt64();
              break;
            }
            case 344: {

              shuffleMergedFetchFallbackCount_ = input.readInt64();
              break;
            }
            case 352: {

              shuffleMergedRemoteBlocksFetched_ = input.readInt64();
              break;
            }
            case 360: {

              shuffleMergedLocalBlocksFetched_ = input.readInt64();
              break;
            }
            case 368: {

              shuffleMergedRemoteChunksFetched_ = input.readInt64();
              break;
            }
            case 376: {

              shuffleMergedLocalChunksFetched_ = input.readInt64();
              break;
            }
            case 384: {

              shuffleMergedRemoteBytesRead_ = input.readInt64();
              break;
            }
            case 392: {

              shuffleMergedLocalBytesRead_ = input.readInt64();
              break;
            }
            case 400: {

              shuffleRemoteReqsDuration_ = input.readInt64();
              break;
            }
            case 408: {

              shuffleMergedRemoteReqDuration_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          accumulatorUpdates_ = java.util.Collections.unmodifiableList(accumulatorUpdates_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper.class, org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper.Builder.class);
    }

    private int bitField0_;
    public static final int TASK_ID_FIELD_NUMBER = 1;
    private long taskId_;
    /**
     * <code>int64 task_id = 1;</code>
     * @return The taskId.
     */
    @java.lang.Override
    public long getTaskId() {
      return taskId_;
    }

    public static final int INDEX_FIELD_NUMBER = 2;
    private int index_;
    /**
     * <code>int32 index = 2;</code>
     * @return The index.
     */
    @java.lang.Override
    public int getIndex() {
      return index_;
    }

    public static final int ATTEMPT_FIELD_NUMBER = 3;
    private int attempt_;
    /**
     * <code>int32 attempt = 3;</code>
     * @return The attempt.
     */
    @java.lang.Override
    public int getAttempt() {
      return attempt_;
    }

    public static final int PARTITION_ID_FIELD_NUMBER = 4;
    private int partitionId_;
    /**
     * <code>int32 partition_id = 4;</code>
     * @return The partitionId.
     */
    @java.lang.Override
    public int getPartitionId() {
      return partitionId_;
    }

    public static final int LAUNCH_TIME_FIELD_NUMBER = 5;
    private long launchTime_;
    /**
     * <code>int64 launch_time = 5;</code>
     * @return The launchTime.
     */
    @java.lang.Override
    public long getLaunchTime() {
      return launchTime_;
    }

    public static final int RESULT_FETCH_START_FIELD_NUMBER = 6;
    private long resultFetchStart_;
    /**
     * <code>int64 result_fetch_start = 6;</code>
     * @return The resultFetchStart.
     */
    @java.lang.Override
    public long getResultFetchStart() {
      return resultFetchStart_;
    }

    public static final int DURATION_FIELD_NUMBER = 7;
    private long duration_;
    /**
     * <code>int64 duration = 7;</code>
     * @return The duration.
     */
    @java.lang.Override
    public long getDuration() {
      return duration_;
    }

    public static final int EXECUTOR_ID_FIELD_NUMBER = 8;
    private volatile java.lang.Object executorId_;
    /**
     * <code>string executor_id = 8;</code>
     * @return Whether the executorId field is set.
     */
    @java.lang.Override
    public boolean hasExecutorId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string executor_id = 8;</code>
     * @return The executorId.
     */
    @java.lang.Override
    public java.lang.String getExecutorId() {
      java.lang.Object ref = executorId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        executorId_ = s;
        return s;
      }
    }
    /**
     * <code>string executor_id = 8;</code>
     * @return The bytes for executorId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getExecutorIdBytes() {
      java.lang.Object ref = executorId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        executorId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int HOST_FIELD_NUMBER = 9;
    private volatile java.lang.Object host_;
    /**
     * <code>string host = 9;</code>
     * @return Whether the host field is set.
     */
    @java.lang.Override
    public boolean hasHost() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string host = 9;</code>
     * @return The host.
     */
    @java.lang.Override
    public java.lang.String getHost() {
      java.lang.Object ref = host_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        host_ = s;
        return s;
      }
    }
    /**
     * <code>string host = 9;</code>
     * @return The bytes for host.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getHostBytes() {
      java.lang.Object ref = host_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        host_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int STATUS_FIELD_NUMBER = 10;
    private volatile java.lang.Object status_;
    /**
     * <code>string status = 10;</code>
     * @return Whether the status field is set.
     */
    @java.lang.Override
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string status = 10;</code>
     * @return The status.
     */
    @java.lang.Override
    public java.lang.String getStatus() {
      java.lang.Object ref = status_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        status_ = s;
        return s;
      }
    }
    /**
     * <code>string status = 10;</code>
     * @return The bytes for status.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStatusBytes() {
      java.lang.Object ref = status_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        status_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TASK_LOCALITY_FIELD_NUMBER = 11;
    private volatile java.lang.Object taskLocality_;
    /**
     * <code>string task_locality = 11;</code>
     * @return Whether the taskLocality field is set.
     */
    @java.lang.Override
    public boolean hasTaskLocality() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>string task_locality = 11;</code>
     * @return The taskLocality.
     */
    @java.lang.Override
    public java.lang.String getTaskLocality() {
      java.lang.Object ref = taskLocality_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        taskLocality_ = s;
        return s;
      }
    }
    /**
     * <code>string task_locality = 11;</code>
     * @return The bytes for taskLocality.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getTaskLocalityBytes() {
      java.lang.Object ref = taskLocality_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        taskLocality_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int SPECULATIVE_FIELD_NUMBER = 12;
    private boolean speculative_;
    /**
     * <code>bool speculative = 12;</code>
     * @return The speculative.
     */
    @java.lang.Override
    public boolean getSpeculative() {
      return speculative_;
    }

    public static final int ACCUMULATOR_UPDATES_FIELD_NUMBER = 13;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> accumulatorUpdates_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> getAccumulatorUpdatesList() {
      return accumulatorUpdates_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
        getAccumulatorUpdatesOrBuilderList() {
      return accumulatorUpdates_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public int getAccumulatorUpdatesCount() {
      return accumulatorUpdates_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index) {
      return accumulatorUpdates_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
        int index) {
      return accumulatorUpdates_.get(index);
    }

    public static final int ERROR_MESSAGE_FIELD_NUMBER = 14;
    private volatile java.lang.Object errorMessage_;
    /**
     * <code>string error_message = 14;</code>
     * @return Whether the errorMessage field is set.
     */
    @java.lang.Override
    public boolean hasErrorMessage() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>string error_message = 14;</code>
     * @return The errorMessage.
     */
    @java.lang.Override
    public java.lang.String getErrorMessage() {
      java.lang.Object ref = errorMessage_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        errorMessage_ = s;
        return s;
      }
    }
    /**
     * <code>string error_message = 14;</code>
     * @return The bytes for errorMessage.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getErrorMessageBytes() {
      java.lang.Object ref = errorMessage_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        errorMessage_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int HAS_METRICS_FIELD_NUMBER = 15;
    private boolean hasMetrics_;
    /**
     * <code>bool has_metrics = 15;</code>
     * @return The hasMetrics.
     */
    @java.lang.Override
    public boolean getHasMetrics() {
      return hasMetrics_;
    }

    public static final int EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER = 16;
    private long executorDeserializeTime_;
    /**
     * <code>int64 executor_deserialize_time = 16;</code>
     * @return The executorDeserializeTime.
     */
    @java.lang.Override
    public long getExecutorDeserializeTime() {
      return executorDeserializeTime_;
    }

    public static final int EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER = 17;
    private long executorDeserializeCpuTime_;
    /**
     * <code>int64 executor_deserialize_cpu_time = 17;</code>
     * @return The executorDeserializeCpuTime.
     */
    @java.lang.Override
    public long getExecutorDeserializeCpuTime() {
      return executorDeserializeCpuTime_;
    }

    public static final int EXECUTOR_RUN_TIME_FIELD_NUMBER = 18;
    private long executorRunTime_;
    /**
     * <code>int64 executor_run_time = 18;</code>
     * @return The executorRunTime.
     */
    @java.lang.Override
    public long getExecutorRunTime() {
      return executorRunTime_;
    }

    public static final int EXECUTOR_CPU_TIME_FIELD_NUMBER = 19;
    private long executorCpuTime_;
    /**
     * <code>int64 executor_cpu_time = 19;</code>
     * @return The executorCpuTime.
     */
    @java.lang.Override
    public long getExecutorCpuTime() {
      return executorCpuTime_;
    }

    public static final int RESULT_SIZE_FIELD_NUMBER = 20;
    private long resultSize_;
    /**
     * <code>int64 result_size = 20;</code>
     * @return The resultSize.
     */
    @java.lang.Override
    public long getResultSize() {
      return resultSize_;
    }

    public static final int JVM_GC_TIME_FIELD_NUMBER = 21;
    private long jvmGcTime_;
    /**
     * <code>int64 jvm_gc_time = 21;</code>
     * @return The jvmGcTime.
     */
    @java.lang.Override
    public long getJvmGcTime() {
      return jvmGcTime_;
    }

    public static final int RESULT_SERIALIZATION_TIME_FIELD_NUMBER = 22;
    private long resultSerializationTime_;
    /**
     * <code>int64 result_serialization_time = 22;</code>
     * @return The resultSerializationTime.
     */
    @java.lang.Override
    public long getResultSerializationTime() {
      return resultSerializationTime_;
    }

    public static final int MEMORY_BYTES_SPILLED_FIELD_NUMBER = 23;
    private long memoryBytesSpilled_;
    /**
     * <code>int64 memory_bytes_spilled = 23;</code>
     * @return The memoryBytesSpilled.
     */
    @java.lang.Override
    public long getMemoryBytesSpilled() {
      return memoryBytesSpilled_;
    }

    public static final int DISK_BYTES_SPILLED_FIELD_NUMBER = 24;
    private long diskBytesSpilled_;
    /**
     * <code>int64 disk_bytes_spilled = 24;</code>
     * @return The diskBytesSpilled.
     */
    @java.lang.Override
    public long getDiskBytesSpilled() {
      return diskBytesSpilled_;
    }

    public static final int PEAK_EXECUTION_MEMORY_FIELD_NUMBER = 25;
    private long peakExecutionMemory_;
    /**
     * <code>int64 peak_execution_memory = 25;</code>
     * @return The peakExecutionMemory.
     */
    @java.lang.Override
    public long getPeakExecutionMemory() {
      return peakExecutionMemory_;
    }

    public static final int INPUT_BYTES_READ_FIELD_NUMBER = 26;
    private long inputBytesRead_;
    /**
     * <code>int64 input_bytes_read = 26;</code>
     * @return The inputBytesRead.
     */
    @java.lang.Override
    public long getInputBytesRead() {
      return inputBytesRead_;
    }

    public static final int INPUT_RECORDS_READ_FIELD_NUMBER = 27;
    private long inputRecordsRead_;
    /**
     * <code>int64 input_records_read = 27;</code>
     * @return The inputRecordsRead.
     */
    @java.lang.Override
    public long getInputRecordsRead() {
      return inputRecordsRead_;
    }

    public static final int OUTPUT_BYTES_WRITTEN_FIELD_NUMBER = 28;
    private long outputBytesWritten_;
    /**
     * <code>int64 output_bytes_written = 28;</code>
     * @return The outputBytesWritten.
     */
    @java.lang.Override
    public long getOutputBytesWritten() {
      return outputBytesWritten_;
    }

    public static final int OUTPUT_RECORDS_WRITTEN_FIELD_NUMBER = 29;
    private long outputRecordsWritten_;
    /**
     * <code>int64 output_records_written = 29;</code>
     * @return The outputRecordsWritten.
     */
    @java.lang.Override
    public long getOutputRecordsWritten() {
      return outputRecordsWritten_;
    }

    public static final int SHUFFLE_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER = 30;
    private long shuffleRemoteBlocksFetched_;
    /**
     * <code>int64 shuffle_remote_blocks_fetched = 30;</code>
     * @return The shuffleRemoteBlocksFetched.
     */
    @java.lang.Override
    public long getShuffleRemoteBlocksFetched() {
      return shuffleRemoteBlocksFetched_;
    }

    public static final int SHUFFLE_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER = 31;
    private long shuffleLocalBlocksFetched_;
    /**
     * <code>int64 shuffle_local_blocks_fetched = 31;</code>
     * @return The shuffleLocalBlocksFetched.
     */
    @java.lang.Override
    public long getShuffleLocalBlocksFetched() {
      return shuffleLocalBlocksFetched_;
    }

    public static final int SHUFFLE_FETCH_WAIT_TIME_FIELD_NUMBER = 32;
    private long shuffleFetchWaitTime_;
    /**
     * <code>int64 shuffle_fetch_wait_time = 32;</code>
     * @return The shuffleFetchWaitTime.
     */
    @java.lang.Override
    public long getShuffleFetchWaitTime() {
      return shuffleFetchWaitTime_;
    }

    public static final int SHUFFLE_REMOTE_BYTES_READ_FIELD_NUMBER = 33;
    private long shuffleRemoteBytesRead_;
    /**
     * <code>int64 shuffle_remote_bytes_read = 33;</code>
     * @return The shuffleRemoteBytesRead.
     */
    @java.lang.Override
    public long getShuffleRemoteBytesRead() {
      return shuffleRemoteBytesRead_;
    }

    public static final int SHUFFLE_REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER = 34;
    private long shuffleRemoteBytesReadToDisk_;
    /**
     * <code>int64 shuffle_remote_bytes_read_to_disk = 34;</code>
     * @return The shuffleRemoteBytesReadToDisk.
     */
    @java.lang.Override
    public long getShuffleRemoteBytesReadToDisk() {
      return shuffleRemoteBytesReadToDisk_;
    }

    public static final int SHUFFLE_LOCAL_BYTES_READ_FIELD_NUMBER = 35;
    private long shuffleLocalBytesRead_;
    /**
     * <code>int64 shuffle_local_bytes_read = 35;</code>
     * @return The shuffleLocalBytesRead.
     */
    @java.lang.Override
    public long getShuffleLocalBytesRead() {
      return shuffleLocalBytesRead_;
    }

    public static final int SHUFFLE_RECORDS_READ_FIELD_NUMBER = 36;
    private long shuffleRecordsRead_;
    /**
     * <code>int64 shuffle_records_read = 36;</code>
     * @return The shuffleRecordsRead.
     */
    @java.lang.Override
    public long getShuffleRecordsRead() {
      return shuffleRecordsRead_;
    }

    public static final int SHUFFLE_BYTES_WRITTEN_FIELD_NUMBER = 37;
    private long shuffleBytesWritten_;
    /**
     * <code>int64 shuffle_bytes_written = 37;</code>
     * @return The shuffleBytesWritten.
     */
    @java.lang.Override
    public long getShuffleBytesWritten() {
      return shuffleBytesWritten_;
    }

    public static final int SHUFFLE_WRITE_TIME_FIELD_NUMBER = 38;
    private long shuffleWriteTime_;
    /**
     * <code>int64 shuffle_write_time = 38;</code>
     * @return The shuffleWriteTime.
     */
    @java.lang.Override
    public long getShuffleWriteTime() {
      return shuffleWriteTime_;
    }

    public static final int SHUFFLE_RECORDS_WRITTEN_FIELD_NUMBER = 39;
    private long shuffleRecordsWritten_;
    /**
     * <code>int64 shuffle_records_written = 39;</code>
     * @return The shuffleRecordsWritten.
     */
    @java.lang.Override
    public long getShuffleRecordsWritten() {
      return shuffleRecordsWritten_;
    }

    public static final int STAGE_ID_FIELD_NUMBER = 40;
    private long stageId_;
    /**
     * <code>int64 stage_id = 40;</code>
     * @return The stageId.
     */
    @java.lang.Override
    public long getStageId() {
      return stageId_;
    }

    public static final int STAGE_ATTEMPT_ID_FIELD_NUMBER = 41;
    private int stageAttemptId_;
    /**
     * <code>int32 stage_attempt_id = 41;</code>
     * @return The stageAttemptId.
     */
    @java.lang.Override
    public int getStageAttemptId() {
      return stageAttemptId_;
    }

    public static final int SHUFFLE_CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER = 42;
    private long shuffleCorruptMergedBlockChunks_;
    /**
     * <code>int64 shuffle_corrupt_merged_block_chunks = 42;</code>
     * @return The shuffleCorruptMergedBlockChunks.
     */
    @java.lang.Override
    public long getShuffleCorruptMergedBlockChunks() {
      return shuffleCorruptMergedBlockChunks_;
    }

    public static final int SHUFFLE_MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER = 43;
    private long shuffleMergedFetchFallbackCount_;
    /**
     * <code>int64 shuffle_merged_fetch_fallback_count = 43;</code>
     * @return The shuffleMergedFetchFallbackCount.
     */
    @java.lang.Override
    public long getShuffleMergedFetchFallbackCount() {
      return shuffleMergedFetchFallbackCount_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER = 44;
    private long shuffleMergedRemoteBlocksFetched_;
    /**
     * <code>int64 shuffle_merged_remote_blocks_fetched = 44;</code>
     * @return The shuffleMergedRemoteBlocksFetched.
     */
    @java.lang.Override
    public long getShuffleMergedRemoteBlocksFetched() {
      return shuffleMergedRemoteBlocksFetched_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER = 45;
    private long shuffleMergedLocalBlocksFetched_;
    /**
     * <code>int64 shuffle_merged_local_blocks_fetched = 45;</code>
     * @return The shuffleMergedLocalBlocksFetched.
     */
    @java.lang.Override
    public long getShuffleMergedLocalBlocksFetched() {
      return shuffleMergedLocalBlocksFetched_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_CHUNKS_FETCHED_FIELD_NUMBER = 46;
    private long shuffleMergedRemoteChunksFetched_;
    /**
     * <code>int64 shuffle_merged_remote_chunks_fetched = 46;</code>
     * @return The shuffleMergedRemoteChunksFetched.
     */
    @java.lang.Override
    public long getShuffleMergedRemoteChunksFetched() {
      return shuffleMergedRemoteChunksFetched_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_CHUNKS_FETCHED_FIELD_NUMBER = 47;
    private long shuffleMergedLocalChunksFetched_;
    /**
     * <code>int64 shuffle_merged_local_chunks_fetched = 47;</code>
     * @return The shuffleMergedLocalChunksFetched.
     */
    @java.lang.Override
    public long getShuffleMergedLocalChunksFetched() {
      return shuffleMergedLocalChunksFetched_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_BYTES_READ_FIELD_NUMBER = 48;
    private long shuffleMergedRemoteBytesRead_;
    /**
     * <code>int64 shuffle_merged_remote_bytes_read = 48;</code>
     * @return The shuffleMergedRemoteBytesRead.
     */
    @java.lang.Override
    public long getShuffleMergedRemoteBytesRead() {
      return shuffleMergedRemoteBytesRead_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_BYTES_READ_FIELD_NUMBER = 49;
    private long shuffleMergedLocalBytesRead_;
    /**
     * <code>int64 shuffle_merged_local_bytes_read = 49;</code>
     * @return The shuffleMergedLocalBytesRead.
     */
    @java.lang.Override
    public long getShuffleMergedLocalBytesRead() {
      return shuffleMergedLocalBytesRead_;
    }

    public static final int SHUFFLE_REMOTE_REQS_DURATION_FIELD_NUMBER = 50;
    private long shuffleRemoteReqsDuration_;
    /**
     * <code>int64 shuffle_remote_reqs_duration = 50;</code>
     * @return The shuffleRemoteReqsDuration.
     */
    @java.lang.Override
    public long getShuffleRemoteReqsDuration() {
      return shuffleRemoteReqsDuration_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_REQ_DURATION_FIELD_NUMBER = 51;
    private long shuffleMergedRemoteReqDuration_;
    /**
     * <code>int64 shuffle_merged_remote_req_duration = 51;</code>
     * @return The shuffleMergedRemoteReqDuration.
     */
    @java.lang.Override
    public long getShuffleMergedRemoteReqDuration() {
      return shuffleMergedRemoteReqDuration_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (taskId_ != 0L) {
        output.writeInt64(1, taskId_);
      }
      if (index_ != 0) {
        output.writeInt32(2, index_);
      }
      if (attempt_ != 0) {
        output.writeInt32(3, attempt_);
      }
      if (partitionId_ != 0) {
        output.writeInt32(4, partitionId_);
      }
      if (launchTime_ != 0L) {
        output.writeInt64(5, launchTime_);
      }
      if (resultFetchStart_ != 0L) {
        output.writeInt64(6, resultFetchStart_);
      }
      if (duration_ != 0L) {
        output.writeInt64(7, duration_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, executorId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 9, host_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 10, status_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 11, taskLocality_);
      }
      if (speculative_ != false) {
        output.writeBool(12, speculative_);
      }
      for (int i = 0; i < accumulatorUpdates_.size(); i++) {
        output.writeMessage(13, accumulatorUpdates_.get(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 14, errorMessage_);
      }
      if (hasMetrics_ != false) {
        output.writeBool(15, hasMetrics_);
      }
      if (executorDeserializeTime_ != 0L) {
        output.writeInt64(16, executorDeserializeTime_);
      }
      if (executorDeserializeCpuTime_ != 0L) {
        output.writeInt64(17, executorDeserializeCpuTime_);
      }
      if (executorRunTime_ != 0L) {
        output.writeInt64(18, executorRunTime_);
      }
      if (executorCpuTime_ != 0L) {
        output.writeInt64(19, executorCpuTime_);
      }
      if (resultSize_ != 0L) {
        output.writeInt64(20, resultSize_);
      }
      if (jvmGcTime_ != 0L) {
        output.writeInt64(21, jvmGcTime_);
      }
      if (resultSerializationTime_ != 0L) {
        output.writeInt64(22, resultSerializationTime_);
      }
      if (memoryBytesSpilled_ != 0L) {
        output.writeInt64(23, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0L) {
        output.writeInt64(24, diskBytesSpilled_);
      }
      if (peakExecutionMemory_ != 0L) {
        output.writeInt64(25, peakExecutionMemory_);
      }
      if (inputBytesRead_ != 0L) {
        output.writeInt64(26, inputBytesRead_);
      }
      if (inputRecordsRead_ != 0L) {
        output.writeInt64(27, inputRecordsRead_);
      }
      if (outputBytesWritten_ != 0L) {
        output.writeInt64(28, outputBytesWritten_);
      }
      if (outputRecordsWritten_ != 0L) {
        output.writeInt64(29, outputRecordsWritten_);
      }
      if (shuffleRemoteBlocksFetched_ != 0L) {
        output.writeInt64(30, shuffleRemoteBlocksFetched_);
      }
      if (shuffleLocalBlocksFetched_ != 0L) {
        output.writeInt64(31, shuffleLocalBlocksFetched_);
      }
      if (shuffleFetchWaitTime_ != 0L) {
        output.writeInt64(32, shuffleFetchWaitTime_);
      }
      if (shuffleRemoteBytesRead_ != 0L) {
        output.writeInt64(33, shuffleRemoteBytesRead_);
      }
      if (shuffleRemoteBytesReadToDisk_ != 0L) {
        output.writeInt64(34, shuffleRemoteBytesReadToDisk_);
      }
      if (shuffleLocalBytesRead_ != 0L) {
        output.writeInt64(35, shuffleLocalBytesRead_);
      }
      if (shuffleRecordsRead_ != 0L) {
        output.writeInt64(36, shuffleRecordsRead_);
      }
      if (shuffleBytesWritten_ != 0L) {
        output.writeInt64(37, shuffleBytesWritten_);
      }
      if (shuffleWriteTime_ != 0L) {
        output.writeInt64(38, shuffleWriteTime_);
      }
      if (shuffleRecordsWritten_ != 0L) {
        output.writeInt64(39, shuffleRecordsWritten_);
      }
      if (stageId_ != 0L) {
        output.writeInt64(40, stageId_);
      }
      if (stageAttemptId_ != 0) {
        output.writeInt32(41, stageAttemptId_);
      }
      if (shuffleCorruptMergedBlockChunks_ != 0L) {
        output.writeInt64(42, shuffleCorruptMergedBlockChunks_);
      }
      if (shuffleMergedFetchFallbackCount_ != 0L) {
        output.writeInt64(43, shuffleMergedFetchFallbackCount_);
      }
      if (shuffleMergedRemoteBlocksFetched_ != 0L) {
        output.writeInt64(44, shuffleMergedRemoteBlocksFetched_);
      }
      if (shuffleMergedLocalBlocksFetched_ != 0L) {
        output.writeInt64(45, shuffleMergedLocalBlocksFetched_);
      }
      if (shuffleMergedRemoteChunksFetched_ != 0L) {
        output.writeInt64(46, shuffleMergedRemoteChunksFetched_);
      }
      if (shuffleMergedLocalChunksFetched_ != 0L) {
        output.writeInt64(47, shuffleMergedLocalChunksFetched_);
      }
      if (shuffleMergedRemoteBytesRead_ != 0L) {
        output.writeInt64(48, shuffleMergedRemoteBytesRead_);
      }
      if (shuffleMergedLocalBytesRead_ != 0L) {
        output.writeInt64(49, shuffleMergedLocalBytesRead_);
      }
      if (shuffleRemoteReqsDuration_ != 0L) {
        output.writeInt64(50, shuffleRemoteReqsDuration_);
      }
      if (shuffleMergedRemoteReqDuration_ != 0L) {
        output.writeInt64(51, shuffleMergedRemoteReqDuration_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (taskId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, taskId_);
      }
      if (index_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, index_);
      }
      if (attempt_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, attempt_);
      }
      if (partitionId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, partitionId_);
      }
      if (launchTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, launchTime_);
      }
      if (resultFetchStart_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, resultFetchStart_);
      }
      if (duration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, duration_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, executorId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(9, host_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(10, status_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(11, taskLocality_);
      }
      if (speculative_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(12, speculative_);
      }
      for (int i = 0; i < accumulatorUpdates_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(13, accumulatorUpdates_.get(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(14, errorMessage_);
      }
      if (hasMetrics_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(15, hasMetrics_);
      }
      if (executorDeserializeTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(16, executorDeserializeTime_);
      }
      if (executorDeserializeCpuTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(17, executorDeserializeCpuTime_);
      }
      if (executorRunTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(18, executorRunTime_);
      }
      if (executorCpuTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(19, executorCpuTime_);
      }
      if (resultSize_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(20, resultSize_);
      }
      if (jvmGcTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(21, jvmGcTime_);
      }
      if (resultSerializationTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(22, resultSerializationTime_);
      }
      if (memoryBytesSpilled_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(23, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(24, diskBytesSpilled_);
      }
      if (peakExecutionMemory_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(25, peakExecutionMemory_);
      }
      if (inputBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(26, inputBytesRead_);
      }
      if (inputRecordsRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(27, inputRecordsRead_);
      }
      if (outputBytesWritten_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(28, outputBytesWritten_);
      }
      if (outputRecordsWritten_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(29, outputRecordsWritten_);
      }
      if (shuffleRemoteBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(30, shuffleRemoteBlocksFetched_);
      }
      if (shuffleLocalBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(31, shuffleLocalBlocksFetched_);
      }
      if (shuffleFetchWaitTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(32, shuffleFetchWaitTime_);
      }
      if (shuffleRemoteBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(33, shuffleRemoteBytesRead_);
      }
      if (shuffleRemoteBytesReadToDisk_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(34, shuffleRemoteBytesReadToDisk_);
      }
      if (shuffleLocalBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(35, shuffleLocalBytesRead_);
      }
      if (shuffleRecordsRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(36, shuffleRecordsRead_);
      }
      if (shuffleBytesWritten_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(37, shuffleBytesWritten_);
      }
      if (shuffleWriteTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(38, shuffleWriteTime_);
      }
      if (shuffleRecordsWritten_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(39, shuffleRecordsWritten_);
      }
      if (stageId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(40, stageId_);
      }
      if (stageAttemptId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(41, stageAttemptId_);
      }
      if (shuffleCorruptMergedBlockChunks_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(42, shuffleCorruptMergedBlockChunks_);
      }
      if (shuffleMergedFetchFallbackCount_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(43, shuffleMergedFetchFallbackCount_);
      }
      if (shuffleMergedRemoteBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(44, shuffleMergedRemoteBlocksFetched_);
      }
      if (shuffleMergedLocalBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(45, shuffleMergedLocalBlocksFetched_);
      }
      if (shuffleMergedRemoteChunksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(46, shuffleMergedRemoteChunksFetched_);
      }
      if (shuffleMergedLocalChunksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(47, shuffleMergedLocalChunksFetched_);
      }
      if (shuffleMergedRemoteBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(48, shuffleMergedRemoteBytesRead_);
      }
      if (shuffleMergedLocalBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(49, shuffleMergedLocalBytesRead_);
      }
      if (shuffleRemoteReqsDuration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(50, shuffleRemoteReqsDuration_);
      }
      if (shuffleMergedRemoteReqDuration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(51, shuffleMergedRemoteReqDuration_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper other = (org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper) obj;

      if (getTaskId()
          != other.getTaskId()) return false;
      if (getIndex()
          != other.getIndex()) return false;
      if (getAttempt()
          != other.getAttempt()) return false;
      if (getPartitionId()
          != other.getPartitionId()) return false;
      if (getLaunchTime()
          != other.getLaunchTime()) return false;
      if (getResultFetchStart()
          != other.getResultFetchStart()) return false;
      if (getDuration()
          != other.getDuration()) return false;
      if (hasExecutorId() != other.hasExecutorId()) return false;
      if (hasExecutorId()) {
        if (!getExecutorId()
            .equals(other.getExecutorId())) return false;
      }
      if (hasHost() != other.hasHost()) return false;
      if (hasHost()) {
        if (!getHost()
            .equals(other.getHost())) return false;
      }
      if (hasStatus() != other.hasStatus()) return false;
      if (hasStatus()) {
        if (!getStatus()
            .equals(other.getStatus())) return false;
      }
      if (hasTaskLocality() != other.hasTaskLocality()) return false;
      if (hasTaskLocality()) {
        if (!getTaskLocality()
            .equals(other.getTaskLocality())) return false;
      }
      if (getSpeculative()
          != other.getSpeculative()) return false;
      if (!getAccumulatorUpdatesList()
          .equals(other.getAccumulatorUpdatesList())) return false;
      if (hasErrorMessage() != other.hasErrorMessage()) return false;
      if (hasErrorMessage()) {
        if (!getErrorMessage()
            .equals(other.getErrorMessage())) return false;
      }
      if (getHasMetrics()
          != other.getHasMetrics()) return false;
      if (getExecutorDeserializeTime()
          != other.getExecutorDeserializeTime()) return false;
      if (getExecutorDeserializeCpuTime()
          != other.getExecutorDeserializeCpuTime()) return false;
      if (getExecutorRunTime()
          != other.getExecutorRunTime()) return false;
      if (getExecutorCpuTime()
          != other.getExecutorCpuTime()) return false;
      if (getResultSize()
          != other.getResultSize()) return false;
      if (getJvmGcTime()
          != other.getJvmGcTime()) return false;
      if (getResultSerializationTime()
          != other.getResultSerializationTime()) return false;
      if (getMemoryBytesSpilled()
          != other.getMemoryBytesSpilled()) return false;
      if (getDiskBytesSpilled()
          != other.getDiskBytesSpilled()) return false;
      if (getPeakExecutionMemory()
          != other.getPeakExecutionMemory()) return false;
      if (getInputBytesRead()
          != other.getInputBytesRead()) return false;
      if (getInputRecordsRead()
          != other.getInputRecordsRead()) return false;
      if (getOutputBytesWritten()
          != other.getOutputBytesWritten()) return false;
      if (getOutputRecordsWritten()
          != other.getOutputRecordsWritten()) return false;
      if (getShuffleRemoteBlocksFetched()
          != other.getShuffleRemoteBlocksFetched()) return false;
      if (getShuffleLocalBlocksFetched()
          != other.getShuffleLocalBlocksFetched()) return false;
      if (getShuffleFetchWaitTime()
          != other.getShuffleFetchWaitTime()) return false;
      if (getShuffleRemoteBytesRead()
          != other.getShuffleRemoteBytesRead()) return false;
      if (getShuffleRemoteBytesReadToDisk()
          != other.getShuffleRemoteBytesReadToDisk()) return false;
      if (getShuffleLocalBytesRead()
          != other.getShuffleLocalBytesRead()) return false;
      if (getShuffleRecordsRead()
          != other.getShuffleRecordsRead()) return false;
      if (getShuffleBytesWritten()
          != other.getShuffleBytesWritten()) return false;
      if (getShuffleWriteTime()
          != other.getShuffleWriteTime()) return false;
      if (getShuffleRecordsWritten()
          != other.getShuffleRecordsWritten()) return false;
      if (getStageId()
          != other.getStageId()) return false;
      if (getStageAttemptId()
          != other.getStageAttemptId()) return false;
      if (getShuffleCorruptMergedBlockChunks()
          != other.getShuffleCorruptMergedBlockChunks()) return false;
      if (getShuffleMergedFetchFallbackCount()
          != other.getShuffleMergedFetchFallbackCount()) return false;
      if (getShuffleMergedRemoteBlocksFetched()
          != other.getShuffleMergedRemoteBlocksFetched()) return false;
      if (getShuffleMergedLocalBlocksFetched()
          != other.getShuffleMergedLocalBlocksFetched()) return false;
      if (getShuffleMergedRemoteChunksFetched()
          != other.getShuffleMergedRemoteChunksFetched()) return false;
      if (getShuffleMergedLocalChunksFetched()
          != other.getShuffleMergedLocalChunksFetched()) return false;
      if (getShuffleMergedRemoteBytesRead()
          != other.getShuffleMergedRemoteBytesRead()) return false;
      if (getShuffleMergedLocalBytesRead()
          != other.getShuffleMergedLocalBytesRead()) return false;
      if (getShuffleRemoteReqsDuration()
          != other.getShuffleRemoteReqsDuration()) return false;
      if (getShuffleMergedRemoteReqDuration()
          != other.getShuffleMergedRemoteReqDuration()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + TASK_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTaskId());
      hash = (37 * hash) + INDEX_FIELD_NUMBER;
      hash = (53 * hash) + getIndex();
      hash = (37 * hash) + ATTEMPT_FIELD_NUMBER;
      hash = (53 * hash) + getAttempt();
      hash = (37 * hash) + PARTITION_ID_FIELD_NUMBER;
      hash = (53 * hash) + getPartitionId();
      hash = (37 * hash) + LAUNCH_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getLaunchTime());
      hash = (37 * hash) + RESULT_FETCH_START_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getResultFetchStart());
      hash = (37 * hash) + DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDuration());
      if (hasExecutorId()) {
        hash = (37 * hash) + EXECUTOR_ID_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorId().hashCode();
      }
      if (hasHost()) {
        hash = (37 * hash) + HOST_FIELD_NUMBER;
        hash = (53 * hash) + getHost().hashCode();
      }
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getStatus().hashCode();
      }
      if (hasTaskLocality()) {
        hash = (37 * hash) + TASK_LOCALITY_FIELD_NUMBER;
        hash = (53 * hash) + getTaskLocality().hashCode();
      }
      hash = (37 * hash) + SPECULATIVE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getSpeculative());
      if (getAccumulatorUpdatesCount() > 0) {
        hash = (37 * hash) + ACCUMULATOR_UPDATES_FIELD_NUMBER;
        hash = (53 * hash) + getAccumulatorUpdatesList().hashCode();
      }
      if (hasErrorMessage()) {
        hash = (37 * hash) + ERROR_MESSAGE_FIELD_NUMBER;
        hash = (53 * hash) + getErrorMessage().hashCode();
      }
      hash = (37 * hash) + HAS_METRICS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getHasMetrics());
      hash = (37 * hash) + EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorDeserializeTime());
      hash = (37 * hash) + EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorDeserializeCpuTime());
      hash = (37 * hash) + EXECUTOR_RUN_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorRunTime());
      hash = (37 * hash) + EXECUTOR_CPU_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorCpuTime());
      hash = (37 * hash) + RESULT_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getResultSize());
      hash = (37 * hash) + JVM_GC_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getJvmGcTime());
      hash = (37 * hash) + RESULT_SERIALIZATION_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getResultSerializationTime());
      hash = (37 * hash) + MEMORY_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryBytesSpilled());
      hash = (37 * hash) + DISK_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskBytesSpilled());
      hash = (37 * hash) + PEAK_EXECUTION_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getPeakExecutionMemory());
      hash = (37 * hash) + INPUT_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getInputBytesRead());
      hash = (37 * hash) + INPUT_RECORDS_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getInputRecordsRead());
      hash = (37 * hash) + OUTPUT_BYTES_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getOutputBytesWritten());
      hash = (37 * hash) + OUTPUT_RECORDS_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getOutputRecordsWritten());
      hash = (37 * hash) + SHUFFLE_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRemoteBlocksFetched());
      hash = (37 * hash) + SHUFFLE_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleLocalBlocksFetched());
      hash = (37 * hash) + SHUFFLE_FETCH_WAIT_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleFetchWaitTime());
      hash = (37 * hash) + SHUFFLE_REMOTE_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRemoteBytesRead());
      hash = (37 * hash) + SHUFFLE_REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRemoteBytesReadToDisk());
      hash = (37 * hash) + SHUFFLE_LOCAL_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleLocalBytesRead());
      hash = (37 * hash) + SHUFFLE_RECORDS_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRecordsRead());
      hash = (37 * hash) + SHUFFLE_BYTES_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleBytesWritten());
      hash = (37 * hash) + SHUFFLE_WRITE_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleWriteTime());
      hash = (37 * hash) + SHUFFLE_RECORDS_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRecordsWritten());
      hash = (37 * hash) + STAGE_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getStageId());
      hash = (37 * hash) + STAGE_ATTEMPT_ID_FIELD_NUMBER;
      hash = (53 * hash) + getStageAttemptId();
      hash = (37 * hash) + SHUFFLE_CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleCorruptMergedBlockChunks());
      hash = (37 * hash) + SHUFFLE_MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedFetchFallbackCount());
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedRemoteBlocksFetched());
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedLocalBlocksFetched());
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_CHUNKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedRemoteChunksFetched());
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_CHUNKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedLocalChunksFetched());
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedRemoteBytesRead());
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedLocalBytesRead());
      hash = (37 * hash) + SHUFFLE_REMOTE_REQS_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRemoteReqsDuration());
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_REQ_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedRemoteReqDuration());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.TaskDataWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.TaskDataWrapper)
        org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper.class, org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAccumulatorUpdatesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        taskId_ = 0L;

        index_ = 0;

        attempt_ = 0;

        partitionId_ = 0;

        launchTime_ = 0L;

        resultFetchStart_ = 0L;

        duration_ = 0L;

        executorId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        host_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        status_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        taskLocality_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        speculative_ = false;

        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdates_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          accumulatorUpdatesBuilder_.clear();
        }
        errorMessage_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        hasMetrics_ = false;

        executorDeserializeTime_ = 0L;

        executorDeserializeCpuTime_ = 0L;

        executorRunTime_ = 0L;

        executorCpuTime_ = 0L;

        resultSize_ = 0L;

        jvmGcTime_ = 0L;

        resultSerializationTime_ = 0L;

        memoryBytesSpilled_ = 0L;

        diskBytesSpilled_ = 0L;

        peakExecutionMemory_ = 0L;

        inputBytesRead_ = 0L;

        inputRecordsRead_ = 0L;

        outputBytesWritten_ = 0L;

        outputRecordsWritten_ = 0L;

        shuffleRemoteBlocksFetched_ = 0L;

        shuffleLocalBlocksFetched_ = 0L;

        shuffleFetchWaitTime_ = 0L;

        shuffleRemoteBytesRead_ = 0L;

        shuffleRemoteBytesReadToDisk_ = 0L;

        shuffleLocalBytesRead_ = 0L;

        shuffleRecordsRead_ = 0L;

        shuffleBytesWritten_ = 0L;

        shuffleWriteTime_ = 0L;

        shuffleRecordsWritten_ = 0L;

        stageId_ = 0L;

        stageAttemptId_ = 0;

        shuffleCorruptMergedBlockChunks_ = 0L;

        shuffleMergedFetchFallbackCount_ = 0L;

        shuffleMergedRemoteBlocksFetched_ = 0L;

        shuffleMergedLocalBlocksFetched_ = 0L;

        shuffleMergedRemoteChunksFetched_ = 0L;

        shuffleMergedLocalChunksFetched_ = 0L;

        shuffleMergedRemoteBytesRead_ = 0L;

        shuffleMergedLocalBytesRead_ = 0L;

        shuffleRemoteReqsDuration_ = 0L;

        shuffleMergedRemoteReqDuration_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper result = new org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.taskId_ = taskId_;
        result.index_ = index_;
        result.attempt_ = attempt_;
        result.partitionId_ = partitionId_;
        result.launchTime_ = launchTime_;
        result.resultFetchStart_ = resultFetchStart_;
        result.duration_ = duration_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.executorId_ = executorId_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.host_ = host_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.status_ = status_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.taskLocality_ = taskLocality_;
        result.speculative_ = speculative_;
        if (accumulatorUpdatesBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0)) {
            accumulatorUpdates_ = java.util.Collections.unmodifiableList(accumulatorUpdates_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.accumulatorUpdates_ = accumulatorUpdates_;
        } else {
          result.accumulatorUpdates_ = accumulatorUpdatesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.errorMessage_ = errorMessage_;
        result.hasMetrics_ = hasMetrics_;
        result.executorDeserializeTime_ = executorDeserializeTime_;
        result.executorDeserializeCpuTime_ = executorDeserializeCpuTime_;
        result.executorRunTime_ = executorRunTime_;
        result.executorCpuTime_ = executorCpuTime_;
        result.resultSize_ = resultSize_;
        result.jvmGcTime_ = jvmGcTime_;
        result.resultSerializationTime_ = resultSerializationTime_;
        result.memoryBytesSpilled_ = memoryBytesSpilled_;
        result.diskBytesSpilled_ = diskBytesSpilled_;
        result.peakExecutionMemory_ = peakExecutionMemory_;
        result.inputBytesRead_ = inputBytesRead_;
        result.inputRecordsRead_ = inputRecordsRead_;
        result.outputBytesWritten_ = outputBytesWritten_;
        result.outputRecordsWritten_ = outputRecordsWritten_;
        result.shuffleRemoteBlocksFetched_ = shuffleRemoteBlocksFetched_;
        result.shuffleLocalBlocksFetched_ = shuffleLocalBlocksFetched_;
        result.shuffleFetchWaitTime_ = shuffleFetchWaitTime_;
        result.shuffleRemoteBytesRead_ = shuffleRemoteBytesRead_;
        result.shuffleRemoteBytesReadToDisk_ = shuffleRemoteBytesReadToDisk_;
        result.shuffleLocalBytesRead_ = shuffleLocalBytesRead_;
        result.shuffleRecordsRead_ = shuffleRecordsRead_;
        result.shuffleBytesWritten_ = shuffleBytesWritten_;
        result.shuffleWriteTime_ = shuffleWriteTime_;
        result.shuffleRecordsWritten_ = shuffleRecordsWritten_;
        result.stageId_ = stageId_;
        result.stageAttemptId_ = stageAttemptId_;
        result.shuffleCorruptMergedBlockChunks_ = shuffleCorruptMergedBlockChunks_;
        result.shuffleMergedFetchFallbackCount_ = shuffleMergedFetchFallbackCount_;
        result.shuffleMergedRemoteBlocksFetched_ = shuffleMergedRemoteBlocksFetched_;
        result.shuffleMergedLocalBlocksFetched_ = shuffleMergedLocalBlocksFetched_;
        result.shuffleMergedRemoteChunksFetched_ = shuffleMergedRemoteChunksFetched_;
        result.shuffleMergedLocalChunksFetched_ = shuffleMergedLocalChunksFetched_;
        result.shuffleMergedRemoteBytesRead_ = shuffleMergedRemoteBytesRead_;
        result.shuffleMergedLocalBytesRead_ = shuffleMergedLocalBytesRead_;
        result.shuffleRemoteReqsDuration_ = shuffleRemoteReqsDuration_;
        result.shuffleMergedRemoteReqDuration_ = shuffleMergedRemoteReqDuration_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper.getDefaultInstance()) return this;
        if (other.getTaskId() != 0L) {
          setTaskId(other.getTaskId());
        }
        if (other.getIndex() != 0) {
          setIndex(other.getIndex());
        }
        if (other.getAttempt() != 0) {
          setAttempt(other.getAttempt());
        }
        if (other.getPartitionId() != 0) {
          setPartitionId(other.getPartitionId());
        }
        if (other.getLaunchTime() != 0L) {
          setLaunchTime(other.getLaunchTime());
        }
        if (other.getResultFetchStart() != 0L) {
          setResultFetchStart(other.getResultFetchStart());
        }
        if (other.getDuration() != 0L) {
          setDuration(other.getDuration());
        }
        if (other.hasExecutorId()) {
          bitField0_ |= 0x00000001;
          executorId_ = other.executorId_;
          onChanged();
        }
        if (other.hasHost()) {
          bitField0_ |= 0x00000002;
          host_ = other.host_;
          onChanged();
        }
        if (other.hasStatus()) {
          bitField0_ |= 0x00000004;
          status_ = other.status_;
          onChanged();
        }
        if (other.hasTaskLocality()) {
          bitField0_ |= 0x00000008;
          taskLocality_ = other.taskLocality_;
          onChanged();
        }
        if (other.getSpeculative() != false) {
          setSpeculative(other.getSpeculative());
        }
        if (accumulatorUpdatesBuilder_ == null) {
          if (!other.accumulatorUpdates_.isEmpty()) {
            if (accumulatorUpdates_.isEmpty()) {
              accumulatorUpdates_ = other.accumulatorUpdates_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureAccumulatorUpdatesIsMutable();
              accumulatorUpdates_.addAll(other.accumulatorUpdates_);
            }
            onChanged();
          }
        } else {
          if (!other.accumulatorUpdates_.isEmpty()) {
            if (accumulatorUpdatesBuilder_.isEmpty()) {
              accumulatorUpdatesBuilder_.dispose();
              accumulatorUpdatesBuilder_ = null;
              accumulatorUpdates_ = other.accumulatorUpdates_;
              bitField0_ = (bitField0_ & ~0x00000010);
              accumulatorUpdatesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getAccumulatorUpdatesFieldBuilder() : null;
            } else {
              accumulatorUpdatesBuilder_.addAllMessages(other.accumulatorUpdates_);
            }
          }
        }
        if (other.hasErrorMessage()) {
          bitField0_ |= 0x00000020;
          errorMessage_ = other.errorMessage_;
          onChanged();
        }
        if (other.getHasMetrics() != false) {
          setHasMetrics(other.getHasMetrics());
        }
        if (other.getExecutorDeserializeTime() != 0L) {
          setExecutorDeserializeTime(other.getExecutorDeserializeTime());
        }
        if (other.getExecutorDeserializeCpuTime() != 0L) {
          setExecutorDeserializeCpuTime(other.getExecutorDeserializeCpuTime());
        }
        if (other.getExecutorRunTime() != 0L) {
          setExecutorRunTime(other.getExecutorRunTime());
        }
        if (other.getExecutorCpuTime() != 0L) {
          setExecutorCpuTime(other.getExecutorCpuTime());
        }
        if (other.getResultSize() != 0L) {
          setResultSize(other.getResultSize());
        }
        if (other.getJvmGcTime() != 0L) {
          setJvmGcTime(other.getJvmGcTime());
        }
        if (other.getResultSerializationTime() != 0L) {
          setResultSerializationTime(other.getResultSerializationTime());
        }
        if (other.getMemoryBytesSpilled() != 0L) {
          setMemoryBytesSpilled(other.getMemoryBytesSpilled());
        }
        if (other.getDiskBytesSpilled() != 0L) {
          setDiskBytesSpilled(other.getDiskBytesSpilled());
        }
        if (other.getPeakExecutionMemory() != 0L) {
          setPeakExecutionMemory(other.getPeakExecutionMemory());
        }
        if (other.getInputBytesRead() != 0L) {
          setInputBytesRead(other.getInputBytesRead());
        }
        if (other.getInputRecordsRead() != 0L) {
          setInputRecordsRead(other.getInputRecordsRead());
        }
        if (other.getOutputBytesWritten() != 0L) {
          setOutputBytesWritten(other.getOutputBytesWritten());
        }
        if (other.getOutputRecordsWritten() != 0L) {
          setOutputRecordsWritten(other.getOutputRecordsWritten());
        }
        if (other.getShuffleRemoteBlocksFetched() != 0L) {
          setShuffleRemoteBlocksFetched(other.getShuffleRemoteBlocksFetched());
        }
        if (other.getShuffleLocalBlocksFetched() != 0L) {
          setShuffleLocalBlocksFetched(other.getShuffleLocalBlocksFetched());
        }
        if (other.getShuffleFetchWaitTime() != 0L) {
          setShuffleFetchWaitTime(other.getShuffleFetchWaitTime());
        }
        if (other.getShuffleRemoteBytesRead() != 0L) {
          setShuffleRemoteBytesRead(other.getShuffleRemoteBytesRead());
        }
        if (other.getShuffleRemoteBytesReadToDisk() != 0L) {
          setShuffleRemoteBytesReadToDisk(other.getShuffleRemoteBytesReadToDisk());
        }
        if (other.getShuffleLocalBytesRead() != 0L) {
          setShuffleLocalBytesRead(other.getShuffleLocalBytesRead());
        }
        if (other.getShuffleRecordsRead() != 0L) {
          setShuffleRecordsRead(other.getShuffleRecordsRead());
        }
        if (other.getShuffleBytesWritten() != 0L) {
          setShuffleBytesWritten(other.getShuffleBytesWritten());
        }
        if (other.getShuffleWriteTime() != 0L) {
          setShuffleWriteTime(other.getShuffleWriteTime());
        }
        if (other.getShuffleRecordsWritten() != 0L) {
          setShuffleRecordsWritten(other.getShuffleRecordsWritten());
        }
        if (other.getStageId() != 0L) {
          setStageId(other.getStageId());
        }
        if (other.getStageAttemptId() != 0) {
          setStageAttemptId(other.getStageAttemptId());
        }
        if (other.getShuffleCorruptMergedBlockChunks() != 0L) {
          setShuffleCorruptMergedBlockChunks(other.getShuffleCorruptMergedBlockChunks());
        }
        if (other.getShuffleMergedFetchFallbackCount() != 0L) {
          setShuffleMergedFetchFallbackCount(other.getShuffleMergedFetchFallbackCount());
        }
        if (other.getShuffleMergedRemoteBlocksFetched() != 0L) {
          setShuffleMergedRemoteBlocksFetched(other.getShuffleMergedRemoteBlocksFetched());
        }
        if (other.getShuffleMergedLocalBlocksFetched() != 0L) {
          setShuffleMergedLocalBlocksFetched(other.getShuffleMergedLocalBlocksFetched());
        }
        if (other.getShuffleMergedRemoteChunksFetched() != 0L) {
          setShuffleMergedRemoteChunksFetched(other.getShuffleMergedRemoteChunksFetched());
        }
        if (other.getShuffleMergedLocalChunksFetched() != 0L) {
          setShuffleMergedLocalChunksFetched(other.getShuffleMergedLocalChunksFetched());
        }
        if (other.getShuffleMergedRemoteBytesRead() != 0L) {
          setShuffleMergedRemoteBytesRead(other.getShuffleMergedRemoteBytesRead());
        }
        if (other.getShuffleMergedLocalBytesRead() != 0L) {
          setShuffleMergedLocalBytesRead(other.getShuffleMergedLocalBytesRead());
        }
        if (other.getShuffleRemoteReqsDuration() != 0L) {
          setShuffleRemoteReqsDuration(other.getShuffleRemoteReqsDuration());
        }
        if (other.getShuffleMergedRemoteReqDuration() != 0L) {
          setShuffleMergedRemoteReqDuration(other.getShuffleMergedRemoteReqDuration());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long taskId_ ;
      /**
       * <code>int64 task_id = 1;</code>
       * @return The taskId.
       */
      @java.lang.Override
      public long getTaskId() {
        return taskId_;
      }
      /**
       * <code>int64 task_id = 1;</code>
       * @param value The taskId to set.
       * @return This builder for chaining.
       */
      public Builder setTaskId(long value) {
        
        taskId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 task_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTaskId() {
        
        taskId_ = 0L;
        onChanged();
        return this;
      }

      private int index_ ;
      /**
       * <code>int32 index = 2;</code>
       * @return The index.
       */
      @java.lang.Override
      public int getIndex() {
        return index_;
      }
      /**
       * <code>int32 index = 2;</code>
       * @param value The index to set.
       * @return This builder for chaining.
       */
      public Builder setIndex(int value) {
        
        index_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 index = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearIndex() {
        
        index_ = 0;
        onChanged();
        return this;
      }

      private int attempt_ ;
      /**
       * <code>int32 attempt = 3;</code>
       * @return The attempt.
       */
      @java.lang.Override
      public int getAttempt() {
        return attempt_;
      }
      /**
       * <code>int32 attempt = 3;</code>
       * @param value The attempt to set.
       * @return This builder for chaining.
       */
      public Builder setAttempt(int value) {
        
        attempt_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 attempt = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearAttempt() {
        
        attempt_ = 0;
        onChanged();
        return this;
      }

      private int partitionId_ ;
      /**
       * <code>int32 partition_id = 4;</code>
       * @return The partitionId.
       */
      @java.lang.Override
      public int getPartitionId() {
        return partitionId_;
      }
      /**
       * <code>int32 partition_id = 4;</code>
       * @param value The partitionId to set.
       * @return This builder for chaining.
       */
      public Builder setPartitionId(int value) {
        
        partitionId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 partition_id = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearPartitionId() {
        
        partitionId_ = 0;
        onChanged();
        return this;
      }

      private long launchTime_ ;
      /**
       * <code>int64 launch_time = 5;</code>
       * @return The launchTime.
       */
      @java.lang.Override
      public long getLaunchTime() {
        return launchTime_;
      }
      /**
       * <code>int64 launch_time = 5;</code>
       * @param value The launchTime to set.
       * @return This builder for chaining.
       */
      public Builder setLaunchTime(long value) {
        
        launchTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 launch_time = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearLaunchTime() {
        
        launchTime_ = 0L;
        onChanged();
        return this;
      }

      private long resultFetchStart_ ;
      /**
       * <code>int64 result_fetch_start = 6;</code>
       * @return The resultFetchStart.
       */
      @java.lang.Override
      public long getResultFetchStart() {
        return resultFetchStart_;
      }
      /**
       * <code>int64 result_fetch_start = 6;</code>
       * @param value The resultFetchStart to set.
       * @return This builder for chaining.
       */
      public Builder setResultFetchStart(long value) {
        
        resultFetchStart_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 result_fetch_start = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultFetchStart() {
        
        resultFetchStart_ = 0L;
        onChanged();
        return this;
      }

      private long duration_ ;
      /**
       * <code>int64 duration = 7;</code>
       * @return The duration.
       */
      @java.lang.Override
      public long getDuration() {
        return duration_;
      }
      /**
       * <code>int64 duration = 7;</code>
       * @param value The duration to set.
       * @return This builder for chaining.
       */
      public Builder setDuration(long value) {
        
        duration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 duration = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearDuration() {
        
        duration_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object executorId_ = "";
      /**
       * <code>string executor_id = 8;</code>
       * @return Whether the executorId field is set.
       */
      public boolean hasExecutorId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string executor_id = 8;</code>
       * @return The executorId.
       */
      public java.lang.String getExecutorId() {
        java.lang.Object ref = executorId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          executorId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string executor_id = 8;</code>
       * @return The bytes for executorId.
       */
      public com.google.protobuf.ByteString
          getExecutorIdBytes() {
        java.lang.Object ref = executorId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          executorId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string executor_id = 8;</code>
       * @param value The executorId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        executorId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string executor_id = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        executorId_ = getDefaultInstance().getExecutorId();
        onChanged();
        return this;
      }
      /**
       * <code>string executor_id = 8;</code>
       * @param value The bytes for executorId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        executorId_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object host_ = "";
      /**
       * <code>string host = 9;</code>
       * @return Whether the host field is set.
       */
      public boolean hasHost() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string host = 9;</code>
       * @return The host.
       */
      public java.lang.String getHost() {
        java.lang.Object ref = host_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          host_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string host = 9;</code>
       * @return The bytes for host.
       */
      public com.google.protobuf.ByteString
          getHostBytes() {
        java.lang.Object ref = host_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          host_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string host = 9;</code>
       * @param value The host to set.
       * @return This builder for chaining.
       */
      public Builder setHost(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        host_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string host = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearHost() {
        bitField0_ = (bitField0_ & ~0x00000002);
        host_ = getDefaultInstance().getHost();
        onChanged();
        return this;
      }
      /**
       * <code>string host = 9;</code>
       * @param value The bytes for host to set.
       * @return This builder for chaining.
       */
      public Builder setHostBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        host_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object status_ = "";
      /**
       * <code>string status = 10;</code>
       * @return Whether the status field is set.
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string status = 10;</code>
       * @return The status.
       */
      public java.lang.String getStatus() {
        java.lang.Object ref = status_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          status_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string status = 10;</code>
       * @return The bytes for status.
       */
      public com.google.protobuf.ByteString
          getStatusBytes() {
        java.lang.Object ref = status_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          status_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string status = 10;</code>
       * @param value The status to set.
       * @return This builder for chaining.
       */
      public Builder setStatus(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string status = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000004);
        status_ = getDefaultInstance().getStatus();
        onChanged();
        return this;
      }
      /**
       * <code>string status = 10;</code>
       * @param value The bytes for status to set.
       * @return This builder for chaining.
       */
      public Builder setStatusBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        status_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object taskLocality_ = "";
      /**
       * <code>string task_locality = 11;</code>
       * @return Whether the taskLocality field is set.
       */
      public boolean hasTaskLocality() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>string task_locality = 11;</code>
       * @return The taskLocality.
       */
      public java.lang.String getTaskLocality() {
        java.lang.Object ref = taskLocality_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          taskLocality_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string task_locality = 11;</code>
       * @return The bytes for taskLocality.
       */
      public com.google.protobuf.ByteString
          getTaskLocalityBytes() {
        java.lang.Object ref = taskLocality_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          taskLocality_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string task_locality = 11;</code>
       * @param value The taskLocality to set.
       * @return This builder for chaining.
       */
      public Builder setTaskLocality(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        taskLocality_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string task_locality = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearTaskLocality() {
        bitField0_ = (bitField0_ & ~0x00000008);
        taskLocality_ = getDefaultInstance().getTaskLocality();
        onChanged();
        return this;
      }
      /**
       * <code>string task_locality = 11;</code>
       * @param value The bytes for taskLocality to set.
       * @return This builder for chaining.
       */
      public Builder setTaskLocalityBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000008;
        taskLocality_ = value;
        onChanged();
        return this;
      }

      private boolean speculative_ ;
      /**
       * <code>bool speculative = 12;</code>
       * @return The speculative.
       */
      @java.lang.Override
      public boolean getSpeculative() {
        return speculative_;
      }
      /**
       * <code>bool speculative = 12;</code>
       * @param value The speculative to set.
       * @return This builder for chaining.
       */
      public Builder setSpeculative(boolean value) {
        
        speculative_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool speculative = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearSpeculative() {
        
        speculative_ = false;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> accumulatorUpdates_ =
        java.util.Collections.emptyList();
      private void ensureAccumulatorUpdatesIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          accumulatorUpdates_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo>(accumulatorUpdates_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> accumulatorUpdatesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> getAccumulatorUpdatesList() {
        if (accumulatorUpdatesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(accumulatorUpdates_);
        } else {
          return accumulatorUpdatesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public int getAccumulatorUpdatesCount() {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.size();
        } else {
          return accumulatorUpdatesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.get(index);
        } else {
          return accumulatorUpdatesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder setAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.set(index, value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder setAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.set(index, builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAccumulatorUpdates(org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(index, value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAccumulatorUpdates(
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(index, builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAllAccumulatorUpdates(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> values) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, accumulatorUpdates_);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder clearAccumulatorUpdates() {
        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdates_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder removeAccumulatorUpdates(int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.remove(index);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder getAccumulatorUpdatesBuilder(
          int index) {
        return getAccumulatorUpdatesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
          int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.get(index);  } else {
          return accumulatorUpdatesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
           getAccumulatorUpdatesOrBuilderList() {
        if (accumulatorUpdatesBuilder_ != null) {
          return accumulatorUpdatesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(accumulatorUpdates_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder addAccumulatorUpdatesBuilder() {
        return getAccumulatorUpdatesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder addAccumulatorUpdatesBuilder(
          int index) {
        return getAccumulatorUpdatesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder> 
           getAccumulatorUpdatesBuilderList() {
        return getAccumulatorUpdatesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
          getAccumulatorUpdatesFieldBuilder() {
        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdatesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder>(
                  accumulatorUpdates_,
                  ((bitField0_ & 0x00000010) != 0),
                  getParentForChildren(),
                  isClean());
          accumulatorUpdates_ = null;
        }
        return accumulatorUpdatesBuilder_;
      }

      private java.lang.Object errorMessage_ = "";
      /**
       * <code>string error_message = 14;</code>
       * @return Whether the errorMessage field is set.
       */
      public boolean hasErrorMessage() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>string error_message = 14;</code>
       * @return The errorMessage.
       */
      public java.lang.String getErrorMessage() {
        java.lang.Object ref = errorMessage_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          errorMessage_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string error_message = 14;</code>
       * @return The bytes for errorMessage.
       */
      public com.google.protobuf.ByteString
          getErrorMessageBytes() {
        java.lang.Object ref = errorMessage_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          errorMessage_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string error_message = 14;</code>
       * @param value The errorMessage to set.
       * @return This builder for chaining.
       */
      public Builder setErrorMessage(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        errorMessage_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string error_message = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearErrorMessage() {
        bitField0_ = (bitField0_ & ~0x00000020);
        errorMessage_ = getDefaultInstance().getErrorMessage();
        onChanged();
        return this;
      }
      /**
       * <code>string error_message = 14;</code>
       * @param value The bytes for errorMessage to set.
       * @return This builder for chaining.
       */
      public Builder setErrorMessageBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000020;
        errorMessage_ = value;
        onChanged();
        return this;
      }

      private boolean hasMetrics_ ;
      /**
       * <code>bool has_metrics = 15;</code>
       * @return The hasMetrics.
       */
      @java.lang.Override
      public boolean getHasMetrics() {
        return hasMetrics_;
      }
      /**
       * <code>bool has_metrics = 15;</code>
       * @param value The hasMetrics to set.
       * @return This builder for chaining.
       */
      public Builder setHasMetrics(boolean value) {
        
        hasMetrics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool has_metrics = 15;</code>
       * @return This builder for chaining.
       */
      public Builder clearHasMetrics() {
        
        hasMetrics_ = false;
        onChanged();
        return this;
      }

      private long executorDeserializeTime_ ;
      /**
       * <code>int64 executor_deserialize_time = 16;</code>
       * @return The executorDeserializeTime.
       */
      @java.lang.Override
      public long getExecutorDeserializeTime() {
        return executorDeserializeTime_;
      }
      /**
       * <code>int64 executor_deserialize_time = 16;</code>
       * @param value The executorDeserializeTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeTime(long value) {
        
        executorDeserializeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_deserialize_time = 16;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeTime() {
        
        executorDeserializeTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorDeserializeCpuTime_ ;
      /**
       * <code>int64 executor_deserialize_cpu_time = 17;</code>
       * @return The executorDeserializeCpuTime.
       */
      @java.lang.Override
      public long getExecutorDeserializeCpuTime() {
        return executorDeserializeCpuTime_;
      }
      /**
       * <code>int64 executor_deserialize_cpu_time = 17;</code>
       * @param value The executorDeserializeCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeCpuTime(long value) {
        
        executorDeserializeCpuTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_deserialize_cpu_time = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeCpuTime() {
        
        executorDeserializeCpuTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorRunTime_ ;
      /**
       * <code>int64 executor_run_time = 18;</code>
       * @return The executorRunTime.
       */
      @java.lang.Override
      public long getExecutorRunTime() {
        return executorRunTime_;
      }
      /**
       * <code>int64 executor_run_time = 18;</code>
       * @param value The executorRunTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorRunTime(long value) {
        
        executorRunTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_run_time = 18;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorRunTime() {
        
        executorRunTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorCpuTime_ ;
      /**
       * <code>int64 executor_cpu_time = 19;</code>
       * @return The executorCpuTime.
       */
      @java.lang.Override
      public long getExecutorCpuTime() {
        return executorCpuTime_;
      }
      /**
       * <code>int64 executor_cpu_time = 19;</code>
       * @param value The executorCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorCpuTime(long value) {
        
        executorCpuTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_cpu_time = 19;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorCpuTime() {
        
        executorCpuTime_ = 0L;
        onChanged();
        return this;
      }

      private long resultSize_ ;
      /**
       * <code>int64 result_size = 20;</code>
       * @return The resultSize.
       */
      @java.lang.Override
      public long getResultSize() {
        return resultSize_;
      }
      /**
       * <code>int64 result_size = 20;</code>
       * @param value The resultSize to set.
       * @return This builder for chaining.
       */
      public Builder setResultSize(long value) {
        
        resultSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 result_size = 20;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSize() {
        
        resultSize_ = 0L;
        onChanged();
        return this;
      }

      private long jvmGcTime_ ;
      /**
       * <code>int64 jvm_gc_time = 21;</code>
       * @return The jvmGcTime.
       */
      @java.lang.Override
      public long getJvmGcTime() {
        return jvmGcTime_;
      }
      /**
       * <code>int64 jvm_gc_time = 21;</code>
       * @param value The jvmGcTime to set.
       * @return This builder for chaining.
       */
      public Builder setJvmGcTime(long value) {
        
        jvmGcTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 jvm_gc_time = 21;</code>
       * @return This builder for chaining.
       */
      public Builder clearJvmGcTime() {
        
        jvmGcTime_ = 0L;
        onChanged();
        return this;
      }

      private long resultSerializationTime_ ;
      /**
       * <code>int64 result_serialization_time = 22;</code>
       * @return The resultSerializationTime.
       */
      @java.lang.Override
      public long getResultSerializationTime() {
        return resultSerializationTime_;
      }
      /**
       * <code>int64 result_serialization_time = 22;</code>
       * @param value The resultSerializationTime to set.
       * @return This builder for chaining.
       */
      public Builder setResultSerializationTime(long value) {
        
        resultSerializationTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 result_serialization_time = 22;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSerializationTime() {
        
        resultSerializationTime_ = 0L;
        onChanged();
        return this;
      }

      private long memoryBytesSpilled_ ;
      /**
       * <code>int64 memory_bytes_spilled = 23;</code>
       * @return The memoryBytesSpilled.
       */
      @java.lang.Override
      public long getMemoryBytesSpilled() {
        return memoryBytesSpilled_;
      }
      /**
       * <code>int64 memory_bytes_spilled = 23;</code>
       * @param value The memoryBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryBytesSpilled(long value) {
        
        memoryBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_bytes_spilled = 23;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryBytesSpilled() {
        
        memoryBytesSpilled_ = 0L;
        onChanged();
        return this;
      }

      private long diskBytesSpilled_ ;
      /**
       * <code>int64 disk_bytes_spilled = 24;</code>
       * @return The diskBytesSpilled.
       */
      @java.lang.Override
      public long getDiskBytesSpilled() {
        return diskBytesSpilled_;
      }
      /**
       * <code>int64 disk_bytes_spilled = 24;</code>
       * @param value The diskBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setDiskBytesSpilled(long value) {
        
        diskBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_bytes_spilled = 24;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskBytesSpilled() {
        
        diskBytesSpilled_ = 0L;
        onChanged();
        return this;
      }

      private long peakExecutionMemory_ ;
      /**
       * <code>int64 peak_execution_memory = 25;</code>
       * @return The peakExecutionMemory.
       */
      @java.lang.Override
      public long getPeakExecutionMemory() {
        return peakExecutionMemory_;
      }
      /**
       * <code>int64 peak_execution_memory = 25;</code>
       * @param value The peakExecutionMemory to set.
       * @return This builder for chaining.
       */
      public Builder setPeakExecutionMemory(long value) {
        
        peakExecutionMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 peak_execution_memory = 25;</code>
       * @return This builder for chaining.
       */
      public Builder clearPeakExecutionMemory() {
        
        peakExecutionMemory_ = 0L;
        onChanged();
        return this;
      }

      private long inputBytesRead_ ;
      /**
       * <code>int64 input_bytes_read = 26;</code>
       * @return The inputBytesRead.
       */
      @java.lang.Override
      public long getInputBytesRead() {
        return inputBytesRead_;
      }
      /**
       * <code>int64 input_bytes_read = 26;</code>
       * @param value The inputBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setInputBytesRead(long value) {
        
        inputBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 input_bytes_read = 26;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputBytesRead() {
        
        inputBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long inputRecordsRead_ ;
      /**
       * <code>int64 input_records_read = 27;</code>
       * @return The inputRecordsRead.
       */
      @java.lang.Override
      public long getInputRecordsRead() {
        return inputRecordsRead_;
      }
      /**
       * <code>int64 input_records_read = 27;</code>
       * @param value The inputRecordsRead to set.
       * @return This builder for chaining.
       */
      public Builder setInputRecordsRead(long value) {
        
        inputRecordsRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 input_records_read = 27;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputRecordsRead() {
        
        inputRecordsRead_ = 0L;
        onChanged();
        return this;
      }

      private long outputBytesWritten_ ;
      /**
       * <code>int64 output_bytes_written = 28;</code>
       * @return The outputBytesWritten.
       */
      @java.lang.Override
      public long getOutputBytesWritten() {
        return outputBytesWritten_;
      }
      /**
       * <code>int64 output_bytes_written = 28;</code>
       * @param value The outputBytesWritten to set.
       * @return This builder for chaining.
       */
      public Builder setOutputBytesWritten(long value) {
        
        outputBytesWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 output_bytes_written = 28;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputBytesWritten() {
        
        outputBytesWritten_ = 0L;
        onChanged();
        return this;
      }

      private long outputRecordsWritten_ ;
      /**
       * <code>int64 output_records_written = 29;</code>
       * @return The outputRecordsWritten.
       */
      @java.lang.Override
      public long getOutputRecordsWritten() {
        return outputRecordsWritten_;
      }
      /**
       * <code>int64 output_records_written = 29;</code>
       * @param value The outputRecordsWritten to set.
       * @return This builder for chaining.
       */
      public Builder setOutputRecordsWritten(long value) {
        
        outputRecordsWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 output_records_written = 29;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputRecordsWritten() {
        
        outputRecordsWritten_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRemoteBlocksFetched_ ;
      /**
       * <code>int64 shuffle_remote_blocks_fetched = 30;</code>
       * @return The shuffleRemoteBlocksFetched.
       */
      @java.lang.Override
      public long getShuffleRemoteBlocksFetched() {
        return shuffleRemoteBlocksFetched_;
      }
      /**
       * <code>int64 shuffle_remote_blocks_fetched = 30;</code>
       * @param value The shuffleRemoteBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBlocksFetched(long value) {
        
        shuffleRemoteBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_remote_blocks_fetched = 30;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBlocksFetched() {
        
        shuffleRemoteBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleLocalBlocksFetched_ ;
      /**
       * <code>int64 shuffle_local_blocks_fetched = 31;</code>
       * @return The shuffleLocalBlocksFetched.
       */
      @java.lang.Override
      public long getShuffleLocalBlocksFetched() {
        return shuffleLocalBlocksFetched_;
      }
      /**
       * <code>int64 shuffle_local_blocks_fetched = 31;</code>
       * @param value The shuffleLocalBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleLocalBlocksFetched(long value) {
        
        shuffleLocalBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_local_blocks_fetched = 31;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleLocalBlocksFetched() {
        
        shuffleLocalBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleFetchWaitTime_ ;
      /**
       * <code>int64 shuffle_fetch_wait_time = 32;</code>
       * @return The shuffleFetchWaitTime.
       */
      @java.lang.Override
      public long getShuffleFetchWaitTime() {
        return shuffleFetchWaitTime_;
      }
      /**
       * <code>int64 shuffle_fetch_wait_time = 32;</code>
       * @param value The shuffleFetchWaitTime to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleFetchWaitTime(long value) {
        
        shuffleFetchWaitTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_fetch_wait_time = 32;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleFetchWaitTime() {
        
        shuffleFetchWaitTime_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRemoteBytesRead_ ;
      /**
       * <code>int64 shuffle_remote_bytes_read = 33;</code>
       * @return The shuffleRemoteBytesRead.
       */
      @java.lang.Override
      public long getShuffleRemoteBytesRead() {
        return shuffleRemoteBytesRead_;
      }
      /**
       * <code>int64 shuffle_remote_bytes_read = 33;</code>
       * @param value The shuffleRemoteBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBytesRead(long value) {
        
        shuffleRemoteBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_remote_bytes_read = 33;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBytesRead() {
        
        shuffleRemoteBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRemoteBytesReadToDisk_ ;
      /**
       * <code>int64 shuffle_remote_bytes_read_to_disk = 34;</code>
       * @return The shuffleRemoteBytesReadToDisk.
       */
      @java.lang.Override
      public long getShuffleRemoteBytesReadToDisk() {
        return shuffleRemoteBytesReadToDisk_;
      }
      /**
       * <code>int64 shuffle_remote_bytes_read_to_disk = 34;</code>
       * @param value The shuffleRemoteBytesReadToDisk to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBytesReadToDisk(long value) {
        
        shuffleRemoteBytesReadToDisk_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_remote_bytes_read_to_disk = 34;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBytesReadToDisk() {
        
        shuffleRemoteBytesReadToDisk_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleLocalBytesRead_ ;
      /**
       * <code>int64 shuffle_local_bytes_read = 35;</code>
       * @return The shuffleLocalBytesRead.
       */
      @java.lang.Override
      public long getShuffleLocalBytesRead() {
        return shuffleLocalBytesRead_;
      }
      /**
       * <code>int64 shuffle_local_bytes_read = 35;</code>
       * @param value The shuffleLocalBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleLocalBytesRead(long value) {
        
        shuffleLocalBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_local_bytes_read = 35;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleLocalBytesRead() {
        
        shuffleLocalBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRecordsRead_ ;
      /**
       * <code>int64 shuffle_records_read = 36;</code>
       * @return The shuffleRecordsRead.
       */
      @java.lang.Override
      public long getShuffleRecordsRead() {
        return shuffleRecordsRead_;
      }
      /**
       * <code>int64 shuffle_records_read = 36;</code>
       * @param value The shuffleRecordsRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRecordsRead(long value) {
        
        shuffleRecordsRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_records_read = 36;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRecordsRead() {
        
        shuffleRecordsRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleBytesWritten_ ;
      /**
       * <code>int64 shuffle_bytes_written = 37;</code>
       * @return The shuffleBytesWritten.
       */
      @java.lang.Override
      public long getShuffleBytesWritten() {
        return shuffleBytesWritten_;
      }
      /**
       * <code>int64 shuffle_bytes_written = 37;</code>
       * @param value The shuffleBytesWritten to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleBytesWritten(long value) {
        
        shuffleBytesWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_bytes_written = 37;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleBytesWritten() {
        
        shuffleBytesWritten_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleWriteTime_ ;
      /**
       * <code>int64 shuffle_write_time = 38;</code>
       * @return The shuffleWriteTime.
       */
      @java.lang.Override
      public long getShuffleWriteTime() {
        return shuffleWriteTime_;
      }
      /**
       * <code>int64 shuffle_write_time = 38;</code>
       * @param value The shuffleWriteTime to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteTime(long value) {
        
        shuffleWriteTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_write_time = 38;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteTime() {
        
        shuffleWriteTime_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRecordsWritten_ ;
      /**
       * <code>int64 shuffle_records_written = 39;</code>
       * @return The shuffleRecordsWritten.
       */
      @java.lang.Override
      public long getShuffleRecordsWritten() {
        return shuffleRecordsWritten_;
      }
      /**
       * <code>int64 shuffle_records_written = 39;</code>
       * @param value The shuffleRecordsWritten to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRecordsWritten(long value) {
        
        shuffleRecordsWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_records_written = 39;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRecordsWritten() {
        
        shuffleRecordsWritten_ = 0L;
        onChanged();
        return this;
      }

      private long stageId_ ;
      /**
       * <code>int64 stage_id = 40;</code>
       * @return The stageId.
       */
      @java.lang.Override
      public long getStageId() {
        return stageId_;
      }
      /**
       * <code>int64 stage_id = 40;</code>
       * @param value The stageId to set.
       * @return This builder for chaining.
       */
      public Builder setStageId(long value) {
        
        stageId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 stage_id = 40;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageId() {
        
        stageId_ = 0L;
        onChanged();
        return this;
      }

      private int stageAttemptId_ ;
      /**
       * <code>int32 stage_attempt_id = 41;</code>
       * @return The stageAttemptId.
       */
      @java.lang.Override
      public int getStageAttemptId() {
        return stageAttemptId_;
      }
      /**
       * <code>int32 stage_attempt_id = 41;</code>
       * @param value The stageAttemptId to set.
       * @return This builder for chaining.
       */
      public Builder setStageAttemptId(int value) {
        
        stageAttemptId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 stage_attempt_id = 41;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageAttemptId() {
        
        stageAttemptId_ = 0;
        onChanged();
        return this;
      }

      private long shuffleCorruptMergedBlockChunks_ ;
      /**
       * <code>int64 shuffle_corrupt_merged_block_chunks = 42;</code>
       * @return The shuffleCorruptMergedBlockChunks.
       */
      @java.lang.Override
      public long getShuffleCorruptMergedBlockChunks() {
        return shuffleCorruptMergedBlockChunks_;
      }
      /**
       * <code>int64 shuffle_corrupt_merged_block_chunks = 42;</code>
       * @param value The shuffleCorruptMergedBlockChunks to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleCorruptMergedBlockChunks(long value) {
        
        shuffleCorruptMergedBlockChunks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_corrupt_merged_block_chunks = 42;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleCorruptMergedBlockChunks() {
        
        shuffleCorruptMergedBlockChunks_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedFetchFallbackCount_ ;
      /**
       * <code>int64 shuffle_merged_fetch_fallback_count = 43;</code>
       * @return The shuffleMergedFetchFallbackCount.
       */
      @java.lang.Override
      public long getShuffleMergedFetchFallbackCount() {
        return shuffleMergedFetchFallbackCount_;
      }
      /**
       * <code>int64 shuffle_merged_fetch_fallback_count = 43;</code>
       * @param value The shuffleMergedFetchFallbackCount to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedFetchFallbackCount(long value) {
        
        shuffleMergedFetchFallbackCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_fetch_fallback_count = 43;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedFetchFallbackCount() {
        
        shuffleMergedFetchFallbackCount_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedRemoteBlocksFetched_ ;
      /**
       * <code>int64 shuffle_merged_remote_blocks_fetched = 44;</code>
       * @return The shuffleMergedRemoteBlocksFetched.
       */
      @java.lang.Override
      public long getShuffleMergedRemoteBlocksFetched() {
        return shuffleMergedRemoteBlocksFetched_;
      }
      /**
       * <code>int64 shuffle_merged_remote_blocks_fetched = 44;</code>
       * @param value The shuffleMergedRemoteBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteBlocksFetched(long value) {
        
        shuffleMergedRemoteBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_remote_blocks_fetched = 44;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteBlocksFetched() {
        
        shuffleMergedRemoteBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedLocalBlocksFetched_ ;
      /**
       * <code>int64 shuffle_merged_local_blocks_fetched = 45;</code>
       * @return The shuffleMergedLocalBlocksFetched.
       */
      @java.lang.Override
      public long getShuffleMergedLocalBlocksFetched() {
        return shuffleMergedLocalBlocksFetched_;
      }
      /**
       * <code>int64 shuffle_merged_local_blocks_fetched = 45;</code>
       * @param value The shuffleMergedLocalBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalBlocksFetched(long value) {
        
        shuffleMergedLocalBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_local_blocks_fetched = 45;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalBlocksFetched() {
        
        shuffleMergedLocalBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedRemoteChunksFetched_ ;
      /**
       * <code>int64 shuffle_merged_remote_chunks_fetched = 46;</code>
       * @return The shuffleMergedRemoteChunksFetched.
       */
      @java.lang.Override
      public long getShuffleMergedRemoteChunksFetched() {
        return shuffleMergedRemoteChunksFetched_;
      }
      /**
       * <code>int64 shuffle_merged_remote_chunks_fetched = 46;</code>
       * @param value The shuffleMergedRemoteChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteChunksFetched(long value) {
        
        shuffleMergedRemoteChunksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_remote_chunks_fetched = 46;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteChunksFetched() {
        
        shuffleMergedRemoteChunksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedLocalChunksFetched_ ;
      /**
       * <code>int64 shuffle_merged_local_chunks_fetched = 47;</code>
       * @return The shuffleMergedLocalChunksFetched.
       */
      @java.lang.Override
      public long getShuffleMergedLocalChunksFetched() {
        return shuffleMergedLocalChunksFetched_;
      }
      /**
       * <code>int64 shuffle_merged_local_chunks_fetched = 47;</code>
       * @param value The shuffleMergedLocalChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalChunksFetched(long value) {
        
        shuffleMergedLocalChunksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_local_chunks_fetched = 47;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalChunksFetched() {
        
        shuffleMergedLocalChunksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedRemoteBytesRead_ ;
      /**
       * <code>int64 shuffle_merged_remote_bytes_read = 48;</code>
       * @return The shuffleMergedRemoteBytesRead.
       */
      @java.lang.Override
      public long getShuffleMergedRemoteBytesRead() {
        return shuffleMergedRemoteBytesRead_;
      }
      /**
       * <code>int64 shuffle_merged_remote_bytes_read = 48;</code>
       * @param value The shuffleMergedRemoteBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteBytesRead(long value) {
        
        shuffleMergedRemoteBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_remote_bytes_read = 48;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteBytesRead() {
        
        shuffleMergedRemoteBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedLocalBytesRead_ ;
      /**
       * <code>int64 shuffle_merged_local_bytes_read = 49;</code>
       * @return The shuffleMergedLocalBytesRead.
       */
      @java.lang.Override
      public long getShuffleMergedLocalBytesRead() {
        return shuffleMergedLocalBytesRead_;
      }
      /**
       * <code>int64 shuffle_merged_local_bytes_read = 49;</code>
       * @param value The shuffleMergedLocalBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalBytesRead(long value) {
        
        shuffleMergedLocalBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_local_bytes_read = 49;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalBytesRead() {
        
        shuffleMergedLocalBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRemoteReqsDuration_ ;
      /**
       * <code>int64 shuffle_remote_reqs_duration = 50;</code>
       * @return The shuffleRemoteReqsDuration.
       */
      @java.lang.Override
      public long getShuffleRemoteReqsDuration() {
        return shuffleRemoteReqsDuration_;
      }
      /**
       * <code>int64 shuffle_remote_reqs_duration = 50;</code>
       * @param value The shuffleRemoteReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteReqsDuration(long value) {
        
        shuffleRemoteReqsDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_remote_reqs_duration = 50;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteReqsDuration() {
        
        shuffleRemoteReqsDuration_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedRemoteReqDuration_ ;
      /**
       * <code>int64 shuffle_merged_remote_req_duration = 51;</code>
       * @return The shuffleMergedRemoteReqDuration.
       */
      @java.lang.Override
      public long getShuffleMergedRemoteReqDuration() {
        return shuffleMergedRemoteReqDuration_;
      }
      /**
       * <code>int64 shuffle_merged_remote_req_duration = 51;</code>
       * @param value The shuffleMergedRemoteReqDuration to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteReqDuration(long value) {
        
        shuffleMergedRemoteReqDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_remote_req_duration = 51;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteReqDuration() {
        
        shuffleMergedRemoteReqDuration_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.TaskDataWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.TaskDataWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<TaskDataWrapper>
        PARSER = new com.google.protobuf.AbstractParser<TaskDataWrapper>() {
      @java.lang.Override
      public TaskDataWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TaskDataWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<TaskDataWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<TaskDataWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskDataWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExecutorMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ExecutorMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */
    int getMetricsCount();
    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */
    boolean containsMetrics(
        java.lang.String key);
    /**
     * Use {@link #getMetricsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.Long>
    getMetrics();
    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */
    java.util.Map<java.lang.String, java.lang.Long>
    getMetricsMap();
    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */

    long getMetricsOrDefault(
        java.lang.String key,
        long defaultValue);
    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */

    long getMetricsOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorMetrics}
   */
  public static final class ExecutorMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ExecutorMetrics)
      ExecutorMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ExecutorMetrics.newBuilder() to construct.
    private ExecutorMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ExecutorMetrics() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ExecutorMetrics();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ExecutorMetrics(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                metrics_ = com.google.protobuf.MapField.newMapField(
                    MetricsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000001;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.Long>
              metrics__ = input.readMessage(
                  MetricsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              metrics_.getMutableMap().put(
                  metrics__.getKey(), metrics__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 1:
          return internalGetMetrics();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder.class);
    }

    public static final int METRICS_FIELD_NUMBER = 1;
    private static final class MetricsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.Long> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.Long>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_MetricsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.INT64,
                  0L);
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.Long> metrics_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
    internalGetMetrics() {
      if (metrics_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            MetricsDefaultEntryHolder.defaultEntry);
      }
      return metrics_;
    }

    public int getMetricsCount() {
      return internalGetMetrics().getMap().size();
    }
    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */

    @java.lang.Override
    public boolean containsMetrics(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetMetrics().getMap().containsKey(key);
    }
    /**
     * Use {@link #getMetricsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.Long> getMetrics() {
      return getMetricsMap();
    }
    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.Long> getMetricsMap() {
      return internalGetMetrics().getMap();
    }
    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */
    @java.lang.Override

    public long getMetricsOrDefault(
        java.lang.String key,
        long defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Long> map =
          internalGetMetrics().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, int64&gt; metrics = 1;</code>
     */
    @java.lang.Override

    public long getMetricsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Long> map =
          internalGetMetrics().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetMetrics(),
          MetricsDefaultEntryHolder.defaultEntry,
          1);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (java.util.Map.Entry<java.lang.String, java.lang.Long> entry
           : internalGetMetrics().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.Long>
        metrics__ = MetricsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(1, metrics__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics other = (org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics) obj;

      if (!internalGetMetrics().equals(
          other.internalGetMetrics())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (!internalGetMetrics().getMap().isEmpty()) {
        hash = (37 * hash) + METRICS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ExecutorMetrics)
        org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 1:
            return internalGetMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 1:
            return internalGetMutableMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        internalGetMutableMetrics().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics build() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics result = new org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics(this);
        int from_bitField0_ = bitField0_;
        result.metrics_ = internalGetMetrics();
        result.metrics_.makeImmutable();
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance()) return this;
        internalGetMutableMetrics().mergeFrom(
            other.internalGetMetrics());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.Long> metrics_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
      internalGetMetrics() {
        if (metrics_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              MetricsDefaultEntryHolder.defaultEntry);
        }
        return metrics_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
      internalGetMutableMetrics() {
        onChanged();;
        if (metrics_ == null) {
          metrics_ = com.google.protobuf.MapField.newMapField(
              MetricsDefaultEntryHolder.defaultEntry);
        }
        if (!metrics_.isMutable()) {
          metrics_ = metrics_.copy();
        }
        return metrics_;
      }

      public int getMetricsCount() {
        return internalGetMetrics().getMap().size();
      }
      /**
       * <code>map&lt;string, int64&gt; metrics = 1;</code>
       */

      @java.lang.Override
      public boolean containsMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetMetrics().getMap().containsKey(key);
      }
      /**
       * Use {@link #getMetricsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Long> getMetrics() {
        return getMetricsMap();
      }
      /**
       * <code>map&lt;string, int64&gt; metrics = 1;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.Long> getMetricsMap() {
        return internalGetMetrics().getMap();
      }
      /**
       * <code>map&lt;string, int64&gt; metrics = 1;</code>
       */
      @java.lang.Override

      public long getMetricsOrDefault(
          java.lang.String key,
          long defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Long> map =
            internalGetMetrics().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, int64&gt; metrics = 1;</code>
       */
      @java.lang.Override

      public long getMetricsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Long> map =
            internalGetMetrics().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearMetrics() {
        internalGetMutableMetrics().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, int64&gt; metrics = 1;</code>
       */

      public Builder removeMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetrics().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Long>
      getMutableMetrics() {
        return internalGetMutableMetrics().getMutableMap();
      }
      /**
       * <code>map&lt;string, int64&gt; metrics = 1;</code>
       */
      public Builder putMetrics(
          java.lang.String key,
          long value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        
        internalGetMutableMetrics().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, int64&gt; metrics = 1;</code>
       */

      public Builder putAllMetrics(
          java.util.Map<java.lang.String, java.lang.Long> values) {
        internalGetMutableMetrics().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ExecutorMetrics)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ExecutorMetrics)
    private static final org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ExecutorMetrics>
        PARSER = new com.google.protobuf.AbstractParser<ExecutorMetrics>() {
      @java.lang.Override
      public ExecutorMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ExecutorMetrics(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ExecutorMetrics> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ExecutorMetrics> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExecutorStageSummaryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ExecutorStageSummary)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 task_time = 1;</code>
     * @return The taskTime.
     */
    long getTaskTime();

    /**
     * <code>int32 failed_tasks = 2;</code>
     * @return The failedTasks.
     */
    int getFailedTasks();

    /**
     * <code>int32 succeeded_tasks = 3;</code>
     * @return The succeededTasks.
     */
    int getSucceededTasks();

    /**
     * <code>int32 killed_tasks = 4;</code>
     * @return The killedTasks.
     */
    int getKilledTasks();

    /**
     * <code>int64 input_bytes = 5;</code>
     * @return The inputBytes.
     */
    long getInputBytes();

    /**
     * <code>int64 input_records = 6;</code>
     * @return The inputRecords.
     */
    long getInputRecords();

    /**
     * <code>int64 output_bytes = 7;</code>
     * @return The outputBytes.
     */
    long getOutputBytes();

    /**
     * <code>int64 output_records = 8;</code>
     * @return The outputRecords.
     */
    long getOutputRecords();

    /**
     * <code>int64 shuffle_read = 9;</code>
     * @return The shuffleRead.
     */
    long getShuffleRead();

    /**
     * <code>int64 shuffle_read_records = 10;</code>
     * @return The shuffleReadRecords.
     */
    long getShuffleReadRecords();

    /**
     * <code>int64 shuffle_write = 11;</code>
     * @return The shuffleWrite.
     */
    long getShuffleWrite();

    /**
     * <code>int64 shuffle_write_records = 12;</code>
     * @return The shuffleWriteRecords.
     */
    long getShuffleWriteRecords();

    /**
     * <code>int64 memory_bytes_spilled = 13;</code>
     * @return The memoryBytesSpilled.
     */
    long getMemoryBytesSpilled();

    /**
     * <code>int64 disk_bytes_spilled = 14;</code>
     * @return The diskBytesSpilled.
     */
    long getDiskBytesSpilled();

    /**
     * <code>bool is_blacklisted_for_stage = 15;</code>
     * @return The isBlacklistedForStage.
     */
    boolean getIsBlacklistedForStage();

    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
     * @return Whether the peakMemoryMetrics field is set.
     */
    boolean hasPeakMemoryMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
     * @return The peakMemoryMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakMemoryMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakMemoryMetricsOrBuilder();

    /**
     * <code>bool is_excluded_for_stage = 17;</code>
     * @return The isExcludedForStage.
     */
    boolean getIsExcludedForStage();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorStageSummary}
   */
  public static final class ExecutorStageSummary extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ExecutorStageSummary)
      ExecutorStageSummaryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ExecutorStageSummary.newBuilder() to construct.
    private ExecutorStageSummary(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ExecutorStageSummary() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ExecutorStageSummary();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ExecutorStageSummary(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              taskTime_ = input.readInt64();
              break;
            }
            case 16: {

              failedTasks_ = input.readInt32();
              break;
            }
            case 24: {

              succeededTasks_ = input.readInt32();
              break;
            }
            case 32: {

              killedTasks_ = input.readInt32();
              break;
            }
            case 40: {

              inputBytes_ = input.readInt64();
              break;
            }
            case 48: {

              inputRecords_ = input.readInt64();
              break;
            }
            case 56: {

              outputBytes_ = input.readInt64();
              break;
            }
            case 64: {

              outputRecords_ = input.readInt64();
              break;
            }
            case 72: {

              shuffleRead_ = input.readInt64();
              break;
            }
            case 80: {

              shuffleReadRecords_ = input.readInt64();
              break;
            }
            case 88: {

              shuffleWrite_ = input.readInt64();
              break;
            }
            case 96: {

              shuffleWriteRecords_ = input.readInt64();
              break;
            }
            case 104: {

              memoryBytesSpilled_ = input.readInt64();
              break;
            }
            case 112: {

              diskBytesSpilled_ = input.readInt64();
              break;
            }
            case 120: {

              isBlacklistedForStage_ = input.readBool();
              break;
            }
            case 130: {
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = peakMemoryMetrics_.toBuilder();
              }
              peakMemoryMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(peakMemoryMetrics_);
                peakMemoryMetrics_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 136: {

              isExcludedForStage_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.Builder.class);
    }

    private int bitField0_;
    public static final int TASK_TIME_FIELD_NUMBER = 1;
    private long taskTime_;
    /**
     * <code>int64 task_time = 1;</code>
     * @return The taskTime.
     */
    @java.lang.Override
    public long getTaskTime() {
      return taskTime_;
    }

    public static final int FAILED_TASKS_FIELD_NUMBER = 2;
    private int failedTasks_;
    /**
     * <code>int32 failed_tasks = 2;</code>
     * @return The failedTasks.
     */
    @java.lang.Override
    public int getFailedTasks() {
      return failedTasks_;
    }

    public static final int SUCCEEDED_TASKS_FIELD_NUMBER = 3;
    private int succeededTasks_;
    /**
     * <code>int32 succeeded_tasks = 3;</code>
     * @return The succeededTasks.
     */
    @java.lang.Override
    public int getSucceededTasks() {
      return succeededTasks_;
    }

    public static final int KILLED_TASKS_FIELD_NUMBER = 4;
    private int killedTasks_;
    /**
     * <code>int32 killed_tasks = 4;</code>
     * @return The killedTasks.
     */
    @java.lang.Override
    public int getKilledTasks() {
      return killedTasks_;
    }

    public static final int INPUT_BYTES_FIELD_NUMBER = 5;
    private long inputBytes_;
    /**
     * <code>int64 input_bytes = 5;</code>
     * @return The inputBytes.
     */
    @java.lang.Override
    public long getInputBytes() {
      return inputBytes_;
    }

    public static final int INPUT_RECORDS_FIELD_NUMBER = 6;
    private long inputRecords_;
    /**
     * <code>int64 input_records = 6;</code>
     * @return The inputRecords.
     */
    @java.lang.Override
    public long getInputRecords() {
      return inputRecords_;
    }

    public static final int OUTPUT_BYTES_FIELD_NUMBER = 7;
    private long outputBytes_;
    /**
     * <code>int64 output_bytes = 7;</code>
     * @return The outputBytes.
     */
    @java.lang.Override
    public long getOutputBytes() {
      return outputBytes_;
    }

    public static final int OUTPUT_RECORDS_FIELD_NUMBER = 8;
    private long outputRecords_;
    /**
     * <code>int64 output_records = 8;</code>
     * @return The outputRecords.
     */
    @java.lang.Override
    public long getOutputRecords() {
      return outputRecords_;
    }

    public static final int SHUFFLE_READ_FIELD_NUMBER = 9;
    private long shuffleRead_;
    /**
     * <code>int64 shuffle_read = 9;</code>
     * @return The shuffleRead.
     */
    @java.lang.Override
    public long getShuffleRead() {
      return shuffleRead_;
    }

    public static final int SHUFFLE_READ_RECORDS_FIELD_NUMBER = 10;
    private long shuffleReadRecords_;
    /**
     * <code>int64 shuffle_read_records = 10;</code>
     * @return The shuffleReadRecords.
     */
    @java.lang.Override
    public long getShuffleReadRecords() {
      return shuffleReadRecords_;
    }

    public static final int SHUFFLE_WRITE_FIELD_NUMBER = 11;
    private long shuffleWrite_;
    /**
     * <code>int64 shuffle_write = 11;</code>
     * @return The shuffleWrite.
     */
    @java.lang.Override
    public long getShuffleWrite() {
      return shuffleWrite_;
    }

    public static final int SHUFFLE_WRITE_RECORDS_FIELD_NUMBER = 12;
    private long shuffleWriteRecords_;
    /**
     * <code>int64 shuffle_write_records = 12;</code>
     * @return The shuffleWriteRecords.
     */
    @java.lang.Override
    public long getShuffleWriteRecords() {
      return shuffleWriteRecords_;
    }

    public static final int MEMORY_BYTES_SPILLED_FIELD_NUMBER = 13;
    private long memoryBytesSpilled_;
    /**
     * <code>int64 memory_bytes_spilled = 13;</code>
     * @return The memoryBytesSpilled.
     */
    @java.lang.Override
    public long getMemoryBytesSpilled() {
      return memoryBytesSpilled_;
    }

    public static final int DISK_BYTES_SPILLED_FIELD_NUMBER = 14;
    private long diskBytesSpilled_;
    /**
     * <code>int64 disk_bytes_spilled = 14;</code>
     * @return The diskBytesSpilled.
     */
    @java.lang.Override
    public long getDiskBytesSpilled() {
      return diskBytesSpilled_;
    }

    public static final int IS_BLACKLISTED_FOR_STAGE_FIELD_NUMBER = 15;
    private boolean isBlacklistedForStage_;
    /**
     * <code>bool is_blacklisted_for_stage = 15;</code>
     * @return The isBlacklistedForStage.
     */
    @java.lang.Override
    public boolean getIsBlacklistedForStage() {
      return isBlacklistedForStage_;
    }

    public static final int PEAK_MEMORY_METRICS_FIELD_NUMBER = 16;
    private org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics peakMemoryMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
     * @return Whether the peakMemoryMetrics field is set.
     */
    @java.lang.Override
    public boolean hasPeakMemoryMetrics() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
     * @return The peakMemoryMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakMemoryMetrics() {
      return peakMemoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakMemoryMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakMemoryMetricsOrBuilder() {
      return peakMemoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakMemoryMetrics_;
    }

    public static final int IS_EXCLUDED_FOR_STAGE_FIELD_NUMBER = 17;
    private boolean isExcludedForStage_;
    /**
     * <code>bool is_excluded_for_stage = 17;</code>
     * @return The isExcludedForStage.
     */
    @java.lang.Override
    public boolean getIsExcludedForStage() {
      return isExcludedForStage_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (taskTime_ != 0L) {
        output.writeInt64(1, taskTime_);
      }
      if (failedTasks_ != 0) {
        output.writeInt32(2, failedTasks_);
      }
      if (succeededTasks_ != 0) {
        output.writeInt32(3, succeededTasks_);
      }
      if (killedTasks_ != 0) {
        output.writeInt32(4, killedTasks_);
      }
      if (inputBytes_ != 0L) {
        output.writeInt64(5, inputBytes_);
      }
      if (inputRecords_ != 0L) {
        output.writeInt64(6, inputRecords_);
      }
      if (outputBytes_ != 0L) {
        output.writeInt64(7, outputBytes_);
      }
      if (outputRecords_ != 0L) {
        output.writeInt64(8, outputRecords_);
      }
      if (shuffleRead_ != 0L) {
        output.writeInt64(9, shuffleRead_);
      }
      if (shuffleReadRecords_ != 0L) {
        output.writeInt64(10, shuffleReadRecords_);
      }
      if (shuffleWrite_ != 0L) {
        output.writeInt64(11, shuffleWrite_);
      }
      if (shuffleWriteRecords_ != 0L) {
        output.writeInt64(12, shuffleWriteRecords_);
      }
      if (memoryBytesSpilled_ != 0L) {
        output.writeInt64(13, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0L) {
        output.writeInt64(14, diskBytesSpilled_);
      }
      if (isBlacklistedForStage_ != false) {
        output.writeBool(15, isBlacklistedForStage_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(16, getPeakMemoryMetrics());
      }
      if (isExcludedForStage_ != false) {
        output.writeBool(17, isExcludedForStage_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (taskTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, taskTime_);
      }
      if (failedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, failedTasks_);
      }
      if (succeededTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, succeededTasks_);
      }
      if (killedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, killedTasks_);
      }
      if (inputBytes_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, inputBytes_);
      }
      if (inputRecords_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, inputRecords_);
      }
      if (outputBytes_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, outputBytes_);
      }
      if (outputRecords_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, outputRecords_);
      }
      if (shuffleRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(9, shuffleRead_);
      }
      if (shuffleReadRecords_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(10, shuffleReadRecords_);
      }
      if (shuffleWrite_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(11, shuffleWrite_);
      }
      if (shuffleWriteRecords_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(12, shuffleWriteRecords_);
      }
      if (memoryBytesSpilled_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(13, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(14, diskBytesSpilled_);
      }
      if (isBlacklistedForStage_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(15, isBlacklistedForStage_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(16, getPeakMemoryMetrics());
      }
      if (isExcludedForStage_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(17, isExcludedForStage_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary other = (org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary) obj;

      if (getTaskTime()
          != other.getTaskTime()) return false;
      if (getFailedTasks()
          != other.getFailedTasks()) return false;
      if (getSucceededTasks()
          != other.getSucceededTasks()) return false;
      if (getKilledTasks()
          != other.getKilledTasks()) return false;
      if (getInputBytes()
          != other.getInputBytes()) return false;
      if (getInputRecords()
          != other.getInputRecords()) return false;
      if (getOutputBytes()
          != other.getOutputBytes()) return false;
      if (getOutputRecords()
          != other.getOutputRecords()) return false;
      if (getShuffleRead()
          != other.getShuffleRead()) return false;
      if (getShuffleReadRecords()
          != other.getShuffleReadRecords()) return false;
      if (getShuffleWrite()
          != other.getShuffleWrite()) return false;
      if (getShuffleWriteRecords()
          != other.getShuffleWriteRecords()) return false;
      if (getMemoryBytesSpilled()
          != other.getMemoryBytesSpilled()) return false;
      if (getDiskBytesSpilled()
          != other.getDiskBytesSpilled()) return false;
      if (getIsBlacklistedForStage()
          != other.getIsBlacklistedForStage()) return false;
      if (hasPeakMemoryMetrics() != other.hasPeakMemoryMetrics()) return false;
      if (hasPeakMemoryMetrics()) {
        if (!getPeakMemoryMetrics()
            .equals(other.getPeakMemoryMetrics())) return false;
      }
      if (getIsExcludedForStage()
          != other.getIsExcludedForStage()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + TASK_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTaskTime());
      hash = (37 * hash) + FAILED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getFailedTasks();
      hash = (37 * hash) + SUCCEEDED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getSucceededTasks();
      hash = (37 * hash) + KILLED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getKilledTasks();
      hash = (37 * hash) + INPUT_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getInputBytes());
      hash = (37 * hash) + INPUT_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getInputRecords());
      hash = (37 * hash) + OUTPUT_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getOutputBytes());
      hash = (37 * hash) + OUTPUT_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getOutputRecords());
      hash = (37 * hash) + SHUFFLE_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRead());
      hash = (37 * hash) + SHUFFLE_READ_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleReadRecords());
      hash = (37 * hash) + SHUFFLE_WRITE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleWrite());
      hash = (37 * hash) + SHUFFLE_WRITE_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleWriteRecords());
      hash = (37 * hash) + MEMORY_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryBytesSpilled());
      hash = (37 * hash) + DISK_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskBytesSpilled());
      hash = (37 * hash) + IS_BLACKLISTED_FOR_STAGE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsBlacklistedForStage());
      if (hasPeakMemoryMetrics()) {
        hash = (37 * hash) + PEAK_MEMORY_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getPeakMemoryMetrics().hashCode();
      }
      hash = (37 * hash) + IS_EXCLUDED_FOR_STAGE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsExcludedForStage());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorStageSummary}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ExecutorStageSummary)
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getPeakMemoryMetricsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        taskTime_ = 0L;

        failedTasks_ = 0;

        succeededTasks_ = 0;

        killedTasks_ = 0;

        inputBytes_ = 0L;

        inputRecords_ = 0L;

        outputBytes_ = 0L;

        outputRecords_ = 0L;

        shuffleRead_ = 0L;

        shuffleReadRecords_ = 0L;

        shuffleWrite_ = 0L;

        shuffleWriteRecords_ = 0L;

        memoryBytesSpilled_ = 0L;

        diskBytesSpilled_ = 0L;

        isBlacklistedForStage_ = false;

        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = null;
        } else {
          peakMemoryMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        isExcludedForStage_ = false;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary build() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary result = new org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.taskTime_ = taskTime_;
        result.failedTasks_ = failedTasks_;
        result.succeededTasks_ = succeededTasks_;
        result.killedTasks_ = killedTasks_;
        result.inputBytes_ = inputBytes_;
        result.inputRecords_ = inputRecords_;
        result.outputBytes_ = outputBytes_;
        result.outputRecords_ = outputRecords_;
        result.shuffleRead_ = shuffleRead_;
        result.shuffleReadRecords_ = shuffleReadRecords_;
        result.shuffleWrite_ = shuffleWrite_;
        result.shuffleWriteRecords_ = shuffleWriteRecords_;
        result.memoryBytesSpilled_ = memoryBytesSpilled_;
        result.diskBytesSpilled_ = diskBytesSpilled_;
        result.isBlacklistedForStage_ = isBlacklistedForStage_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (peakMemoryMetricsBuilder_ == null) {
            result.peakMemoryMetrics_ = peakMemoryMetrics_;
          } else {
            result.peakMemoryMetrics_ = peakMemoryMetricsBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.isExcludedForStage_ = isExcludedForStage_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.getDefaultInstance()) return this;
        if (other.getTaskTime() != 0L) {
          setTaskTime(other.getTaskTime());
        }
        if (other.getFailedTasks() != 0) {
          setFailedTasks(other.getFailedTasks());
        }
        if (other.getSucceededTasks() != 0) {
          setSucceededTasks(other.getSucceededTasks());
        }
        if (other.getKilledTasks() != 0) {
          setKilledTasks(other.getKilledTasks());
        }
        if (other.getInputBytes() != 0L) {
          setInputBytes(other.getInputBytes());
        }
        if (other.getInputRecords() != 0L) {
          setInputRecords(other.getInputRecords());
        }
        if (other.getOutputBytes() != 0L) {
          setOutputBytes(other.getOutputBytes());
        }
        if (other.getOutputRecords() != 0L) {
          setOutputRecords(other.getOutputRecords());
        }
        if (other.getShuffleRead() != 0L) {
          setShuffleRead(other.getShuffleRead());
        }
        if (other.getShuffleReadRecords() != 0L) {
          setShuffleReadRecords(other.getShuffleReadRecords());
        }
        if (other.getShuffleWrite() != 0L) {
          setShuffleWrite(other.getShuffleWrite());
        }
        if (other.getShuffleWriteRecords() != 0L) {
          setShuffleWriteRecords(other.getShuffleWriteRecords());
        }
        if (other.getMemoryBytesSpilled() != 0L) {
          setMemoryBytesSpilled(other.getMemoryBytesSpilled());
        }
        if (other.getDiskBytesSpilled() != 0L) {
          setDiskBytesSpilled(other.getDiskBytesSpilled());
        }
        if (other.getIsBlacklistedForStage() != false) {
          setIsBlacklistedForStage(other.getIsBlacklistedForStage());
        }
        if (other.hasPeakMemoryMetrics()) {
          mergePeakMemoryMetrics(other.getPeakMemoryMetrics());
        }
        if (other.getIsExcludedForStage() != false) {
          setIsExcludedForStage(other.getIsExcludedForStage());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long taskTime_ ;
      /**
       * <code>int64 task_time = 1;</code>
       * @return The taskTime.
       */
      @java.lang.Override
      public long getTaskTime() {
        return taskTime_;
      }
      /**
       * <code>int64 task_time = 1;</code>
       * @param value The taskTime to set.
       * @return This builder for chaining.
       */
      public Builder setTaskTime(long value) {
        
        taskTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 task_time = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTaskTime() {
        
        taskTime_ = 0L;
        onChanged();
        return this;
      }

      private int failedTasks_ ;
      /**
       * <code>int32 failed_tasks = 2;</code>
       * @return The failedTasks.
       */
      @java.lang.Override
      public int getFailedTasks() {
        return failedTasks_;
      }
      /**
       * <code>int32 failed_tasks = 2;</code>
       * @param value The failedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setFailedTasks(int value) {
        
        failedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 failed_tasks = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearFailedTasks() {
        
        failedTasks_ = 0;
        onChanged();
        return this;
      }

      private int succeededTasks_ ;
      /**
       * <code>int32 succeeded_tasks = 3;</code>
       * @return The succeededTasks.
       */
      @java.lang.Override
      public int getSucceededTasks() {
        return succeededTasks_;
      }
      /**
       * <code>int32 succeeded_tasks = 3;</code>
       * @param value The succeededTasks to set.
       * @return This builder for chaining.
       */
      public Builder setSucceededTasks(int value) {
        
        succeededTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 succeeded_tasks = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearSucceededTasks() {
        
        succeededTasks_ = 0;
        onChanged();
        return this;
      }

      private int killedTasks_ ;
      /**
       * <code>int32 killed_tasks = 4;</code>
       * @return The killedTasks.
       */
      @java.lang.Override
      public int getKilledTasks() {
        return killedTasks_;
      }
      /**
       * <code>int32 killed_tasks = 4;</code>
       * @param value The killedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setKilledTasks(int value) {
        
        killedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 killed_tasks = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearKilledTasks() {
        
        killedTasks_ = 0;
        onChanged();
        return this;
      }

      private long inputBytes_ ;
      /**
       * <code>int64 input_bytes = 5;</code>
       * @return The inputBytes.
       */
      @java.lang.Override
      public long getInputBytes() {
        return inputBytes_;
      }
      /**
       * <code>int64 input_bytes = 5;</code>
       * @param value The inputBytes to set.
       * @return This builder for chaining.
       */
      public Builder setInputBytes(long value) {
        
        inputBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 input_bytes = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputBytes() {
        
        inputBytes_ = 0L;
        onChanged();
        return this;
      }

      private long inputRecords_ ;
      /**
       * <code>int64 input_records = 6;</code>
       * @return The inputRecords.
       */
      @java.lang.Override
      public long getInputRecords() {
        return inputRecords_;
      }
      /**
       * <code>int64 input_records = 6;</code>
       * @param value The inputRecords to set.
       * @return This builder for chaining.
       */
      public Builder setInputRecords(long value) {
        
        inputRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 input_records = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputRecords() {
        
        inputRecords_ = 0L;
        onChanged();
        return this;
      }

      private long outputBytes_ ;
      /**
       * <code>int64 output_bytes = 7;</code>
       * @return The outputBytes.
       */
      @java.lang.Override
      public long getOutputBytes() {
        return outputBytes_;
      }
      /**
       * <code>int64 output_bytes = 7;</code>
       * @param value The outputBytes to set.
       * @return This builder for chaining.
       */
      public Builder setOutputBytes(long value) {
        
        outputBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 output_bytes = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputBytes() {
        
        outputBytes_ = 0L;
        onChanged();
        return this;
      }

      private long outputRecords_ ;
      /**
       * <code>int64 output_records = 8;</code>
       * @return The outputRecords.
       */
      @java.lang.Override
      public long getOutputRecords() {
        return outputRecords_;
      }
      /**
       * <code>int64 output_records = 8;</code>
       * @param value The outputRecords to set.
       * @return This builder for chaining.
       */
      public Builder setOutputRecords(long value) {
        
        outputRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 output_records = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputRecords() {
        
        outputRecords_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRead_ ;
      /**
       * <code>int64 shuffle_read = 9;</code>
       * @return The shuffleRead.
       */
      @java.lang.Override
      public long getShuffleRead() {
        return shuffleRead_;
      }
      /**
       * <code>int64 shuffle_read = 9;</code>
       * @param value The shuffleRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRead(long value) {
        
        shuffleRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_read = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRead() {
        
        shuffleRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleReadRecords_ ;
      /**
       * <code>int64 shuffle_read_records = 10;</code>
       * @return The shuffleReadRecords.
       */
      @java.lang.Override
      public long getShuffleReadRecords() {
        return shuffleReadRecords_;
      }
      /**
       * <code>int64 shuffle_read_records = 10;</code>
       * @param value The shuffleReadRecords to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleReadRecords(long value) {
        
        shuffleReadRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_read_records = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleReadRecords() {
        
        shuffleReadRecords_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleWrite_ ;
      /**
       * <code>int64 shuffle_write = 11;</code>
       * @return The shuffleWrite.
       */
      @java.lang.Override
      public long getShuffleWrite() {
        return shuffleWrite_;
      }
      /**
       * <code>int64 shuffle_write = 11;</code>
       * @param value The shuffleWrite to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWrite(long value) {
        
        shuffleWrite_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_write = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWrite() {
        
        shuffleWrite_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleWriteRecords_ ;
      /**
       * <code>int64 shuffle_write_records = 12;</code>
       * @return The shuffleWriteRecords.
       */
      @java.lang.Override
      public long getShuffleWriteRecords() {
        return shuffleWriteRecords_;
      }
      /**
       * <code>int64 shuffle_write_records = 12;</code>
       * @param value The shuffleWriteRecords to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteRecords(long value) {
        
        shuffleWriteRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_write_records = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteRecords() {
        
        shuffleWriteRecords_ = 0L;
        onChanged();
        return this;
      }

      private long memoryBytesSpilled_ ;
      /**
       * <code>int64 memory_bytes_spilled = 13;</code>
       * @return The memoryBytesSpilled.
       */
      @java.lang.Override
      public long getMemoryBytesSpilled() {
        return memoryBytesSpilled_;
      }
      /**
       * <code>int64 memory_bytes_spilled = 13;</code>
       * @param value The memoryBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryBytesSpilled(long value) {
        
        memoryBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_bytes_spilled = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryBytesSpilled() {
        
        memoryBytesSpilled_ = 0L;
        onChanged();
        return this;
      }

      private long diskBytesSpilled_ ;
      /**
       * <code>int64 disk_bytes_spilled = 14;</code>
       * @return The diskBytesSpilled.
       */
      @java.lang.Override
      public long getDiskBytesSpilled() {
        return diskBytesSpilled_;
      }
      /**
       * <code>int64 disk_bytes_spilled = 14;</code>
       * @param value The diskBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setDiskBytesSpilled(long value) {
        
        diskBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_bytes_spilled = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskBytesSpilled() {
        
        diskBytesSpilled_ = 0L;
        onChanged();
        return this;
      }

      private boolean isBlacklistedForStage_ ;
      /**
       * <code>bool is_blacklisted_for_stage = 15;</code>
       * @return The isBlacklistedForStage.
       */
      @java.lang.Override
      public boolean getIsBlacklistedForStage() {
        return isBlacklistedForStage_;
      }
      /**
       * <code>bool is_blacklisted_for_stage = 15;</code>
       * @param value The isBlacklistedForStage to set.
       * @return This builder for chaining.
       */
      public Builder setIsBlacklistedForStage(boolean value) {
        
        isBlacklistedForStage_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool is_blacklisted_for_stage = 15;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsBlacklistedForStage() {
        
        isBlacklistedForStage_ = false;
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics peakMemoryMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> peakMemoryMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       * @return Whether the peakMemoryMetrics field is set.
       */
      public boolean hasPeakMemoryMetrics() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       * @return The peakMemoryMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakMemoryMetrics() {
        if (peakMemoryMetricsBuilder_ == null) {
          return peakMemoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakMemoryMetrics_;
        } else {
          return peakMemoryMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       */
      public Builder setPeakMemoryMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (peakMemoryMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          peakMemoryMetrics_ = value;
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       */
      public Builder setPeakMemoryMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder builderForValue) {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = builderForValue.build();
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       */
      public Builder mergePeakMemoryMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (peakMemoryMetricsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              peakMemoryMetrics_ != null &&
              peakMemoryMetrics_ != org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance()) {
            peakMemoryMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.newBuilder(peakMemoryMetrics_).mergeFrom(value).buildPartial();
          } else {
            peakMemoryMetrics_ = value;
          }
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       */
      public Builder clearPeakMemoryMetrics() {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = null;
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder getPeakMemoryMetricsBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPeakMemoryMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakMemoryMetricsOrBuilder() {
        if (peakMemoryMetricsBuilder_ != null) {
          return peakMemoryMetricsBuilder_.getMessageOrBuilder();
        } else {
          return peakMemoryMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakMemoryMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 16;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> 
          getPeakMemoryMetricsFieldBuilder() {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder>(
                  getPeakMemoryMetrics(),
                  getParentForChildren(),
                  isClean());
          peakMemoryMetrics_ = null;
        }
        return peakMemoryMetricsBuilder_;
      }

      private boolean isExcludedForStage_ ;
      /**
       * <code>bool is_excluded_for_stage = 17;</code>
       * @return The isExcludedForStage.
       */
      @java.lang.Override
      public boolean getIsExcludedForStage() {
        return isExcludedForStage_;
      }
      /**
       * <code>bool is_excluded_for_stage = 17;</code>
       * @param value The isExcludedForStage to set.
       * @return This builder for chaining.
       */
      public Builder setIsExcludedForStage(boolean value) {
        
        isExcludedForStage_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool is_excluded_for_stage = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsExcludedForStage() {
        
        isExcludedForStage_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ExecutorStageSummary)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ExecutorStageSummary)
    private static final org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ExecutorStageSummary>
        PARSER = new com.google.protobuf.AbstractParser<ExecutorStageSummary>() {
      @java.lang.Override
      public ExecutorStageSummary parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ExecutorStageSummary(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ExecutorStageSummary> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ExecutorStageSummary> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExecutorStageSummaryWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ExecutorStageSummaryWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 stage_id = 1;</code>
     * @return The stageId.
     */
    long getStageId();

    /**
     * <code>int32 stage_attempt_id = 2;</code>
     * @return The stageAttemptId.
     */
    int getStageAttemptId();

    /**
     * <code>string executor_id = 3;</code>
     * @return Whether the executorId field is set.
     */
    boolean hasExecutorId();
    /**
     * <code>string executor_id = 3;</code>
     * @return The executorId.
     */
    java.lang.String getExecutorId();
    /**
     * <code>string executor_id = 3;</code>
     * @return The bytes for executorId.
     */
    com.google.protobuf.ByteString
        getExecutorIdBytes();

    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryOrBuilder getInfoOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorStageSummaryWrapper}
   */
  public static final class ExecutorStageSummaryWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ExecutorStageSummaryWrapper)
      ExecutorStageSummaryWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ExecutorStageSummaryWrapper.newBuilder() to construct.
    private ExecutorStageSummaryWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ExecutorStageSummaryWrapper() {
      executorId_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ExecutorStageSummaryWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ExecutorStageSummaryWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              stageId_ = input.readInt64();
              break;
            }
            case 16: {

              stageAttemptId_ = input.readInt32();
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              executorId_ = s;
              break;
            }
            case 34: {
              org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper.Builder.class);
    }

    private int bitField0_;
    public static final int STAGE_ID_FIELD_NUMBER = 1;
    private long stageId_;
    /**
     * <code>int64 stage_id = 1;</code>
     * @return The stageId.
     */
    @java.lang.Override
    public long getStageId() {
      return stageId_;
    }

    public static final int STAGE_ATTEMPT_ID_FIELD_NUMBER = 2;
    private int stageAttemptId_;
    /**
     * <code>int32 stage_attempt_id = 2;</code>
     * @return The stageAttemptId.
     */
    @java.lang.Override
    public int getStageAttemptId() {
      return stageAttemptId_;
    }

    public static final int EXECUTOR_ID_FIELD_NUMBER = 3;
    private volatile java.lang.Object executorId_;
    /**
     * <code>string executor_id = 3;</code>
     * @return Whether the executorId field is set.
     */
    @java.lang.Override
    public boolean hasExecutorId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string executor_id = 3;</code>
     * @return The executorId.
     */
    @java.lang.Override
    public java.lang.String getExecutorId() {
      java.lang.Object ref = executorId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        executorId_ = s;
        return s;
      }
    }
    /**
     * <code>string executor_id = 3;</code>
     * @return The bytes for executorId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getExecutorIdBytes() {
      java.lang.Object ref = executorId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        executorId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int INFO_FIELD_NUMBER = 4;
    private org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary info_;
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (stageId_ != 0L) {
        output.writeInt64(1, stageId_);
      }
      if (stageAttemptId_ != 0) {
        output.writeInt32(2, stageAttemptId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, executorId_);
      }
      if (info_ != null) {
        output.writeMessage(4, getInfo());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (stageId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, stageId_);
      }
      if (stageAttemptId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, stageAttemptId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, executorId_);
      }
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, getInfo());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper other = (org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper) obj;

      if (getStageId()
          != other.getStageId()) return false;
      if (getStageAttemptId()
          != other.getStageAttemptId()) return false;
      if (hasExecutorId() != other.hasExecutorId()) return false;
      if (hasExecutorId()) {
        if (!getExecutorId()
            .equals(other.getExecutorId())) return false;
      }
      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STAGE_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getStageId());
      hash = (37 * hash) + STAGE_ATTEMPT_ID_FIELD_NUMBER;
      hash = (53 * hash) + getStageAttemptId();
      if (hasExecutorId()) {
        hash = (37 * hash) + EXECUTOR_ID_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorId().hashCode();
      }
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorStageSummaryWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ExecutorStageSummaryWrapper)
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        stageId_ = 0L;

        stageAttemptId_ = 0;

        executorId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper result = new org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.stageId_ = stageId_;
        result.stageAttemptId_ = stageAttemptId_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.executorId_ = executorId_;
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper.getDefaultInstance()) return this;
        if (other.getStageId() != 0L) {
          setStageId(other.getStageId());
        }
        if (other.getStageAttemptId() != 0) {
          setStageAttemptId(other.getStageAttemptId());
        }
        if (other.hasExecutorId()) {
          bitField0_ |= 0x00000001;
          executorId_ = other.executorId_;
          onChanged();
        }
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long stageId_ ;
      /**
       * <code>int64 stage_id = 1;</code>
       * @return The stageId.
       */
      @java.lang.Override
      public long getStageId() {
        return stageId_;
      }
      /**
       * <code>int64 stage_id = 1;</code>
       * @param value The stageId to set.
       * @return This builder for chaining.
       */
      public Builder setStageId(long value) {
        
        stageId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 stage_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageId() {
        
        stageId_ = 0L;
        onChanged();
        return this;
      }

      private int stageAttemptId_ ;
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @return The stageAttemptId.
       */
      @java.lang.Override
      public int getStageAttemptId() {
        return stageAttemptId_;
      }
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @param value The stageAttemptId to set.
       * @return This builder for chaining.
       */
      public Builder setStageAttemptId(int value) {
        
        stageAttemptId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageAttemptId() {
        
        stageAttemptId_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object executorId_ = "";
      /**
       * <code>string executor_id = 3;</code>
       * @return Whether the executorId field is set.
       */
      public boolean hasExecutorId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string executor_id = 3;</code>
       * @return The executorId.
       */
      public java.lang.String getExecutorId() {
        java.lang.Object ref = executorId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          executorId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string executor_id = 3;</code>
       * @return The bytes for executorId.
       */
      public com.google.protobuf.ByteString
          getExecutorIdBytes() {
        java.lang.Object ref = executorId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          executorId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string executor_id = 3;</code>
       * @param value The executorId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        executorId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string executor_id = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        executorId_ = getDefaultInstance().getExecutorId();
        onChanged();
        return this;
      }
      /**
       * <code>string executor_id = 3;</code>
       * @param value The bytes for executorId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        executorId_ = value;
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorStageSummary info = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ExecutorStageSummaryWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ExecutorStageSummaryWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ExecutorStageSummaryWrapper>
        PARSER = new com.google.protobuf.AbstractParser<ExecutorStageSummaryWrapper>() {
      @java.lang.Override
      public ExecutorStageSummaryWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ExecutorStageSummaryWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ExecutorStageSummaryWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ExecutorStageSummaryWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummaryWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExecutorResourceRequestOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ExecutorResourceRequest)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string resource_name = 1;</code>
     * @return Whether the resourceName field is set.
     */
    boolean hasResourceName();
    /**
     * <code>string resource_name = 1;</code>
     * @return The resourceName.
     */
    java.lang.String getResourceName();
    /**
     * <code>string resource_name = 1;</code>
     * @return The bytes for resourceName.
     */
    com.google.protobuf.ByteString
        getResourceNameBytes();

    /**
     * <code>int64 amount = 2;</code>
     * @return The amount.
     */
    long getAmount();

    /**
     * <code>string discoveryScript = 3;</code>
     * @return Whether the discoveryScript field is set.
     */
    boolean hasDiscoveryScript();
    /**
     * <code>string discoveryScript = 3;</code>
     * @return The discoveryScript.
     */
    java.lang.String getDiscoveryScript();
    /**
     * <code>string discoveryScript = 3;</code>
     * @return The bytes for discoveryScript.
     */
    com.google.protobuf.ByteString
        getDiscoveryScriptBytes();

    /**
     * <code>string vendor = 4;</code>
     * @return Whether the vendor field is set.
     */
    boolean hasVendor();
    /**
     * <code>string vendor = 4;</code>
     * @return The vendor.
     */
    java.lang.String getVendor();
    /**
     * <code>string vendor = 4;</code>
     * @return The bytes for vendor.
     */
    com.google.protobuf.ByteString
        getVendorBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorResourceRequest}
   */
  public static final class ExecutorResourceRequest extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ExecutorResourceRequest)
      ExecutorResourceRequestOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ExecutorResourceRequest.newBuilder() to construct.
    private ExecutorResourceRequest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ExecutorResourceRequest() {
      resourceName_ = "";
      discoveryScript_ = "";
      vendor_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ExecutorResourceRequest();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ExecutorResourceRequest(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              resourceName_ = s;
              break;
            }
            case 16: {

              amount_ = input.readInt64();
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              discoveryScript_ = s;
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              vendor_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest.Builder.class);
    }

    private int bitField0_;
    public static final int RESOURCE_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object resourceName_;
    /**
     * <code>string resource_name = 1;</code>
     * @return Whether the resourceName field is set.
     */
    @java.lang.Override
    public boolean hasResourceName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string resource_name = 1;</code>
     * @return The resourceName.
     */
    @java.lang.Override
    public java.lang.String getResourceName() {
      java.lang.Object ref = resourceName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        resourceName_ = s;
        return s;
      }
    }
    /**
     * <code>string resource_name = 1;</code>
     * @return The bytes for resourceName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getResourceNameBytes() {
      java.lang.Object ref = resourceName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        resourceName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int AMOUNT_FIELD_NUMBER = 2;
    private long amount_;
    /**
     * <code>int64 amount = 2;</code>
     * @return The amount.
     */
    @java.lang.Override
    public long getAmount() {
      return amount_;
    }

    public static final int DISCOVERYSCRIPT_FIELD_NUMBER = 3;
    private volatile java.lang.Object discoveryScript_;
    /**
     * <code>string discoveryScript = 3;</code>
     * @return Whether the discoveryScript field is set.
     */
    @java.lang.Override
    public boolean hasDiscoveryScript() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string discoveryScript = 3;</code>
     * @return The discoveryScript.
     */
    @java.lang.Override
    public java.lang.String getDiscoveryScript() {
      java.lang.Object ref = discoveryScript_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        discoveryScript_ = s;
        return s;
      }
    }
    /**
     * <code>string discoveryScript = 3;</code>
     * @return The bytes for discoveryScript.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDiscoveryScriptBytes() {
      java.lang.Object ref = discoveryScript_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        discoveryScript_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VENDOR_FIELD_NUMBER = 4;
    private volatile java.lang.Object vendor_;
    /**
     * <code>string vendor = 4;</code>
     * @return Whether the vendor field is set.
     */
    @java.lang.Override
    public boolean hasVendor() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string vendor = 4;</code>
     * @return The vendor.
     */
    @java.lang.Override
    public java.lang.String getVendor() {
      java.lang.Object ref = vendor_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        vendor_ = s;
        return s;
      }
    }
    /**
     * <code>string vendor = 4;</code>
     * @return The bytes for vendor.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getVendorBytes() {
      java.lang.Object ref = vendor_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        vendor_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, resourceName_);
      }
      if (amount_ != 0L) {
        output.writeInt64(2, amount_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, discoveryScript_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, vendor_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, resourceName_);
      }
      if (amount_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, amount_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, discoveryScript_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, vendor_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest other = (org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest) obj;

      if (hasResourceName() != other.hasResourceName()) return false;
      if (hasResourceName()) {
        if (!getResourceName()
            .equals(other.getResourceName())) return false;
      }
      if (getAmount()
          != other.getAmount()) return false;
      if (hasDiscoveryScript() != other.hasDiscoveryScript()) return false;
      if (hasDiscoveryScript()) {
        if (!getDiscoveryScript()
            .equals(other.getDiscoveryScript())) return false;
      }
      if (hasVendor() != other.hasVendor()) return false;
      if (hasVendor()) {
        if (!getVendor()
            .equals(other.getVendor())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasResourceName()) {
        hash = (37 * hash) + RESOURCE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getResourceName().hashCode();
      }
      hash = (37 * hash) + AMOUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getAmount());
      if (hasDiscoveryScript()) {
        hash = (37 * hash) + DISCOVERYSCRIPT_FIELD_NUMBER;
        hash = (53 * hash) + getDiscoveryScript().hashCode();
      }
      if (hasVendor()) {
        hash = (37 * hash) + VENDOR_FIELD_NUMBER;
        hash = (53 * hash) + getVendor().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorResourceRequest}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ExecutorResourceRequest)
        org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        resourceName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        amount_ = 0L;

        discoveryScript_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        vendor_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest build() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest result = new org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.resourceName_ = resourceName_;
        result.amount_ = amount_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.discoveryScript_ = discoveryScript_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.vendor_ = vendor_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest.getDefaultInstance()) return this;
        if (other.hasResourceName()) {
          bitField0_ |= 0x00000001;
          resourceName_ = other.resourceName_;
          onChanged();
        }
        if (other.getAmount() != 0L) {
          setAmount(other.getAmount());
        }
        if (other.hasDiscoveryScript()) {
          bitField0_ |= 0x00000002;
          discoveryScript_ = other.discoveryScript_;
          onChanged();
        }
        if (other.hasVendor()) {
          bitField0_ |= 0x00000004;
          vendor_ = other.vendor_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object resourceName_ = "";
      /**
       * <code>string resource_name = 1;</code>
       * @return Whether the resourceName field is set.
       */
      public boolean hasResourceName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string resource_name = 1;</code>
       * @return The resourceName.
       */
      public java.lang.String getResourceName() {
        java.lang.Object ref = resourceName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          resourceName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string resource_name = 1;</code>
       * @return The bytes for resourceName.
       */
      public com.google.protobuf.ByteString
          getResourceNameBytes() {
        java.lang.Object ref = resourceName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          resourceName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string resource_name = 1;</code>
       * @param value The resourceName to set.
       * @return This builder for chaining.
       */
      public Builder setResourceName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        resourceName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string resource_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearResourceName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        resourceName_ = getDefaultInstance().getResourceName();
        onChanged();
        return this;
      }
      /**
       * <code>string resource_name = 1;</code>
       * @param value The bytes for resourceName to set.
       * @return This builder for chaining.
       */
      public Builder setResourceNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        resourceName_ = value;
        onChanged();
        return this;
      }

      private long amount_ ;
      /**
       * <code>int64 amount = 2;</code>
       * @return The amount.
       */
      @java.lang.Override
      public long getAmount() {
        return amount_;
      }
      /**
       * <code>int64 amount = 2;</code>
       * @param value The amount to set.
       * @return This builder for chaining.
       */
      public Builder setAmount(long value) {
        
        amount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 amount = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearAmount() {
        
        amount_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object discoveryScript_ = "";
      /**
       * <code>string discoveryScript = 3;</code>
       * @return Whether the discoveryScript field is set.
       */
      public boolean hasDiscoveryScript() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string discoveryScript = 3;</code>
       * @return The discoveryScript.
       */
      public java.lang.String getDiscoveryScript() {
        java.lang.Object ref = discoveryScript_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          discoveryScript_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string discoveryScript = 3;</code>
       * @return The bytes for discoveryScript.
       */
      public com.google.protobuf.ByteString
          getDiscoveryScriptBytes() {
        java.lang.Object ref = discoveryScript_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          discoveryScript_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string discoveryScript = 3;</code>
       * @param value The discoveryScript to set.
       * @return This builder for chaining.
       */
      public Builder setDiscoveryScript(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        discoveryScript_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string discoveryScript = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiscoveryScript() {
        bitField0_ = (bitField0_ & ~0x00000002);
        discoveryScript_ = getDefaultInstance().getDiscoveryScript();
        onChanged();
        return this;
      }
      /**
       * <code>string discoveryScript = 3;</code>
       * @param value The bytes for discoveryScript to set.
       * @return This builder for chaining.
       */
      public Builder setDiscoveryScriptBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        discoveryScript_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object vendor_ = "";
      /**
       * <code>string vendor = 4;</code>
       * @return Whether the vendor field is set.
       */
      public boolean hasVendor() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string vendor = 4;</code>
       * @return The vendor.
       */
      public java.lang.String getVendor() {
        java.lang.Object ref = vendor_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          vendor_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string vendor = 4;</code>
       * @return The bytes for vendor.
       */
      public com.google.protobuf.ByteString
          getVendorBytes() {
        java.lang.Object ref = vendor_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          vendor_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string vendor = 4;</code>
       * @param value The vendor to set.
       * @return This builder for chaining.
       */
      public Builder setVendor(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        vendor_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string vendor = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearVendor() {
        bitField0_ = (bitField0_ & ~0x00000004);
        vendor_ = getDefaultInstance().getVendor();
        onChanged();
        return this;
      }
      /**
       * <code>string vendor = 4;</code>
       * @param value The bytes for vendor to set.
       * @return This builder for chaining.
       */
      public Builder setVendorBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        vendor_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ExecutorResourceRequest)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ExecutorResourceRequest)
    private static final org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ExecutorResourceRequest>
        PARSER = new com.google.protobuf.AbstractParser<ExecutorResourceRequest>() {
      @java.lang.Override
      public ExecutorResourceRequest parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ExecutorResourceRequest(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ExecutorResourceRequest> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ExecutorResourceRequest> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskResourceRequestOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.TaskResourceRequest)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string resource_name = 1;</code>
     * @return Whether the resourceName field is set.
     */
    boolean hasResourceName();
    /**
     * <code>string resource_name = 1;</code>
     * @return The resourceName.
     */
    java.lang.String getResourceName();
    /**
     * <code>string resource_name = 1;</code>
     * @return The bytes for resourceName.
     */
    com.google.protobuf.ByteString
        getResourceNameBytes();

    /**
     * <code>double amount = 2;</code>
     * @return The amount.
     */
    double getAmount();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.TaskResourceRequest}
   */
  public static final class TaskResourceRequest extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.TaskResourceRequest)
      TaskResourceRequestOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskResourceRequest.newBuilder() to construct.
    private TaskResourceRequest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskResourceRequest() {
      resourceName_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new TaskResourceRequest();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskResourceRequest(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              resourceName_ = s;
              break;
            }
            case 17: {

              amount_ = input.readDouble();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest.class, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest.Builder.class);
    }

    private int bitField0_;
    public static final int RESOURCE_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object resourceName_;
    /**
     * <code>string resource_name = 1;</code>
     * @return Whether the resourceName field is set.
     */
    @java.lang.Override
    public boolean hasResourceName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string resource_name = 1;</code>
     * @return The resourceName.
     */
    @java.lang.Override
    public java.lang.String getResourceName() {
      java.lang.Object ref = resourceName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        resourceName_ = s;
        return s;
      }
    }
    /**
     * <code>string resource_name = 1;</code>
     * @return The bytes for resourceName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getResourceNameBytes() {
      java.lang.Object ref = resourceName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        resourceName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int AMOUNT_FIELD_NUMBER = 2;
    private double amount_;
    /**
     * <code>double amount = 2;</code>
     * @return The amount.
     */
    @java.lang.Override
    public double getAmount() {
      return amount_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, resourceName_);
      }
      if (amount_ != 0D) {
        output.writeDouble(2, amount_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, resourceName_);
      }
      if (amount_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(2, amount_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest other = (org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest) obj;

      if (hasResourceName() != other.hasResourceName()) return false;
      if (hasResourceName()) {
        if (!getResourceName()
            .equals(other.getResourceName())) return false;
      }
      if (java.lang.Double.doubleToLongBits(getAmount())
          != java.lang.Double.doubleToLongBits(
              other.getAmount())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasResourceName()) {
        hash = (37 * hash) + RESOURCE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getResourceName().hashCode();
      }
      hash = (37 * hash) + AMOUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getAmount()));
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.TaskResourceRequest}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.TaskResourceRequest)
        org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest.class, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        resourceName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        amount_ = 0D;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest build() {
        org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest result = new org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.resourceName_ = resourceName_;
        result.amount_ = amount_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest.getDefaultInstance()) return this;
        if (other.hasResourceName()) {
          bitField0_ |= 0x00000001;
          resourceName_ = other.resourceName_;
          onChanged();
        }
        if (other.getAmount() != 0D) {
          setAmount(other.getAmount());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object resourceName_ = "";
      /**
       * <code>string resource_name = 1;</code>
       * @return Whether the resourceName field is set.
       */
      public boolean hasResourceName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string resource_name = 1;</code>
       * @return The resourceName.
       */
      public java.lang.String getResourceName() {
        java.lang.Object ref = resourceName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          resourceName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string resource_name = 1;</code>
       * @return The bytes for resourceName.
       */
      public com.google.protobuf.ByteString
          getResourceNameBytes() {
        java.lang.Object ref = resourceName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          resourceName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string resource_name = 1;</code>
       * @param value The resourceName to set.
       * @return This builder for chaining.
       */
      public Builder setResourceName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        resourceName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string resource_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearResourceName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        resourceName_ = getDefaultInstance().getResourceName();
        onChanged();
        return this;
      }
      /**
       * <code>string resource_name = 1;</code>
       * @param value The bytes for resourceName to set.
       * @return This builder for chaining.
       */
      public Builder setResourceNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        resourceName_ = value;
        onChanged();
        return this;
      }

      private double amount_ ;
      /**
       * <code>double amount = 2;</code>
       * @return The amount.
       */
      @java.lang.Override
      public double getAmount() {
        return amount_;
      }
      /**
       * <code>double amount = 2;</code>
       * @param value The amount to set.
       * @return This builder for chaining.
       */
      public Builder setAmount(double value) {
        
        amount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double amount = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearAmount() {
        
        amount_ = 0D;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.TaskResourceRequest)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.TaskResourceRequest)
    private static final org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<TaskResourceRequest>
        PARSER = new com.google.protobuf.AbstractParser<TaskResourceRequest>() {
      @java.lang.Override
      public TaskResourceRequest parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TaskResourceRequest(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<TaskResourceRequest> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<TaskResourceRequest> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ResourceProfileInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ResourceProfileInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int32 id = 1;</code>
     * @return The id.
     */
    int getId();

    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */
    int getExecutorResourcesCount();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */
    boolean containsExecutorResources(
        java.lang.String key);
    /**
     * Use {@link #getExecutorResourcesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>
    getExecutorResources();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */
    java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>
    getExecutorResourcesMap();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getExecutorResourcesOrDefault(
        java.lang.String key,
        org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest defaultValue);
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getExecutorResourcesOrThrow(
        java.lang.String key);

    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */
    int getTaskResourcesCount();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */
    boolean containsTaskResources(
        java.lang.String key);
    /**
     * Use {@link #getTaskResourcesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>
    getTaskResources();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */
    java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>
    getTaskResourcesMap();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getTaskResourcesOrDefault(
        java.lang.String key,
        org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest defaultValue);
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getTaskResourcesOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ResourceProfileInfo}
   */
  public static final class ResourceProfileInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ResourceProfileInfo)
      ResourceProfileInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ResourceProfileInfo.newBuilder() to construct.
    private ResourceProfileInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ResourceProfileInfo() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ResourceProfileInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ResourceProfileInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              id_ = input.readInt32();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                executorResources_ = com.google.protobuf.MapField.newMapField(
                    ExecutorResourcesDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000001;
              }
              com.google.protobuf.MapEntry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>
              executorResources__ = input.readMessage(
                  ExecutorResourcesDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              executorResources_.getMutableMap().put(
                  executorResources__.getKey(), executorResources__.getValue());
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                taskResources_ = com.google.protobuf.MapField.newMapField(
                    TaskResourcesDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000002;
              }
              com.google.protobuf.MapEntry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>
              taskResources__ = input.readMessage(
                  TaskResourcesDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              taskResources_.getMutableMap().put(
                  taskResources__.getKey(), taskResources__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 2:
          return internalGetExecutorResources();
        case 3:
          return internalGetTaskResources();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.class, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder.class);
    }

    public static final int ID_FIELD_NUMBER = 1;
    private int id_;
    /**
     * <code>int32 id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public int getId() {
      return id_;
    }

    public static final int EXECUTOR_RESOURCES_FIELD_NUMBER = 2;
    private static final class ExecutorResourcesDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_ExecutorResourcesEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> executorResources_;
    private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>
    internalGetExecutorResources() {
      if (executorResources_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ExecutorResourcesDefaultEntryHolder.defaultEntry);
      }
      return executorResources_;
    }

    public int getExecutorResourcesCount() {
      return internalGetExecutorResources().getMap().size();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */

    @java.lang.Override
    public boolean containsExecutorResources(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetExecutorResources().getMap().containsKey(key);
    }
    /**
     * Use {@link #getExecutorResourcesMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> getExecutorResources() {
      return getExecutorResourcesMap();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> getExecutorResourcesMap() {
      return internalGetExecutorResources().getMap();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getExecutorResourcesOrDefault(
        java.lang.String key,
        org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> map =
          internalGetExecutorResources().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getExecutorResourcesOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> map =
          internalGetExecutorResources().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int TASK_RESOURCES_FIELD_NUMBER = 3;
    private static final class TaskResourcesDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_TaskResourcesEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> taskResources_;
    private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>
    internalGetTaskResources() {
      if (taskResources_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            TaskResourcesDefaultEntryHolder.defaultEntry);
      }
      return taskResources_;
    }

    public int getTaskResourcesCount() {
      return internalGetTaskResources().getMap().size();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */

    @java.lang.Override
    public boolean containsTaskResources(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetTaskResources().getMap().containsKey(key);
    }
    /**
     * Use {@link #getTaskResourcesMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> getTaskResources() {
      return getTaskResourcesMap();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> getTaskResourcesMap() {
      return internalGetTaskResources().getMap();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getTaskResourcesOrDefault(
        java.lang.String key,
        org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> map =
          internalGetTaskResources().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getTaskResourcesOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> map =
          internalGetTaskResources().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (id_ != 0) {
        output.writeInt32(1, id_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetExecutorResources(),
          ExecutorResourcesDefaultEntryHolder.defaultEntry,
          2);
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetTaskResources(),
          TaskResourcesDefaultEntryHolder.defaultEntry,
          3);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (id_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, id_);
      }
      for (java.util.Map.Entry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> entry
           : internalGetExecutorResources().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>
        executorResources__ = ExecutorResourcesDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, executorResources__);
      }
      for (java.util.Map.Entry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> entry
           : internalGetTaskResources().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>
        taskResources__ = TaskResourcesDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(3, taskResources__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo other = (org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo) obj;

      if (getId()
          != other.getId()) return false;
      if (!internalGetExecutorResources().equals(
          other.internalGetExecutorResources())) return false;
      if (!internalGetTaskResources().equals(
          other.internalGetTaskResources())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + getId();
      if (!internalGetExecutorResources().getMap().isEmpty()) {
        hash = (37 * hash) + EXECUTOR_RESOURCES_FIELD_NUMBER;
        hash = (53 * hash) + internalGetExecutorResources().hashCode();
      }
      if (!internalGetTaskResources().getMap().isEmpty()) {
        hash = (37 * hash) + TASK_RESOURCES_FIELD_NUMBER;
        hash = (53 * hash) + internalGetTaskResources().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ResourceProfileInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ResourceProfileInfo)
        org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 2:
            return internalGetExecutorResources();
          case 3:
            return internalGetTaskResources();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 2:
            return internalGetMutableExecutorResources();
          case 3:
            return internalGetMutableTaskResources();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.class, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0;

        internalGetMutableExecutorResources().clear();
        internalGetMutableTaskResources().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo build() {
        org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo result = new org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo(this);
        int from_bitField0_ = bitField0_;
        result.id_ = id_;
        result.executorResources_ = internalGetExecutorResources();
        result.executorResources_.makeImmutable();
        result.taskResources_ = internalGetTaskResources();
        result.taskResources_.makeImmutable();
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.getDefaultInstance()) return this;
        if (other.getId() != 0) {
          setId(other.getId());
        }
        internalGetMutableExecutorResources().mergeFrom(
            other.internalGetExecutorResources());
        internalGetMutableTaskResources().mergeFrom(
            other.internalGetTaskResources());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int id_ ;
      /**
       * <code>int32 id = 1;</code>
       * @return The id.
       */
      @java.lang.Override
      public int getId() {
        return id_;
      }
      /**
       * <code>int32 id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(int value) {
        
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        
        id_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> executorResources_;
      private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>
      internalGetExecutorResources() {
        if (executorResources_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ExecutorResourcesDefaultEntryHolder.defaultEntry);
        }
        return executorResources_;
      }
      private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>
      internalGetMutableExecutorResources() {
        onChanged();;
        if (executorResources_ == null) {
          executorResources_ = com.google.protobuf.MapField.newMapField(
              ExecutorResourcesDefaultEntryHolder.defaultEntry);
        }
        if (!executorResources_.isMutable()) {
          executorResources_ = executorResources_.copy();
        }
        return executorResources_;
      }

      public int getExecutorResourcesCount() {
        return internalGetExecutorResources().getMap().size();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
       */

      @java.lang.Override
      public boolean containsExecutorResources(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetExecutorResources().getMap().containsKey(key);
      }
      /**
       * Use {@link #getExecutorResourcesMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> getExecutorResources() {
        return getExecutorResourcesMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> getExecutorResourcesMap() {
        return internalGetExecutorResources().getMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getExecutorResourcesOrDefault(
          java.lang.String key,
          org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> map =
            internalGetExecutorResources().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest getExecutorResourcesOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> map =
            internalGetExecutorResources().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearExecutorResources() {
        internalGetMutableExecutorResources().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
       */

      public Builder removeExecutorResources(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableExecutorResources().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest>
      getMutableExecutorResources() {
        return internalGetMutableExecutorResources().getMutableMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
       */
      public Builder putExecutorResources(
          java.lang.String key,
          org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableExecutorResources().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorResourceRequest&gt; executor_resources = 2;</code>
       */

      public Builder putAllExecutorResources(
          java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorResourceRequest> values) {
        internalGetMutableExecutorResources().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> taskResources_;
      private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>
      internalGetTaskResources() {
        if (taskResources_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              TaskResourcesDefaultEntryHolder.defaultEntry);
        }
        return taskResources_;
      }
      private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>
      internalGetMutableTaskResources() {
        onChanged();;
        if (taskResources_ == null) {
          taskResources_ = com.google.protobuf.MapField.newMapField(
              TaskResourcesDefaultEntryHolder.defaultEntry);
        }
        if (!taskResources_.isMutable()) {
          taskResources_ = taskResources_.copy();
        }
        return taskResources_;
      }

      public int getTaskResourcesCount() {
        return internalGetTaskResources().getMap().size();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
       */

      @java.lang.Override
      public boolean containsTaskResources(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetTaskResources().getMap().containsKey(key);
      }
      /**
       * Use {@link #getTaskResourcesMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> getTaskResources() {
        return getTaskResourcesMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> getTaskResourcesMap() {
        return internalGetTaskResources().getMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getTaskResourcesOrDefault(
          java.lang.String key,
          org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> map =
            internalGetTaskResources().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest getTaskResourcesOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> map =
            internalGetTaskResources().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearTaskResources() {
        internalGetMutableTaskResources().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
       */

      public Builder removeTaskResources(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableTaskResources().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest>
      getMutableTaskResources() {
        return internalGetMutableTaskResources().getMutableMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
       */
      public Builder putTaskResources(
          java.lang.String key,
          org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableTaskResources().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.TaskResourceRequest&gt; task_resources = 3;</code>
       */

      public Builder putAllTaskResources(
          java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.TaskResourceRequest> values) {
        internalGetMutableTaskResources().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ResourceProfileInfo)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ResourceProfileInfo)
    private static final org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ResourceProfileInfo>
        PARSER = new com.google.protobuf.AbstractParser<ResourceProfileInfo>() {
      @java.lang.Override
      public ResourceProfileInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ResourceProfileInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ResourceProfileInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ResourceProfileInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RuntimeInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RuntimeInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string java_version = 1;</code>
     * @return Whether the javaVersion field is set.
     */
    boolean hasJavaVersion();
    /**
     * <code>string java_version = 1;</code>
     * @return The javaVersion.
     */
    java.lang.String getJavaVersion();
    /**
     * <code>string java_version = 1;</code>
     * @return The bytes for javaVersion.
     */
    com.google.protobuf.ByteString
        getJavaVersionBytes();

    /**
     * <code>string java_home = 2;</code>
     * @return Whether the javaHome field is set.
     */
    boolean hasJavaHome();
    /**
     * <code>string java_home = 2;</code>
     * @return The javaHome.
     */
    java.lang.String getJavaHome();
    /**
     * <code>string java_home = 2;</code>
     * @return The bytes for javaHome.
     */
    com.google.protobuf.ByteString
        getJavaHomeBytes();

    /**
     * <code>string scala_version = 3;</code>
     * @return Whether the scalaVersion field is set.
     */
    boolean hasScalaVersion();
    /**
     * <code>string scala_version = 3;</code>
     * @return The scalaVersion.
     */
    java.lang.String getScalaVersion();
    /**
     * <code>string scala_version = 3;</code>
     * @return The bytes for scalaVersion.
     */
    com.google.protobuf.ByteString
        getScalaVersionBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RuntimeInfo}
   */
  public static final class RuntimeInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RuntimeInfo)
      RuntimeInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RuntimeInfo.newBuilder() to construct.
    private RuntimeInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RuntimeInfo() {
      javaVersion_ = "";
      javaHome_ = "";
      scalaVersion_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RuntimeInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RuntimeInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              javaVersion_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              javaHome_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              scalaVersion_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RuntimeInfo_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RuntimeInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.class, org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.Builder.class);
    }

    private int bitField0_;
    public static final int JAVA_VERSION_FIELD_NUMBER = 1;
    private volatile java.lang.Object javaVersion_;
    /**
     * <code>string java_version = 1;</code>
     * @return Whether the javaVersion field is set.
     */
    @java.lang.Override
    public boolean hasJavaVersion() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string java_version = 1;</code>
     * @return The javaVersion.
     */
    @java.lang.Override
    public java.lang.String getJavaVersion() {
      java.lang.Object ref = javaVersion_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        javaVersion_ = s;
        return s;
      }
    }
    /**
     * <code>string java_version = 1;</code>
     * @return The bytes for javaVersion.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getJavaVersionBytes() {
      java.lang.Object ref = javaVersion_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        javaVersion_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int JAVA_HOME_FIELD_NUMBER = 2;
    private volatile java.lang.Object javaHome_;
    /**
     * <code>string java_home = 2;</code>
     * @return Whether the javaHome field is set.
     */
    @java.lang.Override
    public boolean hasJavaHome() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string java_home = 2;</code>
     * @return The javaHome.
     */
    @java.lang.Override
    public java.lang.String getJavaHome() {
      java.lang.Object ref = javaHome_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        javaHome_ = s;
        return s;
      }
    }
    /**
     * <code>string java_home = 2;</code>
     * @return The bytes for javaHome.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getJavaHomeBytes() {
      java.lang.Object ref = javaHome_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        javaHome_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int SCALA_VERSION_FIELD_NUMBER = 3;
    private volatile java.lang.Object scalaVersion_;
    /**
     * <code>string scala_version = 3;</code>
     * @return Whether the scalaVersion field is set.
     */
    @java.lang.Override
    public boolean hasScalaVersion() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string scala_version = 3;</code>
     * @return The scalaVersion.
     */
    @java.lang.Override
    public java.lang.String getScalaVersion() {
      java.lang.Object ref = scalaVersion_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        scalaVersion_ = s;
        return s;
      }
    }
    /**
     * <code>string scala_version = 3;</code>
     * @return The bytes for scalaVersion.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getScalaVersionBytes() {
      java.lang.Object ref = scalaVersion_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        scalaVersion_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, javaVersion_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, javaHome_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, scalaVersion_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, javaVersion_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, javaHome_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, scalaVersion_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo other = (org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo) obj;

      if (hasJavaVersion() != other.hasJavaVersion()) return false;
      if (hasJavaVersion()) {
        if (!getJavaVersion()
            .equals(other.getJavaVersion())) return false;
      }
      if (hasJavaHome() != other.hasJavaHome()) return false;
      if (hasJavaHome()) {
        if (!getJavaHome()
            .equals(other.getJavaHome())) return false;
      }
      if (hasScalaVersion() != other.hasScalaVersion()) return false;
      if (hasScalaVersion()) {
        if (!getScalaVersion()
            .equals(other.getScalaVersion())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasJavaVersion()) {
        hash = (37 * hash) + JAVA_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getJavaVersion().hashCode();
      }
      if (hasJavaHome()) {
        hash = (37 * hash) + JAVA_HOME_FIELD_NUMBER;
        hash = (53 * hash) + getJavaHome().hashCode();
      }
      if (hasScalaVersion()) {
        hash = (37 * hash) + SCALA_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getScalaVersion().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RuntimeInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RuntimeInfo)
        org.apache.spark.status.protobuf.StoreTypes.RuntimeInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RuntimeInfo_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RuntimeInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.class, org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        javaVersion_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        javaHome_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        scalaVersion_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RuntimeInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo build() {
        org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo result = new org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.javaVersion_ = javaVersion_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.javaHome_ = javaHome_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.scalaVersion_ = scalaVersion_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.getDefaultInstance()) return this;
        if (other.hasJavaVersion()) {
          bitField0_ |= 0x00000001;
          javaVersion_ = other.javaVersion_;
          onChanged();
        }
        if (other.hasJavaHome()) {
          bitField0_ |= 0x00000002;
          javaHome_ = other.javaHome_;
          onChanged();
        }
        if (other.hasScalaVersion()) {
          bitField0_ |= 0x00000004;
          scalaVersion_ = other.scalaVersion_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object javaVersion_ = "";
      /**
       * <code>string java_version = 1;</code>
       * @return Whether the javaVersion field is set.
       */
      public boolean hasJavaVersion() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string java_version = 1;</code>
       * @return The javaVersion.
       */
      public java.lang.String getJavaVersion() {
        java.lang.Object ref = javaVersion_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          javaVersion_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string java_version = 1;</code>
       * @return The bytes for javaVersion.
       */
      public com.google.protobuf.ByteString
          getJavaVersionBytes() {
        java.lang.Object ref = javaVersion_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          javaVersion_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string java_version = 1;</code>
       * @param value The javaVersion to set.
       * @return This builder for chaining.
       */
      public Builder setJavaVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        javaVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string java_version = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearJavaVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        javaVersion_ = getDefaultInstance().getJavaVersion();
        onChanged();
        return this;
      }
      /**
       * <code>string java_version = 1;</code>
       * @param value The bytes for javaVersion to set.
       * @return This builder for chaining.
       */
      public Builder setJavaVersionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        javaVersion_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object javaHome_ = "";
      /**
       * <code>string java_home = 2;</code>
       * @return Whether the javaHome field is set.
       */
      public boolean hasJavaHome() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string java_home = 2;</code>
       * @return The javaHome.
       */
      public java.lang.String getJavaHome() {
        java.lang.Object ref = javaHome_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          javaHome_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string java_home = 2;</code>
       * @return The bytes for javaHome.
       */
      public com.google.protobuf.ByteString
          getJavaHomeBytes() {
        java.lang.Object ref = javaHome_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          javaHome_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string java_home = 2;</code>
       * @param value The javaHome to set.
       * @return This builder for chaining.
       */
      public Builder setJavaHome(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        javaHome_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string java_home = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearJavaHome() {
        bitField0_ = (bitField0_ & ~0x00000002);
        javaHome_ = getDefaultInstance().getJavaHome();
        onChanged();
        return this;
      }
      /**
       * <code>string java_home = 2;</code>
       * @param value The bytes for javaHome to set.
       * @return This builder for chaining.
       */
      public Builder setJavaHomeBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        javaHome_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object scalaVersion_ = "";
      /**
       * <code>string scala_version = 3;</code>
       * @return Whether the scalaVersion field is set.
       */
      public boolean hasScalaVersion() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string scala_version = 3;</code>
       * @return The scalaVersion.
       */
      public java.lang.String getScalaVersion() {
        java.lang.Object ref = scalaVersion_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          scalaVersion_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string scala_version = 3;</code>
       * @return The bytes for scalaVersion.
       */
      public com.google.protobuf.ByteString
          getScalaVersionBytes() {
        java.lang.Object ref = scalaVersion_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          scalaVersion_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string scala_version = 3;</code>
       * @param value The scalaVersion to set.
       * @return This builder for chaining.
       */
      public Builder setScalaVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        scalaVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string scala_version = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearScalaVersion() {
        bitField0_ = (bitField0_ & ~0x00000004);
        scalaVersion_ = getDefaultInstance().getScalaVersion();
        onChanged();
        return this;
      }
      /**
       * <code>string scala_version = 3;</code>
       * @param value The bytes for scalaVersion to set.
       * @return This builder for chaining.
       */
      public Builder setScalaVersionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        scalaVersion_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RuntimeInfo)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RuntimeInfo)
    private static final org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RuntimeInfo>
        PARSER = new com.google.protobuf.AbstractParser<RuntimeInfo>() {
      @java.lang.Override
      public RuntimeInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RuntimeInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RuntimeInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RuntimeInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface PairStringsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.PairStrings)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string value1 = 1;</code>
     * @return Whether the value1 field is set.
     */
    boolean hasValue1();
    /**
     * <code>string value1 = 1;</code>
     * @return The value1.
     */
    java.lang.String getValue1();
    /**
     * <code>string value1 = 1;</code>
     * @return The bytes for value1.
     */
    com.google.protobuf.ByteString
        getValue1Bytes();

    /**
     * <code>string value2 = 2;</code>
     * @return Whether the value2 field is set.
     */
    boolean hasValue2();
    /**
     * <code>string value2 = 2;</code>
     * @return The value2.
     */
    java.lang.String getValue2();
    /**
     * <code>string value2 = 2;</code>
     * @return The bytes for value2.
     */
    com.google.protobuf.ByteString
        getValue2Bytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.PairStrings}
   */
  public static final class PairStrings extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.PairStrings)
      PairStringsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use PairStrings.newBuilder() to construct.
    private PairStrings(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private PairStrings() {
      value1_ = "";
      value2_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new PairStrings();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private PairStrings(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              value1_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              value2_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PairStrings_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PairStrings_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.PairStrings.class, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder.class);
    }

    private int bitField0_;
    public static final int VALUE1_FIELD_NUMBER = 1;
    private volatile java.lang.Object value1_;
    /**
     * <code>string value1 = 1;</code>
     * @return Whether the value1 field is set.
     */
    @java.lang.Override
    public boolean hasValue1() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string value1 = 1;</code>
     * @return The value1.
     */
    @java.lang.Override
    public java.lang.String getValue1() {
      java.lang.Object ref = value1_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        value1_ = s;
        return s;
      }
    }
    /**
     * <code>string value1 = 1;</code>
     * @return The bytes for value1.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getValue1Bytes() {
      java.lang.Object ref = value1_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        value1_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VALUE2_FIELD_NUMBER = 2;
    private volatile java.lang.Object value2_;
    /**
     * <code>string value2 = 2;</code>
     * @return Whether the value2 field is set.
     */
    @java.lang.Override
    public boolean hasValue2() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string value2 = 2;</code>
     * @return The value2.
     */
    @java.lang.Override
    public java.lang.String getValue2() {
      java.lang.Object ref = value2_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        value2_ = s;
        return s;
      }
    }
    /**
     * <code>string value2 = 2;</code>
     * @return The bytes for value2.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getValue2Bytes() {
      java.lang.Object ref = value2_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        value2_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, value1_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, value2_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, value1_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, value2_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.PairStrings)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.PairStrings other = (org.apache.spark.status.protobuf.StoreTypes.PairStrings) obj;

      if (hasValue1() != other.hasValue1()) return false;
      if (hasValue1()) {
        if (!getValue1()
            .equals(other.getValue1())) return false;
      }
      if (hasValue2() != other.hasValue2()) return false;
      if (hasValue2()) {
        if (!getValue2()
            .equals(other.getValue2())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasValue1()) {
        hash = (37 * hash) + VALUE1_FIELD_NUMBER;
        hash = (53 * hash) + getValue1().hashCode();
      }
      if (hasValue2()) {
        hash = (37 * hash) + VALUE2_FIELD_NUMBER;
        hash = (53 * hash) + getValue2().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.PairStrings prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.PairStrings}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.PairStrings)
        org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PairStrings_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PairStrings_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.PairStrings.class, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.PairStrings.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        value1_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        value2_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PairStrings_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings build() {
        org.apache.spark.status.protobuf.StoreTypes.PairStrings result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.PairStrings result = new org.apache.spark.status.protobuf.StoreTypes.PairStrings(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.value1_ = value1_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.value2_ = value2_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.PairStrings) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.PairStrings)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.PairStrings other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance()) return this;
        if (other.hasValue1()) {
          bitField0_ |= 0x00000001;
          value1_ = other.value1_;
          onChanged();
        }
        if (other.hasValue2()) {
          bitField0_ |= 0x00000002;
          value2_ = other.value2_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.PairStrings parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.PairStrings) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object value1_ = "";
      /**
       * <code>string value1 = 1;</code>
       * @return Whether the value1 field is set.
       */
      public boolean hasValue1() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string value1 = 1;</code>
       * @return The value1.
       */
      public java.lang.String getValue1() {
        java.lang.Object ref = value1_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          value1_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string value1 = 1;</code>
       * @return The bytes for value1.
       */
      public com.google.protobuf.ByteString
          getValue1Bytes() {
        java.lang.Object ref = value1_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          value1_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string value1 = 1;</code>
       * @param value The value1 to set.
       * @return This builder for chaining.
       */
      public Builder setValue1(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        value1_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string value1 = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearValue1() {
        bitField0_ = (bitField0_ & ~0x00000001);
        value1_ = getDefaultInstance().getValue1();
        onChanged();
        return this;
      }
      /**
       * <code>string value1 = 1;</code>
       * @param value The bytes for value1 to set.
       * @return This builder for chaining.
       */
      public Builder setValue1Bytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        value1_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object value2_ = "";
      /**
       * <code>string value2 = 2;</code>
       * @return Whether the value2 field is set.
       */
      public boolean hasValue2() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string value2 = 2;</code>
       * @return The value2.
       */
      public java.lang.String getValue2() {
        java.lang.Object ref = value2_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          value2_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string value2 = 2;</code>
       * @return The bytes for value2.
       */
      public com.google.protobuf.ByteString
          getValue2Bytes() {
        java.lang.Object ref = value2_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          value2_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string value2 = 2;</code>
       * @param value The value2 to set.
       * @return This builder for chaining.
       */
      public Builder setValue2(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        value2_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string value2 = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearValue2() {
        bitField0_ = (bitField0_ & ~0x00000002);
        value2_ = getDefaultInstance().getValue2();
        onChanged();
        return this;
      }
      /**
       * <code>string value2 = 2;</code>
       * @param value The bytes for value2 to set.
       * @return This builder for chaining.
       */
      public Builder setValue2Bytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        value2_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.PairStrings)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.PairStrings)
    private static final org.apache.spark.status.protobuf.StoreTypes.PairStrings DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.PairStrings();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.PairStrings getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<PairStrings>
        PARSER = new com.google.protobuf.AbstractParser<PairStrings>() {
      @java.lang.Override
      public PairStrings parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new PairStrings(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<PairStrings> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<PairStrings> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStrings getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ApplicationEnvironmentInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ApplicationEnvironmentInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
     * @return Whether the runtime field is set.
     */
    boolean hasRuntime();
    /**
     * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
     * @return The runtime.
     */
    org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo getRuntime();
    /**
     * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RuntimeInfoOrBuilder getRuntimeOrBuilder();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> 
        getSparkPropertiesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStrings getSparkProperties(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    int getSparkPropertiesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getSparkPropertiesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getSparkPropertiesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> 
        getHadoopPropertiesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStrings getHadoopProperties(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    int getHadoopPropertiesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getHadoopPropertiesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getHadoopPropertiesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> 
        getSystemPropertiesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStrings getSystemProperties(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    int getSystemPropertiesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getSystemPropertiesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getSystemPropertiesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> 
        getMetricsPropertiesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStrings getMetricsProperties(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    int getMetricsPropertiesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getMetricsPropertiesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getMetricsPropertiesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> 
        getClasspathEntriesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStrings getClasspathEntries(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    int getClasspathEntriesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getClasspathEntriesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getClasspathEntriesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo> 
        getResourceProfilesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getResourceProfiles(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    int getResourceProfilesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder> 
        getResourceProfilesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder getResourceProfilesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationEnvironmentInfo}
   */
  public static final class ApplicationEnvironmentInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ApplicationEnvironmentInfo)
      ApplicationEnvironmentInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ApplicationEnvironmentInfo.newBuilder() to construct.
    private ApplicationEnvironmentInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ApplicationEnvironmentInfo() {
      sparkProperties_ = java.util.Collections.emptyList();
      hadoopProperties_ = java.util.Collections.emptyList();
      systemProperties_ = java.util.Collections.emptyList();
      metricsProperties_ = java.util.Collections.emptyList();
      classpathEntries_ = java.util.Collections.emptyList();
      resourceProfiles_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ApplicationEnvironmentInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ApplicationEnvironmentInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.Builder subBuilder = null;
              if (runtime_ != null) {
                subBuilder = runtime_.toBuilder();
              }
              runtime_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(runtime_);
                runtime_ = subBuilder.buildPartial();
              }

              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                sparkProperties_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>();
                mutable_bitField0_ |= 0x00000001;
              }
              sparkProperties_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.PairStrings.parser(), extensionRegistry));
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                hadoopProperties_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>();
                mutable_bitField0_ |= 0x00000002;
              }
              hadoopProperties_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.PairStrings.parser(), extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                systemProperties_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>();
                mutable_bitField0_ |= 0x00000004;
              }
              systemProperties_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.PairStrings.parser(), extensionRegistry));
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                metricsProperties_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>();
                mutable_bitField0_ |= 0x00000008;
              }
              metricsProperties_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.PairStrings.parser(), extensionRegistry));
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                classpathEntries_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>();
                mutable_bitField0_ |= 0x00000010;
              }
              classpathEntries_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.PairStrings.parser(), extensionRegistry));
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                resourceProfiles_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo>();
                mutable_bitField0_ |= 0x00000020;
              }
              resourceProfiles_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          sparkProperties_ = java.util.Collections.unmodifiableList(sparkProperties_);
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          hadoopProperties_ = java.util.Collections.unmodifiableList(hadoopProperties_);
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          systemProperties_ = java.util.Collections.unmodifiableList(systemProperties_);
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          metricsProperties_ = java.util.Collections.unmodifiableList(metricsProperties_);
        }
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          classpathEntries_ = java.util.Collections.unmodifiableList(classpathEntries_);
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          resourceProfiles_ = java.util.Collections.unmodifiableList(resourceProfiles_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.Builder.class);
    }

    public static final int RUNTIME_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo runtime_;
    /**
     * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
     * @return Whether the runtime field is set.
     */
    @java.lang.Override
    public boolean hasRuntime() {
      return runtime_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
     * @return The runtime.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo getRuntime() {
      return runtime_ == null ? org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.getDefaultInstance() : runtime_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfoOrBuilder getRuntimeOrBuilder() {
      return getRuntime();
    }

    public static final int SPARK_PROPERTIES_FIELD_NUMBER = 2;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> sparkProperties_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getSparkPropertiesList() {
      return sparkProperties_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getSparkPropertiesOrBuilderList() {
      return sparkProperties_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    @java.lang.Override
    public int getSparkPropertiesCount() {
      return sparkProperties_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStrings getSparkProperties(int index) {
      return sparkProperties_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getSparkPropertiesOrBuilder(
        int index) {
      return sparkProperties_.get(index);
    }

    public static final int HADOOP_PROPERTIES_FIELD_NUMBER = 3;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> hadoopProperties_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getHadoopPropertiesList() {
      return hadoopProperties_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getHadoopPropertiesOrBuilderList() {
      return hadoopProperties_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    @java.lang.Override
    public int getHadoopPropertiesCount() {
      return hadoopProperties_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStrings getHadoopProperties(int index) {
      return hadoopProperties_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getHadoopPropertiesOrBuilder(
        int index) {
      return hadoopProperties_.get(index);
    }

    public static final int SYSTEM_PROPERTIES_FIELD_NUMBER = 4;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> systemProperties_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getSystemPropertiesList() {
      return systemProperties_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getSystemPropertiesOrBuilderList() {
      return systemProperties_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    @java.lang.Override
    public int getSystemPropertiesCount() {
      return systemProperties_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStrings getSystemProperties(int index) {
      return systemProperties_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getSystemPropertiesOrBuilder(
        int index) {
      return systemProperties_.get(index);
    }

    public static final int METRICS_PROPERTIES_FIELD_NUMBER = 5;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> metricsProperties_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getMetricsPropertiesList() {
      return metricsProperties_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getMetricsPropertiesOrBuilderList() {
      return metricsProperties_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    @java.lang.Override
    public int getMetricsPropertiesCount() {
      return metricsProperties_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStrings getMetricsProperties(int index) {
      return metricsProperties_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getMetricsPropertiesOrBuilder(
        int index) {
      return metricsProperties_.get(index);
    }

    public static final int CLASSPATH_ENTRIES_FIELD_NUMBER = 6;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> classpathEntries_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getClasspathEntriesList() {
      return classpathEntries_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
        getClasspathEntriesOrBuilderList() {
      return classpathEntries_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    @java.lang.Override
    public int getClasspathEntriesCount() {
      return classpathEntries_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStrings getClasspathEntries(int index) {
      return classpathEntries_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getClasspathEntriesOrBuilder(
        int index) {
      return classpathEntries_.get(index);
    }

    public static final int RESOURCE_PROFILES_FIELD_NUMBER = 7;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo> resourceProfiles_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo> getResourceProfilesList() {
      return resourceProfiles_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder> 
        getResourceProfilesOrBuilderList() {
      return resourceProfiles_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    @java.lang.Override
    public int getResourceProfilesCount() {
      return resourceProfiles_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getResourceProfiles(int index) {
      return resourceProfiles_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder getResourceProfilesOrBuilder(
        int index) {
      return resourceProfiles_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (runtime_ != null) {
        output.writeMessage(1, getRuntime());
      }
      for (int i = 0; i < sparkProperties_.size(); i++) {
        output.writeMessage(2, sparkProperties_.get(i));
      }
      for (int i = 0; i < hadoopProperties_.size(); i++) {
        output.writeMessage(3, hadoopProperties_.get(i));
      }
      for (int i = 0; i < systemProperties_.size(); i++) {
        output.writeMessage(4, systemProperties_.get(i));
      }
      for (int i = 0; i < metricsProperties_.size(); i++) {
        output.writeMessage(5, metricsProperties_.get(i));
      }
      for (int i = 0; i < classpathEntries_.size(); i++) {
        output.writeMessage(6, classpathEntries_.get(i));
      }
      for (int i = 0; i < resourceProfiles_.size(); i++) {
        output.writeMessage(7, resourceProfiles_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (runtime_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRuntime());
      }
      for (int i = 0; i < sparkProperties_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, sparkProperties_.get(i));
      }
      for (int i = 0; i < hadoopProperties_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, hadoopProperties_.get(i));
      }
      for (int i = 0; i < systemProperties_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, systemProperties_.get(i));
      }
      for (int i = 0; i < metricsProperties_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, metricsProperties_.get(i));
      }
      for (int i = 0; i < classpathEntries_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, classpathEntries_.get(i));
      }
      for (int i = 0; i < resourceProfiles_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, resourceProfiles_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo other = (org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo) obj;

      if (hasRuntime() != other.hasRuntime()) return false;
      if (hasRuntime()) {
        if (!getRuntime()
            .equals(other.getRuntime())) return false;
      }
      if (!getSparkPropertiesList()
          .equals(other.getSparkPropertiesList())) return false;
      if (!getHadoopPropertiesList()
          .equals(other.getHadoopPropertiesList())) return false;
      if (!getSystemPropertiesList()
          .equals(other.getSystemPropertiesList())) return false;
      if (!getMetricsPropertiesList()
          .equals(other.getMetricsPropertiesList())) return false;
      if (!getClasspathEntriesList()
          .equals(other.getClasspathEntriesList())) return false;
      if (!getResourceProfilesList()
          .equals(other.getResourceProfilesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRuntime()) {
        hash = (37 * hash) + RUNTIME_FIELD_NUMBER;
        hash = (53 * hash) + getRuntime().hashCode();
      }
      if (getSparkPropertiesCount() > 0) {
        hash = (37 * hash) + SPARK_PROPERTIES_FIELD_NUMBER;
        hash = (53 * hash) + getSparkPropertiesList().hashCode();
      }
      if (getHadoopPropertiesCount() > 0) {
        hash = (37 * hash) + HADOOP_PROPERTIES_FIELD_NUMBER;
        hash = (53 * hash) + getHadoopPropertiesList().hashCode();
      }
      if (getSystemPropertiesCount() > 0) {
        hash = (37 * hash) + SYSTEM_PROPERTIES_FIELD_NUMBER;
        hash = (53 * hash) + getSystemPropertiesList().hashCode();
      }
      if (getMetricsPropertiesCount() > 0) {
        hash = (37 * hash) + METRICS_PROPERTIES_FIELD_NUMBER;
        hash = (53 * hash) + getMetricsPropertiesList().hashCode();
      }
      if (getClasspathEntriesCount() > 0) {
        hash = (37 * hash) + CLASSPATH_ENTRIES_FIELD_NUMBER;
        hash = (53 * hash) + getClasspathEntriesList().hashCode();
      }
      if (getResourceProfilesCount() > 0) {
        hash = (37 * hash) + RESOURCE_PROFILES_FIELD_NUMBER;
        hash = (53 * hash) + getResourceProfilesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationEnvironmentInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ApplicationEnvironmentInfo)
        org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getSparkPropertiesFieldBuilder();
          getHadoopPropertiesFieldBuilder();
          getSystemPropertiesFieldBuilder();
          getMetricsPropertiesFieldBuilder();
          getClasspathEntriesFieldBuilder();
          getResourceProfilesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (runtimeBuilder_ == null) {
          runtime_ = null;
        } else {
          runtime_ = null;
          runtimeBuilder_ = null;
        }
        if (sparkPropertiesBuilder_ == null) {
          sparkProperties_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          sparkPropertiesBuilder_.clear();
        }
        if (hadoopPropertiesBuilder_ == null) {
          hadoopProperties_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          hadoopPropertiesBuilder_.clear();
        }
        if (systemPropertiesBuilder_ == null) {
          systemProperties_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          systemPropertiesBuilder_.clear();
        }
        if (metricsPropertiesBuilder_ == null) {
          metricsProperties_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          metricsPropertiesBuilder_.clear();
        }
        if (classpathEntriesBuilder_ == null) {
          classpathEntries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          classpathEntriesBuilder_.clear();
        }
        if (resourceProfilesBuilder_ == null) {
          resourceProfiles_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          resourceProfilesBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo build() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo result = new org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo(this);
        int from_bitField0_ = bitField0_;
        if (runtimeBuilder_ == null) {
          result.runtime_ = runtime_;
        } else {
          result.runtime_ = runtimeBuilder_.build();
        }
        if (sparkPropertiesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            sparkProperties_ = java.util.Collections.unmodifiableList(sparkProperties_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.sparkProperties_ = sparkProperties_;
        } else {
          result.sparkProperties_ = sparkPropertiesBuilder_.build();
        }
        if (hadoopPropertiesBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            hadoopProperties_ = java.util.Collections.unmodifiableList(hadoopProperties_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.hadoopProperties_ = hadoopProperties_;
        } else {
          result.hadoopProperties_ = hadoopPropertiesBuilder_.build();
        }
        if (systemPropertiesBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            systemProperties_ = java.util.Collections.unmodifiableList(systemProperties_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.systemProperties_ = systemProperties_;
        } else {
          result.systemProperties_ = systemPropertiesBuilder_.build();
        }
        if (metricsPropertiesBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0)) {
            metricsProperties_ = java.util.Collections.unmodifiableList(metricsProperties_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.metricsProperties_ = metricsProperties_;
        } else {
          result.metricsProperties_ = metricsPropertiesBuilder_.build();
        }
        if (classpathEntriesBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0)) {
            classpathEntries_ = java.util.Collections.unmodifiableList(classpathEntries_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.classpathEntries_ = classpathEntries_;
        } else {
          result.classpathEntries_ = classpathEntriesBuilder_.build();
        }
        if (resourceProfilesBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0)) {
            resourceProfiles_ = java.util.Collections.unmodifiableList(resourceProfiles_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.resourceProfiles_ = resourceProfiles_;
        } else {
          result.resourceProfiles_ = resourceProfilesBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.getDefaultInstance()) return this;
        if (other.hasRuntime()) {
          mergeRuntime(other.getRuntime());
        }
        if (sparkPropertiesBuilder_ == null) {
          if (!other.sparkProperties_.isEmpty()) {
            if (sparkProperties_.isEmpty()) {
              sparkProperties_ = other.sparkProperties_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureSparkPropertiesIsMutable();
              sparkProperties_.addAll(other.sparkProperties_);
            }
            onChanged();
          }
        } else {
          if (!other.sparkProperties_.isEmpty()) {
            if (sparkPropertiesBuilder_.isEmpty()) {
              sparkPropertiesBuilder_.dispose();
              sparkPropertiesBuilder_ = null;
              sparkProperties_ = other.sparkProperties_;
              bitField0_ = (bitField0_ & ~0x00000001);
              sparkPropertiesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getSparkPropertiesFieldBuilder() : null;
            } else {
              sparkPropertiesBuilder_.addAllMessages(other.sparkProperties_);
            }
          }
        }
        if (hadoopPropertiesBuilder_ == null) {
          if (!other.hadoopProperties_.isEmpty()) {
            if (hadoopProperties_.isEmpty()) {
              hadoopProperties_ = other.hadoopProperties_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureHadoopPropertiesIsMutable();
              hadoopProperties_.addAll(other.hadoopProperties_);
            }
            onChanged();
          }
        } else {
          if (!other.hadoopProperties_.isEmpty()) {
            if (hadoopPropertiesBuilder_.isEmpty()) {
              hadoopPropertiesBuilder_.dispose();
              hadoopPropertiesBuilder_ = null;
              hadoopProperties_ = other.hadoopProperties_;
              bitField0_ = (bitField0_ & ~0x00000002);
              hadoopPropertiesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getHadoopPropertiesFieldBuilder() : null;
            } else {
              hadoopPropertiesBuilder_.addAllMessages(other.hadoopProperties_);
            }
          }
        }
        if (systemPropertiesBuilder_ == null) {
          if (!other.systemProperties_.isEmpty()) {
            if (systemProperties_.isEmpty()) {
              systemProperties_ = other.systemProperties_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureSystemPropertiesIsMutable();
              systemProperties_.addAll(other.systemProperties_);
            }
            onChanged();
          }
        } else {
          if (!other.systemProperties_.isEmpty()) {
            if (systemPropertiesBuilder_.isEmpty()) {
              systemPropertiesBuilder_.dispose();
              systemPropertiesBuilder_ = null;
              systemProperties_ = other.systemProperties_;
              bitField0_ = (bitField0_ & ~0x00000004);
              systemPropertiesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getSystemPropertiesFieldBuilder() : null;
            } else {
              systemPropertiesBuilder_.addAllMessages(other.systemProperties_);
            }
          }
        }
        if (metricsPropertiesBuilder_ == null) {
          if (!other.metricsProperties_.isEmpty()) {
            if (metricsProperties_.isEmpty()) {
              metricsProperties_ = other.metricsProperties_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureMetricsPropertiesIsMutable();
              metricsProperties_.addAll(other.metricsProperties_);
            }
            onChanged();
          }
        } else {
          if (!other.metricsProperties_.isEmpty()) {
            if (metricsPropertiesBuilder_.isEmpty()) {
              metricsPropertiesBuilder_.dispose();
              metricsPropertiesBuilder_ = null;
              metricsProperties_ = other.metricsProperties_;
              bitField0_ = (bitField0_ & ~0x00000008);
              metricsPropertiesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getMetricsPropertiesFieldBuilder() : null;
            } else {
              metricsPropertiesBuilder_.addAllMessages(other.metricsProperties_);
            }
          }
        }
        if (classpathEntriesBuilder_ == null) {
          if (!other.classpathEntries_.isEmpty()) {
            if (classpathEntries_.isEmpty()) {
              classpathEntries_ = other.classpathEntries_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureClasspathEntriesIsMutable();
              classpathEntries_.addAll(other.classpathEntries_);
            }
            onChanged();
          }
        } else {
          if (!other.classpathEntries_.isEmpty()) {
            if (classpathEntriesBuilder_.isEmpty()) {
              classpathEntriesBuilder_.dispose();
              classpathEntriesBuilder_ = null;
              classpathEntries_ = other.classpathEntries_;
              bitField0_ = (bitField0_ & ~0x00000010);
              classpathEntriesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getClasspathEntriesFieldBuilder() : null;
            } else {
              classpathEntriesBuilder_.addAllMessages(other.classpathEntries_);
            }
          }
        }
        if (resourceProfilesBuilder_ == null) {
          if (!other.resourceProfiles_.isEmpty()) {
            if (resourceProfiles_.isEmpty()) {
              resourceProfiles_ = other.resourceProfiles_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureResourceProfilesIsMutable();
              resourceProfiles_.addAll(other.resourceProfiles_);
            }
            onChanged();
          }
        } else {
          if (!other.resourceProfiles_.isEmpty()) {
            if (resourceProfilesBuilder_.isEmpty()) {
              resourceProfilesBuilder_.dispose();
              resourceProfilesBuilder_ = null;
              resourceProfiles_ = other.resourceProfiles_;
              bitField0_ = (bitField0_ & ~0x00000020);
              resourceProfilesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getResourceProfilesFieldBuilder() : null;
            } else {
              resourceProfilesBuilder_.addAllMessages(other.resourceProfiles_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo runtime_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo, org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RuntimeInfoOrBuilder> runtimeBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       * @return Whether the runtime field is set.
       */
      public boolean hasRuntime() {
        return runtimeBuilder_ != null || runtime_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       * @return The runtime.
       */
      public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo getRuntime() {
        if (runtimeBuilder_ == null) {
          return runtime_ == null ? org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.getDefaultInstance() : runtime_;
        } else {
          return runtimeBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       */
      public Builder setRuntime(org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo value) {
        if (runtimeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          runtime_ = value;
          onChanged();
        } else {
          runtimeBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       */
      public Builder setRuntime(
          org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.Builder builderForValue) {
        if (runtimeBuilder_ == null) {
          runtime_ = builderForValue.build();
          onChanged();
        } else {
          runtimeBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       */
      public Builder mergeRuntime(org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo value) {
        if (runtimeBuilder_ == null) {
          if (runtime_ != null) {
            runtime_ =
              org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.newBuilder(runtime_).mergeFrom(value).buildPartial();
          } else {
            runtime_ = value;
          }
          onChanged();
        } else {
          runtimeBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       */
      public Builder clearRuntime() {
        if (runtimeBuilder_ == null) {
          runtime_ = null;
          onChanged();
        } else {
          runtime_ = null;
          runtimeBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.Builder getRuntimeBuilder() {
        
        onChanged();
        return getRuntimeFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RuntimeInfoOrBuilder getRuntimeOrBuilder() {
        if (runtimeBuilder_ != null) {
          return runtimeBuilder_.getMessageOrBuilder();
        } else {
          return runtime_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.getDefaultInstance() : runtime_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RuntimeInfo runtime = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo, org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RuntimeInfoOrBuilder> 
          getRuntimeFieldBuilder() {
        if (runtimeBuilder_ == null) {
          runtimeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo, org.apache.spark.status.protobuf.StoreTypes.RuntimeInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RuntimeInfoOrBuilder>(
                  getRuntime(),
                  getParentForChildren(),
                  isClean());
          runtime_ = null;
        }
        return runtimeBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> sparkProperties_ =
        java.util.Collections.emptyList();
      private void ensureSparkPropertiesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          sparkProperties_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>(sparkProperties_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> sparkPropertiesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getSparkPropertiesList() {
        if (sparkPropertiesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(sparkProperties_);
        } else {
          return sparkPropertiesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public int getSparkPropertiesCount() {
        if (sparkPropertiesBuilder_ == null) {
          return sparkProperties_.size();
        } else {
          return sparkPropertiesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings getSparkProperties(int index) {
        if (sparkPropertiesBuilder_ == null) {
          return sparkProperties_.get(index);
        } else {
          return sparkPropertiesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder setSparkProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (sparkPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSparkPropertiesIsMutable();
          sparkProperties_.set(index, value);
          onChanged();
        } else {
          sparkPropertiesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder setSparkProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (sparkPropertiesBuilder_ == null) {
          ensureSparkPropertiesIsMutable();
          sparkProperties_.set(index, builderForValue.build());
          onChanged();
        } else {
          sparkPropertiesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder addSparkProperties(org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (sparkPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSparkPropertiesIsMutable();
          sparkProperties_.add(value);
          onChanged();
        } else {
          sparkPropertiesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder addSparkProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (sparkPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSparkPropertiesIsMutable();
          sparkProperties_.add(index, value);
          onChanged();
        } else {
          sparkPropertiesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder addSparkProperties(
          org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (sparkPropertiesBuilder_ == null) {
          ensureSparkPropertiesIsMutable();
          sparkProperties_.add(builderForValue.build());
          onChanged();
        } else {
          sparkPropertiesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder addSparkProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (sparkPropertiesBuilder_ == null) {
          ensureSparkPropertiesIsMutable();
          sparkProperties_.add(index, builderForValue.build());
          onChanged();
        } else {
          sparkPropertiesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder addAllSparkProperties(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.PairStrings> values) {
        if (sparkPropertiesBuilder_ == null) {
          ensureSparkPropertiesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, sparkProperties_);
          onChanged();
        } else {
          sparkPropertiesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder clearSparkProperties() {
        if (sparkPropertiesBuilder_ == null) {
          sparkProperties_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          sparkPropertiesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public Builder removeSparkProperties(int index) {
        if (sparkPropertiesBuilder_ == null) {
          ensureSparkPropertiesIsMutable();
          sparkProperties_.remove(index);
          onChanged();
        } else {
          sparkPropertiesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder getSparkPropertiesBuilder(
          int index) {
        return getSparkPropertiesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getSparkPropertiesOrBuilder(
          int index) {
        if (sparkPropertiesBuilder_ == null) {
          return sparkProperties_.get(index);  } else {
          return sparkPropertiesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
           getSparkPropertiesOrBuilderList() {
        if (sparkPropertiesBuilder_ != null) {
          return sparkPropertiesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(sparkProperties_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addSparkPropertiesBuilder() {
        return getSparkPropertiesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addSparkPropertiesBuilder(
          int index) {
        return getSparkPropertiesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings spark_properties = 2;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder> 
           getSparkPropertiesBuilderList() {
        return getSparkPropertiesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
          getSparkPropertiesFieldBuilder() {
        if (sparkPropertiesBuilder_ == null) {
          sparkPropertiesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder>(
                  sparkProperties_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          sparkProperties_ = null;
        }
        return sparkPropertiesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> hadoopProperties_ =
        java.util.Collections.emptyList();
      private void ensureHadoopPropertiesIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          hadoopProperties_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>(hadoopProperties_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> hadoopPropertiesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getHadoopPropertiesList() {
        if (hadoopPropertiesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(hadoopProperties_);
        } else {
          return hadoopPropertiesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public int getHadoopPropertiesCount() {
        if (hadoopPropertiesBuilder_ == null) {
          return hadoopProperties_.size();
        } else {
          return hadoopPropertiesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings getHadoopProperties(int index) {
        if (hadoopPropertiesBuilder_ == null) {
          return hadoopProperties_.get(index);
        } else {
          return hadoopPropertiesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder setHadoopProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (hadoopPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureHadoopPropertiesIsMutable();
          hadoopProperties_.set(index, value);
          onChanged();
        } else {
          hadoopPropertiesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder setHadoopProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (hadoopPropertiesBuilder_ == null) {
          ensureHadoopPropertiesIsMutable();
          hadoopProperties_.set(index, builderForValue.build());
          onChanged();
        } else {
          hadoopPropertiesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder addHadoopProperties(org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (hadoopPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureHadoopPropertiesIsMutable();
          hadoopProperties_.add(value);
          onChanged();
        } else {
          hadoopPropertiesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder addHadoopProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (hadoopPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureHadoopPropertiesIsMutable();
          hadoopProperties_.add(index, value);
          onChanged();
        } else {
          hadoopPropertiesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder addHadoopProperties(
          org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (hadoopPropertiesBuilder_ == null) {
          ensureHadoopPropertiesIsMutable();
          hadoopProperties_.add(builderForValue.build());
          onChanged();
        } else {
          hadoopPropertiesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder addHadoopProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (hadoopPropertiesBuilder_ == null) {
          ensureHadoopPropertiesIsMutable();
          hadoopProperties_.add(index, builderForValue.build());
          onChanged();
        } else {
          hadoopPropertiesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder addAllHadoopProperties(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.PairStrings> values) {
        if (hadoopPropertiesBuilder_ == null) {
          ensureHadoopPropertiesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, hadoopProperties_);
          onChanged();
        } else {
          hadoopPropertiesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder clearHadoopProperties() {
        if (hadoopPropertiesBuilder_ == null) {
          hadoopProperties_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          hadoopPropertiesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public Builder removeHadoopProperties(int index) {
        if (hadoopPropertiesBuilder_ == null) {
          ensureHadoopPropertiesIsMutable();
          hadoopProperties_.remove(index);
          onChanged();
        } else {
          hadoopPropertiesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder getHadoopPropertiesBuilder(
          int index) {
        return getHadoopPropertiesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getHadoopPropertiesOrBuilder(
          int index) {
        if (hadoopPropertiesBuilder_ == null) {
          return hadoopProperties_.get(index);  } else {
          return hadoopPropertiesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
           getHadoopPropertiesOrBuilderList() {
        if (hadoopPropertiesBuilder_ != null) {
          return hadoopPropertiesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(hadoopProperties_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addHadoopPropertiesBuilder() {
        return getHadoopPropertiesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addHadoopPropertiesBuilder(
          int index) {
        return getHadoopPropertiesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings hadoop_properties = 3;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder> 
           getHadoopPropertiesBuilderList() {
        return getHadoopPropertiesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
          getHadoopPropertiesFieldBuilder() {
        if (hadoopPropertiesBuilder_ == null) {
          hadoopPropertiesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder>(
                  hadoopProperties_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          hadoopProperties_ = null;
        }
        return hadoopPropertiesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> systemProperties_ =
        java.util.Collections.emptyList();
      private void ensureSystemPropertiesIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          systemProperties_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>(systemProperties_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> systemPropertiesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getSystemPropertiesList() {
        if (systemPropertiesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(systemProperties_);
        } else {
          return systemPropertiesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public int getSystemPropertiesCount() {
        if (systemPropertiesBuilder_ == null) {
          return systemProperties_.size();
        } else {
          return systemPropertiesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings getSystemProperties(int index) {
        if (systemPropertiesBuilder_ == null) {
          return systemProperties_.get(index);
        } else {
          return systemPropertiesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder setSystemProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (systemPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSystemPropertiesIsMutable();
          systemProperties_.set(index, value);
          onChanged();
        } else {
          systemPropertiesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder setSystemProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (systemPropertiesBuilder_ == null) {
          ensureSystemPropertiesIsMutable();
          systemProperties_.set(index, builderForValue.build());
          onChanged();
        } else {
          systemPropertiesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder addSystemProperties(org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (systemPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSystemPropertiesIsMutable();
          systemProperties_.add(value);
          onChanged();
        } else {
          systemPropertiesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder addSystemProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (systemPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSystemPropertiesIsMutable();
          systemProperties_.add(index, value);
          onChanged();
        } else {
          systemPropertiesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder addSystemProperties(
          org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (systemPropertiesBuilder_ == null) {
          ensureSystemPropertiesIsMutable();
          systemProperties_.add(builderForValue.build());
          onChanged();
        } else {
          systemPropertiesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder addSystemProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (systemPropertiesBuilder_ == null) {
          ensureSystemPropertiesIsMutable();
          systemProperties_.add(index, builderForValue.build());
          onChanged();
        } else {
          systemPropertiesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder addAllSystemProperties(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.PairStrings> values) {
        if (systemPropertiesBuilder_ == null) {
          ensureSystemPropertiesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, systemProperties_);
          onChanged();
        } else {
          systemPropertiesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder clearSystemProperties() {
        if (systemPropertiesBuilder_ == null) {
          systemProperties_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          systemPropertiesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public Builder removeSystemProperties(int index) {
        if (systemPropertiesBuilder_ == null) {
          ensureSystemPropertiesIsMutable();
          systemProperties_.remove(index);
          onChanged();
        } else {
          systemPropertiesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder getSystemPropertiesBuilder(
          int index) {
        return getSystemPropertiesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getSystemPropertiesOrBuilder(
          int index) {
        if (systemPropertiesBuilder_ == null) {
          return systemProperties_.get(index);  } else {
          return systemPropertiesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
           getSystemPropertiesOrBuilderList() {
        if (systemPropertiesBuilder_ != null) {
          return systemPropertiesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(systemProperties_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addSystemPropertiesBuilder() {
        return getSystemPropertiesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addSystemPropertiesBuilder(
          int index) {
        return getSystemPropertiesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings system_properties = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder> 
           getSystemPropertiesBuilderList() {
        return getSystemPropertiesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
          getSystemPropertiesFieldBuilder() {
        if (systemPropertiesBuilder_ == null) {
          systemPropertiesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder>(
                  systemProperties_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          systemProperties_ = null;
        }
        return systemPropertiesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> metricsProperties_ =
        java.util.Collections.emptyList();
      private void ensureMetricsPropertiesIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          metricsProperties_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>(metricsProperties_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> metricsPropertiesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getMetricsPropertiesList() {
        if (metricsPropertiesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(metricsProperties_);
        } else {
          return metricsPropertiesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public int getMetricsPropertiesCount() {
        if (metricsPropertiesBuilder_ == null) {
          return metricsProperties_.size();
        } else {
          return metricsPropertiesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings getMetricsProperties(int index) {
        if (metricsPropertiesBuilder_ == null) {
          return metricsProperties_.get(index);
        } else {
          return metricsPropertiesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder setMetricsProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (metricsPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsPropertiesIsMutable();
          metricsProperties_.set(index, value);
          onChanged();
        } else {
          metricsPropertiesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder setMetricsProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (metricsPropertiesBuilder_ == null) {
          ensureMetricsPropertiesIsMutable();
          metricsProperties_.set(index, builderForValue.build());
          onChanged();
        } else {
          metricsPropertiesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder addMetricsProperties(org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (metricsPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsPropertiesIsMutable();
          metricsProperties_.add(value);
          onChanged();
        } else {
          metricsPropertiesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder addMetricsProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (metricsPropertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsPropertiesIsMutable();
          metricsProperties_.add(index, value);
          onChanged();
        } else {
          metricsPropertiesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder addMetricsProperties(
          org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (metricsPropertiesBuilder_ == null) {
          ensureMetricsPropertiesIsMutable();
          metricsProperties_.add(builderForValue.build());
          onChanged();
        } else {
          metricsPropertiesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder addMetricsProperties(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (metricsPropertiesBuilder_ == null) {
          ensureMetricsPropertiesIsMutable();
          metricsProperties_.add(index, builderForValue.build());
          onChanged();
        } else {
          metricsPropertiesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder addAllMetricsProperties(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.PairStrings> values) {
        if (metricsPropertiesBuilder_ == null) {
          ensureMetricsPropertiesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, metricsProperties_);
          onChanged();
        } else {
          metricsPropertiesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder clearMetricsProperties() {
        if (metricsPropertiesBuilder_ == null) {
          metricsProperties_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          metricsPropertiesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public Builder removeMetricsProperties(int index) {
        if (metricsPropertiesBuilder_ == null) {
          ensureMetricsPropertiesIsMutable();
          metricsProperties_.remove(index);
          onChanged();
        } else {
          metricsPropertiesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder getMetricsPropertiesBuilder(
          int index) {
        return getMetricsPropertiesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getMetricsPropertiesOrBuilder(
          int index) {
        if (metricsPropertiesBuilder_ == null) {
          return metricsProperties_.get(index);  } else {
          return metricsPropertiesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
           getMetricsPropertiesOrBuilderList() {
        if (metricsPropertiesBuilder_ != null) {
          return metricsPropertiesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(metricsProperties_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addMetricsPropertiesBuilder() {
        return getMetricsPropertiesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addMetricsPropertiesBuilder(
          int index) {
        return getMetricsPropertiesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings metrics_properties = 5;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder> 
           getMetricsPropertiesBuilderList() {
        return getMetricsPropertiesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
          getMetricsPropertiesFieldBuilder() {
        if (metricsPropertiesBuilder_ == null) {
          metricsPropertiesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder>(
                  metricsProperties_,
                  ((bitField0_ & 0x00000008) != 0),
                  getParentForChildren(),
                  isClean());
          metricsProperties_ = null;
        }
        return metricsPropertiesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> classpathEntries_ =
        java.util.Collections.emptyList();
      private void ensureClasspathEntriesIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          classpathEntries_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.PairStrings>(classpathEntries_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> classpathEntriesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings> getClasspathEntriesList() {
        if (classpathEntriesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(classpathEntries_);
        } else {
          return classpathEntriesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public int getClasspathEntriesCount() {
        if (classpathEntriesBuilder_ == null) {
          return classpathEntries_.size();
        } else {
          return classpathEntriesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings getClasspathEntries(int index) {
        if (classpathEntriesBuilder_ == null) {
          return classpathEntries_.get(index);
        } else {
          return classpathEntriesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder setClasspathEntries(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (classpathEntriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClasspathEntriesIsMutable();
          classpathEntries_.set(index, value);
          onChanged();
        } else {
          classpathEntriesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder setClasspathEntries(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (classpathEntriesBuilder_ == null) {
          ensureClasspathEntriesIsMutable();
          classpathEntries_.set(index, builderForValue.build());
          onChanged();
        } else {
          classpathEntriesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder addClasspathEntries(org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (classpathEntriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClasspathEntriesIsMutable();
          classpathEntries_.add(value);
          onChanged();
        } else {
          classpathEntriesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder addClasspathEntries(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings value) {
        if (classpathEntriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClasspathEntriesIsMutable();
          classpathEntries_.add(index, value);
          onChanged();
        } else {
          classpathEntriesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder addClasspathEntries(
          org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (classpathEntriesBuilder_ == null) {
          ensureClasspathEntriesIsMutable();
          classpathEntries_.add(builderForValue.build());
          onChanged();
        } else {
          classpathEntriesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder addClasspathEntries(
          int index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder builderForValue) {
        if (classpathEntriesBuilder_ == null) {
          ensureClasspathEntriesIsMutable();
          classpathEntries_.add(index, builderForValue.build());
          onChanged();
        } else {
          classpathEntriesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder addAllClasspathEntries(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.PairStrings> values) {
        if (classpathEntriesBuilder_ == null) {
          ensureClasspathEntriesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, classpathEntries_);
          onChanged();
        } else {
          classpathEntriesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder clearClasspathEntries() {
        if (classpathEntriesBuilder_ == null) {
          classpathEntries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          classpathEntriesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public Builder removeClasspathEntries(int index) {
        if (classpathEntriesBuilder_ == null) {
          ensureClasspathEntriesIsMutable();
          classpathEntries_.remove(index);
          onChanged();
        } else {
          classpathEntriesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder getClasspathEntriesBuilder(
          int index) {
        return getClasspathEntriesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder getClasspathEntriesOrBuilder(
          int index) {
        if (classpathEntriesBuilder_ == null) {
          return classpathEntries_.get(index);  } else {
          return classpathEntriesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
           getClasspathEntriesOrBuilderList() {
        if (classpathEntriesBuilder_ != null) {
          return classpathEntriesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(classpathEntries_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addClasspathEntriesBuilder() {
        return getClasspathEntriesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder addClasspathEntriesBuilder(
          int index) {
        return getClasspathEntriesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.PairStrings.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.PairStrings classpath_entries = 6;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder> 
           getClasspathEntriesBuilderList() {
        return getClasspathEntriesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder> 
          getClasspathEntriesFieldBuilder() {
        if (classpathEntriesBuilder_ == null) {
          classpathEntriesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.PairStrings, org.apache.spark.status.protobuf.StoreTypes.PairStrings.Builder, org.apache.spark.status.protobuf.StoreTypes.PairStringsOrBuilder>(
                  classpathEntries_,
                  ((bitField0_ & 0x00000010) != 0),
                  getParentForChildren(),
                  isClean());
          classpathEntries_ = null;
        }
        return classpathEntriesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo> resourceProfiles_ =
        java.util.Collections.emptyList();
      private void ensureResourceProfilesIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          resourceProfiles_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo>(resourceProfiles_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder> resourceProfilesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo> getResourceProfilesList() {
        if (resourceProfilesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(resourceProfiles_);
        } else {
          return resourceProfilesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public int getResourceProfilesCount() {
        if (resourceProfilesBuilder_ == null) {
          return resourceProfiles_.size();
        } else {
          return resourceProfilesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getResourceProfiles(int index) {
        if (resourceProfilesBuilder_ == null) {
          return resourceProfiles_.get(index);
        } else {
          return resourceProfilesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder setResourceProfiles(
          int index, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo value) {
        if (resourceProfilesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourceProfilesIsMutable();
          resourceProfiles_.set(index, value);
          onChanged();
        } else {
          resourceProfilesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder setResourceProfiles(
          int index, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder builderForValue) {
        if (resourceProfilesBuilder_ == null) {
          ensureResourceProfilesIsMutable();
          resourceProfiles_.set(index, builderForValue.build());
          onChanged();
        } else {
          resourceProfilesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder addResourceProfiles(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo value) {
        if (resourceProfilesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourceProfilesIsMutable();
          resourceProfiles_.add(value);
          onChanged();
        } else {
          resourceProfilesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder addResourceProfiles(
          int index, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo value) {
        if (resourceProfilesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourceProfilesIsMutable();
          resourceProfiles_.add(index, value);
          onChanged();
        } else {
          resourceProfilesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder addResourceProfiles(
          org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder builderForValue) {
        if (resourceProfilesBuilder_ == null) {
          ensureResourceProfilesIsMutable();
          resourceProfiles_.add(builderForValue.build());
          onChanged();
        } else {
          resourceProfilesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder addResourceProfiles(
          int index, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder builderForValue) {
        if (resourceProfilesBuilder_ == null) {
          ensureResourceProfilesIsMutable();
          resourceProfiles_.add(index, builderForValue.build());
          onChanged();
        } else {
          resourceProfilesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder addAllResourceProfiles(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo> values) {
        if (resourceProfilesBuilder_ == null) {
          ensureResourceProfilesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, resourceProfiles_);
          onChanged();
        } else {
          resourceProfilesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder clearResourceProfiles() {
        if (resourceProfilesBuilder_ == null) {
          resourceProfiles_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          resourceProfilesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public Builder removeResourceProfiles(int index) {
        if (resourceProfilesBuilder_ == null) {
          ensureResourceProfilesIsMutable();
          resourceProfiles_.remove(index);
          onChanged();
        } else {
          resourceProfilesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder getResourceProfilesBuilder(
          int index) {
        return getResourceProfilesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder getResourceProfilesOrBuilder(
          int index) {
        if (resourceProfilesBuilder_ == null) {
          return resourceProfiles_.get(index);  } else {
          return resourceProfilesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder> 
           getResourceProfilesOrBuilderList() {
        if (resourceProfilesBuilder_ != null) {
          return resourceProfilesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(resourceProfiles_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder addResourceProfilesBuilder() {
        return getResourceProfilesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder addResourceProfilesBuilder(
          int index) {
        return getResourceProfilesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ResourceProfileInfo resource_profiles = 7;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder> 
           getResourceProfilesBuilderList() {
        return getResourceProfilesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder> 
          getResourceProfilesFieldBuilder() {
        if (resourceProfilesBuilder_ == null) {
          resourceProfilesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder>(
                  resourceProfiles_,
                  ((bitField0_ & 0x00000020) != 0),
                  getParentForChildren(),
                  isClean());
          resourceProfiles_ = null;
        }
        return resourceProfilesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ApplicationEnvironmentInfo)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ApplicationEnvironmentInfo)
    private static final org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ApplicationEnvironmentInfo>
        PARSER = new com.google.protobuf.AbstractParser<ApplicationEnvironmentInfo>() {
      @java.lang.Override
      public ApplicationEnvironmentInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ApplicationEnvironmentInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ApplicationEnvironmentInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ApplicationEnvironmentInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ApplicationEnvironmentInfoWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ApplicationEnvironmentInfoWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoOrBuilder getInfoOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationEnvironmentInfoWrapper}
   */
  public static final class ApplicationEnvironmentInfoWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ApplicationEnvironmentInfoWrapper)
      ApplicationEnvironmentInfoWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ApplicationEnvironmentInfoWrapper.newBuilder() to construct.
    private ApplicationEnvironmentInfoWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ApplicationEnvironmentInfoWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ApplicationEnvironmentInfoWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ApplicationEnvironmentInfoWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper.Builder.class);
    }

    public static final int INFO_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo info_;
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (info_ != null) {
        output.writeMessage(1, getInfo());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getInfo());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper other = (org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper) obj;

      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationEnvironmentInfoWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ApplicationEnvironmentInfoWrapper)
        org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper result = new org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper(this);
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper.getDefaultInstance()) return this;
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationEnvironmentInfo info = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ApplicationEnvironmentInfoWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ApplicationEnvironmentInfoWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ApplicationEnvironmentInfoWrapper>
        PARSER = new com.google.protobuf.AbstractParser<ApplicationEnvironmentInfoWrapper>() {
      @java.lang.Override
      public ApplicationEnvironmentInfoWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ApplicationEnvironmentInfoWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ApplicationEnvironmentInfoWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ApplicationEnvironmentInfoWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationEnvironmentInfoWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ApplicationAttemptInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ApplicationAttemptInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string attempt_id = 1;</code>
     * @return Whether the attemptId field is set.
     */
    boolean hasAttemptId();
    /**
     * <code>string attempt_id = 1;</code>
     * @return The attemptId.
     */
    java.lang.String getAttemptId();
    /**
     * <code>string attempt_id = 1;</code>
     * @return The bytes for attemptId.
     */
    com.google.protobuf.ByteString
        getAttemptIdBytes();

    /**
     * <code>int64 start_time = 2;</code>
     * @return The startTime.
     */
    long getStartTime();

    /**
     * <code>int64 end_time = 3;</code>
     * @return The endTime.
     */
    long getEndTime();

    /**
     * <code>int64 last_updated = 4;</code>
     * @return The lastUpdated.
     */
    long getLastUpdated();

    /**
     * <code>int64 duration = 5;</code>
     * @return The duration.
     */
    long getDuration();

    /**
     * <code>string spark_user = 6;</code>
     * @return Whether the sparkUser field is set.
     */
    boolean hasSparkUser();
    /**
     * <code>string spark_user = 6;</code>
     * @return The sparkUser.
     */
    java.lang.String getSparkUser();
    /**
     * <code>string spark_user = 6;</code>
     * @return The bytes for sparkUser.
     */
    com.google.protobuf.ByteString
        getSparkUserBytes();

    /**
     * <code>bool completed = 7;</code>
     * @return The completed.
     */
    boolean getCompleted();

    /**
     * <code>string app_spark_version = 8;</code>
     * @return Whether the appSparkVersion field is set.
     */
    boolean hasAppSparkVersion();
    /**
     * <code>string app_spark_version = 8;</code>
     * @return The appSparkVersion.
     */
    java.lang.String getAppSparkVersion();
    /**
     * <code>string app_spark_version = 8;</code>
     * @return The bytes for appSparkVersion.
     */
    com.google.protobuf.ByteString
        getAppSparkVersionBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationAttemptInfo}
   */
  public static final class ApplicationAttemptInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ApplicationAttemptInfo)
      ApplicationAttemptInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ApplicationAttemptInfo.newBuilder() to construct.
    private ApplicationAttemptInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ApplicationAttemptInfo() {
      attemptId_ = "";
      sparkUser_ = "";
      appSparkVersion_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ApplicationAttemptInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ApplicationAttemptInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              attemptId_ = s;
              break;
            }
            case 16: {

              startTime_ = input.readInt64();
              break;
            }
            case 24: {

              endTime_ = input.readInt64();
              break;
            }
            case 32: {

              lastUpdated_ = input.readInt64();
              break;
            }
            case 40: {

              duration_ = input.readInt64();
              break;
            }
            case 50: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              sparkUser_ = s;
              break;
            }
            case 56: {

              completed_ = input.readBool();
              break;
            }
            case 66: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              appSparkVersion_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder.class);
    }

    private int bitField0_;
    public static final int ATTEMPT_ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object attemptId_;
    /**
     * <code>string attempt_id = 1;</code>
     * @return Whether the attemptId field is set.
     */
    @java.lang.Override
    public boolean hasAttemptId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string attempt_id = 1;</code>
     * @return The attemptId.
     */
    @java.lang.Override
    public java.lang.String getAttemptId() {
      java.lang.Object ref = attemptId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        attemptId_ = s;
        return s;
      }
    }
    /**
     * <code>string attempt_id = 1;</code>
     * @return The bytes for attemptId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getAttemptIdBytes() {
      java.lang.Object ref = attemptId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        attemptId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int START_TIME_FIELD_NUMBER = 2;
    private long startTime_;
    /**
     * <code>int64 start_time = 2;</code>
     * @return The startTime.
     */
    @java.lang.Override
    public long getStartTime() {
      return startTime_;
    }

    public static final int END_TIME_FIELD_NUMBER = 3;
    private long endTime_;
    /**
     * <code>int64 end_time = 3;</code>
     * @return The endTime.
     */
    @java.lang.Override
    public long getEndTime() {
      return endTime_;
    }

    public static final int LAST_UPDATED_FIELD_NUMBER = 4;
    private long lastUpdated_;
    /**
     * <code>int64 last_updated = 4;</code>
     * @return The lastUpdated.
     */
    @java.lang.Override
    public long getLastUpdated() {
      return lastUpdated_;
    }

    public static final int DURATION_FIELD_NUMBER = 5;
    private long duration_;
    /**
     * <code>int64 duration = 5;</code>
     * @return The duration.
     */
    @java.lang.Override
    public long getDuration() {
      return duration_;
    }

    public static final int SPARK_USER_FIELD_NUMBER = 6;
    private volatile java.lang.Object sparkUser_;
    /**
     * <code>string spark_user = 6;</code>
     * @return Whether the sparkUser field is set.
     */
    @java.lang.Override
    public boolean hasSparkUser() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string spark_user = 6;</code>
     * @return The sparkUser.
     */
    @java.lang.Override
    public java.lang.String getSparkUser() {
      java.lang.Object ref = sparkUser_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        sparkUser_ = s;
        return s;
      }
    }
    /**
     * <code>string spark_user = 6;</code>
     * @return The bytes for sparkUser.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getSparkUserBytes() {
      java.lang.Object ref = sparkUser_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        sparkUser_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int COMPLETED_FIELD_NUMBER = 7;
    private boolean completed_;
    /**
     * <code>bool completed = 7;</code>
     * @return The completed.
     */
    @java.lang.Override
    public boolean getCompleted() {
      return completed_;
    }

    public static final int APP_SPARK_VERSION_FIELD_NUMBER = 8;
    private volatile java.lang.Object appSparkVersion_;
    /**
     * <code>string app_spark_version = 8;</code>
     * @return Whether the appSparkVersion field is set.
     */
    @java.lang.Override
    public boolean hasAppSparkVersion() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string app_spark_version = 8;</code>
     * @return The appSparkVersion.
     */
    @java.lang.Override
    public java.lang.String getAppSparkVersion() {
      java.lang.Object ref = appSparkVersion_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        appSparkVersion_ = s;
        return s;
      }
    }
    /**
     * <code>string app_spark_version = 8;</code>
     * @return The bytes for appSparkVersion.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getAppSparkVersionBytes() {
      java.lang.Object ref = appSparkVersion_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        appSparkVersion_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, attemptId_);
      }
      if (startTime_ != 0L) {
        output.writeInt64(2, startTime_);
      }
      if (endTime_ != 0L) {
        output.writeInt64(3, endTime_);
      }
      if (lastUpdated_ != 0L) {
        output.writeInt64(4, lastUpdated_);
      }
      if (duration_ != 0L) {
        output.writeInt64(5, duration_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 6, sparkUser_);
      }
      if (completed_ != false) {
        output.writeBool(7, completed_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, appSparkVersion_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, attemptId_);
      }
      if (startTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, startTime_);
      }
      if (endTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, endTime_);
      }
      if (lastUpdated_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, lastUpdated_);
      }
      if (duration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, duration_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(6, sparkUser_);
      }
      if (completed_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(7, completed_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, appSparkVersion_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo other = (org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo) obj;

      if (hasAttemptId() != other.hasAttemptId()) return false;
      if (hasAttemptId()) {
        if (!getAttemptId()
            .equals(other.getAttemptId())) return false;
      }
      if (getStartTime()
          != other.getStartTime()) return false;
      if (getEndTime()
          != other.getEndTime()) return false;
      if (getLastUpdated()
          != other.getLastUpdated()) return false;
      if (getDuration()
          != other.getDuration()) return false;
      if (hasSparkUser() != other.hasSparkUser()) return false;
      if (hasSparkUser()) {
        if (!getSparkUser()
            .equals(other.getSparkUser())) return false;
      }
      if (getCompleted()
          != other.getCompleted()) return false;
      if (hasAppSparkVersion() != other.hasAppSparkVersion()) return false;
      if (hasAppSparkVersion()) {
        if (!getAppSparkVersion()
            .equals(other.getAppSparkVersion())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAttemptId()) {
        hash = (37 * hash) + ATTEMPT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getAttemptId().hashCode();
      }
      hash = (37 * hash) + START_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getStartTime());
      hash = (37 * hash) + END_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getEndTime());
      hash = (37 * hash) + LAST_UPDATED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getLastUpdated());
      hash = (37 * hash) + DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDuration());
      if (hasSparkUser()) {
        hash = (37 * hash) + SPARK_USER_FIELD_NUMBER;
        hash = (53 * hash) + getSparkUser().hashCode();
      }
      hash = (37 * hash) + COMPLETED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getCompleted());
      if (hasAppSparkVersion()) {
        hash = (37 * hash) + APP_SPARK_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getAppSparkVersion().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationAttemptInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ApplicationAttemptInfo)
        org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        attemptId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        startTime_ = 0L;

        endTime_ = 0L;

        lastUpdated_ = 0L;

        duration_ = 0L;

        sparkUser_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        completed_ = false;

        appSparkVersion_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo build() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo result = new org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.attemptId_ = attemptId_;
        result.startTime_ = startTime_;
        result.endTime_ = endTime_;
        result.lastUpdated_ = lastUpdated_;
        result.duration_ = duration_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.sparkUser_ = sparkUser_;
        result.completed_ = completed_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.appSparkVersion_ = appSparkVersion_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.getDefaultInstance()) return this;
        if (other.hasAttemptId()) {
          bitField0_ |= 0x00000001;
          attemptId_ = other.attemptId_;
          onChanged();
        }
        if (other.getStartTime() != 0L) {
          setStartTime(other.getStartTime());
        }
        if (other.getEndTime() != 0L) {
          setEndTime(other.getEndTime());
        }
        if (other.getLastUpdated() != 0L) {
          setLastUpdated(other.getLastUpdated());
        }
        if (other.getDuration() != 0L) {
          setDuration(other.getDuration());
        }
        if (other.hasSparkUser()) {
          bitField0_ |= 0x00000002;
          sparkUser_ = other.sparkUser_;
          onChanged();
        }
        if (other.getCompleted() != false) {
          setCompleted(other.getCompleted());
        }
        if (other.hasAppSparkVersion()) {
          bitField0_ |= 0x00000004;
          appSparkVersion_ = other.appSparkVersion_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object attemptId_ = "";
      /**
       * <code>string attempt_id = 1;</code>
       * @return Whether the attemptId field is set.
       */
      public boolean hasAttemptId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string attempt_id = 1;</code>
       * @return The attemptId.
       */
      public java.lang.String getAttemptId() {
        java.lang.Object ref = attemptId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          attemptId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string attempt_id = 1;</code>
       * @return The bytes for attemptId.
       */
      public com.google.protobuf.ByteString
          getAttemptIdBytes() {
        java.lang.Object ref = attemptId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          attemptId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string attempt_id = 1;</code>
       * @param value The attemptId to set.
       * @return This builder for chaining.
       */
      public Builder setAttemptId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        attemptId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string attempt_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearAttemptId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        attemptId_ = getDefaultInstance().getAttemptId();
        onChanged();
        return this;
      }
      /**
       * <code>string attempt_id = 1;</code>
       * @param value The bytes for attemptId to set.
       * @return This builder for chaining.
       */
      public Builder setAttemptIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        attemptId_ = value;
        onChanged();
        return this;
      }

      private long startTime_ ;
      /**
       * <code>int64 start_time = 2;</code>
       * @return The startTime.
       */
      @java.lang.Override
      public long getStartTime() {
        return startTime_;
      }
      /**
       * <code>int64 start_time = 2;</code>
       * @param value The startTime to set.
       * @return This builder for chaining.
       */
      public Builder setStartTime(long value) {
        
        startTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 start_time = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearStartTime() {
        
        startTime_ = 0L;
        onChanged();
        return this;
      }

      private long endTime_ ;
      /**
       * <code>int64 end_time = 3;</code>
       * @return The endTime.
       */
      @java.lang.Override
      public long getEndTime() {
        return endTime_;
      }
      /**
       * <code>int64 end_time = 3;</code>
       * @param value The endTime to set.
       * @return This builder for chaining.
       */
      public Builder setEndTime(long value) {
        
        endTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 end_time = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearEndTime() {
        
        endTime_ = 0L;
        onChanged();
        return this;
      }

      private long lastUpdated_ ;
      /**
       * <code>int64 last_updated = 4;</code>
       * @return The lastUpdated.
       */
      @java.lang.Override
      public long getLastUpdated() {
        return lastUpdated_;
      }
      /**
       * <code>int64 last_updated = 4;</code>
       * @param value The lastUpdated to set.
       * @return This builder for chaining.
       */
      public Builder setLastUpdated(long value) {
        
        lastUpdated_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 last_updated = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearLastUpdated() {
        
        lastUpdated_ = 0L;
        onChanged();
        return this;
      }

      private long duration_ ;
      /**
       * <code>int64 duration = 5;</code>
       * @return The duration.
       */
      @java.lang.Override
      public long getDuration() {
        return duration_;
      }
      /**
       * <code>int64 duration = 5;</code>
       * @param value The duration to set.
       * @return This builder for chaining.
       */
      public Builder setDuration(long value) {
        
        duration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 duration = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearDuration() {
        
        duration_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object sparkUser_ = "";
      /**
       * <code>string spark_user = 6;</code>
       * @return Whether the sparkUser field is set.
       */
      public boolean hasSparkUser() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string spark_user = 6;</code>
       * @return The sparkUser.
       */
      public java.lang.String getSparkUser() {
        java.lang.Object ref = sparkUser_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          sparkUser_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string spark_user = 6;</code>
       * @return The bytes for sparkUser.
       */
      public com.google.protobuf.ByteString
          getSparkUserBytes() {
        java.lang.Object ref = sparkUser_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          sparkUser_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string spark_user = 6;</code>
       * @param value The sparkUser to set.
       * @return This builder for chaining.
       */
      public Builder setSparkUser(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        sparkUser_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string spark_user = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearSparkUser() {
        bitField0_ = (bitField0_ & ~0x00000002);
        sparkUser_ = getDefaultInstance().getSparkUser();
        onChanged();
        return this;
      }
      /**
       * <code>string spark_user = 6;</code>
       * @param value The bytes for sparkUser to set.
       * @return This builder for chaining.
       */
      public Builder setSparkUserBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        sparkUser_ = value;
        onChanged();
        return this;
      }

      private boolean completed_ ;
      /**
       * <code>bool completed = 7;</code>
       * @return The completed.
       */
      @java.lang.Override
      public boolean getCompleted() {
        return completed_;
      }
      /**
       * <code>bool completed = 7;</code>
       * @param value The completed to set.
       * @return This builder for chaining.
       */
      public Builder setCompleted(boolean value) {
        
        completed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool completed = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearCompleted() {
        
        completed_ = false;
        onChanged();
        return this;
      }

      private java.lang.Object appSparkVersion_ = "";
      /**
       * <code>string app_spark_version = 8;</code>
       * @return Whether the appSparkVersion field is set.
       */
      public boolean hasAppSparkVersion() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string app_spark_version = 8;</code>
       * @return The appSparkVersion.
       */
      public java.lang.String getAppSparkVersion() {
        java.lang.Object ref = appSparkVersion_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          appSparkVersion_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string app_spark_version = 8;</code>
       * @return The bytes for appSparkVersion.
       */
      public com.google.protobuf.ByteString
          getAppSparkVersionBytes() {
        java.lang.Object ref = appSparkVersion_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          appSparkVersion_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string app_spark_version = 8;</code>
       * @param value The appSparkVersion to set.
       * @return This builder for chaining.
       */
      public Builder setAppSparkVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        appSparkVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string app_spark_version = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearAppSparkVersion() {
        bitField0_ = (bitField0_ & ~0x00000004);
        appSparkVersion_ = getDefaultInstance().getAppSparkVersion();
        onChanged();
        return this;
      }
      /**
       * <code>string app_spark_version = 8;</code>
       * @param value The bytes for appSparkVersion to set.
       * @return This builder for chaining.
       */
      public Builder setAppSparkVersionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        appSparkVersion_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ApplicationAttemptInfo)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ApplicationAttemptInfo)
    private static final org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ApplicationAttemptInfo>
        PARSER = new com.google.protobuf.AbstractParser<ApplicationAttemptInfo>() {
      @java.lang.Override
      public ApplicationAttemptInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ApplicationAttemptInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ApplicationAttemptInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ApplicationAttemptInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ApplicationInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ApplicationInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    boolean hasId();
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    java.lang.String getId();
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    com.google.protobuf.ByteString
        getIdBytes();

    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>int32 cores_granted = 3;</code>
     * @return Whether the coresGranted field is set.
     */
    boolean hasCoresGranted();
    /**
     * <code>int32 cores_granted = 3;</code>
     * @return The coresGranted.
     */
    int getCoresGranted();

    /**
     * <code>int32 max_cores = 4;</code>
     * @return Whether the maxCores field is set.
     */
    boolean hasMaxCores();
    /**
     * <code>int32 max_cores = 4;</code>
     * @return The maxCores.
     */
    int getMaxCores();

    /**
     * <code>int32 cores_per_executor = 5;</code>
     * @return Whether the coresPerExecutor field is set.
     */
    boolean hasCoresPerExecutor();
    /**
     * <code>int32 cores_per_executor = 5;</code>
     * @return The coresPerExecutor.
     */
    int getCoresPerExecutor();

    /**
     * <code>int32 memory_per_executor_mb = 6;</code>
     * @return Whether the memoryPerExecutorMb field is set.
     */
    boolean hasMemoryPerExecutorMb();
    /**
     * <code>int32 memory_per_executor_mb = 6;</code>
     * @return The memoryPerExecutorMb.
     */
    int getMemoryPerExecutorMb();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo> 
        getAttemptsList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo getAttempts(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    int getAttemptsCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder> 
        getAttemptsOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder getAttemptsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationInfo}
   */
  public static final class ApplicationInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ApplicationInfo)
      ApplicationInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ApplicationInfo.newBuilder() to construct.
    private ApplicationInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ApplicationInfo() {
      id_ = "";
      name_ = "";
      attempts_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ApplicationInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ApplicationInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              id_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              name_ = s;
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              coresGranted_ = input.readInt32();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              maxCores_ = input.readInt32();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              coresPerExecutor_ = input.readInt32();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              memoryPerExecutorMb_ = input.readInt32();
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                attempts_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo>();
                mutable_bitField0_ |= 0x00000040;
              }
              attempts_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          attempts_ = java.util.Collections.unmodifiableList(attempts_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfo_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object id_;
    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    @java.lang.Override
    public boolean hasId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public java.lang.String getId() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        id_ = s;
        return s;
      }
    }
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getIdBytes() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        id_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int CORES_GRANTED_FIELD_NUMBER = 3;
    private int coresGranted_;
    /**
     * <code>int32 cores_granted = 3;</code>
     * @return Whether the coresGranted field is set.
     */
    @java.lang.Override
    public boolean hasCoresGranted() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>int32 cores_granted = 3;</code>
     * @return The coresGranted.
     */
    @java.lang.Override
    public int getCoresGranted() {
      return coresGranted_;
    }

    public static final int MAX_CORES_FIELD_NUMBER = 4;
    private int maxCores_;
    /**
     * <code>int32 max_cores = 4;</code>
     * @return Whether the maxCores field is set.
     */
    @java.lang.Override
    public boolean hasMaxCores() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>int32 max_cores = 4;</code>
     * @return The maxCores.
     */
    @java.lang.Override
    public int getMaxCores() {
      return maxCores_;
    }

    public static final int CORES_PER_EXECUTOR_FIELD_NUMBER = 5;
    private int coresPerExecutor_;
    /**
     * <code>int32 cores_per_executor = 5;</code>
     * @return Whether the coresPerExecutor field is set.
     */
    @java.lang.Override
    public boolean hasCoresPerExecutor() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>int32 cores_per_executor = 5;</code>
     * @return The coresPerExecutor.
     */
    @java.lang.Override
    public int getCoresPerExecutor() {
      return coresPerExecutor_;
    }

    public static final int MEMORY_PER_EXECUTOR_MB_FIELD_NUMBER = 6;
    private int memoryPerExecutorMb_;
    /**
     * <code>int32 memory_per_executor_mb = 6;</code>
     * @return Whether the memoryPerExecutorMb field is set.
     */
    @java.lang.Override
    public boolean hasMemoryPerExecutorMb() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>int32 memory_per_executor_mb = 6;</code>
     * @return The memoryPerExecutorMb.
     */
    @java.lang.Override
    public int getMemoryPerExecutorMb() {
      return memoryPerExecutorMb_;
    }

    public static final int ATTEMPTS_FIELD_NUMBER = 7;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo> attempts_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo> getAttemptsList() {
      return attempts_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder> 
        getAttemptsOrBuilderList() {
      return attempts_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    @java.lang.Override
    public int getAttemptsCount() {
      return attempts_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo getAttempts(int index) {
      return attempts_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder getAttemptsOrBuilder(
        int index) {
      return attempts_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, name_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt32(3, coresGranted_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt32(4, maxCores_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt32(5, coresPerExecutor_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeInt32(6, memoryPerExecutorMb_);
      }
      for (int i = 0; i < attempts_.size(); i++) {
        output.writeMessage(7, attempts_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, name_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, coresGranted_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, maxCores_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(5, coresPerExecutor_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(6, memoryPerExecutorMb_);
      }
      for (int i = 0; i < attempts_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, attempts_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo other = (org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo) obj;

      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (!getId()
            .equals(other.getId())) return false;
      }
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasCoresGranted() != other.hasCoresGranted()) return false;
      if (hasCoresGranted()) {
        if (getCoresGranted()
            != other.getCoresGranted()) return false;
      }
      if (hasMaxCores() != other.hasMaxCores()) return false;
      if (hasMaxCores()) {
        if (getMaxCores()
            != other.getMaxCores()) return false;
      }
      if (hasCoresPerExecutor() != other.hasCoresPerExecutor()) return false;
      if (hasCoresPerExecutor()) {
        if (getCoresPerExecutor()
            != other.getCoresPerExecutor()) return false;
      }
      if (hasMemoryPerExecutorMb() != other.hasMemoryPerExecutorMb()) return false;
      if (hasMemoryPerExecutorMb()) {
        if (getMemoryPerExecutorMb()
            != other.getMemoryPerExecutorMb()) return false;
      }
      if (!getAttemptsList()
          .equals(other.getAttemptsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId().hashCode();
      }
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasCoresGranted()) {
        hash = (37 * hash) + CORES_GRANTED_FIELD_NUMBER;
        hash = (53 * hash) + getCoresGranted();
      }
      if (hasMaxCores()) {
        hash = (37 * hash) + MAX_CORES_FIELD_NUMBER;
        hash = (53 * hash) + getMaxCores();
      }
      if (hasCoresPerExecutor()) {
        hash = (37 * hash) + CORES_PER_EXECUTOR_FIELD_NUMBER;
        hash = (53 * hash) + getCoresPerExecutor();
      }
      if (hasMemoryPerExecutorMb()) {
        hash = (37 * hash) + MEMORY_PER_EXECUTOR_MB_FIELD_NUMBER;
        hash = (53 * hash) + getMemoryPerExecutorMb();
      }
      if (getAttemptsCount() > 0) {
        hash = (37 * hash) + ATTEMPTS_FIELD_NUMBER;
        hash = (53 * hash) + getAttemptsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ApplicationInfo)
        org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfo_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAttemptsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        coresGranted_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        maxCores_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        coresPerExecutor_ = 0;
        bitField0_ = (bitField0_ & ~0x00000010);
        memoryPerExecutorMb_ = 0;
        bitField0_ = (bitField0_ & ~0x00000020);
        if (attemptsBuilder_ == null) {
          attempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
        } else {
          attemptsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo build() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo result = new org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.coresGranted_ = coresGranted_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.maxCores_ = maxCores_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.coresPerExecutor_ = coresPerExecutor_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.memoryPerExecutorMb_ = memoryPerExecutorMb_;
          to_bitField0_ |= 0x00000020;
        }
        if (attemptsBuilder_ == null) {
          if (((bitField0_ & 0x00000040) != 0)) {
            attempts_ = java.util.Collections.unmodifiableList(attempts_);
            bitField0_ = (bitField0_ & ~0x00000040);
          }
          result.attempts_ = attempts_;
        } else {
          result.attempts_ = attemptsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.getDefaultInstance()) return this;
        if (other.hasId()) {
          bitField0_ |= 0x00000001;
          id_ = other.id_;
          onChanged();
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000002;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasCoresGranted()) {
          setCoresGranted(other.getCoresGranted());
        }
        if (other.hasMaxCores()) {
          setMaxCores(other.getMaxCores());
        }
        if (other.hasCoresPerExecutor()) {
          setCoresPerExecutor(other.getCoresPerExecutor());
        }
        if (other.hasMemoryPerExecutorMb()) {
          setMemoryPerExecutorMb(other.getMemoryPerExecutorMb());
        }
        if (attemptsBuilder_ == null) {
          if (!other.attempts_.isEmpty()) {
            if (attempts_.isEmpty()) {
              attempts_ = other.attempts_;
              bitField0_ = (bitField0_ & ~0x00000040);
            } else {
              ensureAttemptsIsMutable();
              attempts_.addAll(other.attempts_);
            }
            onChanged();
          }
        } else {
          if (!other.attempts_.isEmpty()) {
            if (attemptsBuilder_.isEmpty()) {
              attemptsBuilder_.dispose();
              attemptsBuilder_ = null;
              attempts_ = other.attempts_;
              bitField0_ = (bitField0_ & ~0x00000040);
              attemptsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getAttemptsFieldBuilder() : null;
            } else {
              attemptsBuilder_.addAllMessages(other.attempts_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object id_ = "";
      /**
       * <code>string id = 1;</code>
       * @return Whether the id field is set.
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string id = 1;</code>
       * @return The id.
       */
      public java.lang.String getId() {
        java.lang.Object ref = id_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          id_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @return The bytes for id.
       */
      public com.google.protobuf.ByteString
          getIdBytes() {
        java.lang.Object ref = id_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          id_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = getDefaultInstance().getId();
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @param value The bytes for id to set.
       * @return This builder for chaining.
       */
      public Builder setIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 2;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string name = 2;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        name_ = value;
        onChanged();
        return this;
      }

      private int coresGranted_ ;
      /**
       * <code>int32 cores_granted = 3;</code>
       * @return Whether the coresGranted field is set.
       */
      @java.lang.Override
      public boolean hasCoresGranted() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>int32 cores_granted = 3;</code>
       * @return The coresGranted.
       */
      @java.lang.Override
      public int getCoresGranted() {
        return coresGranted_;
      }
      /**
       * <code>int32 cores_granted = 3;</code>
       * @param value The coresGranted to set.
       * @return This builder for chaining.
       */
      public Builder setCoresGranted(int value) {
        bitField0_ |= 0x00000004;
        coresGranted_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 cores_granted = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearCoresGranted() {
        bitField0_ = (bitField0_ & ~0x00000004);
        coresGranted_ = 0;
        onChanged();
        return this;
      }

      private int maxCores_ ;
      /**
       * <code>int32 max_cores = 4;</code>
       * @return Whether the maxCores field is set.
       */
      @java.lang.Override
      public boolean hasMaxCores() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>int32 max_cores = 4;</code>
       * @return The maxCores.
       */
      @java.lang.Override
      public int getMaxCores() {
        return maxCores_;
      }
      /**
       * <code>int32 max_cores = 4;</code>
       * @param value The maxCores to set.
       * @return This builder for chaining.
       */
      public Builder setMaxCores(int value) {
        bitField0_ |= 0x00000008;
        maxCores_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 max_cores = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxCores() {
        bitField0_ = (bitField0_ & ~0x00000008);
        maxCores_ = 0;
        onChanged();
        return this;
      }

      private int coresPerExecutor_ ;
      /**
       * <code>int32 cores_per_executor = 5;</code>
       * @return Whether the coresPerExecutor field is set.
       */
      @java.lang.Override
      public boolean hasCoresPerExecutor() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>int32 cores_per_executor = 5;</code>
       * @return The coresPerExecutor.
       */
      @java.lang.Override
      public int getCoresPerExecutor() {
        return coresPerExecutor_;
      }
      /**
       * <code>int32 cores_per_executor = 5;</code>
       * @param value The coresPerExecutor to set.
       * @return This builder for chaining.
       */
      public Builder setCoresPerExecutor(int value) {
        bitField0_ |= 0x00000010;
        coresPerExecutor_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 cores_per_executor = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearCoresPerExecutor() {
        bitField0_ = (bitField0_ & ~0x00000010);
        coresPerExecutor_ = 0;
        onChanged();
        return this;
      }

      private int memoryPerExecutorMb_ ;
      /**
       * <code>int32 memory_per_executor_mb = 6;</code>
       * @return Whether the memoryPerExecutorMb field is set.
       */
      @java.lang.Override
      public boolean hasMemoryPerExecutorMb() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>int32 memory_per_executor_mb = 6;</code>
       * @return The memoryPerExecutorMb.
       */
      @java.lang.Override
      public int getMemoryPerExecutorMb() {
        return memoryPerExecutorMb_;
      }
      /**
       * <code>int32 memory_per_executor_mb = 6;</code>
       * @param value The memoryPerExecutorMb to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryPerExecutorMb(int value) {
        bitField0_ |= 0x00000020;
        memoryPerExecutorMb_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 memory_per_executor_mb = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryPerExecutorMb() {
        bitField0_ = (bitField0_ & ~0x00000020);
        memoryPerExecutorMb_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo> attempts_ =
        java.util.Collections.emptyList();
      private void ensureAttemptsIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          attempts_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo>(attempts_);
          bitField0_ |= 0x00000040;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder> attemptsBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo> getAttemptsList() {
        if (attemptsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(attempts_);
        } else {
          return attemptsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public int getAttemptsCount() {
        if (attemptsBuilder_ == null) {
          return attempts_.size();
        } else {
          return attemptsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo getAttempts(int index) {
        if (attemptsBuilder_ == null) {
          return attempts_.get(index);
        } else {
          return attemptsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder setAttempts(
          int index, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo value) {
        if (attemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAttemptsIsMutable();
          attempts_.set(index, value);
          onChanged();
        } else {
          attemptsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder setAttempts(
          int index, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder builderForValue) {
        if (attemptsBuilder_ == null) {
          ensureAttemptsIsMutable();
          attempts_.set(index, builderForValue.build());
          onChanged();
        } else {
          attemptsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder addAttempts(org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo value) {
        if (attemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAttemptsIsMutable();
          attempts_.add(value);
          onChanged();
        } else {
          attemptsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder addAttempts(
          int index, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo value) {
        if (attemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAttemptsIsMutable();
          attempts_.add(index, value);
          onChanged();
        } else {
          attemptsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder addAttempts(
          org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder builderForValue) {
        if (attemptsBuilder_ == null) {
          ensureAttemptsIsMutable();
          attempts_.add(builderForValue.build());
          onChanged();
        } else {
          attemptsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder addAttempts(
          int index, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder builderForValue) {
        if (attemptsBuilder_ == null) {
          ensureAttemptsIsMutable();
          attempts_.add(index, builderForValue.build());
          onChanged();
        } else {
          attemptsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder addAllAttempts(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo> values) {
        if (attemptsBuilder_ == null) {
          ensureAttemptsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, attempts_);
          onChanged();
        } else {
          attemptsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder clearAttempts() {
        if (attemptsBuilder_ == null) {
          attempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
          onChanged();
        } else {
          attemptsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public Builder removeAttempts(int index) {
        if (attemptsBuilder_ == null) {
          ensureAttemptsIsMutable();
          attempts_.remove(index);
          onChanged();
        } else {
          attemptsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder getAttemptsBuilder(
          int index) {
        return getAttemptsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder getAttemptsOrBuilder(
          int index) {
        if (attemptsBuilder_ == null) {
          return attempts_.get(index);  } else {
          return attemptsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder> 
           getAttemptsOrBuilderList() {
        if (attemptsBuilder_ != null) {
          return attemptsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(attempts_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder addAttemptsBuilder() {
        return getAttemptsFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder addAttemptsBuilder(
          int index) {
        return getAttemptsFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ApplicationAttemptInfo attempts = 7;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder> 
           getAttemptsBuilderList() {
        return getAttemptsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder> 
          getAttemptsFieldBuilder() {
        if (attemptsBuilder_ == null) {
          attemptsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationAttemptInfoOrBuilder>(
                  attempts_,
                  ((bitField0_ & 0x00000040) != 0),
                  getParentForChildren(),
                  isClean());
          attempts_ = null;
        }
        return attemptsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ApplicationInfo)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ApplicationInfo)
    private static final org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ApplicationInfo>
        PARSER = new com.google.protobuf.AbstractParser<ApplicationInfo>() {
      @java.lang.Override
      public ApplicationInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ApplicationInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ApplicationInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ApplicationInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ApplicationInfoWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ApplicationInfoWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoOrBuilder getInfoOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationInfoWrapper}
   */
  public static final class ApplicationInfoWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ApplicationInfoWrapper)
      ApplicationInfoWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ApplicationInfoWrapper.newBuilder() to construct.
    private ApplicationInfoWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ApplicationInfoWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ApplicationInfoWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ApplicationInfoWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper.Builder.class);
    }

    public static final int INFO_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo info_;
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (info_ != null) {
        output.writeMessage(1, getInfo());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getInfo());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper other = (org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper) obj;

      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ApplicationInfoWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ApplicationInfoWrapper)
        org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper result = new org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper(this);
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper.getDefaultInstance()) return this;
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ApplicationInfo info = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ApplicationInfoWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ApplicationInfoWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ApplicationInfoWrapper>
        PARSER = new com.google.protobuf.AbstractParser<ApplicationInfoWrapper>() {
      @java.lang.Override
      public ApplicationInfoWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ApplicationInfoWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ApplicationInfoWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ApplicationInfoWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ApplicationInfoWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StreamBlockDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.StreamBlockData)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>string executor_id = 2;</code>
     * @return Whether the executorId field is set.
     */
    boolean hasExecutorId();
    /**
     * <code>string executor_id = 2;</code>
     * @return The executorId.
     */
    java.lang.String getExecutorId();
    /**
     * <code>string executor_id = 2;</code>
     * @return The bytes for executorId.
     */
    com.google.protobuf.ByteString
        getExecutorIdBytes();

    /**
     * <code>string host_port = 3;</code>
     * @return Whether the hostPort field is set.
     */
    boolean hasHostPort();
    /**
     * <code>string host_port = 3;</code>
     * @return The hostPort.
     */
    java.lang.String getHostPort();
    /**
     * <code>string host_port = 3;</code>
     * @return The bytes for hostPort.
     */
    com.google.protobuf.ByteString
        getHostPortBytes();

    /**
     * <code>string storage_level = 4;</code>
     * @return Whether the storageLevel field is set.
     */
    boolean hasStorageLevel();
    /**
     * <code>string storage_level = 4;</code>
     * @return The storageLevel.
     */
    java.lang.String getStorageLevel();
    /**
     * <code>string storage_level = 4;</code>
     * @return The bytes for storageLevel.
     */
    com.google.protobuf.ByteString
        getStorageLevelBytes();

    /**
     * <code>bool use_memory = 5;</code>
     * @return The useMemory.
     */
    boolean getUseMemory();

    /**
     * <code>bool use_disk = 6;</code>
     * @return The useDisk.
     */
    boolean getUseDisk();

    /**
     * <code>bool deserialized = 7;</code>
     * @return The deserialized.
     */
    boolean getDeserialized();

    /**
     * <code>int64 mem_size = 8;</code>
     * @return The memSize.
     */
    long getMemSize();

    /**
     * <code>int64 disk_size = 9;</code>
     * @return The diskSize.
     */
    long getDiskSize();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.StreamBlockData}
   */
  public static final class StreamBlockData extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.StreamBlockData)
      StreamBlockDataOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StreamBlockData.newBuilder() to construct.
    private StreamBlockData(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StreamBlockData() {
      name_ = "";
      executorId_ = "";
      hostPort_ = "";
      storageLevel_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StreamBlockData();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StreamBlockData(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              executorId_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              hostPort_ = s;
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000008;
              storageLevel_ = s;
              break;
            }
            case 40: {

              useMemory_ = input.readBool();
              break;
            }
            case 48: {

              useDisk_ = input.readBool();
              break;
            }
            case 56: {

              deserialized_ = input.readBool();
              break;
            }
            case 64: {

              memSize_ = input.readInt64();
              break;
            }
            case 72: {

              diskSize_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamBlockData_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamBlockData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.StreamBlockData.class, org.apache.spark.status.protobuf.StoreTypes.StreamBlockData.Builder.class);
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int EXECUTOR_ID_FIELD_NUMBER = 2;
    private volatile java.lang.Object executorId_;
    /**
     * <code>string executor_id = 2;</code>
     * @return Whether the executorId field is set.
     */
    @java.lang.Override
    public boolean hasExecutorId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string executor_id = 2;</code>
     * @return The executorId.
     */
    @java.lang.Override
    public java.lang.String getExecutorId() {
      java.lang.Object ref = executorId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        executorId_ = s;
        return s;
      }
    }
    /**
     * <code>string executor_id = 2;</code>
     * @return The bytes for executorId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getExecutorIdBytes() {
      java.lang.Object ref = executorId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        executorId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int HOST_PORT_FIELD_NUMBER = 3;
    private volatile java.lang.Object hostPort_;
    /**
     * <code>string host_port = 3;</code>
     * @return Whether the hostPort field is set.
     */
    @java.lang.Override
    public boolean hasHostPort() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string host_port = 3;</code>
     * @return The hostPort.
     */
    @java.lang.Override
    public java.lang.String getHostPort() {
      java.lang.Object ref = hostPort_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        hostPort_ = s;
        return s;
      }
    }
    /**
     * <code>string host_port = 3;</code>
     * @return The bytes for hostPort.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getHostPortBytes() {
      java.lang.Object ref = hostPort_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        hostPort_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int STORAGE_LEVEL_FIELD_NUMBER = 4;
    private volatile java.lang.Object storageLevel_;
    /**
     * <code>string storage_level = 4;</code>
     * @return Whether the storageLevel field is set.
     */
    @java.lang.Override
    public boolean hasStorageLevel() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>string storage_level = 4;</code>
     * @return The storageLevel.
     */
    @java.lang.Override
    public java.lang.String getStorageLevel() {
      java.lang.Object ref = storageLevel_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        storageLevel_ = s;
        return s;
      }
    }
    /**
     * <code>string storage_level = 4;</code>
     * @return The bytes for storageLevel.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStorageLevelBytes() {
      java.lang.Object ref = storageLevel_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        storageLevel_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int USE_MEMORY_FIELD_NUMBER = 5;
    private boolean useMemory_;
    /**
     * <code>bool use_memory = 5;</code>
     * @return The useMemory.
     */
    @java.lang.Override
    public boolean getUseMemory() {
      return useMemory_;
    }

    public static final int USE_DISK_FIELD_NUMBER = 6;
    private boolean useDisk_;
    /**
     * <code>bool use_disk = 6;</code>
     * @return The useDisk.
     */
    @java.lang.Override
    public boolean getUseDisk() {
      return useDisk_;
    }

    public static final int DESERIALIZED_FIELD_NUMBER = 7;
    private boolean deserialized_;
    /**
     * <code>bool deserialized = 7;</code>
     * @return The deserialized.
     */
    @java.lang.Override
    public boolean getDeserialized() {
      return deserialized_;
    }

    public static final int MEM_SIZE_FIELD_NUMBER = 8;
    private long memSize_;
    /**
     * <code>int64 mem_size = 8;</code>
     * @return The memSize.
     */
    @java.lang.Override
    public long getMemSize() {
      return memSize_;
    }

    public static final int DISK_SIZE_FIELD_NUMBER = 9;
    private long diskSize_;
    /**
     * <code>int64 disk_size = 9;</code>
     * @return The diskSize.
     */
    @java.lang.Override
    public long getDiskSize() {
      return diskSize_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, executorId_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, hostPort_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, storageLevel_);
      }
      if (useMemory_ != false) {
        output.writeBool(5, useMemory_);
      }
      if (useDisk_ != false) {
        output.writeBool(6, useDisk_);
      }
      if (deserialized_ != false) {
        output.writeBool(7, deserialized_);
      }
      if (memSize_ != 0L) {
        output.writeInt64(8, memSize_);
      }
      if (diskSize_ != 0L) {
        output.writeInt64(9, diskSize_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, executorId_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, hostPort_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, storageLevel_);
      }
      if (useMemory_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(5, useMemory_);
      }
      if (useDisk_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(6, useDisk_);
      }
      if (deserialized_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(7, deserialized_);
      }
      if (memSize_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, memSize_);
      }
      if (diskSize_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(9, diskSize_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.StreamBlockData)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.StreamBlockData other = (org.apache.spark.status.protobuf.StoreTypes.StreamBlockData) obj;

      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasExecutorId() != other.hasExecutorId()) return false;
      if (hasExecutorId()) {
        if (!getExecutorId()
            .equals(other.getExecutorId())) return false;
      }
      if (hasHostPort() != other.hasHostPort()) return false;
      if (hasHostPort()) {
        if (!getHostPort()
            .equals(other.getHostPort())) return false;
      }
      if (hasStorageLevel() != other.hasStorageLevel()) return false;
      if (hasStorageLevel()) {
        if (!getStorageLevel()
            .equals(other.getStorageLevel())) return false;
      }
      if (getUseMemory()
          != other.getUseMemory()) return false;
      if (getUseDisk()
          != other.getUseDisk()) return false;
      if (getDeserialized()
          != other.getDeserialized()) return false;
      if (getMemSize()
          != other.getMemSize()) return false;
      if (getDiskSize()
          != other.getDiskSize()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasExecutorId()) {
        hash = (37 * hash) + EXECUTOR_ID_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorId().hashCode();
      }
      if (hasHostPort()) {
        hash = (37 * hash) + HOST_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getHostPort().hashCode();
      }
      if (hasStorageLevel()) {
        hash = (37 * hash) + STORAGE_LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getStorageLevel().hashCode();
      }
      hash = (37 * hash) + USE_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getUseMemory());
      hash = (37 * hash) + USE_DISK_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getUseDisk());
      hash = (37 * hash) + DESERIALIZED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getDeserialized());
      hash = (37 * hash) + MEM_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemSize());
      hash = (37 * hash) + DISK_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskSize());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.StreamBlockData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.StreamBlockData}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.StreamBlockData)
        org.apache.spark.status.protobuf.StoreTypes.StreamBlockDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamBlockData_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamBlockData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.StreamBlockData.class, org.apache.spark.status.protobuf.StoreTypes.StreamBlockData.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.StreamBlockData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        executorId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        hostPort_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        storageLevel_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        useMemory_ = false;

        useDisk_ = false;

        deserialized_ = false;

        memSize_ = 0L;

        diskSize_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamBlockData_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamBlockData getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.StreamBlockData.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamBlockData build() {
        org.apache.spark.status.protobuf.StoreTypes.StreamBlockData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamBlockData buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.StreamBlockData result = new org.apache.spark.status.protobuf.StoreTypes.StreamBlockData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.executorId_ = executorId_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.hostPort_ = hostPort_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.storageLevel_ = storageLevel_;
        result.useMemory_ = useMemory_;
        result.useDisk_ = useDisk_;
        result.deserialized_ = deserialized_;
        result.memSize_ = memSize_;
        result.diskSize_ = diskSize_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.StreamBlockData) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.StreamBlockData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.StreamBlockData other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.StreamBlockData.getDefaultInstance()) return this;
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasExecutorId()) {
          bitField0_ |= 0x00000002;
          executorId_ = other.executorId_;
          onChanged();
        }
        if (other.hasHostPort()) {
          bitField0_ |= 0x00000004;
          hostPort_ = other.hostPort_;
          onChanged();
        }
        if (other.hasStorageLevel()) {
          bitField0_ |= 0x00000008;
          storageLevel_ = other.storageLevel_;
          onChanged();
        }
        if (other.getUseMemory() != false) {
          setUseMemory(other.getUseMemory());
        }
        if (other.getUseDisk() != false) {
          setUseDisk(other.getUseDisk());
        }
        if (other.getDeserialized() != false) {
          setDeserialized(other.getDeserialized());
        }
        if (other.getMemSize() != 0L) {
          setMemSize(other.getMemSize());
        }
        if (other.getDiskSize() != 0L) {
          setDiskSize(other.getDiskSize());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.StreamBlockData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.StreamBlockData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object executorId_ = "";
      /**
       * <code>string executor_id = 2;</code>
       * @return Whether the executorId field is set.
       */
      public boolean hasExecutorId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string executor_id = 2;</code>
       * @return The executorId.
       */
      public java.lang.String getExecutorId() {
        java.lang.Object ref = executorId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          executorId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string executor_id = 2;</code>
       * @return The bytes for executorId.
       */
      public com.google.protobuf.ByteString
          getExecutorIdBytes() {
        java.lang.Object ref = executorId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          executorId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string executor_id = 2;</code>
       * @param value The executorId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        executorId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string executor_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        executorId_ = getDefaultInstance().getExecutorId();
        onChanged();
        return this;
      }
      /**
       * <code>string executor_id = 2;</code>
       * @param value The bytes for executorId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        executorId_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object hostPort_ = "";
      /**
       * <code>string host_port = 3;</code>
       * @return Whether the hostPort field is set.
       */
      public boolean hasHostPort() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string host_port = 3;</code>
       * @return The hostPort.
       */
      public java.lang.String getHostPort() {
        java.lang.Object ref = hostPort_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          hostPort_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string host_port = 3;</code>
       * @return The bytes for hostPort.
       */
      public com.google.protobuf.ByteString
          getHostPortBytes() {
        java.lang.Object ref = hostPort_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          hostPort_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string host_port = 3;</code>
       * @param value The hostPort to set.
       * @return This builder for chaining.
       */
      public Builder setHostPort(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        hostPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string host_port = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearHostPort() {
        bitField0_ = (bitField0_ & ~0x00000004);
        hostPort_ = getDefaultInstance().getHostPort();
        onChanged();
        return this;
      }
      /**
       * <code>string host_port = 3;</code>
       * @param value The bytes for hostPort to set.
       * @return This builder for chaining.
       */
      public Builder setHostPortBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        hostPort_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object storageLevel_ = "";
      /**
       * <code>string storage_level = 4;</code>
       * @return Whether the storageLevel field is set.
       */
      public boolean hasStorageLevel() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>string storage_level = 4;</code>
       * @return The storageLevel.
       */
      public java.lang.String getStorageLevel() {
        java.lang.Object ref = storageLevel_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          storageLevel_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string storage_level = 4;</code>
       * @return The bytes for storageLevel.
       */
      public com.google.protobuf.ByteString
          getStorageLevelBytes() {
        java.lang.Object ref = storageLevel_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storageLevel_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string storage_level = 4;</code>
       * @param value The storageLevel to set.
       * @return This builder for chaining.
       */
      public Builder setStorageLevel(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        storageLevel_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string storage_level = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearStorageLevel() {
        bitField0_ = (bitField0_ & ~0x00000008);
        storageLevel_ = getDefaultInstance().getStorageLevel();
        onChanged();
        return this;
      }
      /**
       * <code>string storage_level = 4;</code>
       * @param value The bytes for storageLevel to set.
       * @return This builder for chaining.
       */
      public Builder setStorageLevelBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000008;
        storageLevel_ = value;
        onChanged();
        return this;
      }

      private boolean useMemory_ ;
      /**
       * <code>bool use_memory = 5;</code>
       * @return The useMemory.
       */
      @java.lang.Override
      public boolean getUseMemory() {
        return useMemory_;
      }
      /**
       * <code>bool use_memory = 5;</code>
       * @param value The useMemory to set.
       * @return This builder for chaining.
       */
      public Builder setUseMemory(boolean value) {
        
        useMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool use_memory = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearUseMemory() {
        
        useMemory_ = false;
        onChanged();
        return this;
      }

      private boolean useDisk_ ;
      /**
       * <code>bool use_disk = 6;</code>
       * @return The useDisk.
       */
      @java.lang.Override
      public boolean getUseDisk() {
        return useDisk_;
      }
      /**
       * <code>bool use_disk = 6;</code>
       * @param value The useDisk to set.
       * @return This builder for chaining.
       */
      public Builder setUseDisk(boolean value) {
        
        useDisk_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool use_disk = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearUseDisk() {
        
        useDisk_ = false;
        onChanged();
        return this;
      }

      private boolean deserialized_ ;
      /**
       * <code>bool deserialized = 7;</code>
       * @return The deserialized.
       */
      @java.lang.Override
      public boolean getDeserialized() {
        return deserialized_;
      }
      /**
       * <code>bool deserialized = 7;</code>
       * @param value The deserialized to set.
       * @return This builder for chaining.
       */
      public Builder setDeserialized(boolean value) {
        
        deserialized_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool deserialized = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearDeserialized() {
        
        deserialized_ = false;
        onChanged();
        return this;
      }

      private long memSize_ ;
      /**
       * <code>int64 mem_size = 8;</code>
       * @return The memSize.
       */
      @java.lang.Override
      public long getMemSize() {
        return memSize_;
      }
      /**
       * <code>int64 mem_size = 8;</code>
       * @param value The memSize to set.
       * @return This builder for chaining.
       */
      public Builder setMemSize(long value) {
        
        memSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 mem_size = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemSize() {
        
        memSize_ = 0L;
        onChanged();
        return this;
      }

      private long diskSize_ ;
      /**
       * <code>int64 disk_size = 9;</code>
       * @return The diskSize.
       */
      @java.lang.Override
      public long getDiskSize() {
        return diskSize_;
      }
      /**
       * <code>int64 disk_size = 9;</code>
       * @param value The diskSize to set.
       * @return This builder for chaining.
       */
      public Builder setDiskSize(long value) {
        
        diskSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_size = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskSize() {
        
        diskSize_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.StreamBlockData)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.StreamBlockData)
    private static final org.apache.spark.status.protobuf.StoreTypes.StreamBlockData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.StreamBlockData();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StreamBlockData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StreamBlockData>
        PARSER = new com.google.protobuf.AbstractParser<StreamBlockData>() {
      @java.lang.Override
      public StreamBlockData parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StreamBlockData(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StreamBlockData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StreamBlockData> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StreamBlockData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RDDDataDistributionOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RDDDataDistribution)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string address = 1;</code>
     * @return Whether the address field is set.
     */
    boolean hasAddress();
    /**
     * <code>string address = 1;</code>
     * @return The address.
     */
    java.lang.String getAddress();
    /**
     * <code>string address = 1;</code>
     * @return The bytes for address.
     */
    com.google.protobuf.ByteString
        getAddressBytes();

    /**
     * <code>int64 memory_used = 2;</code>
     * @return The memoryUsed.
     */
    long getMemoryUsed();

    /**
     * <code>int64 memory_remaining = 3;</code>
     * @return The memoryRemaining.
     */
    long getMemoryRemaining();

    /**
     * <code>int64 disk_used = 4;</code>
     * @return The diskUsed.
     */
    long getDiskUsed();

    /**
     * <code>int64 on_heap_memory_used = 5;</code>
     * @return Whether the onHeapMemoryUsed field is set.
     */
    boolean hasOnHeapMemoryUsed();
    /**
     * <code>int64 on_heap_memory_used = 5;</code>
     * @return The onHeapMemoryUsed.
     */
    long getOnHeapMemoryUsed();

    /**
     * <code>int64 off_heap_memory_used = 6;</code>
     * @return Whether the offHeapMemoryUsed field is set.
     */
    boolean hasOffHeapMemoryUsed();
    /**
     * <code>int64 off_heap_memory_used = 6;</code>
     * @return The offHeapMemoryUsed.
     */
    long getOffHeapMemoryUsed();

    /**
     * <code>int64 on_heap_memory_remaining = 7;</code>
     * @return Whether the onHeapMemoryRemaining field is set.
     */
    boolean hasOnHeapMemoryRemaining();
    /**
     * <code>int64 on_heap_memory_remaining = 7;</code>
     * @return The onHeapMemoryRemaining.
     */
    long getOnHeapMemoryRemaining();

    /**
     * <code>int64 off_heap_memory_remaining = 8;</code>
     * @return Whether the offHeapMemoryRemaining field is set.
     */
    boolean hasOffHeapMemoryRemaining();
    /**
     * <code>int64 off_heap_memory_remaining = 8;</code>
     * @return The offHeapMemoryRemaining.
     */
    long getOffHeapMemoryRemaining();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RDDDataDistribution}
   */
  public static final class RDDDataDistribution extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RDDDataDistribution)
      RDDDataDistributionOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RDDDataDistribution.newBuilder() to construct.
    private RDDDataDistribution(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RDDDataDistribution() {
      address_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RDDDataDistribution();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RDDDataDistribution(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              address_ = s;
              break;
            }
            case 16: {

              memoryUsed_ = input.readInt64();
              break;
            }
            case 24: {

              memoryRemaining_ = input.readInt64();
              break;
            }
            case 32: {

              diskUsed_ = input.readInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000002;
              onHeapMemoryUsed_ = input.readInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000004;
              offHeapMemoryUsed_ = input.readInt64();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000008;
              onHeapMemoryRemaining_ = input.readInt64();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000010;
              offHeapMemoryRemaining_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.class, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder.class);
    }

    private int bitField0_;
    public static final int ADDRESS_FIELD_NUMBER = 1;
    private volatile java.lang.Object address_;
    /**
     * <code>string address = 1;</code>
     * @return Whether the address field is set.
     */
    @java.lang.Override
    public boolean hasAddress() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string address = 1;</code>
     * @return The address.
     */
    @java.lang.Override
    public java.lang.String getAddress() {
      java.lang.Object ref = address_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        address_ = s;
        return s;
      }
    }
    /**
     * <code>string address = 1;</code>
     * @return The bytes for address.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getAddressBytes() {
      java.lang.Object ref = address_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        address_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int MEMORY_USED_FIELD_NUMBER = 2;
    private long memoryUsed_;
    /**
     * <code>int64 memory_used = 2;</code>
     * @return The memoryUsed.
     */
    @java.lang.Override
    public long getMemoryUsed() {
      return memoryUsed_;
    }

    public static final int MEMORY_REMAINING_FIELD_NUMBER = 3;
    private long memoryRemaining_;
    /**
     * <code>int64 memory_remaining = 3;</code>
     * @return The memoryRemaining.
     */
    @java.lang.Override
    public long getMemoryRemaining() {
      return memoryRemaining_;
    }

    public static final int DISK_USED_FIELD_NUMBER = 4;
    private long diskUsed_;
    /**
     * <code>int64 disk_used = 4;</code>
     * @return The diskUsed.
     */
    @java.lang.Override
    public long getDiskUsed() {
      return diskUsed_;
    }

    public static final int ON_HEAP_MEMORY_USED_FIELD_NUMBER = 5;
    private long onHeapMemoryUsed_;
    /**
     * <code>int64 on_heap_memory_used = 5;</code>
     * @return Whether the onHeapMemoryUsed field is set.
     */
    @java.lang.Override
    public boolean hasOnHeapMemoryUsed() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>int64 on_heap_memory_used = 5;</code>
     * @return The onHeapMemoryUsed.
     */
    @java.lang.Override
    public long getOnHeapMemoryUsed() {
      return onHeapMemoryUsed_;
    }

    public static final int OFF_HEAP_MEMORY_USED_FIELD_NUMBER = 6;
    private long offHeapMemoryUsed_;
    /**
     * <code>int64 off_heap_memory_used = 6;</code>
     * @return Whether the offHeapMemoryUsed field is set.
     */
    @java.lang.Override
    public boolean hasOffHeapMemoryUsed() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>int64 off_heap_memory_used = 6;</code>
     * @return The offHeapMemoryUsed.
     */
    @java.lang.Override
    public long getOffHeapMemoryUsed() {
      return offHeapMemoryUsed_;
    }

    public static final int ON_HEAP_MEMORY_REMAINING_FIELD_NUMBER = 7;
    private long onHeapMemoryRemaining_;
    /**
     * <code>int64 on_heap_memory_remaining = 7;</code>
     * @return Whether the onHeapMemoryRemaining field is set.
     */
    @java.lang.Override
    public boolean hasOnHeapMemoryRemaining() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>int64 on_heap_memory_remaining = 7;</code>
     * @return The onHeapMemoryRemaining.
     */
    @java.lang.Override
    public long getOnHeapMemoryRemaining() {
      return onHeapMemoryRemaining_;
    }

    public static final int OFF_HEAP_MEMORY_REMAINING_FIELD_NUMBER = 8;
    private long offHeapMemoryRemaining_;
    /**
     * <code>int64 off_heap_memory_remaining = 8;</code>
     * @return Whether the offHeapMemoryRemaining field is set.
     */
    @java.lang.Override
    public boolean hasOffHeapMemoryRemaining() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>int64 off_heap_memory_remaining = 8;</code>
     * @return The offHeapMemoryRemaining.
     */
    @java.lang.Override
    public long getOffHeapMemoryRemaining() {
      return offHeapMemoryRemaining_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, address_);
      }
      if (memoryUsed_ != 0L) {
        output.writeInt64(2, memoryUsed_);
      }
      if (memoryRemaining_ != 0L) {
        output.writeInt64(3, memoryRemaining_);
      }
      if (diskUsed_ != 0L) {
        output.writeInt64(4, diskUsed_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(5, onHeapMemoryUsed_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt64(6, offHeapMemoryUsed_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(7, onHeapMemoryRemaining_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt64(8, offHeapMemoryRemaining_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, address_);
      }
      if (memoryUsed_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, memoryUsed_);
      }
      if (memoryRemaining_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, memoryRemaining_);
      }
      if (diskUsed_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, diskUsed_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, onHeapMemoryUsed_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, offHeapMemoryUsed_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, onHeapMemoryRemaining_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, offHeapMemoryRemaining_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution other = (org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution) obj;

      if (hasAddress() != other.hasAddress()) return false;
      if (hasAddress()) {
        if (!getAddress()
            .equals(other.getAddress())) return false;
      }
      if (getMemoryUsed()
          != other.getMemoryUsed()) return false;
      if (getMemoryRemaining()
          != other.getMemoryRemaining()) return false;
      if (getDiskUsed()
          != other.getDiskUsed()) return false;
      if (hasOnHeapMemoryUsed() != other.hasOnHeapMemoryUsed()) return false;
      if (hasOnHeapMemoryUsed()) {
        if (getOnHeapMemoryUsed()
            != other.getOnHeapMemoryUsed()) return false;
      }
      if (hasOffHeapMemoryUsed() != other.hasOffHeapMemoryUsed()) return false;
      if (hasOffHeapMemoryUsed()) {
        if (getOffHeapMemoryUsed()
            != other.getOffHeapMemoryUsed()) return false;
      }
      if (hasOnHeapMemoryRemaining() != other.hasOnHeapMemoryRemaining()) return false;
      if (hasOnHeapMemoryRemaining()) {
        if (getOnHeapMemoryRemaining()
            != other.getOnHeapMemoryRemaining()) return false;
      }
      if (hasOffHeapMemoryRemaining() != other.hasOffHeapMemoryRemaining()) return false;
      if (hasOffHeapMemoryRemaining()) {
        if (getOffHeapMemoryRemaining()
            != other.getOffHeapMemoryRemaining()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAddress()) {
        hash = (37 * hash) + ADDRESS_FIELD_NUMBER;
        hash = (53 * hash) + getAddress().hashCode();
      }
      hash = (37 * hash) + MEMORY_USED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryUsed());
      hash = (37 * hash) + MEMORY_REMAINING_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryRemaining());
      hash = (37 * hash) + DISK_USED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskUsed());
      if (hasOnHeapMemoryUsed()) {
        hash = (37 * hash) + ON_HEAP_MEMORY_USED_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getOnHeapMemoryUsed());
      }
      if (hasOffHeapMemoryUsed()) {
        hash = (37 * hash) + OFF_HEAP_MEMORY_USED_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getOffHeapMemoryUsed());
      }
      if (hasOnHeapMemoryRemaining()) {
        hash = (37 * hash) + ON_HEAP_MEMORY_REMAINING_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getOnHeapMemoryRemaining());
      }
      if (hasOffHeapMemoryRemaining()) {
        hash = (37 * hash) + OFF_HEAP_MEMORY_REMAINING_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getOffHeapMemoryRemaining());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RDDDataDistribution}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RDDDataDistribution)
        org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.class, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        address_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        memoryUsed_ = 0L;

        memoryRemaining_ = 0L;

        diskUsed_ = 0L;

        onHeapMemoryUsed_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        offHeapMemoryUsed_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        onHeapMemoryRemaining_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        offHeapMemoryRemaining_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution build() {
        org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution result = new org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.address_ = address_;
        result.memoryUsed_ = memoryUsed_;
        result.memoryRemaining_ = memoryRemaining_;
        result.diskUsed_ = diskUsed_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.onHeapMemoryUsed_ = onHeapMemoryUsed_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.offHeapMemoryUsed_ = offHeapMemoryUsed_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.onHeapMemoryRemaining_ = onHeapMemoryRemaining_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.offHeapMemoryRemaining_ = offHeapMemoryRemaining_;
          to_bitField0_ |= 0x00000010;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.getDefaultInstance()) return this;
        if (other.hasAddress()) {
          bitField0_ |= 0x00000001;
          address_ = other.address_;
          onChanged();
        }
        if (other.getMemoryUsed() != 0L) {
          setMemoryUsed(other.getMemoryUsed());
        }
        if (other.getMemoryRemaining() != 0L) {
          setMemoryRemaining(other.getMemoryRemaining());
        }
        if (other.getDiskUsed() != 0L) {
          setDiskUsed(other.getDiskUsed());
        }
        if (other.hasOnHeapMemoryUsed()) {
          setOnHeapMemoryUsed(other.getOnHeapMemoryUsed());
        }
        if (other.hasOffHeapMemoryUsed()) {
          setOffHeapMemoryUsed(other.getOffHeapMemoryUsed());
        }
        if (other.hasOnHeapMemoryRemaining()) {
          setOnHeapMemoryRemaining(other.getOnHeapMemoryRemaining());
        }
        if (other.hasOffHeapMemoryRemaining()) {
          setOffHeapMemoryRemaining(other.getOffHeapMemoryRemaining());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object address_ = "";
      /**
       * <code>string address = 1;</code>
       * @return Whether the address field is set.
       */
      public boolean hasAddress() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string address = 1;</code>
       * @return The address.
       */
      public java.lang.String getAddress() {
        java.lang.Object ref = address_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          address_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string address = 1;</code>
       * @return The bytes for address.
       */
      public com.google.protobuf.ByteString
          getAddressBytes() {
        java.lang.Object ref = address_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          address_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string address = 1;</code>
       * @param value The address to set.
       * @return This builder for chaining.
       */
      public Builder setAddress(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        address_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string address = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearAddress() {
        bitField0_ = (bitField0_ & ~0x00000001);
        address_ = getDefaultInstance().getAddress();
        onChanged();
        return this;
      }
      /**
       * <code>string address = 1;</code>
       * @param value The bytes for address to set.
       * @return This builder for chaining.
       */
      public Builder setAddressBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        address_ = value;
        onChanged();
        return this;
      }

      private long memoryUsed_ ;
      /**
       * <code>int64 memory_used = 2;</code>
       * @return The memoryUsed.
       */
      @java.lang.Override
      public long getMemoryUsed() {
        return memoryUsed_;
      }
      /**
       * <code>int64 memory_used = 2;</code>
       * @param value The memoryUsed to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryUsed(long value) {
        
        memoryUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_used = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryUsed() {
        
        memoryUsed_ = 0L;
        onChanged();
        return this;
      }

      private long memoryRemaining_ ;
      /**
       * <code>int64 memory_remaining = 3;</code>
       * @return The memoryRemaining.
       */
      @java.lang.Override
      public long getMemoryRemaining() {
        return memoryRemaining_;
      }
      /**
       * <code>int64 memory_remaining = 3;</code>
       * @param value The memoryRemaining to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryRemaining(long value) {
        
        memoryRemaining_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_remaining = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryRemaining() {
        
        memoryRemaining_ = 0L;
        onChanged();
        return this;
      }

      private long diskUsed_ ;
      /**
       * <code>int64 disk_used = 4;</code>
       * @return The diskUsed.
       */
      @java.lang.Override
      public long getDiskUsed() {
        return diskUsed_;
      }
      /**
       * <code>int64 disk_used = 4;</code>
       * @param value The diskUsed to set.
       * @return This builder for chaining.
       */
      public Builder setDiskUsed(long value) {
        
        diskUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_used = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskUsed() {
        
        diskUsed_ = 0L;
        onChanged();
        return this;
      }

      private long onHeapMemoryUsed_ ;
      /**
       * <code>int64 on_heap_memory_used = 5;</code>
       * @return Whether the onHeapMemoryUsed field is set.
       */
      @java.lang.Override
      public boolean hasOnHeapMemoryUsed() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>int64 on_heap_memory_used = 5;</code>
       * @return The onHeapMemoryUsed.
       */
      @java.lang.Override
      public long getOnHeapMemoryUsed() {
        return onHeapMemoryUsed_;
      }
      /**
       * <code>int64 on_heap_memory_used = 5;</code>
       * @param value The onHeapMemoryUsed to set.
       * @return This builder for chaining.
       */
      public Builder setOnHeapMemoryUsed(long value) {
        bitField0_ |= 0x00000002;
        onHeapMemoryUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 on_heap_memory_used = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearOnHeapMemoryUsed() {
        bitField0_ = (bitField0_ & ~0x00000002);
        onHeapMemoryUsed_ = 0L;
        onChanged();
        return this;
      }

      private long offHeapMemoryUsed_ ;
      /**
       * <code>int64 off_heap_memory_used = 6;</code>
       * @return Whether the offHeapMemoryUsed field is set.
       */
      @java.lang.Override
      public boolean hasOffHeapMemoryUsed() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>int64 off_heap_memory_used = 6;</code>
       * @return The offHeapMemoryUsed.
       */
      @java.lang.Override
      public long getOffHeapMemoryUsed() {
        return offHeapMemoryUsed_;
      }
      /**
       * <code>int64 off_heap_memory_used = 6;</code>
       * @param value The offHeapMemoryUsed to set.
       * @return This builder for chaining.
       */
      public Builder setOffHeapMemoryUsed(long value) {
        bitField0_ |= 0x00000004;
        offHeapMemoryUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 off_heap_memory_used = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearOffHeapMemoryUsed() {
        bitField0_ = (bitField0_ & ~0x00000004);
        offHeapMemoryUsed_ = 0L;
        onChanged();
        return this;
      }

      private long onHeapMemoryRemaining_ ;
      /**
       * <code>int64 on_heap_memory_remaining = 7;</code>
       * @return Whether the onHeapMemoryRemaining field is set.
       */
      @java.lang.Override
      public boolean hasOnHeapMemoryRemaining() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>int64 on_heap_memory_remaining = 7;</code>
       * @return The onHeapMemoryRemaining.
       */
      @java.lang.Override
      public long getOnHeapMemoryRemaining() {
        return onHeapMemoryRemaining_;
      }
      /**
       * <code>int64 on_heap_memory_remaining = 7;</code>
       * @param value The onHeapMemoryRemaining to set.
       * @return This builder for chaining.
       */
      public Builder setOnHeapMemoryRemaining(long value) {
        bitField0_ |= 0x00000008;
        onHeapMemoryRemaining_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 on_heap_memory_remaining = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearOnHeapMemoryRemaining() {
        bitField0_ = (bitField0_ & ~0x00000008);
        onHeapMemoryRemaining_ = 0L;
        onChanged();
        return this;
      }

      private long offHeapMemoryRemaining_ ;
      /**
       * <code>int64 off_heap_memory_remaining = 8;</code>
       * @return Whether the offHeapMemoryRemaining field is set.
       */
      @java.lang.Override
      public boolean hasOffHeapMemoryRemaining() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>int64 off_heap_memory_remaining = 8;</code>
       * @return The offHeapMemoryRemaining.
       */
      @java.lang.Override
      public long getOffHeapMemoryRemaining() {
        return offHeapMemoryRemaining_;
      }
      /**
       * <code>int64 off_heap_memory_remaining = 8;</code>
       * @param value The offHeapMemoryRemaining to set.
       * @return This builder for chaining.
       */
      public Builder setOffHeapMemoryRemaining(long value) {
        bitField0_ |= 0x00000010;
        offHeapMemoryRemaining_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 off_heap_memory_remaining = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearOffHeapMemoryRemaining() {
        bitField0_ = (bitField0_ & ~0x00000010);
        offHeapMemoryRemaining_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RDDDataDistribution)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RDDDataDistribution)
    private static final org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RDDDataDistribution>
        PARSER = new com.google.protobuf.AbstractParser<RDDDataDistribution>() {
      @java.lang.Override
      public RDDDataDistribution parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RDDDataDistribution(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RDDDataDistribution> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RDDDataDistribution> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RDDPartitionInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RDDPartitionInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string block_name = 1;</code>
     * @return Whether the blockName field is set.
     */
    boolean hasBlockName();
    /**
     * <code>string block_name = 1;</code>
     * @return The blockName.
     */
    java.lang.String getBlockName();
    /**
     * <code>string block_name = 1;</code>
     * @return The bytes for blockName.
     */
    com.google.protobuf.ByteString
        getBlockNameBytes();

    /**
     * <code>string storage_level = 2;</code>
     * @return Whether the storageLevel field is set.
     */
    boolean hasStorageLevel();
    /**
     * <code>string storage_level = 2;</code>
     * @return The storageLevel.
     */
    java.lang.String getStorageLevel();
    /**
     * <code>string storage_level = 2;</code>
     * @return The bytes for storageLevel.
     */
    com.google.protobuf.ByteString
        getStorageLevelBytes();

    /**
     * <code>int64 memory_used = 3;</code>
     * @return The memoryUsed.
     */
    long getMemoryUsed();

    /**
     * <code>int64 disk_used = 4;</code>
     * @return The diskUsed.
     */
    long getDiskUsed();

    /**
     * <code>repeated string executors = 5;</code>
     * @return A list containing the executors.
     */
    java.util.List<java.lang.String>
        getExecutorsList();
    /**
     * <code>repeated string executors = 5;</code>
     * @return The count of executors.
     */
    int getExecutorsCount();
    /**
     * <code>repeated string executors = 5;</code>
     * @param index The index of the element to return.
     * @return The executors at the given index.
     */
    java.lang.String getExecutors(int index);
    /**
     * <code>repeated string executors = 5;</code>
     * @param index The index of the value to return.
     * @return The bytes of the executors at the given index.
     */
    com.google.protobuf.ByteString
        getExecutorsBytes(int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RDDPartitionInfo}
   */
  public static final class RDDPartitionInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RDDPartitionInfo)
      RDDPartitionInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RDDPartitionInfo.newBuilder() to construct.
    private RDDPartitionInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RDDPartitionInfo() {
      blockName_ = "";
      storageLevel_ = "";
      executors_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RDDPartitionInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RDDPartitionInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              blockName_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              storageLevel_ = s;
              break;
            }
            case 24: {

              memoryUsed_ = input.readInt64();
              break;
            }
            case 32: {

              diskUsed_ = input.readInt64();
              break;
            }
            case 42: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                executors_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000004;
              }
              executors_.add(s);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          executors_ = executors_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.class, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder.class);
    }

    private int bitField0_;
    public static final int BLOCK_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object blockName_;
    /**
     * <code>string block_name = 1;</code>
     * @return Whether the blockName field is set.
     */
    @java.lang.Override
    public boolean hasBlockName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string block_name = 1;</code>
     * @return The blockName.
     */
    @java.lang.Override
    public java.lang.String getBlockName() {
      java.lang.Object ref = blockName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        blockName_ = s;
        return s;
      }
    }
    /**
     * <code>string block_name = 1;</code>
     * @return The bytes for blockName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBlockNameBytes() {
      java.lang.Object ref = blockName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int STORAGE_LEVEL_FIELD_NUMBER = 2;
    private volatile java.lang.Object storageLevel_;
    /**
     * <code>string storage_level = 2;</code>
     * @return Whether the storageLevel field is set.
     */
    @java.lang.Override
    public boolean hasStorageLevel() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string storage_level = 2;</code>
     * @return The storageLevel.
     */
    @java.lang.Override
    public java.lang.String getStorageLevel() {
      java.lang.Object ref = storageLevel_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        storageLevel_ = s;
        return s;
      }
    }
    /**
     * <code>string storage_level = 2;</code>
     * @return The bytes for storageLevel.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStorageLevelBytes() {
      java.lang.Object ref = storageLevel_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        storageLevel_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int MEMORY_USED_FIELD_NUMBER = 3;
    private long memoryUsed_;
    /**
     * <code>int64 memory_used = 3;</code>
     * @return The memoryUsed.
     */
    @java.lang.Override
    public long getMemoryUsed() {
      return memoryUsed_;
    }

    public static final int DISK_USED_FIELD_NUMBER = 4;
    private long diskUsed_;
    /**
     * <code>int64 disk_used = 4;</code>
     * @return The diskUsed.
     */
    @java.lang.Override
    public long getDiskUsed() {
      return diskUsed_;
    }

    public static final int EXECUTORS_FIELD_NUMBER = 5;
    private com.google.protobuf.LazyStringList executors_;
    /**
     * <code>repeated string executors = 5;</code>
     * @return A list containing the executors.
     */
    public com.google.protobuf.ProtocolStringList
        getExecutorsList() {
      return executors_;
    }
    /**
     * <code>repeated string executors = 5;</code>
     * @return The count of executors.
     */
    public int getExecutorsCount() {
      return executors_.size();
    }
    /**
     * <code>repeated string executors = 5;</code>
     * @param index The index of the element to return.
     * @return The executors at the given index.
     */
    public java.lang.String getExecutors(int index) {
      return executors_.get(index);
    }
    /**
     * <code>repeated string executors = 5;</code>
     * @param index The index of the value to return.
     * @return The bytes of the executors at the given index.
     */
    public com.google.protobuf.ByteString
        getExecutorsBytes(int index) {
      return executors_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, blockName_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, storageLevel_);
      }
      if (memoryUsed_ != 0L) {
        output.writeInt64(3, memoryUsed_);
      }
      if (diskUsed_ != 0L) {
        output.writeInt64(4, diskUsed_);
      }
      for (int i = 0; i < executors_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, executors_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, blockName_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, storageLevel_);
      }
      if (memoryUsed_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, memoryUsed_);
      }
      if (diskUsed_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, diskUsed_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < executors_.size(); i++) {
          dataSize += computeStringSizeNoTag(executors_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getExecutorsList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo other = (org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo) obj;

      if (hasBlockName() != other.hasBlockName()) return false;
      if (hasBlockName()) {
        if (!getBlockName()
            .equals(other.getBlockName())) return false;
      }
      if (hasStorageLevel() != other.hasStorageLevel()) return false;
      if (hasStorageLevel()) {
        if (!getStorageLevel()
            .equals(other.getStorageLevel())) return false;
      }
      if (getMemoryUsed()
          != other.getMemoryUsed()) return false;
      if (getDiskUsed()
          != other.getDiskUsed()) return false;
      if (!getExecutorsList()
          .equals(other.getExecutorsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasBlockName()) {
        hash = (37 * hash) + BLOCK_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getBlockName().hashCode();
      }
      if (hasStorageLevel()) {
        hash = (37 * hash) + STORAGE_LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getStorageLevel().hashCode();
      }
      hash = (37 * hash) + MEMORY_USED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryUsed());
      hash = (37 * hash) + DISK_USED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskUsed());
      if (getExecutorsCount() > 0) {
        hash = (37 * hash) + EXECUTORS_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RDDPartitionInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RDDPartitionInfo)
        org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.class, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        blockName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        storageLevel_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        memoryUsed_ = 0L;

        diskUsed_ = 0L;

        executors_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo build() {
        org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo result = new org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.blockName_ = blockName_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.storageLevel_ = storageLevel_;
        result.memoryUsed_ = memoryUsed_;
        result.diskUsed_ = diskUsed_;
        if (((bitField0_ & 0x00000004) != 0)) {
          executors_ = executors_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.executors_ = executors_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.getDefaultInstance()) return this;
        if (other.hasBlockName()) {
          bitField0_ |= 0x00000001;
          blockName_ = other.blockName_;
          onChanged();
        }
        if (other.hasStorageLevel()) {
          bitField0_ |= 0x00000002;
          storageLevel_ = other.storageLevel_;
          onChanged();
        }
        if (other.getMemoryUsed() != 0L) {
          setMemoryUsed(other.getMemoryUsed());
        }
        if (other.getDiskUsed() != 0L) {
          setDiskUsed(other.getDiskUsed());
        }
        if (!other.executors_.isEmpty()) {
          if (executors_.isEmpty()) {
            executors_ = other.executors_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureExecutorsIsMutable();
            executors_.addAll(other.executors_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object blockName_ = "";
      /**
       * <code>string block_name = 1;</code>
       * @return Whether the blockName field is set.
       */
      public boolean hasBlockName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string block_name = 1;</code>
       * @return The blockName.
       */
      public java.lang.String getBlockName() {
        java.lang.Object ref = blockName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          blockName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string block_name = 1;</code>
       * @return The bytes for blockName.
       */
      public com.google.protobuf.ByteString
          getBlockNameBytes() {
        java.lang.Object ref = blockName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string block_name = 1;</code>
       * @param value The blockName to set.
       * @return This builder for chaining.
       */
      public Builder setBlockName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        blockName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string block_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlockName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        blockName_ = getDefaultInstance().getBlockName();
        onChanged();
        return this;
      }
      /**
       * <code>string block_name = 1;</code>
       * @param value The bytes for blockName to set.
       * @return This builder for chaining.
       */
      public Builder setBlockNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        blockName_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object storageLevel_ = "";
      /**
       * <code>string storage_level = 2;</code>
       * @return Whether the storageLevel field is set.
       */
      public boolean hasStorageLevel() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string storage_level = 2;</code>
       * @return The storageLevel.
       */
      public java.lang.String getStorageLevel() {
        java.lang.Object ref = storageLevel_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          storageLevel_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string storage_level = 2;</code>
       * @return The bytes for storageLevel.
       */
      public com.google.protobuf.ByteString
          getStorageLevelBytes() {
        java.lang.Object ref = storageLevel_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storageLevel_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string storage_level = 2;</code>
       * @param value The storageLevel to set.
       * @return This builder for chaining.
       */
      public Builder setStorageLevel(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        storageLevel_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string storage_level = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearStorageLevel() {
        bitField0_ = (bitField0_ & ~0x00000002);
        storageLevel_ = getDefaultInstance().getStorageLevel();
        onChanged();
        return this;
      }
      /**
       * <code>string storage_level = 2;</code>
       * @param value The bytes for storageLevel to set.
       * @return This builder for chaining.
       */
      public Builder setStorageLevelBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        storageLevel_ = value;
        onChanged();
        return this;
      }

      private long memoryUsed_ ;
      /**
       * <code>int64 memory_used = 3;</code>
       * @return The memoryUsed.
       */
      @java.lang.Override
      public long getMemoryUsed() {
        return memoryUsed_;
      }
      /**
       * <code>int64 memory_used = 3;</code>
       * @param value The memoryUsed to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryUsed(long value) {
        
        memoryUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_used = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryUsed() {
        
        memoryUsed_ = 0L;
        onChanged();
        return this;
      }

      private long diskUsed_ ;
      /**
       * <code>int64 disk_used = 4;</code>
       * @return The diskUsed.
       */
      @java.lang.Override
      public long getDiskUsed() {
        return diskUsed_;
      }
      /**
       * <code>int64 disk_used = 4;</code>
       * @param value The diskUsed to set.
       * @return This builder for chaining.
       */
      public Builder setDiskUsed(long value) {
        
        diskUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_used = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskUsed() {
        
        diskUsed_ = 0L;
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList executors_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureExecutorsIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          executors_ = new com.google.protobuf.LazyStringArrayList(executors_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @return A list containing the executors.
       */
      public com.google.protobuf.ProtocolStringList
          getExecutorsList() {
        return executors_.getUnmodifiableView();
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @return The count of executors.
       */
      public int getExecutorsCount() {
        return executors_.size();
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @param index The index of the element to return.
       * @return The executors at the given index.
       */
      public java.lang.String getExecutors(int index) {
        return executors_.get(index);
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @param index The index of the value to return.
       * @return The bytes of the executors at the given index.
       */
      public com.google.protobuf.ByteString
          getExecutorsBytes(int index) {
        return executors_.getByteString(index);
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @param index The index to set the value at.
       * @param value The executors to set.
       * @return This builder for chaining.
       */
      public Builder setExecutors(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureExecutorsIsMutable();
        executors_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @param value The executors to add.
       * @return This builder for chaining.
       */
      public Builder addExecutors(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureExecutorsIsMutable();
        executors_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @param values The executors to add.
       * @return This builder for chaining.
       */
      public Builder addAllExecutors(
          java.lang.Iterable<java.lang.String> values) {
        ensureExecutorsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, executors_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutors() {
        executors_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string executors = 5;</code>
       * @param value The bytes of the executors to add.
       * @return This builder for chaining.
       */
      public Builder addExecutorsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureExecutorsIsMutable();
        executors_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RDDPartitionInfo)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RDDPartitionInfo)
    private static final org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RDDPartitionInfo>
        PARSER = new com.google.protobuf.AbstractParser<RDDPartitionInfo>() {
      @java.lang.Override
      public RDDPartitionInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RDDPartitionInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RDDPartitionInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RDDPartitionInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RDDStorageInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RDDStorageInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int32 id = 1;</code>
     * @return The id.
     */
    int getId();

    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>int32 num_partitions = 3;</code>
     * @return The numPartitions.
     */
    int getNumPartitions();

    /**
     * <code>int32 num_cached_partitions = 4;</code>
     * @return The numCachedPartitions.
     */
    int getNumCachedPartitions();

    /**
     * <code>string storage_level = 5;</code>
     * @return Whether the storageLevel field is set.
     */
    boolean hasStorageLevel();
    /**
     * <code>string storage_level = 5;</code>
     * @return The storageLevel.
     */
    java.lang.String getStorageLevel();
    /**
     * <code>string storage_level = 5;</code>
     * @return The bytes for storageLevel.
     */
    com.google.protobuf.ByteString
        getStorageLevelBytes();

    /**
     * <code>int64 memory_used = 6;</code>
     * @return The memoryUsed.
     */
    long getMemoryUsed();

    /**
     * <code>int64 disk_used = 7;</code>
     * @return The diskUsed.
     */
    long getDiskUsed();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution> 
        getDataDistributionList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution getDataDistribution(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    int getDataDistributionCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder> 
        getDataDistributionOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder getDataDistributionOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo> 
        getPartitionsList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo getPartitions(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    int getPartitionsCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder> 
        getPartitionsOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder getPartitionsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RDDStorageInfo}
   */
  public static final class RDDStorageInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RDDStorageInfo)
      RDDStorageInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RDDStorageInfo.newBuilder() to construct.
    private RDDStorageInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RDDStorageInfo() {
      name_ = "";
      storageLevel_ = "";
      dataDistribution_ = java.util.Collections.emptyList();
      partitions_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RDDStorageInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RDDStorageInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              id_ = input.readInt32();
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 24: {

              numPartitions_ = input.readInt32();
              break;
            }
            case 32: {

              numCachedPartitions_ = input.readInt32();
              break;
            }
            case 42: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              storageLevel_ = s;
              break;
            }
            case 48: {

              memoryUsed_ = input.readInt64();
              break;
            }
            case 56: {

              diskUsed_ = input.readInt64();
              break;
            }
            case 66: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                dataDistribution_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution>();
                mutable_bitField0_ |= 0x00000004;
              }
              dataDistribution_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.parser(), extensionRegistry));
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                partitions_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo>();
                mutable_bitField0_ |= 0x00000008;
              }
              partitions_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          dataDistribution_ = java.util.Collections.unmodifiableList(dataDistribution_);
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          partitions_ = java.util.Collections.unmodifiableList(partitions_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.class, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private int id_;
    /**
     * <code>int32 id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public int getId() {
      return id_;
    }

    public static final int NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int NUM_PARTITIONS_FIELD_NUMBER = 3;
    private int numPartitions_;
    /**
     * <code>int32 num_partitions = 3;</code>
     * @return The numPartitions.
     */
    @java.lang.Override
    public int getNumPartitions() {
      return numPartitions_;
    }

    public static final int NUM_CACHED_PARTITIONS_FIELD_NUMBER = 4;
    private int numCachedPartitions_;
    /**
     * <code>int32 num_cached_partitions = 4;</code>
     * @return The numCachedPartitions.
     */
    @java.lang.Override
    public int getNumCachedPartitions() {
      return numCachedPartitions_;
    }

    public static final int STORAGE_LEVEL_FIELD_NUMBER = 5;
    private volatile java.lang.Object storageLevel_;
    /**
     * <code>string storage_level = 5;</code>
     * @return Whether the storageLevel field is set.
     */
    @java.lang.Override
    public boolean hasStorageLevel() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string storage_level = 5;</code>
     * @return The storageLevel.
     */
    @java.lang.Override
    public java.lang.String getStorageLevel() {
      java.lang.Object ref = storageLevel_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        storageLevel_ = s;
        return s;
      }
    }
    /**
     * <code>string storage_level = 5;</code>
     * @return The bytes for storageLevel.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStorageLevelBytes() {
      java.lang.Object ref = storageLevel_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        storageLevel_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int MEMORY_USED_FIELD_NUMBER = 6;
    private long memoryUsed_;
    /**
     * <code>int64 memory_used = 6;</code>
     * @return The memoryUsed.
     */
    @java.lang.Override
    public long getMemoryUsed() {
      return memoryUsed_;
    }

    public static final int DISK_USED_FIELD_NUMBER = 7;
    private long diskUsed_;
    /**
     * <code>int64 disk_used = 7;</code>
     * @return The diskUsed.
     */
    @java.lang.Override
    public long getDiskUsed() {
      return diskUsed_;
    }

    public static final int DATA_DISTRIBUTION_FIELD_NUMBER = 8;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution> dataDistribution_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution> getDataDistributionList() {
      return dataDistribution_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder> 
        getDataDistributionOrBuilderList() {
      return dataDistribution_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    @java.lang.Override
    public int getDataDistributionCount() {
      return dataDistribution_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution getDataDistribution(int index) {
      return dataDistribution_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder getDataDistributionOrBuilder(
        int index) {
      return dataDistribution_.get(index);
    }

    public static final int PARTITIONS_FIELD_NUMBER = 9;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo> partitions_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo> getPartitionsList() {
      return partitions_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder> 
        getPartitionsOrBuilderList() {
      return partitions_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    @java.lang.Override
    public int getPartitionsCount() {
      return partitions_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo getPartitions(int index) {
      return partitions_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder getPartitionsOrBuilder(
        int index) {
      return partitions_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (id_ != 0) {
        output.writeInt32(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, name_);
      }
      if (numPartitions_ != 0) {
        output.writeInt32(3, numPartitions_);
      }
      if (numCachedPartitions_ != 0) {
        output.writeInt32(4, numCachedPartitions_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, storageLevel_);
      }
      if (memoryUsed_ != 0L) {
        output.writeInt64(6, memoryUsed_);
      }
      if (diskUsed_ != 0L) {
        output.writeInt64(7, diskUsed_);
      }
      for (int i = 0; i < dataDistribution_.size(); i++) {
        output.writeMessage(8, dataDistribution_.get(i));
      }
      for (int i = 0; i < partitions_.size(); i++) {
        output.writeMessage(9, partitions_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (id_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, name_);
      }
      if (numPartitions_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, numPartitions_);
      }
      if (numCachedPartitions_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, numCachedPartitions_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, storageLevel_);
      }
      if (memoryUsed_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, memoryUsed_);
      }
      if (diskUsed_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, diskUsed_);
      }
      for (int i = 0; i < dataDistribution_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, dataDistribution_.get(i));
      }
      for (int i = 0; i < partitions_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, partitions_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo other = (org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo) obj;

      if (getId()
          != other.getId()) return false;
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (getNumPartitions()
          != other.getNumPartitions()) return false;
      if (getNumCachedPartitions()
          != other.getNumCachedPartitions()) return false;
      if (hasStorageLevel() != other.hasStorageLevel()) return false;
      if (hasStorageLevel()) {
        if (!getStorageLevel()
            .equals(other.getStorageLevel())) return false;
      }
      if (getMemoryUsed()
          != other.getMemoryUsed()) return false;
      if (getDiskUsed()
          != other.getDiskUsed()) return false;
      if (!getDataDistributionList()
          .equals(other.getDataDistributionList())) return false;
      if (!getPartitionsList()
          .equals(other.getPartitionsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + getId();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      hash = (37 * hash) + NUM_PARTITIONS_FIELD_NUMBER;
      hash = (53 * hash) + getNumPartitions();
      hash = (37 * hash) + NUM_CACHED_PARTITIONS_FIELD_NUMBER;
      hash = (53 * hash) + getNumCachedPartitions();
      if (hasStorageLevel()) {
        hash = (37 * hash) + STORAGE_LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getStorageLevel().hashCode();
      }
      hash = (37 * hash) + MEMORY_USED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryUsed());
      hash = (37 * hash) + DISK_USED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskUsed());
      if (getDataDistributionCount() > 0) {
        hash = (37 * hash) + DATA_DISTRIBUTION_FIELD_NUMBER;
        hash = (53 * hash) + getDataDistributionList().hashCode();
      }
      if (getPartitionsCount() > 0) {
        hash = (37 * hash) + PARTITIONS_FIELD_NUMBER;
        hash = (53 * hash) + getPartitionsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RDDStorageInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RDDStorageInfo)
        org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.class, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getDataDistributionFieldBuilder();
          getPartitionsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0;

        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        numPartitions_ = 0;

        numCachedPartitions_ = 0;

        storageLevel_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        memoryUsed_ = 0L;

        diskUsed_ = 0L;

        if (dataDistributionBuilder_ == null) {
          dataDistribution_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          dataDistributionBuilder_.clear();
        }
        if (partitionsBuilder_ == null) {
          partitions_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          partitionsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo build() {
        org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo result = new org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        result.numPartitions_ = numPartitions_;
        result.numCachedPartitions_ = numCachedPartitions_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.storageLevel_ = storageLevel_;
        result.memoryUsed_ = memoryUsed_;
        result.diskUsed_ = diskUsed_;
        if (dataDistributionBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            dataDistribution_ = java.util.Collections.unmodifiableList(dataDistribution_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.dataDistribution_ = dataDistribution_;
        } else {
          result.dataDistribution_ = dataDistributionBuilder_.build();
        }
        if (partitionsBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0)) {
            partitions_ = java.util.Collections.unmodifiableList(partitions_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.partitions_ = partitions_;
        } else {
          result.partitions_ = partitionsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.getDefaultInstance()) return this;
        if (other.getId() != 0) {
          setId(other.getId());
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.getNumPartitions() != 0) {
          setNumPartitions(other.getNumPartitions());
        }
        if (other.getNumCachedPartitions() != 0) {
          setNumCachedPartitions(other.getNumCachedPartitions());
        }
        if (other.hasStorageLevel()) {
          bitField0_ |= 0x00000002;
          storageLevel_ = other.storageLevel_;
          onChanged();
        }
        if (other.getMemoryUsed() != 0L) {
          setMemoryUsed(other.getMemoryUsed());
        }
        if (other.getDiskUsed() != 0L) {
          setDiskUsed(other.getDiskUsed());
        }
        if (dataDistributionBuilder_ == null) {
          if (!other.dataDistribution_.isEmpty()) {
            if (dataDistribution_.isEmpty()) {
              dataDistribution_ = other.dataDistribution_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureDataDistributionIsMutable();
              dataDistribution_.addAll(other.dataDistribution_);
            }
            onChanged();
          }
        } else {
          if (!other.dataDistribution_.isEmpty()) {
            if (dataDistributionBuilder_.isEmpty()) {
              dataDistributionBuilder_.dispose();
              dataDistributionBuilder_ = null;
              dataDistribution_ = other.dataDistribution_;
              bitField0_ = (bitField0_ & ~0x00000004);
              dataDistributionBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getDataDistributionFieldBuilder() : null;
            } else {
              dataDistributionBuilder_.addAllMessages(other.dataDistribution_);
            }
          }
        }
        if (partitionsBuilder_ == null) {
          if (!other.partitions_.isEmpty()) {
            if (partitions_.isEmpty()) {
              partitions_ = other.partitions_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensurePartitionsIsMutable();
              partitions_.addAll(other.partitions_);
            }
            onChanged();
          }
        } else {
          if (!other.partitions_.isEmpty()) {
            if (partitionsBuilder_.isEmpty()) {
              partitionsBuilder_.dispose();
              partitionsBuilder_ = null;
              partitions_ = other.partitions_;
              bitField0_ = (bitField0_ & ~0x00000008);
              partitionsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getPartitionsFieldBuilder() : null;
            } else {
              partitionsBuilder_.addAllMessages(other.partitions_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int id_ ;
      /**
       * <code>int32 id = 1;</code>
       * @return The id.
       */
      @java.lang.Override
      public int getId() {
        return id_;
      }
      /**
       * <code>int32 id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(int value) {
        
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        
        id_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 2;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 2;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private int numPartitions_ ;
      /**
       * <code>int32 num_partitions = 3;</code>
       * @return The numPartitions.
       */
      @java.lang.Override
      public int getNumPartitions() {
        return numPartitions_;
      }
      /**
       * <code>int32 num_partitions = 3;</code>
       * @param value The numPartitions to set.
       * @return This builder for chaining.
       */
      public Builder setNumPartitions(int value) {
        
        numPartitions_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_partitions = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumPartitions() {
        
        numPartitions_ = 0;
        onChanged();
        return this;
      }

      private int numCachedPartitions_ ;
      /**
       * <code>int32 num_cached_partitions = 4;</code>
       * @return The numCachedPartitions.
       */
      @java.lang.Override
      public int getNumCachedPartitions() {
        return numCachedPartitions_;
      }
      /**
       * <code>int32 num_cached_partitions = 4;</code>
       * @param value The numCachedPartitions to set.
       * @return This builder for chaining.
       */
      public Builder setNumCachedPartitions(int value) {
        
        numCachedPartitions_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_cached_partitions = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCachedPartitions() {
        
        numCachedPartitions_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object storageLevel_ = "";
      /**
       * <code>string storage_level = 5;</code>
       * @return Whether the storageLevel field is set.
       */
      public boolean hasStorageLevel() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string storage_level = 5;</code>
       * @return The storageLevel.
       */
      public java.lang.String getStorageLevel() {
        java.lang.Object ref = storageLevel_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          storageLevel_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string storage_level = 5;</code>
       * @return The bytes for storageLevel.
       */
      public com.google.protobuf.ByteString
          getStorageLevelBytes() {
        java.lang.Object ref = storageLevel_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storageLevel_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string storage_level = 5;</code>
       * @param value The storageLevel to set.
       * @return This builder for chaining.
       */
      public Builder setStorageLevel(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        storageLevel_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string storage_level = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearStorageLevel() {
        bitField0_ = (bitField0_ & ~0x00000002);
        storageLevel_ = getDefaultInstance().getStorageLevel();
        onChanged();
        return this;
      }
      /**
       * <code>string storage_level = 5;</code>
       * @param value The bytes for storageLevel to set.
       * @return This builder for chaining.
       */
      public Builder setStorageLevelBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        storageLevel_ = value;
        onChanged();
        return this;
      }

      private long memoryUsed_ ;
      /**
       * <code>int64 memory_used = 6;</code>
       * @return The memoryUsed.
       */
      @java.lang.Override
      public long getMemoryUsed() {
        return memoryUsed_;
      }
      /**
       * <code>int64 memory_used = 6;</code>
       * @param value The memoryUsed to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryUsed(long value) {
        
        memoryUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_used = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryUsed() {
        
        memoryUsed_ = 0L;
        onChanged();
        return this;
      }

      private long diskUsed_ ;
      /**
       * <code>int64 disk_used = 7;</code>
       * @return The diskUsed.
       */
      @java.lang.Override
      public long getDiskUsed() {
        return diskUsed_;
      }
      /**
       * <code>int64 disk_used = 7;</code>
       * @param value The diskUsed to set.
       * @return This builder for chaining.
       */
      public Builder setDiskUsed(long value) {
        
        diskUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_used = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskUsed() {
        
        diskUsed_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution> dataDistribution_ =
        java.util.Collections.emptyList();
      private void ensureDataDistributionIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          dataDistribution_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution>(dataDistribution_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder> dataDistributionBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution> getDataDistributionList() {
        if (dataDistributionBuilder_ == null) {
          return java.util.Collections.unmodifiableList(dataDistribution_);
        } else {
          return dataDistributionBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public int getDataDistributionCount() {
        if (dataDistributionBuilder_ == null) {
          return dataDistribution_.size();
        } else {
          return dataDistributionBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution getDataDistribution(int index) {
        if (dataDistributionBuilder_ == null) {
          return dataDistribution_.get(index);
        } else {
          return dataDistributionBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder setDataDistribution(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution value) {
        if (dataDistributionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDataDistributionIsMutable();
          dataDistribution_.set(index, value);
          onChanged();
        } else {
          dataDistributionBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder setDataDistribution(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder builderForValue) {
        if (dataDistributionBuilder_ == null) {
          ensureDataDistributionIsMutable();
          dataDistribution_.set(index, builderForValue.build());
          onChanged();
        } else {
          dataDistributionBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder addDataDistribution(org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution value) {
        if (dataDistributionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDataDistributionIsMutable();
          dataDistribution_.add(value);
          onChanged();
        } else {
          dataDistributionBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder addDataDistribution(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution value) {
        if (dataDistributionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDataDistributionIsMutable();
          dataDistribution_.add(index, value);
          onChanged();
        } else {
          dataDistributionBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder addDataDistribution(
          org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder builderForValue) {
        if (dataDistributionBuilder_ == null) {
          ensureDataDistributionIsMutable();
          dataDistribution_.add(builderForValue.build());
          onChanged();
        } else {
          dataDistributionBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder addDataDistribution(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder builderForValue) {
        if (dataDistributionBuilder_ == null) {
          ensureDataDistributionIsMutable();
          dataDistribution_.add(index, builderForValue.build());
          onChanged();
        } else {
          dataDistributionBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder addAllDataDistribution(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution> values) {
        if (dataDistributionBuilder_ == null) {
          ensureDataDistributionIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, dataDistribution_);
          onChanged();
        } else {
          dataDistributionBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder clearDataDistribution() {
        if (dataDistributionBuilder_ == null) {
          dataDistribution_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          dataDistributionBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public Builder removeDataDistribution(int index) {
        if (dataDistributionBuilder_ == null) {
          ensureDataDistributionIsMutable();
          dataDistribution_.remove(index);
          onChanged();
        } else {
          dataDistributionBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder getDataDistributionBuilder(
          int index) {
        return getDataDistributionFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder getDataDistributionOrBuilder(
          int index) {
        if (dataDistributionBuilder_ == null) {
          return dataDistribution_.get(index);  } else {
          return dataDistributionBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder> 
           getDataDistributionOrBuilderList() {
        if (dataDistributionBuilder_ != null) {
          return dataDistributionBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(dataDistribution_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder addDataDistributionBuilder() {
        return getDataDistributionFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder addDataDistributionBuilder(
          int index) {
        return getDataDistributionFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDDataDistribution data_distribution = 8;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder> 
           getDataDistributionBuilderList() {
        return getDataDistributionFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder> 
          getDataDistributionFieldBuilder() {
        if (dataDistributionBuilder_ == null) {
          dataDistributionBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistribution.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDDataDistributionOrBuilder>(
                  dataDistribution_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          dataDistribution_ = null;
        }
        return dataDistributionBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo> partitions_ =
        java.util.Collections.emptyList();
      private void ensurePartitionsIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          partitions_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo>(partitions_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder> partitionsBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo> getPartitionsList() {
        if (partitionsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(partitions_);
        } else {
          return partitionsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public int getPartitionsCount() {
        if (partitionsBuilder_ == null) {
          return partitions_.size();
        } else {
          return partitionsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo getPartitions(int index) {
        if (partitionsBuilder_ == null) {
          return partitions_.get(index);
        } else {
          return partitionsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder setPartitions(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo value) {
        if (partitionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensurePartitionsIsMutable();
          partitions_.set(index, value);
          onChanged();
        } else {
          partitionsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder setPartitions(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder builderForValue) {
        if (partitionsBuilder_ == null) {
          ensurePartitionsIsMutable();
          partitions_.set(index, builderForValue.build());
          onChanged();
        } else {
          partitionsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder addPartitions(org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo value) {
        if (partitionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensurePartitionsIsMutable();
          partitions_.add(value);
          onChanged();
        } else {
          partitionsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder addPartitions(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo value) {
        if (partitionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensurePartitionsIsMutable();
          partitions_.add(index, value);
          onChanged();
        } else {
          partitionsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder addPartitions(
          org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder builderForValue) {
        if (partitionsBuilder_ == null) {
          ensurePartitionsIsMutable();
          partitions_.add(builderForValue.build());
          onChanged();
        } else {
          partitionsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder addPartitions(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder builderForValue) {
        if (partitionsBuilder_ == null) {
          ensurePartitionsIsMutable();
          partitions_.add(index, builderForValue.build());
          onChanged();
        } else {
          partitionsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder addAllPartitions(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo> values) {
        if (partitionsBuilder_ == null) {
          ensurePartitionsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, partitions_);
          onChanged();
        } else {
          partitionsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder clearPartitions() {
        if (partitionsBuilder_ == null) {
          partitions_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          partitionsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public Builder removePartitions(int index) {
        if (partitionsBuilder_ == null) {
          ensurePartitionsIsMutable();
          partitions_.remove(index);
          onChanged();
        } else {
          partitionsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder getPartitionsBuilder(
          int index) {
        return getPartitionsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder getPartitionsOrBuilder(
          int index) {
        if (partitionsBuilder_ == null) {
          return partitions_.get(index);  } else {
          return partitionsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder> 
           getPartitionsOrBuilderList() {
        if (partitionsBuilder_ != null) {
          return partitionsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(partitions_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder addPartitionsBuilder() {
        return getPartitionsFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder addPartitionsBuilder(
          int index) {
        return getPartitionsFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDPartitionInfo partitions = 9;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder> 
           getPartitionsBuilderList() {
        return getPartitionsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder> 
          getPartitionsFieldBuilder() {
        if (partitionsBuilder_ == null) {
          partitionsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDPartitionInfoOrBuilder>(
                  partitions_,
                  ((bitField0_ & 0x00000008) != 0),
                  getParentForChildren(),
                  isClean());
          partitions_ = null;
        }
        return partitionsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RDDStorageInfo)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RDDStorageInfo)
    private static final org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RDDStorageInfo>
        PARSER = new com.google.protobuf.AbstractParser<RDDStorageInfo>() {
      @java.lang.Override
      public RDDStorageInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RDDStorageInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RDDStorageInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RDDStorageInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RDDStorageInfoWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RDDStorageInfoWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoOrBuilder getInfoOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RDDStorageInfoWrapper}
   */
  public static final class RDDStorageInfoWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RDDStorageInfoWrapper)
      RDDStorageInfoWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RDDStorageInfoWrapper.newBuilder() to construct.
    private RDDStorageInfoWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RDDStorageInfoWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RDDStorageInfoWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RDDStorageInfoWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper.class, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper.Builder.class);
    }

    public static final int INFO_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo info_;
    /**
     * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (info_ != null) {
        output.writeMessage(1, getInfo());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getInfo());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper other = (org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper) obj;

      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RDDStorageInfoWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RDDStorageInfoWrapper)
        org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper.class, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper result = new org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper(this);
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper.getDefaultInstance()) return this;
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDStorageInfo info = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RDDStorageInfoWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RDDStorageInfoWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RDDStorageInfoWrapper>
        PARSER = new com.google.protobuf.AbstractParser<RDDStorageInfoWrapper>() {
      @java.lang.Override
      public RDDStorageInfoWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RDDStorageInfoWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RDDStorageInfoWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RDDStorageInfoWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDStorageInfoWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ResourceProfileWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ResourceProfileWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
     * @return Whether the rpInfo field is set.
     */
    boolean hasRpInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
     * @return The rpInfo.
     */
    org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getRpInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder getRpInfoOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ResourceProfileWrapper}
   */
  public static final class ResourceProfileWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ResourceProfileWrapper)
      ResourceProfileWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ResourceProfileWrapper.newBuilder() to construct.
    private ResourceProfileWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ResourceProfileWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ResourceProfileWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ResourceProfileWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder subBuilder = null;
              if (rpInfo_ != null) {
                subBuilder = rpInfo_.toBuilder();
              }
              rpInfo_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(rpInfo_);
                rpInfo_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper.Builder.class);
    }

    public static final int RPINFO_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo rpInfo_;
    /**
     * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
     * @return Whether the rpInfo field is set.
     */
    @java.lang.Override
    public boolean hasRpInfo() {
      return rpInfo_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
     * @return The rpInfo.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getRpInfo() {
      return rpInfo_ == null ? org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.getDefaultInstance() : rpInfo_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder getRpInfoOrBuilder() {
      return getRpInfo();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (rpInfo_ != null) {
        output.writeMessage(1, getRpInfo());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (rpInfo_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRpInfo());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper other = (org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper) obj;

      if (hasRpInfo() != other.hasRpInfo()) return false;
      if (hasRpInfo()) {
        if (!getRpInfo()
            .equals(other.getRpInfo())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRpInfo()) {
        hash = (37 * hash) + RPINFO_FIELD_NUMBER;
        hash = (53 * hash) + getRpInfo().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ResourceProfileWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ResourceProfileWrapper)
        org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (rpInfoBuilder_ == null) {
          rpInfo_ = null;
        } else {
          rpInfo_ = null;
          rpInfoBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper result = new org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper(this);
        if (rpInfoBuilder_ == null) {
          result.rpInfo_ = rpInfo_;
        } else {
          result.rpInfo_ = rpInfoBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper.getDefaultInstance()) return this;
        if (other.hasRpInfo()) {
          mergeRpInfo(other.getRpInfo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo rpInfo_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder> rpInfoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       * @return Whether the rpInfo field is set.
       */
      public boolean hasRpInfo() {
        return rpInfoBuilder_ != null || rpInfo_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       * @return The rpInfo.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo getRpInfo() {
        if (rpInfoBuilder_ == null) {
          return rpInfo_ == null ? org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.getDefaultInstance() : rpInfo_;
        } else {
          return rpInfoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       */
      public Builder setRpInfo(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo value) {
        if (rpInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          rpInfo_ = value;
          onChanged();
        } else {
          rpInfoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       */
      public Builder setRpInfo(
          org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder builderForValue) {
        if (rpInfoBuilder_ == null) {
          rpInfo_ = builderForValue.build();
          onChanged();
        } else {
          rpInfoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       */
      public Builder mergeRpInfo(org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo value) {
        if (rpInfoBuilder_ == null) {
          if (rpInfo_ != null) {
            rpInfo_ =
              org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.newBuilder(rpInfo_).mergeFrom(value).buildPartial();
          } else {
            rpInfo_ = value;
          }
          onChanged();
        } else {
          rpInfoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       */
      public Builder clearRpInfo() {
        if (rpInfoBuilder_ == null) {
          rpInfo_ = null;
          onChanged();
        } else {
          rpInfo_ = null;
          rpInfoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder getRpInfoBuilder() {
        
        onChanged();
        return getRpInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder getRpInfoOrBuilder() {
        if (rpInfoBuilder_ != null) {
          return rpInfoBuilder_.getMessageOrBuilder();
        } else {
          return rpInfo_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.getDefaultInstance() : rpInfo_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ResourceProfileInfo rpInfo = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder> 
          getRpInfoFieldBuilder() {
        if (rpInfoBuilder_ == null) {
          rpInfoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.ResourceProfileInfoOrBuilder>(
                  getRpInfo(),
                  getParentForChildren(),
                  isClean());
          rpInfo_ = null;
        }
        return rpInfoBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ResourceProfileWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ResourceProfileWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ResourceProfileWrapper>
        PARSER = new com.google.protobuf.AbstractParser<ResourceProfileWrapper>() {
      @java.lang.Override
      public ResourceProfileWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ResourceProfileWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ResourceProfileWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ResourceProfileWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ResourceProfileWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CachedQuantileOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.CachedQuantile)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 stage_id = 1;</code>
     * @return The stageId.
     */
    long getStageId();

    /**
     * <code>int32 stage_attempt_id = 2;</code>
     * @return The stageAttemptId.
     */
    int getStageAttemptId();

    /**
     * <code>string quantile = 3;</code>
     * @return Whether the quantile field is set.
     */
    boolean hasQuantile();
    /**
     * <code>string quantile = 3;</code>
     * @return The quantile.
     */
    java.lang.String getQuantile();
    /**
     * <code>string quantile = 3;</code>
     * @return The bytes for quantile.
     */
    com.google.protobuf.ByteString
        getQuantileBytes();

    /**
     * <code>int64 task_count = 4;</code>
     * @return The taskCount.
     */
    long getTaskCount();

    /**
     * <code>double duration = 5;</code>
     * @return The duration.
     */
    double getDuration();

    /**
     * <code>double executor_deserialize_time = 6;</code>
     * @return The executorDeserializeTime.
     */
    double getExecutorDeserializeTime();

    /**
     * <code>double executor_deserialize_cpu_time = 7;</code>
     * @return The executorDeserializeCpuTime.
     */
    double getExecutorDeserializeCpuTime();

    /**
     * <code>double executor_run_time = 8;</code>
     * @return The executorRunTime.
     */
    double getExecutorRunTime();

    /**
     * <code>double executor_cpu_time = 9;</code>
     * @return The executorCpuTime.
     */
    double getExecutorCpuTime();

    /**
     * <code>double result_size = 10;</code>
     * @return The resultSize.
     */
    double getResultSize();

    /**
     * <code>double jvm_gc_time = 11;</code>
     * @return The jvmGcTime.
     */
    double getJvmGcTime();

    /**
     * <code>double result_serialization_time = 12;</code>
     * @return The resultSerializationTime.
     */
    double getResultSerializationTime();

    /**
     * <code>double getting_result_time = 13;</code>
     * @return The gettingResultTime.
     */
    double getGettingResultTime();

    /**
     * <code>double scheduler_delay = 14;</code>
     * @return The schedulerDelay.
     */
    double getSchedulerDelay();

    /**
     * <code>double peak_execution_memory = 15;</code>
     * @return The peakExecutionMemory.
     */
    double getPeakExecutionMemory();

    /**
     * <code>double memory_bytes_spilled = 16;</code>
     * @return The memoryBytesSpilled.
     */
    double getMemoryBytesSpilled();

    /**
     * <code>double disk_bytes_spilled = 17;</code>
     * @return The diskBytesSpilled.
     */
    double getDiskBytesSpilled();

    /**
     * <code>double bytes_read = 18;</code>
     * @return The bytesRead.
     */
    double getBytesRead();

    /**
     * <code>double records_read = 19;</code>
     * @return The recordsRead.
     */
    double getRecordsRead();

    /**
     * <code>double bytes_written = 20;</code>
     * @return The bytesWritten.
     */
    double getBytesWritten();

    /**
     * <code>double records_written = 21;</code>
     * @return The recordsWritten.
     */
    double getRecordsWritten();

    /**
     * <code>double shuffle_read_bytes = 22;</code>
     * @return The shuffleReadBytes.
     */
    double getShuffleReadBytes();

    /**
     * <code>double shuffle_records_read = 23;</code>
     * @return The shuffleRecordsRead.
     */
    double getShuffleRecordsRead();

    /**
     * <code>double shuffle_remote_blocks_fetched = 24;</code>
     * @return The shuffleRemoteBlocksFetched.
     */
    double getShuffleRemoteBlocksFetched();

    /**
     * <code>double shuffle_local_blocks_fetched = 25;</code>
     * @return The shuffleLocalBlocksFetched.
     */
    double getShuffleLocalBlocksFetched();

    /**
     * <code>double shuffle_fetch_wait_time = 26;</code>
     * @return The shuffleFetchWaitTime.
     */
    double getShuffleFetchWaitTime();

    /**
     * <code>double shuffle_remote_bytes_read = 27;</code>
     * @return The shuffleRemoteBytesRead.
     */
    double getShuffleRemoteBytesRead();

    /**
     * <code>double shuffle_remote_bytes_read_to_disk = 28;</code>
     * @return The shuffleRemoteBytesReadToDisk.
     */
    double getShuffleRemoteBytesReadToDisk();

    /**
     * <code>double shuffle_total_blocks_fetched = 29;</code>
     * @return The shuffleTotalBlocksFetched.
     */
    double getShuffleTotalBlocksFetched();

    /**
     * <code>double shuffle_write_bytes = 30;</code>
     * @return The shuffleWriteBytes.
     */
    double getShuffleWriteBytes();

    /**
     * <code>double shuffle_write_records = 31;</code>
     * @return The shuffleWriteRecords.
     */
    double getShuffleWriteRecords();

    /**
     * <code>double shuffle_write_time = 32;</code>
     * @return The shuffleWriteTime.
     */
    double getShuffleWriteTime();

    /**
     * <code>double shuffle_corrupt_merged_block_chunks = 33;</code>
     * @return The shuffleCorruptMergedBlockChunks.
     */
    double getShuffleCorruptMergedBlockChunks();

    /**
     * <code>double shuffle_merged_fetch_fallback_count = 34;</code>
     * @return The shuffleMergedFetchFallbackCount.
     */
    double getShuffleMergedFetchFallbackCount();

    /**
     * <code>double shuffle_merged_remote_blocks_fetched = 35;</code>
     * @return The shuffleMergedRemoteBlocksFetched.
     */
    double getShuffleMergedRemoteBlocksFetched();

    /**
     * <code>double shuffle_merged_local_blocks_fetched = 36;</code>
     * @return The shuffleMergedLocalBlocksFetched.
     */
    double getShuffleMergedLocalBlocksFetched();

    /**
     * <code>double shuffle_merged_remote_chunks_fetched = 37;</code>
     * @return The shuffleMergedRemoteChunksFetched.
     */
    double getShuffleMergedRemoteChunksFetched();

    /**
     * <code>double shuffle_merged_local_chunks_fetched = 38;</code>
     * @return The shuffleMergedLocalChunksFetched.
     */
    double getShuffleMergedLocalChunksFetched();

    /**
     * <code>double shuffle_merged_remote_bytes_read = 39;</code>
     * @return The shuffleMergedRemoteBytesRead.
     */
    double getShuffleMergedRemoteBytesRead();

    /**
     * <code>double shuffle_merged_local_bytes_read = 40;</code>
     * @return The shuffleMergedLocalBytesRead.
     */
    double getShuffleMergedLocalBytesRead();

    /**
     * <code>double shuffle_remote_reqs_duration = 41;</code>
     * @return The shuffleRemoteReqsDuration.
     */
    double getShuffleRemoteReqsDuration();

    /**
     * <code>double shuffle_merged_remote_reqs_duration = 42;</code>
     * @return The shuffleMergedRemoteReqsDuration.
     */
    double getShuffleMergedRemoteReqsDuration();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.CachedQuantile}
   */
  public static final class CachedQuantile extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.CachedQuantile)
      CachedQuantileOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CachedQuantile.newBuilder() to construct.
    private CachedQuantile(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CachedQuantile() {
      quantile_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CachedQuantile();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CachedQuantile(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              stageId_ = input.readInt64();
              break;
            }
            case 16: {

              stageAttemptId_ = input.readInt32();
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              quantile_ = s;
              break;
            }
            case 32: {

              taskCount_ = input.readInt64();
              break;
            }
            case 41: {

              duration_ = input.readDouble();
              break;
            }
            case 49: {

              executorDeserializeTime_ = input.readDouble();
              break;
            }
            case 57: {

              executorDeserializeCpuTime_ = input.readDouble();
              break;
            }
            case 65: {

              executorRunTime_ = input.readDouble();
              break;
            }
            case 73: {

              executorCpuTime_ = input.readDouble();
              break;
            }
            case 81: {

              resultSize_ = input.readDouble();
              break;
            }
            case 89: {

              jvmGcTime_ = input.readDouble();
              break;
            }
            case 97: {

              resultSerializationTime_ = input.readDouble();
              break;
            }
            case 105: {

              gettingResultTime_ = input.readDouble();
              break;
            }
            case 113: {

              schedulerDelay_ = input.readDouble();
              break;
            }
            case 121: {

              peakExecutionMemory_ = input.readDouble();
              break;
            }
            case 129: {

              memoryBytesSpilled_ = input.readDouble();
              break;
            }
            case 137: {

              diskBytesSpilled_ = input.readDouble();
              break;
            }
            case 145: {

              bytesRead_ = input.readDouble();
              break;
            }
            case 153: {

              recordsRead_ = input.readDouble();
              break;
            }
            case 161: {

              bytesWritten_ = input.readDouble();
              break;
            }
            case 169: {

              recordsWritten_ = input.readDouble();
              break;
            }
            case 177: {

              shuffleReadBytes_ = input.readDouble();
              break;
            }
            case 185: {

              shuffleRecordsRead_ = input.readDouble();
              break;
            }
            case 193: {

              shuffleRemoteBlocksFetched_ = input.readDouble();
              break;
            }
            case 201: {

              shuffleLocalBlocksFetched_ = input.readDouble();
              break;
            }
            case 209: {

              shuffleFetchWaitTime_ = input.readDouble();
              break;
            }
            case 217: {

              shuffleRemoteBytesRead_ = input.readDouble();
              break;
            }
            case 225: {

              shuffleRemoteBytesReadToDisk_ = input.readDouble();
              break;
            }
            case 233: {

              shuffleTotalBlocksFetched_ = input.readDouble();
              break;
            }
            case 241: {

              shuffleWriteBytes_ = input.readDouble();
              break;
            }
            case 249: {

              shuffleWriteRecords_ = input.readDouble();
              break;
            }
            case 257: {

              shuffleWriteTime_ = input.readDouble();
              break;
            }
            case 265: {

              shuffleCorruptMergedBlockChunks_ = input.readDouble();
              break;
            }
            case 273: {

              shuffleMergedFetchFallbackCount_ = input.readDouble();
              break;
            }
            case 281: {

              shuffleMergedRemoteBlocksFetched_ = input.readDouble();
              break;
            }
            case 289: {

              shuffleMergedLocalBlocksFetched_ = input.readDouble();
              break;
            }
            case 297: {

              shuffleMergedRemoteChunksFetched_ = input.readDouble();
              break;
            }
            case 305: {

              shuffleMergedLocalChunksFetched_ = input.readDouble();
              break;
            }
            case 313: {

              shuffleMergedRemoteBytesRead_ = input.readDouble();
              break;
            }
            case 321: {

              shuffleMergedLocalBytesRead_ = input.readDouble();
              break;
            }
            case 329: {

              shuffleRemoteReqsDuration_ = input.readDouble();
              break;
            }
            case 337: {

              shuffleMergedRemoteReqsDuration_ = input.readDouble();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_CachedQuantile_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_CachedQuantile_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.CachedQuantile.class, org.apache.spark.status.protobuf.StoreTypes.CachedQuantile.Builder.class);
    }

    private int bitField0_;
    public static final int STAGE_ID_FIELD_NUMBER = 1;
    private long stageId_;
    /**
     * <code>int64 stage_id = 1;</code>
     * @return The stageId.
     */
    @java.lang.Override
    public long getStageId() {
      return stageId_;
    }

    public static final int STAGE_ATTEMPT_ID_FIELD_NUMBER = 2;
    private int stageAttemptId_;
    /**
     * <code>int32 stage_attempt_id = 2;</code>
     * @return The stageAttemptId.
     */
    @java.lang.Override
    public int getStageAttemptId() {
      return stageAttemptId_;
    }

    public static final int QUANTILE_FIELD_NUMBER = 3;
    private volatile java.lang.Object quantile_;
    /**
     * <code>string quantile = 3;</code>
     * @return Whether the quantile field is set.
     */
    @java.lang.Override
    public boolean hasQuantile() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string quantile = 3;</code>
     * @return The quantile.
     */
    @java.lang.Override
    public java.lang.String getQuantile() {
      java.lang.Object ref = quantile_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        quantile_ = s;
        return s;
      }
    }
    /**
     * <code>string quantile = 3;</code>
     * @return The bytes for quantile.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getQuantileBytes() {
      java.lang.Object ref = quantile_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        quantile_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TASK_COUNT_FIELD_NUMBER = 4;
    private long taskCount_;
    /**
     * <code>int64 task_count = 4;</code>
     * @return The taskCount.
     */
    @java.lang.Override
    public long getTaskCount() {
      return taskCount_;
    }

    public static final int DURATION_FIELD_NUMBER = 5;
    private double duration_;
    /**
     * <code>double duration = 5;</code>
     * @return The duration.
     */
    @java.lang.Override
    public double getDuration() {
      return duration_;
    }

    public static final int EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER = 6;
    private double executorDeserializeTime_;
    /**
     * <code>double executor_deserialize_time = 6;</code>
     * @return The executorDeserializeTime.
     */
    @java.lang.Override
    public double getExecutorDeserializeTime() {
      return executorDeserializeTime_;
    }

    public static final int EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER = 7;
    private double executorDeserializeCpuTime_;
    /**
     * <code>double executor_deserialize_cpu_time = 7;</code>
     * @return The executorDeserializeCpuTime.
     */
    @java.lang.Override
    public double getExecutorDeserializeCpuTime() {
      return executorDeserializeCpuTime_;
    }

    public static final int EXECUTOR_RUN_TIME_FIELD_NUMBER = 8;
    private double executorRunTime_;
    /**
     * <code>double executor_run_time = 8;</code>
     * @return The executorRunTime.
     */
    @java.lang.Override
    public double getExecutorRunTime() {
      return executorRunTime_;
    }

    public static final int EXECUTOR_CPU_TIME_FIELD_NUMBER = 9;
    private double executorCpuTime_;
    /**
     * <code>double executor_cpu_time = 9;</code>
     * @return The executorCpuTime.
     */
    @java.lang.Override
    public double getExecutorCpuTime() {
      return executorCpuTime_;
    }

    public static final int RESULT_SIZE_FIELD_NUMBER = 10;
    private double resultSize_;
    /**
     * <code>double result_size = 10;</code>
     * @return The resultSize.
     */
    @java.lang.Override
    public double getResultSize() {
      return resultSize_;
    }

    public static final int JVM_GC_TIME_FIELD_NUMBER = 11;
    private double jvmGcTime_;
    /**
     * <code>double jvm_gc_time = 11;</code>
     * @return The jvmGcTime.
     */
    @java.lang.Override
    public double getJvmGcTime() {
      return jvmGcTime_;
    }

    public static final int RESULT_SERIALIZATION_TIME_FIELD_NUMBER = 12;
    private double resultSerializationTime_;
    /**
     * <code>double result_serialization_time = 12;</code>
     * @return The resultSerializationTime.
     */
    @java.lang.Override
    public double getResultSerializationTime() {
      return resultSerializationTime_;
    }

    public static final int GETTING_RESULT_TIME_FIELD_NUMBER = 13;
    private double gettingResultTime_;
    /**
     * <code>double getting_result_time = 13;</code>
     * @return The gettingResultTime.
     */
    @java.lang.Override
    public double getGettingResultTime() {
      return gettingResultTime_;
    }

    public static final int SCHEDULER_DELAY_FIELD_NUMBER = 14;
    private double schedulerDelay_;
    /**
     * <code>double scheduler_delay = 14;</code>
     * @return The schedulerDelay.
     */
    @java.lang.Override
    public double getSchedulerDelay() {
      return schedulerDelay_;
    }

    public static final int PEAK_EXECUTION_MEMORY_FIELD_NUMBER = 15;
    private double peakExecutionMemory_;
    /**
     * <code>double peak_execution_memory = 15;</code>
     * @return The peakExecutionMemory.
     */
    @java.lang.Override
    public double getPeakExecutionMemory() {
      return peakExecutionMemory_;
    }

    public static final int MEMORY_BYTES_SPILLED_FIELD_NUMBER = 16;
    private double memoryBytesSpilled_;
    /**
     * <code>double memory_bytes_spilled = 16;</code>
     * @return The memoryBytesSpilled.
     */
    @java.lang.Override
    public double getMemoryBytesSpilled() {
      return memoryBytesSpilled_;
    }

    public static final int DISK_BYTES_SPILLED_FIELD_NUMBER = 17;
    private double diskBytesSpilled_;
    /**
     * <code>double disk_bytes_spilled = 17;</code>
     * @return The diskBytesSpilled.
     */
    @java.lang.Override
    public double getDiskBytesSpilled() {
      return diskBytesSpilled_;
    }

    public static final int BYTES_READ_FIELD_NUMBER = 18;
    private double bytesRead_;
    /**
     * <code>double bytes_read = 18;</code>
     * @return The bytesRead.
     */
    @java.lang.Override
    public double getBytesRead() {
      return bytesRead_;
    }

    public static final int RECORDS_READ_FIELD_NUMBER = 19;
    private double recordsRead_;
    /**
     * <code>double records_read = 19;</code>
     * @return The recordsRead.
     */
    @java.lang.Override
    public double getRecordsRead() {
      return recordsRead_;
    }

    public static final int BYTES_WRITTEN_FIELD_NUMBER = 20;
    private double bytesWritten_;
    /**
     * <code>double bytes_written = 20;</code>
     * @return The bytesWritten.
     */
    @java.lang.Override
    public double getBytesWritten() {
      return bytesWritten_;
    }

    public static final int RECORDS_WRITTEN_FIELD_NUMBER = 21;
    private double recordsWritten_;
    /**
     * <code>double records_written = 21;</code>
     * @return The recordsWritten.
     */
    @java.lang.Override
    public double getRecordsWritten() {
      return recordsWritten_;
    }

    public static final int SHUFFLE_READ_BYTES_FIELD_NUMBER = 22;
    private double shuffleReadBytes_;
    /**
     * <code>double shuffle_read_bytes = 22;</code>
     * @return The shuffleReadBytes.
     */
    @java.lang.Override
    public double getShuffleReadBytes() {
      return shuffleReadBytes_;
    }

    public static final int SHUFFLE_RECORDS_READ_FIELD_NUMBER = 23;
    private double shuffleRecordsRead_;
    /**
     * <code>double shuffle_records_read = 23;</code>
     * @return The shuffleRecordsRead.
     */
    @java.lang.Override
    public double getShuffleRecordsRead() {
      return shuffleRecordsRead_;
    }

    public static final int SHUFFLE_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER = 24;
    private double shuffleRemoteBlocksFetched_;
    /**
     * <code>double shuffle_remote_blocks_fetched = 24;</code>
     * @return The shuffleRemoteBlocksFetched.
     */
    @java.lang.Override
    public double getShuffleRemoteBlocksFetched() {
      return shuffleRemoteBlocksFetched_;
    }

    public static final int SHUFFLE_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER = 25;
    private double shuffleLocalBlocksFetched_;
    /**
     * <code>double shuffle_local_blocks_fetched = 25;</code>
     * @return The shuffleLocalBlocksFetched.
     */
    @java.lang.Override
    public double getShuffleLocalBlocksFetched() {
      return shuffleLocalBlocksFetched_;
    }

    public static final int SHUFFLE_FETCH_WAIT_TIME_FIELD_NUMBER = 26;
    private double shuffleFetchWaitTime_;
    /**
     * <code>double shuffle_fetch_wait_time = 26;</code>
     * @return The shuffleFetchWaitTime.
     */
    @java.lang.Override
    public double getShuffleFetchWaitTime() {
      return shuffleFetchWaitTime_;
    }

    public static final int SHUFFLE_REMOTE_BYTES_READ_FIELD_NUMBER = 27;
    private double shuffleRemoteBytesRead_;
    /**
     * <code>double shuffle_remote_bytes_read = 27;</code>
     * @return The shuffleRemoteBytesRead.
     */
    @java.lang.Override
    public double getShuffleRemoteBytesRead() {
      return shuffleRemoteBytesRead_;
    }

    public static final int SHUFFLE_REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER = 28;
    private double shuffleRemoteBytesReadToDisk_;
    /**
     * <code>double shuffle_remote_bytes_read_to_disk = 28;</code>
     * @return The shuffleRemoteBytesReadToDisk.
     */
    @java.lang.Override
    public double getShuffleRemoteBytesReadToDisk() {
      return shuffleRemoteBytesReadToDisk_;
    }

    public static final int SHUFFLE_TOTAL_BLOCKS_FETCHED_FIELD_NUMBER = 29;
    private double shuffleTotalBlocksFetched_;
    /**
     * <code>double shuffle_total_blocks_fetched = 29;</code>
     * @return The shuffleTotalBlocksFetched.
     */
    @java.lang.Override
    public double getShuffleTotalBlocksFetched() {
      return shuffleTotalBlocksFetched_;
    }

    public static final int SHUFFLE_WRITE_BYTES_FIELD_NUMBER = 30;
    private double shuffleWriteBytes_;
    /**
     * <code>double shuffle_write_bytes = 30;</code>
     * @return The shuffleWriteBytes.
     */
    @java.lang.Override
    public double getShuffleWriteBytes() {
      return shuffleWriteBytes_;
    }

    public static final int SHUFFLE_WRITE_RECORDS_FIELD_NUMBER = 31;
    private double shuffleWriteRecords_;
    /**
     * <code>double shuffle_write_records = 31;</code>
     * @return The shuffleWriteRecords.
     */
    @java.lang.Override
    public double getShuffleWriteRecords() {
      return shuffleWriteRecords_;
    }

    public static final int SHUFFLE_WRITE_TIME_FIELD_NUMBER = 32;
    private double shuffleWriteTime_;
    /**
     * <code>double shuffle_write_time = 32;</code>
     * @return The shuffleWriteTime.
     */
    @java.lang.Override
    public double getShuffleWriteTime() {
      return shuffleWriteTime_;
    }

    public static final int SHUFFLE_CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER = 33;
    private double shuffleCorruptMergedBlockChunks_;
    /**
     * <code>double shuffle_corrupt_merged_block_chunks = 33;</code>
     * @return The shuffleCorruptMergedBlockChunks.
     */
    @java.lang.Override
    public double getShuffleCorruptMergedBlockChunks() {
      return shuffleCorruptMergedBlockChunks_;
    }

    public static final int SHUFFLE_MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER = 34;
    private double shuffleMergedFetchFallbackCount_;
    /**
     * <code>double shuffle_merged_fetch_fallback_count = 34;</code>
     * @return The shuffleMergedFetchFallbackCount.
     */
    @java.lang.Override
    public double getShuffleMergedFetchFallbackCount() {
      return shuffleMergedFetchFallbackCount_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER = 35;
    private double shuffleMergedRemoteBlocksFetched_;
    /**
     * <code>double shuffle_merged_remote_blocks_fetched = 35;</code>
     * @return The shuffleMergedRemoteBlocksFetched.
     */
    @java.lang.Override
    public double getShuffleMergedRemoteBlocksFetched() {
      return shuffleMergedRemoteBlocksFetched_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER = 36;
    private double shuffleMergedLocalBlocksFetched_;
    /**
     * <code>double shuffle_merged_local_blocks_fetched = 36;</code>
     * @return The shuffleMergedLocalBlocksFetched.
     */
    @java.lang.Override
    public double getShuffleMergedLocalBlocksFetched() {
      return shuffleMergedLocalBlocksFetched_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_CHUNKS_FETCHED_FIELD_NUMBER = 37;
    private double shuffleMergedRemoteChunksFetched_;
    /**
     * <code>double shuffle_merged_remote_chunks_fetched = 37;</code>
     * @return The shuffleMergedRemoteChunksFetched.
     */
    @java.lang.Override
    public double getShuffleMergedRemoteChunksFetched() {
      return shuffleMergedRemoteChunksFetched_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_CHUNKS_FETCHED_FIELD_NUMBER = 38;
    private double shuffleMergedLocalChunksFetched_;
    /**
     * <code>double shuffle_merged_local_chunks_fetched = 38;</code>
     * @return The shuffleMergedLocalChunksFetched.
     */
    @java.lang.Override
    public double getShuffleMergedLocalChunksFetched() {
      return shuffleMergedLocalChunksFetched_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_BYTES_READ_FIELD_NUMBER = 39;
    private double shuffleMergedRemoteBytesRead_;
    /**
     * <code>double shuffle_merged_remote_bytes_read = 39;</code>
     * @return The shuffleMergedRemoteBytesRead.
     */
    @java.lang.Override
    public double getShuffleMergedRemoteBytesRead() {
      return shuffleMergedRemoteBytesRead_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_BYTES_READ_FIELD_NUMBER = 40;
    private double shuffleMergedLocalBytesRead_;
    /**
     * <code>double shuffle_merged_local_bytes_read = 40;</code>
     * @return The shuffleMergedLocalBytesRead.
     */
    @java.lang.Override
    public double getShuffleMergedLocalBytesRead() {
      return shuffleMergedLocalBytesRead_;
    }

    public static final int SHUFFLE_REMOTE_REQS_DURATION_FIELD_NUMBER = 41;
    private double shuffleRemoteReqsDuration_;
    /**
     * <code>double shuffle_remote_reqs_duration = 41;</code>
     * @return The shuffleRemoteReqsDuration.
     */
    @java.lang.Override
    public double getShuffleRemoteReqsDuration() {
      return shuffleRemoteReqsDuration_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_REQS_DURATION_FIELD_NUMBER = 42;
    private double shuffleMergedRemoteReqsDuration_;
    /**
     * <code>double shuffle_merged_remote_reqs_duration = 42;</code>
     * @return The shuffleMergedRemoteReqsDuration.
     */
    @java.lang.Override
    public double getShuffleMergedRemoteReqsDuration() {
      return shuffleMergedRemoteReqsDuration_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (stageId_ != 0L) {
        output.writeInt64(1, stageId_);
      }
      if (stageAttemptId_ != 0) {
        output.writeInt32(2, stageAttemptId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, quantile_);
      }
      if (taskCount_ != 0L) {
        output.writeInt64(4, taskCount_);
      }
      if (duration_ != 0D) {
        output.writeDouble(5, duration_);
      }
      if (executorDeserializeTime_ != 0D) {
        output.writeDouble(6, executorDeserializeTime_);
      }
      if (executorDeserializeCpuTime_ != 0D) {
        output.writeDouble(7, executorDeserializeCpuTime_);
      }
      if (executorRunTime_ != 0D) {
        output.writeDouble(8, executorRunTime_);
      }
      if (executorCpuTime_ != 0D) {
        output.writeDouble(9, executorCpuTime_);
      }
      if (resultSize_ != 0D) {
        output.writeDouble(10, resultSize_);
      }
      if (jvmGcTime_ != 0D) {
        output.writeDouble(11, jvmGcTime_);
      }
      if (resultSerializationTime_ != 0D) {
        output.writeDouble(12, resultSerializationTime_);
      }
      if (gettingResultTime_ != 0D) {
        output.writeDouble(13, gettingResultTime_);
      }
      if (schedulerDelay_ != 0D) {
        output.writeDouble(14, schedulerDelay_);
      }
      if (peakExecutionMemory_ != 0D) {
        output.writeDouble(15, peakExecutionMemory_);
      }
      if (memoryBytesSpilled_ != 0D) {
        output.writeDouble(16, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0D) {
        output.writeDouble(17, diskBytesSpilled_);
      }
      if (bytesRead_ != 0D) {
        output.writeDouble(18, bytesRead_);
      }
      if (recordsRead_ != 0D) {
        output.writeDouble(19, recordsRead_);
      }
      if (bytesWritten_ != 0D) {
        output.writeDouble(20, bytesWritten_);
      }
      if (recordsWritten_ != 0D) {
        output.writeDouble(21, recordsWritten_);
      }
      if (shuffleReadBytes_ != 0D) {
        output.writeDouble(22, shuffleReadBytes_);
      }
      if (shuffleRecordsRead_ != 0D) {
        output.writeDouble(23, shuffleRecordsRead_);
      }
      if (shuffleRemoteBlocksFetched_ != 0D) {
        output.writeDouble(24, shuffleRemoteBlocksFetched_);
      }
      if (shuffleLocalBlocksFetched_ != 0D) {
        output.writeDouble(25, shuffleLocalBlocksFetched_);
      }
      if (shuffleFetchWaitTime_ != 0D) {
        output.writeDouble(26, shuffleFetchWaitTime_);
      }
      if (shuffleRemoteBytesRead_ != 0D) {
        output.writeDouble(27, shuffleRemoteBytesRead_);
      }
      if (shuffleRemoteBytesReadToDisk_ != 0D) {
        output.writeDouble(28, shuffleRemoteBytesReadToDisk_);
      }
      if (shuffleTotalBlocksFetched_ != 0D) {
        output.writeDouble(29, shuffleTotalBlocksFetched_);
      }
      if (shuffleWriteBytes_ != 0D) {
        output.writeDouble(30, shuffleWriteBytes_);
      }
      if (shuffleWriteRecords_ != 0D) {
        output.writeDouble(31, shuffleWriteRecords_);
      }
      if (shuffleWriteTime_ != 0D) {
        output.writeDouble(32, shuffleWriteTime_);
      }
      if (shuffleCorruptMergedBlockChunks_ != 0D) {
        output.writeDouble(33, shuffleCorruptMergedBlockChunks_);
      }
      if (shuffleMergedFetchFallbackCount_ != 0D) {
        output.writeDouble(34, shuffleMergedFetchFallbackCount_);
      }
      if (shuffleMergedRemoteBlocksFetched_ != 0D) {
        output.writeDouble(35, shuffleMergedRemoteBlocksFetched_);
      }
      if (shuffleMergedLocalBlocksFetched_ != 0D) {
        output.writeDouble(36, shuffleMergedLocalBlocksFetched_);
      }
      if (shuffleMergedRemoteChunksFetched_ != 0D) {
        output.writeDouble(37, shuffleMergedRemoteChunksFetched_);
      }
      if (shuffleMergedLocalChunksFetched_ != 0D) {
        output.writeDouble(38, shuffleMergedLocalChunksFetched_);
      }
      if (shuffleMergedRemoteBytesRead_ != 0D) {
        output.writeDouble(39, shuffleMergedRemoteBytesRead_);
      }
      if (shuffleMergedLocalBytesRead_ != 0D) {
        output.writeDouble(40, shuffleMergedLocalBytesRead_);
      }
      if (shuffleRemoteReqsDuration_ != 0D) {
        output.writeDouble(41, shuffleRemoteReqsDuration_);
      }
      if (shuffleMergedRemoteReqsDuration_ != 0D) {
        output.writeDouble(42, shuffleMergedRemoteReqsDuration_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (stageId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, stageId_);
      }
      if (stageAttemptId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, stageAttemptId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, quantile_);
      }
      if (taskCount_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, taskCount_);
      }
      if (duration_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(5, duration_);
      }
      if (executorDeserializeTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(6, executorDeserializeTime_);
      }
      if (executorDeserializeCpuTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(7, executorDeserializeCpuTime_);
      }
      if (executorRunTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(8, executorRunTime_);
      }
      if (executorCpuTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(9, executorCpuTime_);
      }
      if (resultSize_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(10, resultSize_);
      }
      if (jvmGcTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(11, jvmGcTime_);
      }
      if (resultSerializationTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(12, resultSerializationTime_);
      }
      if (gettingResultTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(13, gettingResultTime_);
      }
      if (schedulerDelay_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(14, schedulerDelay_);
      }
      if (peakExecutionMemory_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(15, peakExecutionMemory_);
      }
      if (memoryBytesSpilled_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(16, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(17, diskBytesSpilled_);
      }
      if (bytesRead_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(18, bytesRead_);
      }
      if (recordsRead_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(19, recordsRead_);
      }
      if (bytesWritten_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(20, bytesWritten_);
      }
      if (recordsWritten_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(21, recordsWritten_);
      }
      if (shuffleReadBytes_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(22, shuffleReadBytes_);
      }
      if (shuffleRecordsRead_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(23, shuffleRecordsRead_);
      }
      if (shuffleRemoteBlocksFetched_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(24, shuffleRemoteBlocksFetched_);
      }
      if (shuffleLocalBlocksFetched_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(25, shuffleLocalBlocksFetched_);
      }
      if (shuffleFetchWaitTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(26, shuffleFetchWaitTime_);
      }
      if (shuffleRemoteBytesRead_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(27, shuffleRemoteBytesRead_);
      }
      if (shuffleRemoteBytesReadToDisk_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(28, shuffleRemoteBytesReadToDisk_);
      }
      if (shuffleTotalBlocksFetched_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(29, shuffleTotalBlocksFetched_);
      }
      if (shuffleWriteBytes_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(30, shuffleWriteBytes_);
      }
      if (shuffleWriteRecords_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(31, shuffleWriteRecords_);
      }
      if (shuffleWriteTime_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(32, shuffleWriteTime_);
      }
      if (shuffleCorruptMergedBlockChunks_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(33, shuffleCorruptMergedBlockChunks_);
      }
      if (shuffleMergedFetchFallbackCount_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(34, shuffleMergedFetchFallbackCount_);
      }
      if (shuffleMergedRemoteBlocksFetched_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(35, shuffleMergedRemoteBlocksFetched_);
      }
      if (shuffleMergedLocalBlocksFetched_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(36, shuffleMergedLocalBlocksFetched_);
      }
      if (shuffleMergedRemoteChunksFetched_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(37, shuffleMergedRemoteChunksFetched_);
      }
      if (shuffleMergedLocalChunksFetched_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(38, shuffleMergedLocalChunksFetched_);
      }
      if (shuffleMergedRemoteBytesRead_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(39, shuffleMergedRemoteBytesRead_);
      }
      if (shuffleMergedLocalBytesRead_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(40, shuffleMergedLocalBytesRead_);
      }
      if (shuffleRemoteReqsDuration_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(41, shuffleRemoteReqsDuration_);
      }
      if (shuffleMergedRemoteReqsDuration_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(42, shuffleMergedRemoteReqsDuration_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.CachedQuantile)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.CachedQuantile other = (org.apache.spark.status.protobuf.StoreTypes.CachedQuantile) obj;

      if (getStageId()
          != other.getStageId()) return false;
      if (getStageAttemptId()
          != other.getStageAttemptId()) return false;
      if (hasQuantile() != other.hasQuantile()) return false;
      if (hasQuantile()) {
        if (!getQuantile()
            .equals(other.getQuantile())) return false;
      }
      if (getTaskCount()
          != other.getTaskCount()) return false;
      if (java.lang.Double.doubleToLongBits(getDuration())
          != java.lang.Double.doubleToLongBits(
              other.getDuration())) return false;
      if (java.lang.Double.doubleToLongBits(getExecutorDeserializeTime())
          != java.lang.Double.doubleToLongBits(
              other.getExecutorDeserializeTime())) return false;
      if (java.lang.Double.doubleToLongBits(getExecutorDeserializeCpuTime())
          != java.lang.Double.doubleToLongBits(
              other.getExecutorDeserializeCpuTime())) return false;
      if (java.lang.Double.doubleToLongBits(getExecutorRunTime())
          != java.lang.Double.doubleToLongBits(
              other.getExecutorRunTime())) return false;
      if (java.lang.Double.doubleToLongBits(getExecutorCpuTime())
          != java.lang.Double.doubleToLongBits(
              other.getExecutorCpuTime())) return false;
      if (java.lang.Double.doubleToLongBits(getResultSize())
          != java.lang.Double.doubleToLongBits(
              other.getResultSize())) return false;
      if (java.lang.Double.doubleToLongBits(getJvmGcTime())
          != java.lang.Double.doubleToLongBits(
              other.getJvmGcTime())) return false;
      if (java.lang.Double.doubleToLongBits(getResultSerializationTime())
          != java.lang.Double.doubleToLongBits(
              other.getResultSerializationTime())) return false;
      if (java.lang.Double.doubleToLongBits(getGettingResultTime())
          != java.lang.Double.doubleToLongBits(
              other.getGettingResultTime())) return false;
      if (java.lang.Double.doubleToLongBits(getSchedulerDelay())
          != java.lang.Double.doubleToLongBits(
              other.getSchedulerDelay())) return false;
      if (java.lang.Double.doubleToLongBits(getPeakExecutionMemory())
          != java.lang.Double.doubleToLongBits(
              other.getPeakExecutionMemory())) return false;
      if (java.lang.Double.doubleToLongBits(getMemoryBytesSpilled())
          != java.lang.Double.doubleToLongBits(
              other.getMemoryBytesSpilled())) return false;
      if (java.lang.Double.doubleToLongBits(getDiskBytesSpilled())
          != java.lang.Double.doubleToLongBits(
              other.getDiskBytesSpilled())) return false;
      if (java.lang.Double.doubleToLongBits(getBytesRead())
          != java.lang.Double.doubleToLongBits(
              other.getBytesRead())) return false;
      if (java.lang.Double.doubleToLongBits(getRecordsRead())
          != java.lang.Double.doubleToLongBits(
              other.getRecordsRead())) return false;
      if (java.lang.Double.doubleToLongBits(getBytesWritten())
          != java.lang.Double.doubleToLongBits(
              other.getBytesWritten())) return false;
      if (java.lang.Double.doubleToLongBits(getRecordsWritten())
          != java.lang.Double.doubleToLongBits(
              other.getRecordsWritten())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleReadBytes())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleReadBytes())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleRecordsRead())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleRecordsRead())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleRemoteBlocksFetched())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleRemoteBlocksFetched())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleLocalBlocksFetched())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleLocalBlocksFetched())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleFetchWaitTime())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleFetchWaitTime())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleRemoteBytesRead())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleRemoteBytesRead())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleRemoteBytesReadToDisk())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleRemoteBytesReadToDisk())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleTotalBlocksFetched())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleTotalBlocksFetched())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleWriteBytes())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleWriteBytes())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleWriteRecords())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleWriteRecords())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleWriteTime())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleWriteTime())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleCorruptMergedBlockChunks())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleCorruptMergedBlockChunks())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleMergedFetchFallbackCount())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleMergedFetchFallbackCount())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleMergedRemoteBlocksFetched())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleMergedRemoteBlocksFetched())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleMergedLocalBlocksFetched())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleMergedLocalBlocksFetched())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleMergedRemoteChunksFetched())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleMergedRemoteChunksFetched())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleMergedLocalChunksFetched())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleMergedLocalChunksFetched())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleMergedRemoteBytesRead())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleMergedRemoteBytesRead())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleMergedLocalBytesRead())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleMergedLocalBytesRead())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleRemoteReqsDuration())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleRemoteReqsDuration())) return false;
      if (java.lang.Double.doubleToLongBits(getShuffleMergedRemoteReqsDuration())
          != java.lang.Double.doubleToLongBits(
              other.getShuffleMergedRemoteReqsDuration())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STAGE_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getStageId());
      hash = (37 * hash) + STAGE_ATTEMPT_ID_FIELD_NUMBER;
      hash = (53 * hash) + getStageAttemptId();
      if (hasQuantile()) {
        hash = (37 * hash) + QUANTILE_FIELD_NUMBER;
        hash = (53 * hash) + getQuantile().hashCode();
      }
      hash = (37 * hash) + TASK_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTaskCount());
      hash = (37 * hash) + DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getDuration()));
      hash = (37 * hash) + EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getExecutorDeserializeTime()));
      hash = (37 * hash) + EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getExecutorDeserializeCpuTime()));
      hash = (37 * hash) + EXECUTOR_RUN_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getExecutorRunTime()));
      hash = (37 * hash) + EXECUTOR_CPU_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getExecutorCpuTime()));
      hash = (37 * hash) + RESULT_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getResultSize()));
      hash = (37 * hash) + JVM_GC_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getJvmGcTime()));
      hash = (37 * hash) + RESULT_SERIALIZATION_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getResultSerializationTime()));
      hash = (37 * hash) + GETTING_RESULT_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getGettingResultTime()));
      hash = (37 * hash) + SCHEDULER_DELAY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getSchedulerDelay()));
      hash = (37 * hash) + PEAK_EXECUTION_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getPeakExecutionMemory()));
      hash = (37 * hash) + MEMORY_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getMemoryBytesSpilled()));
      hash = (37 * hash) + DISK_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getDiskBytesSpilled()));
      hash = (37 * hash) + BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getBytesRead()));
      hash = (37 * hash) + RECORDS_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getRecordsRead()));
      hash = (37 * hash) + BYTES_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getBytesWritten()));
      hash = (37 * hash) + RECORDS_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getRecordsWritten()));
      hash = (37 * hash) + SHUFFLE_READ_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleReadBytes()));
      hash = (37 * hash) + SHUFFLE_RECORDS_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleRecordsRead()));
      hash = (37 * hash) + SHUFFLE_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleRemoteBlocksFetched()));
      hash = (37 * hash) + SHUFFLE_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleLocalBlocksFetched()));
      hash = (37 * hash) + SHUFFLE_FETCH_WAIT_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleFetchWaitTime()));
      hash = (37 * hash) + SHUFFLE_REMOTE_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleRemoteBytesRead()));
      hash = (37 * hash) + SHUFFLE_REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleRemoteBytesReadToDisk()));
      hash = (37 * hash) + SHUFFLE_TOTAL_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleTotalBlocksFetched()));
      hash = (37 * hash) + SHUFFLE_WRITE_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleWriteBytes()));
      hash = (37 * hash) + SHUFFLE_WRITE_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleWriteRecords()));
      hash = (37 * hash) + SHUFFLE_WRITE_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleWriteTime()));
      hash = (37 * hash) + SHUFFLE_CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleCorruptMergedBlockChunks()));
      hash = (37 * hash) + SHUFFLE_MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleMergedFetchFallbackCount()));
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleMergedRemoteBlocksFetched()));
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleMergedLocalBlocksFetched()));
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_CHUNKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleMergedRemoteChunksFetched()));
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_CHUNKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleMergedLocalChunksFetched()));
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleMergedRemoteBytesRead()));
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleMergedLocalBytesRead()));
      hash = (37 * hash) + SHUFFLE_REMOTE_REQS_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleRemoteReqsDuration()));
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_REQS_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getShuffleMergedRemoteReqsDuration()));
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.CachedQuantile prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.CachedQuantile}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.CachedQuantile)
        org.apache.spark.status.protobuf.StoreTypes.CachedQuantileOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_CachedQuantile_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_CachedQuantile_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.CachedQuantile.class, org.apache.spark.status.protobuf.StoreTypes.CachedQuantile.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.CachedQuantile.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        stageId_ = 0L;

        stageAttemptId_ = 0;

        quantile_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        taskCount_ = 0L;

        duration_ = 0D;

        executorDeserializeTime_ = 0D;

        executorDeserializeCpuTime_ = 0D;

        executorRunTime_ = 0D;

        executorCpuTime_ = 0D;

        resultSize_ = 0D;

        jvmGcTime_ = 0D;

        resultSerializationTime_ = 0D;

        gettingResultTime_ = 0D;

        schedulerDelay_ = 0D;

        peakExecutionMemory_ = 0D;

        memoryBytesSpilled_ = 0D;

        diskBytesSpilled_ = 0D;

        bytesRead_ = 0D;

        recordsRead_ = 0D;

        bytesWritten_ = 0D;

        recordsWritten_ = 0D;

        shuffleReadBytes_ = 0D;

        shuffleRecordsRead_ = 0D;

        shuffleRemoteBlocksFetched_ = 0D;

        shuffleLocalBlocksFetched_ = 0D;

        shuffleFetchWaitTime_ = 0D;

        shuffleRemoteBytesRead_ = 0D;

        shuffleRemoteBytesReadToDisk_ = 0D;

        shuffleTotalBlocksFetched_ = 0D;

        shuffleWriteBytes_ = 0D;

        shuffleWriteRecords_ = 0D;

        shuffleWriteTime_ = 0D;

        shuffleCorruptMergedBlockChunks_ = 0D;

        shuffleMergedFetchFallbackCount_ = 0D;

        shuffleMergedRemoteBlocksFetched_ = 0D;

        shuffleMergedLocalBlocksFetched_ = 0D;

        shuffleMergedRemoteChunksFetched_ = 0D;

        shuffleMergedLocalChunksFetched_ = 0D;

        shuffleMergedRemoteBytesRead_ = 0D;

        shuffleMergedLocalBytesRead_ = 0D;

        shuffleRemoteReqsDuration_ = 0D;

        shuffleMergedRemoteReqsDuration_ = 0D;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_CachedQuantile_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.CachedQuantile getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.CachedQuantile.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.CachedQuantile build() {
        org.apache.spark.status.protobuf.StoreTypes.CachedQuantile result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.CachedQuantile buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.CachedQuantile result = new org.apache.spark.status.protobuf.StoreTypes.CachedQuantile(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.stageId_ = stageId_;
        result.stageAttemptId_ = stageAttemptId_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.quantile_ = quantile_;
        result.taskCount_ = taskCount_;
        result.duration_ = duration_;
        result.executorDeserializeTime_ = executorDeserializeTime_;
        result.executorDeserializeCpuTime_ = executorDeserializeCpuTime_;
        result.executorRunTime_ = executorRunTime_;
        result.executorCpuTime_ = executorCpuTime_;
        result.resultSize_ = resultSize_;
        result.jvmGcTime_ = jvmGcTime_;
        result.resultSerializationTime_ = resultSerializationTime_;
        result.gettingResultTime_ = gettingResultTime_;
        result.schedulerDelay_ = schedulerDelay_;
        result.peakExecutionMemory_ = peakExecutionMemory_;
        result.memoryBytesSpilled_ = memoryBytesSpilled_;
        result.diskBytesSpilled_ = diskBytesSpilled_;
        result.bytesRead_ = bytesRead_;
        result.recordsRead_ = recordsRead_;
        result.bytesWritten_ = bytesWritten_;
        result.recordsWritten_ = recordsWritten_;
        result.shuffleReadBytes_ = shuffleReadBytes_;
        result.shuffleRecordsRead_ = shuffleRecordsRead_;
        result.shuffleRemoteBlocksFetched_ = shuffleRemoteBlocksFetched_;
        result.shuffleLocalBlocksFetched_ = shuffleLocalBlocksFetched_;
        result.shuffleFetchWaitTime_ = shuffleFetchWaitTime_;
        result.shuffleRemoteBytesRead_ = shuffleRemoteBytesRead_;
        result.shuffleRemoteBytesReadToDisk_ = shuffleRemoteBytesReadToDisk_;
        result.shuffleTotalBlocksFetched_ = shuffleTotalBlocksFetched_;
        result.shuffleWriteBytes_ = shuffleWriteBytes_;
        result.shuffleWriteRecords_ = shuffleWriteRecords_;
        result.shuffleWriteTime_ = shuffleWriteTime_;
        result.shuffleCorruptMergedBlockChunks_ = shuffleCorruptMergedBlockChunks_;
        result.shuffleMergedFetchFallbackCount_ = shuffleMergedFetchFallbackCount_;
        result.shuffleMergedRemoteBlocksFetched_ = shuffleMergedRemoteBlocksFetched_;
        result.shuffleMergedLocalBlocksFetched_ = shuffleMergedLocalBlocksFetched_;
        result.shuffleMergedRemoteChunksFetched_ = shuffleMergedRemoteChunksFetched_;
        result.shuffleMergedLocalChunksFetched_ = shuffleMergedLocalChunksFetched_;
        result.shuffleMergedRemoteBytesRead_ = shuffleMergedRemoteBytesRead_;
        result.shuffleMergedLocalBytesRead_ = shuffleMergedLocalBytesRead_;
        result.shuffleRemoteReqsDuration_ = shuffleRemoteReqsDuration_;
        result.shuffleMergedRemoteReqsDuration_ = shuffleMergedRemoteReqsDuration_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.CachedQuantile) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.CachedQuantile)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.CachedQuantile other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.CachedQuantile.getDefaultInstance()) return this;
        if (other.getStageId() != 0L) {
          setStageId(other.getStageId());
        }
        if (other.getStageAttemptId() != 0) {
          setStageAttemptId(other.getStageAttemptId());
        }
        if (other.hasQuantile()) {
          bitField0_ |= 0x00000001;
          quantile_ = other.quantile_;
          onChanged();
        }
        if (other.getTaskCount() != 0L) {
          setTaskCount(other.getTaskCount());
        }
        if (other.getDuration() != 0D) {
          setDuration(other.getDuration());
        }
        if (other.getExecutorDeserializeTime() != 0D) {
          setExecutorDeserializeTime(other.getExecutorDeserializeTime());
        }
        if (other.getExecutorDeserializeCpuTime() != 0D) {
          setExecutorDeserializeCpuTime(other.getExecutorDeserializeCpuTime());
        }
        if (other.getExecutorRunTime() != 0D) {
          setExecutorRunTime(other.getExecutorRunTime());
        }
        if (other.getExecutorCpuTime() != 0D) {
          setExecutorCpuTime(other.getExecutorCpuTime());
        }
        if (other.getResultSize() != 0D) {
          setResultSize(other.getResultSize());
        }
        if (other.getJvmGcTime() != 0D) {
          setJvmGcTime(other.getJvmGcTime());
        }
        if (other.getResultSerializationTime() != 0D) {
          setResultSerializationTime(other.getResultSerializationTime());
        }
        if (other.getGettingResultTime() != 0D) {
          setGettingResultTime(other.getGettingResultTime());
        }
        if (other.getSchedulerDelay() != 0D) {
          setSchedulerDelay(other.getSchedulerDelay());
        }
        if (other.getPeakExecutionMemory() != 0D) {
          setPeakExecutionMemory(other.getPeakExecutionMemory());
        }
        if (other.getMemoryBytesSpilled() != 0D) {
          setMemoryBytesSpilled(other.getMemoryBytesSpilled());
        }
        if (other.getDiskBytesSpilled() != 0D) {
          setDiskBytesSpilled(other.getDiskBytesSpilled());
        }
        if (other.getBytesRead() != 0D) {
          setBytesRead(other.getBytesRead());
        }
        if (other.getRecordsRead() != 0D) {
          setRecordsRead(other.getRecordsRead());
        }
        if (other.getBytesWritten() != 0D) {
          setBytesWritten(other.getBytesWritten());
        }
        if (other.getRecordsWritten() != 0D) {
          setRecordsWritten(other.getRecordsWritten());
        }
        if (other.getShuffleReadBytes() != 0D) {
          setShuffleReadBytes(other.getShuffleReadBytes());
        }
        if (other.getShuffleRecordsRead() != 0D) {
          setShuffleRecordsRead(other.getShuffleRecordsRead());
        }
        if (other.getShuffleRemoteBlocksFetched() != 0D) {
          setShuffleRemoteBlocksFetched(other.getShuffleRemoteBlocksFetched());
        }
        if (other.getShuffleLocalBlocksFetched() != 0D) {
          setShuffleLocalBlocksFetched(other.getShuffleLocalBlocksFetched());
        }
        if (other.getShuffleFetchWaitTime() != 0D) {
          setShuffleFetchWaitTime(other.getShuffleFetchWaitTime());
        }
        if (other.getShuffleRemoteBytesRead() != 0D) {
          setShuffleRemoteBytesRead(other.getShuffleRemoteBytesRead());
        }
        if (other.getShuffleRemoteBytesReadToDisk() != 0D) {
          setShuffleRemoteBytesReadToDisk(other.getShuffleRemoteBytesReadToDisk());
        }
        if (other.getShuffleTotalBlocksFetched() != 0D) {
          setShuffleTotalBlocksFetched(other.getShuffleTotalBlocksFetched());
        }
        if (other.getShuffleWriteBytes() != 0D) {
          setShuffleWriteBytes(other.getShuffleWriteBytes());
        }
        if (other.getShuffleWriteRecords() != 0D) {
          setShuffleWriteRecords(other.getShuffleWriteRecords());
        }
        if (other.getShuffleWriteTime() != 0D) {
          setShuffleWriteTime(other.getShuffleWriteTime());
        }
        if (other.getShuffleCorruptMergedBlockChunks() != 0D) {
          setShuffleCorruptMergedBlockChunks(other.getShuffleCorruptMergedBlockChunks());
        }
        if (other.getShuffleMergedFetchFallbackCount() != 0D) {
          setShuffleMergedFetchFallbackCount(other.getShuffleMergedFetchFallbackCount());
        }
        if (other.getShuffleMergedRemoteBlocksFetched() != 0D) {
          setShuffleMergedRemoteBlocksFetched(other.getShuffleMergedRemoteBlocksFetched());
        }
        if (other.getShuffleMergedLocalBlocksFetched() != 0D) {
          setShuffleMergedLocalBlocksFetched(other.getShuffleMergedLocalBlocksFetched());
        }
        if (other.getShuffleMergedRemoteChunksFetched() != 0D) {
          setShuffleMergedRemoteChunksFetched(other.getShuffleMergedRemoteChunksFetched());
        }
        if (other.getShuffleMergedLocalChunksFetched() != 0D) {
          setShuffleMergedLocalChunksFetched(other.getShuffleMergedLocalChunksFetched());
        }
        if (other.getShuffleMergedRemoteBytesRead() != 0D) {
          setShuffleMergedRemoteBytesRead(other.getShuffleMergedRemoteBytesRead());
        }
        if (other.getShuffleMergedLocalBytesRead() != 0D) {
          setShuffleMergedLocalBytesRead(other.getShuffleMergedLocalBytesRead());
        }
        if (other.getShuffleRemoteReqsDuration() != 0D) {
          setShuffleRemoteReqsDuration(other.getShuffleRemoteReqsDuration());
        }
        if (other.getShuffleMergedRemoteReqsDuration() != 0D) {
          setShuffleMergedRemoteReqsDuration(other.getShuffleMergedRemoteReqsDuration());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.CachedQuantile parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.CachedQuantile) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long stageId_ ;
      /**
       * <code>int64 stage_id = 1;</code>
       * @return The stageId.
       */
      @java.lang.Override
      public long getStageId() {
        return stageId_;
      }
      /**
       * <code>int64 stage_id = 1;</code>
       * @param value The stageId to set.
       * @return This builder for chaining.
       */
      public Builder setStageId(long value) {
        
        stageId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 stage_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageId() {
        
        stageId_ = 0L;
        onChanged();
        return this;
      }

      private int stageAttemptId_ ;
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @return The stageAttemptId.
       */
      @java.lang.Override
      public int getStageAttemptId() {
        return stageAttemptId_;
      }
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @param value The stageAttemptId to set.
       * @return This builder for chaining.
       */
      public Builder setStageAttemptId(int value) {
        
        stageAttemptId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageAttemptId() {
        
        stageAttemptId_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object quantile_ = "";
      /**
       * <code>string quantile = 3;</code>
       * @return Whether the quantile field is set.
       */
      public boolean hasQuantile() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string quantile = 3;</code>
       * @return The quantile.
       */
      public java.lang.String getQuantile() {
        java.lang.Object ref = quantile_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          quantile_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string quantile = 3;</code>
       * @return The bytes for quantile.
       */
      public com.google.protobuf.ByteString
          getQuantileBytes() {
        java.lang.Object ref = quantile_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          quantile_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string quantile = 3;</code>
       * @param value The quantile to set.
       * @return This builder for chaining.
       */
      public Builder setQuantile(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        quantile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string quantile = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearQuantile() {
        bitField0_ = (bitField0_ & ~0x00000001);
        quantile_ = getDefaultInstance().getQuantile();
        onChanged();
        return this;
      }
      /**
       * <code>string quantile = 3;</code>
       * @param value The bytes for quantile to set.
       * @return This builder for chaining.
       */
      public Builder setQuantileBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        quantile_ = value;
        onChanged();
        return this;
      }

      private long taskCount_ ;
      /**
       * <code>int64 task_count = 4;</code>
       * @return The taskCount.
       */
      @java.lang.Override
      public long getTaskCount() {
        return taskCount_;
      }
      /**
       * <code>int64 task_count = 4;</code>
       * @param value The taskCount to set.
       * @return This builder for chaining.
       */
      public Builder setTaskCount(long value) {
        
        taskCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 task_count = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearTaskCount() {
        
        taskCount_ = 0L;
        onChanged();
        return this;
      }

      private double duration_ ;
      /**
       * <code>double duration = 5;</code>
       * @return The duration.
       */
      @java.lang.Override
      public double getDuration() {
        return duration_;
      }
      /**
       * <code>double duration = 5;</code>
       * @param value The duration to set.
       * @return This builder for chaining.
       */
      public Builder setDuration(double value) {
        
        duration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double duration = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearDuration() {
        
        duration_ = 0D;
        onChanged();
        return this;
      }

      private double executorDeserializeTime_ ;
      /**
       * <code>double executor_deserialize_time = 6;</code>
       * @return The executorDeserializeTime.
       */
      @java.lang.Override
      public double getExecutorDeserializeTime() {
        return executorDeserializeTime_;
      }
      /**
       * <code>double executor_deserialize_time = 6;</code>
       * @param value The executorDeserializeTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeTime(double value) {
        
        executorDeserializeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double executor_deserialize_time = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeTime() {
        
        executorDeserializeTime_ = 0D;
        onChanged();
        return this;
      }

      private double executorDeserializeCpuTime_ ;
      /**
       * <code>double executor_deserialize_cpu_time = 7;</code>
       * @return The executorDeserializeCpuTime.
       */
      @java.lang.Override
      public double getExecutorDeserializeCpuTime() {
        return executorDeserializeCpuTime_;
      }
      /**
       * <code>double executor_deserialize_cpu_time = 7;</code>
       * @param value The executorDeserializeCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeCpuTime(double value) {
        
        executorDeserializeCpuTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double executor_deserialize_cpu_time = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeCpuTime() {
        
        executorDeserializeCpuTime_ = 0D;
        onChanged();
        return this;
      }

      private double executorRunTime_ ;
      /**
       * <code>double executor_run_time = 8;</code>
       * @return The executorRunTime.
       */
      @java.lang.Override
      public double getExecutorRunTime() {
        return executorRunTime_;
      }
      /**
       * <code>double executor_run_time = 8;</code>
       * @param value The executorRunTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorRunTime(double value) {
        
        executorRunTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double executor_run_time = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorRunTime() {
        
        executorRunTime_ = 0D;
        onChanged();
        return this;
      }

      private double executorCpuTime_ ;
      /**
       * <code>double executor_cpu_time = 9;</code>
       * @return The executorCpuTime.
       */
      @java.lang.Override
      public double getExecutorCpuTime() {
        return executorCpuTime_;
      }
      /**
       * <code>double executor_cpu_time = 9;</code>
       * @param value The executorCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorCpuTime(double value) {
        
        executorCpuTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double executor_cpu_time = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorCpuTime() {
        
        executorCpuTime_ = 0D;
        onChanged();
        return this;
      }

      private double resultSize_ ;
      /**
       * <code>double result_size = 10;</code>
       * @return The resultSize.
       */
      @java.lang.Override
      public double getResultSize() {
        return resultSize_;
      }
      /**
       * <code>double result_size = 10;</code>
       * @param value The resultSize to set.
       * @return This builder for chaining.
       */
      public Builder setResultSize(double value) {
        
        resultSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double result_size = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSize() {
        
        resultSize_ = 0D;
        onChanged();
        return this;
      }

      private double jvmGcTime_ ;
      /**
       * <code>double jvm_gc_time = 11;</code>
       * @return The jvmGcTime.
       */
      @java.lang.Override
      public double getJvmGcTime() {
        return jvmGcTime_;
      }
      /**
       * <code>double jvm_gc_time = 11;</code>
       * @param value The jvmGcTime to set.
       * @return This builder for chaining.
       */
      public Builder setJvmGcTime(double value) {
        
        jvmGcTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double jvm_gc_time = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearJvmGcTime() {
        
        jvmGcTime_ = 0D;
        onChanged();
        return this;
      }

      private double resultSerializationTime_ ;
      /**
       * <code>double result_serialization_time = 12;</code>
       * @return The resultSerializationTime.
       */
      @java.lang.Override
      public double getResultSerializationTime() {
        return resultSerializationTime_;
      }
      /**
       * <code>double result_serialization_time = 12;</code>
       * @param value The resultSerializationTime to set.
       * @return This builder for chaining.
       */
      public Builder setResultSerializationTime(double value) {
        
        resultSerializationTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double result_serialization_time = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSerializationTime() {
        
        resultSerializationTime_ = 0D;
        onChanged();
        return this;
      }

      private double gettingResultTime_ ;
      /**
       * <code>double getting_result_time = 13;</code>
       * @return The gettingResultTime.
       */
      @java.lang.Override
      public double getGettingResultTime() {
        return gettingResultTime_;
      }
      /**
       * <code>double getting_result_time = 13;</code>
       * @param value The gettingResultTime to set.
       * @return This builder for chaining.
       */
      public Builder setGettingResultTime(double value) {
        
        gettingResultTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double getting_result_time = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearGettingResultTime() {
        
        gettingResultTime_ = 0D;
        onChanged();
        return this;
      }

      private double schedulerDelay_ ;
      /**
       * <code>double scheduler_delay = 14;</code>
       * @return The schedulerDelay.
       */
      @java.lang.Override
      public double getSchedulerDelay() {
        return schedulerDelay_;
      }
      /**
       * <code>double scheduler_delay = 14;</code>
       * @param value The schedulerDelay to set.
       * @return This builder for chaining.
       */
      public Builder setSchedulerDelay(double value) {
        
        schedulerDelay_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double scheduler_delay = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearSchedulerDelay() {
        
        schedulerDelay_ = 0D;
        onChanged();
        return this;
      }

      private double peakExecutionMemory_ ;
      /**
       * <code>double peak_execution_memory = 15;</code>
       * @return The peakExecutionMemory.
       */
      @java.lang.Override
      public double getPeakExecutionMemory() {
        return peakExecutionMemory_;
      }
      /**
       * <code>double peak_execution_memory = 15;</code>
       * @param value The peakExecutionMemory to set.
       * @return This builder for chaining.
       */
      public Builder setPeakExecutionMemory(double value) {
        
        peakExecutionMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double peak_execution_memory = 15;</code>
       * @return This builder for chaining.
       */
      public Builder clearPeakExecutionMemory() {
        
        peakExecutionMemory_ = 0D;
        onChanged();
        return this;
      }

      private double memoryBytesSpilled_ ;
      /**
       * <code>double memory_bytes_spilled = 16;</code>
       * @return The memoryBytesSpilled.
       */
      @java.lang.Override
      public double getMemoryBytesSpilled() {
        return memoryBytesSpilled_;
      }
      /**
       * <code>double memory_bytes_spilled = 16;</code>
       * @param value The memoryBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryBytesSpilled(double value) {
        
        memoryBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double memory_bytes_spilled = 16;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryBytesSpilled() {
        
        memoryBytesSpilled_ = 0D;
        onChanged();
        return this;
      }

      private double diskBytesSpilled_ ;
      /**
       * <code>double disk_bytes_spilled = 17;</code>
       * @return The diskBytesSpilled.
       */
      @java.lang.Override
      public double getDiskBytesSpilled() {
        return diskBytesSpilled_;
      }
      /**
       * <code>double disk_bytes_spilled = 17;</code>
       * @param value The diskBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setDiskBytesSpilled(double value) {
        
        diskBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double disk_bytes_spilled = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskBytesSpilled() {
        
        diskBytesSpilled_ = 0D;
        onChanged();
        return this;
      }

      private double bytesRead_ ;
      /**
       * <code>double bytes_read = 18;</code>
       * @return The bytesRead.
       */
      @java.lang.Override
      public double getBytesRead() {
        return bytesRead_;
      }
      /**
       * <code>double bytes_read = 18;</code>
       * @param value The bytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setBytesRead(double value) {
        
        bytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double bytes_read = 18;</code>
       * @return This builder for chaining.
       */
      public Builder clearBytesRead() {
        
        bytesRead_ = 0D;
        onChanged();
        return this;
      }

      private double recordsRead_ ;
      /**
       * <code>double records_read = 19;</code>
       * @return The recordsRead.
       */
      @java.lang.Override
      public double getRecordsRead() {
        return recordsRead_;
      }
      /**
       * <code>double records_read = 19;</code>
       * @param value The recordsRead to set.
       * @return This builder for chaining.
       */
      public Builder setRecordsRead(double value) {
        
        recordsRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double records_read = 19;</code>
       * @return This builder for chaining.
       */
      public Builder clearRecordsRead() {
        
        recordsRead_ = 0D;
        onChanged();
        return this;
      }

      private double bytesWritten_ ;
      /**
       * <code>double bytes_written = 20;</code>
       * @return The bytesWritten.
       */
      @java.lang.Override
      public double getBytesWritten() {
        return bytesWritten_;
      }
      /**
       * <code>double bytes_written = 20;</code>
       * @param value The bytesWritten to set.
       * @return This builder for chaining.
       */
      public Builder setBytesWritten(double value) {
        
        bytesWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double bytes_written = 20;</code>
       * @return This builder for chaining.
       */
      public Builder clearBytesWritten() {
        
        bytesWritten_ = 0D;
        onChanged();
        return this;
      }

      private double recordsWritten_ ;
      /**
       * <code>double records_written = 21;</code>
       * @return The recordsWritten.
       */
      @java.lang.Override
      public double getRecordsWritten() {
        return recordsWritten_;
      }
      /**
       * <code>double records_written = 21;</code>
       * @param value The recordsWritten to set.
       * @return This builder for chaining.
       */
      public Builder setRecordsWritten(double value) {
        
        recordsWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double records_written = 21;</code>
       * @return This builder for chaining.
       */
      public Builder clearRecordsWritten() {
        
        recordsWritten_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleReadBytes_ ;
      /**
       * <code>double shuffle_read_bytes = 22;</code>
       * @return The shuffleReadBytes.
       */
      @java.lang.Override
      public double getShuffleReadBytes() {
        return shuffleReadBytes_;
      }
      /**
       * <code>double shuffle_read_bytes = 22;</code>
       * @param value The shuffleReadBytes to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleReadBytes(double value) {
        
        shuffleReadBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_read_bytes = 22;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleReadBytes() {
        
        shuffleReadBytes_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleRecordsRead_ ;
      /**
       * <code>double shuffle_records_read = 23;</code>
       * @return The shuffleRecordsRead.
       */
      @java.lang.Override
      public double getShuffleRecordsRead() {
        return shuffleRecordsRead_;
      }
      /**
       * <code>double shuffle_records_read = 23;</code>
       * @param value The shuffleRecordsRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRecordsRead(double value) {
        
        shuffleRecordsRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_records_read = 23;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRecordsRead() {
        
        shuffleRecordsRead_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleRemoteBlocksFetched_ ;
      /**
       * <code>double shuffle_remote_blocks_fetched = 24;</code>
       * @return The shuffleRemoteBlocksFetched.
       */
      @java.lang.Override
      public double getShuffleRemoteBlocksFetched() {
        return shuffleRemoteBlocksFetched_;
      }
      /**
       * <code>double shuffle_remote_blocks_fetched = 24;</code>
       * @param value The shuffleRemoteBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBlocksFetched(double value) {
        
        shuffleRemoteBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_remote_blocks_fetched = 24;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBlocksFetched() {
        
        shuffleRemoteBlocksFetched_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleLocalBlocksFetched_ ;
      /**
       * <code>double shuffle_local_blocks_fetched = 25;</code>
       * @return The shuffleLocalBlocksFetched.
       */
      @java.lang.Override
      public double getShuffleLocalBlocksFetched() {
        return shuffleLocalBlocksFetched_;
      }
      /**
       * <code>double shuffle_local_blocks_fetched = 25;</code>
       * @param value The shuffleLocalBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleLocalBlocksFetched(double value) {
        
        shuffleLocalBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_local_blocks_fetched = 25;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleLocalBlocksFetched() {
        
        shuffleLocalBlocksFetched_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleFetchWaitTime_ ;
      /**
       * <code>double shuffle_fetch_wait_time = 26;</code>
       * @return The shuffleFetchWaitTime.
       */
      @java.lang.Override
      public double getShuffleFetchWaitTime() {
        return shuffleFetchWaitTime_;
      }
      /**
       * <code>double shuffle_fetch_wait_time = 26;</code>
       * @param value The shuffleFetchWaitTime to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleFetchWaitTime(double value) {
        
        shuffleFetchWaitTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_fetch_wait_time = 26;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleFetchWaitTime() {
        
        shuffleFetchWaitTime_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleRemoteBytesRead_ ;
      /**
       * <code>double shuffle_remote_bytes_read = 27;</code>
       * @return The shuffleRemoteBytesRead.
       */
      @java.lang.Override
      public double getShuffleRemoteBytesRead() {
        return shuffleRemoteBytesRead_;
      }
      /**
       * <code>double shuffle_remote_bytes_read = 27;</code>
       * @param value The shuffleRemoteBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBytesRead(double value) {
        
        shuffleRemoteBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_remote_bytes_read = 27;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBytesRead() {
        
        shuffleRemoteBytesRead_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleRemoteBytesReadToDisk_ ;
      /**
       * <code>double shuffle_remote_bytes_read_to_disk = 28;</code>
       * @return The shuffleRemoteBytesReadToDisk.
       */
      @java.lang.Override
      public double getShuffleRemoteBytesReadToDisk() {
        return shuffleRemoteBytesReadToDisk_;
      }
      /**
       * <code>double shuffle_remote_bytes_read_to_disk = 28;</code>
       * @param value The shuffleRemoteBytesReadToDisk to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBytesReadToDisk(double value) {
        
        shuffleRemoteBytesReadToDisk_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_remote_bytes_read_to_disk = 28;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBytesReadToDisk() {
        
        shuffleRemoteBytesReadToDisk_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleTotalBlocksFetched_ ;
      /**
       * <code>double shuffle_total_blocks_fetched = 29;</code>
       * @return The shuffleTotalBlocksFetched.
       */
      @java.lang.Override
      public double getShuffleTotalBlocksFetched() {
        return shuffleTotalBlocksFetched_;
      }
      /**
       * <code>double shuffle_total_blocks_fetched = 29;</code>
       * @param value The shuffleTotalBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleTotalBlocksFetched(double value) {
        
        shuffleTotalBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_total_blocks_fetched = 29;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleTotalBlocksFetched() {
        
        shuffleTotalBlocksFetched_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleWriteBytes_ ;
      /**
       * <code>double shuffle_write_bytes = 30;</code>
       * @return The shuffleWriteBytes.
       */
      @java.lang.Override
      public double getShuffleWriteBytes() {
        return shuffleWriteBytes_;
      }
      /**
       * <code>double shuffle_write_bytes = 30;</code>
       * @param value The shuffleWriteBytes to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteBytes(double value) {
        
        shuffleWriteBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_write_bytes = 30;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteBytes() {
        
        shuffleWriteBytes_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleWriteRecords_ ;
      /**
       * <code>double shuffle_write_records = 31;</code>
       * @return The shuffleWriteRecords.
       */
      @java.lang.Override
      public double getShuffleWriteRecords() {
        return shuffleWriteRecords_;
      }
      /**
       * <code>double shuffle_write_records = 31;</code>
       * @param value The shuffleWriteRecords to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteRecords(double value) {
        
        shuffleWriteRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_write_records = 31;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteRecords() {
        
        shuffleWriteRecords_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleWriteTime_ ;
      /**
       * <code>double shuffle_write_time = 32;</code>
       * @return The shuffleWriteTime.
       */
      @java.lang.Override
      public double getShuffleWriteTime() {
        return shuffleWriteTime_;
      }
      /**
       * <code>double shuffle_write_time = 32;</code>
       * @param value The shuffleWriteTime to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteTime(double value) {
        
        shuffleWriteTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_write_time = 32;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteTime() {
        
        shuffleWriteTime_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleCorruptMergedBlockChunks_ ;
      /**
       * <code>double shuffle_corrupt_merged_block_chunks = 33;</code>
       * @return The shuffleCorruptMergedBlockChunks.
       */
      @java.lang.Override
      public double getShuffleCorruptMergedBlockChunks() {
        return shuffleCorruptMergedBlockChunks_;
      }
      /**
       * <code>double shuffle_corrupt_merged_block_chunks = 33;</code>
       * @param value The shuffleCorruptMergedBlockChunks to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleCorruptMergedBlockChunks(double value) {
        
        shuffleCorruptMergedBlockChunks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_corrupt_merged_block_chunks = 33;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleCorruptMergedBlockChunks() {
        
        shuffleCorruptMergedBlockChunks_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleMergedFetchFallbackCount_ ;
      /**
       * <code>double shuffle_merged_fetch_fallback_count = 34;</code>
       * @return The shuffleMergedFetchFallbackCount.
       */
      @java.lang.Override
      public double getShuffleMergedFetchFallbackCount() {
        return shuffleMergedFetchFallbackCount_;
      }
      /**
       * <code>double shuffle_merged_fetch_fallback_count = 34;</code>
       * @param value The shuffleMergedFetchFallbackCount to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedFetchFallbackCount(double value) {
        
        shuffleMergedFetchFallbackCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_merged_fetch_fallback_count = 34;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedFetchFallbackCount() {
        
        shuffleMergedFetchFallbackCount_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleMergedRemoteBlocksFetched_ ;
      /**
       * <code>double shuffle_merged_remote_blocks_fetched = 35;</code>
       * @return The shuffleMergedRemoteBlocksFetched.
       */
      @java.lang.Override
      public double getShuffleMergedRemoteBlocksFetched() {
        return shuffleMergedRemoteBlocksFetched_;
      }
      /**
       * <code>double shuffle_merged_remote_blocks_fetched = 35;</code>
       * @param value The shuffleMergedRemoteBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteBlocksFetched(double value) {
        
        shuffleMergedRemoteBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_merged_remote_blocks_fetched = 35;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteBlocksFetched() {
        
        shuffleMergedRemoteBlocksFetched_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleMergedLocalBlocksFetched_ ;
      /**
       * <code>double shuffle_merged_local_blocks_fetched = 36;</code>
       * @return The shuffleMergedLocalBlocksFetched.
       */
      @java.lang.Override
      public double getShuffleMergedLocalBlocksFetched() {
        return shuffleMergedLocalBlocksFetched_;
      }
      /**
       * <code>double shuffle_merged_local_blocks_fetched = 36;</code>
       * @param value The shuffleMergedLocalBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalBlocksFetched(double value) {
        
        shuffleMergedLocalBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_merged_local_blocks_fetched = 36;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalBlocksFetched() {
        
        shuffleMergedLocalBlocksFetched_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleMergedRemoteChunksFetched_ ;
      /**
       * <code>double shuffle_merged_remote_chunks_fetched = 37;</code>
       * @return The shuffleMergedRemoteChunksFetched.
       */
      @java.lang.Override
      public double getShuffleMergedRemoteChunksFetched() {
        return shuffleMergedRemoteChunksFetched_;
      }
      /**
       * <code>double shuffle_merged_remote_chunks_fetched = 37;</code>
       * @param value The shuffleMergedRemoteChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteChunksFetched(double value) {
        
        shuffleMergedRemoteChunksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_merged_remote_chunks_fetched = 37;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteChunksFetched() {
        
        shuffleMergedRemoteChunksFetched_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleMergedLocalChunksFetched_ ;
      /**
       * <code>double shuffle_merged_local_chunks_fetched = 38;</code>
       * @return The shuffleMergedLocalChunksFetched.
       */
      @java.lang.Override
      public double getShuffleMergedLocalChunksFetched() {
        return shuffleMergedLocalChunksFetched_;
      }
      /**
       * <code>double shuffle_merged_local_chunks_fetched = 38;</code>
       * @param value The shuffleMergedLocalChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalChunksFetched(double value) {
        
        shuffleMergedLocalChunksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_merged_local_chunks_fetched = 38;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalChunksFetched() {
        
        shuffleMergedLocalChunksFetched_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleMergedRemoteBytesRead_ ;
      /**
       * <code>double shuffle_merged_remote_bytes_read = 39;</code>
       * @return The shuffleMergedRemoteBytesRead.
       */
      @java.lang.Override
      public double getShuffleMergedRemoteBytesRead() {
        return shuffleMergedRemoteBytesRead_;
      }
      /**
       * <code>double shuffle_merged_remote_bytes_read = 39;</code>
       * @param value The shuffleMergedRemoteBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteBytesRead(double value) {
        
        shuffleMergedRemoteBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_merged_remote_bytes_read = 39;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteBytesRead() {
        
        shuffleMergedRemoteBytesRead_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleMergedLocalBytesRead_ ;
      /**
       * <code>double shuffle_merged_local_bytes_read = 40;</code>
       * @return The shuffleMergedLocalBytesRead.
       */
      @java.lang.Override
      public double getShuffleMergedLocalBytesRead() {
        return shuffleMergedLocalBytesRead_;
      }
      /**
       * <code>double shuffle_merged_local_bytes_read = 40;</code>
       * @param value The shuffleMergedLocalBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalBytesRead(double value) {
        
        shuffleMergedLocalBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_merged_local_bytes_read = 40;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalBytesRead() {
        
        shuffleMergedLocalBytesRead_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleRemoteReqsDuration_ ;
      /**
       * <code>double shuffle_remote_reqs_duration = 41;</code>
       * @return The shuffleRemoteReqsDuration.
       */
      @java.lang.Override
      public double getShuffleRemoteReqsDuration() {
        return shuffleRemoteReqsDuration_;
      }
      /**
       * <code>double shuffle_remote_reqs_duration = 41;</code>
       * @param value The shuffleRemoteReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteReqsDuration(double value) {
        
        shuffleRemoteReqsDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_remote_reqs_duration = 41;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteReqsDuration() {
        
        shuffleRemoteReqsDuration_ = 0D;
        onChanged();
        return this;
      }

      private double shuffleMergedRemoteReqsDuration_ ;
      /**
       * <code>double shuffle_merged_remote_reqs_duration = 42;</code>
       * @return The shuffleMergedRemoteReqsDuration.
       */
      @java.lang.Override
      public double getShuffleMergedRemoteReqsDuration() {
        return shuffleMergedRemoteReqsDuration_;
      }
      /**
       * <code>double shuffle_merged_remote_reqs_duration = 42;</code>
       * @param value The shuffleMergedRemoteReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteReqsDuration(double value) {
        
        shuffleMergedRemoteReqsDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double shuffle_merged_remote_reqs_duration = 42;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteReqsDuration() {
        
        shuffleMergedRemoteReqsDuration_ = 0D;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.CachedQuantile)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.CachedQuantile)
    private static final org.apache.spark.status.protobuf.StoreTypes.CachedQuantile DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.CachedQuantile();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.CachedQuantile getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<CachedQuantile>
        PARSER = new com.google.protobuf.AbstractParser<CachedQuantile>() {
      @java.lang.Override
      public CachedQuantile parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CachedQuantile(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CachedQuantile> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CachedQuantile> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.CachedQuantile getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SpeculationStageSummaryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SpeculationStageSummary)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int32 num_tasks = 1;</code>
     * @return The numTasks.
     */
    int getNumTasks();

    /**
     * <code>int32 num_active_tasks = 2;</code>
     * @return The numActiveTasks.
     */
    int getNumActiveTasks();

    /**
     * <code>int32 num_completed_tasks = 3;</code>
     * @return The numCompletedTasks.
     */
    int getNumCompletedTasks();

    /**
     * <code>int32 num_failed_tasks = 4;</code>
     * @return The numFailedTasks.
     */
    int getNumFailedTasks();

    /**
     * <code>int32 num_killed_tasks = 5;</code>
     * @return The numKilledTasks.
     */
    int getNumKilledTasks();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SpeculationStageSummary}
   */
  public static final class SpeculationStageSummary extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SpeculationStageSummary)
      SpeculationStageSummaryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SpeculationStageSummary.newBuilder() to construct.
    private SpeculationStageSummary(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SpeculationStageSummary() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SpeculationStageSummary();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SpeculationStageSummary(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              numTasks_ = input.readInt32();
              break;
            }
            case 16: {

              numActiveTasks_ = input.readInt32();
              break;
            }
            case 24: {

              numCompletedTasks_ = input.readInt32();
              break;
            }
            case 32: {

              numFailedTasks_ = input.readInt32();
              break;
            }
            case 40: {

              numKilledTasks_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.class, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder.class);
    }

    public static final int NUM_TASKS_FIELD_NUMBER = 1;
    private int numTasks_;
    /**
     * <code>int32 num_tasks = 1;</code>
     * @return The numTasks.
     */
    @java.lang.Override
    public int getNumTasks() {
      return numTasks_;
    }

    public static final int NUM_ACTIVE_TASKS_FIELD_NUMBER = 2;
    private int numActiveTasks_;
    /**
     * <code>int32 num_active_tasks = 2;</code>
     * @return The numActiveTasks.
     */
    @java.lang.Override
    public int getNumActiveTasks() {
      return numActiveTasks_;
    }

    public static final int NUM_COMPLETED_TASKS_FIELD_NUMBER = 3;
    private int numCompletedTasks_;
    /**
     * <code>int32 num_completed_tasks = 3;</code>
     * @return The numCompletedTasks.
     */
    @java.lang.Override
    public int getNumCompletedTasks() {
      return numCompletedTasks_;
    }

    public static final int NUM_FAILED_TASKS_FIELD_NUMBER = 4;
    private int numFailedTasks_;
    /**
     * <code>int32 num_failed_tasks = 4;</code>
     * @return The numFailedTasks.
     */
    @java.lang.Override
    public int getNumFailedTasks() {
      return numFailedTasks_;
    }

    public static final int NUM_KILLED_TASKS_FIELD_NUMBER = 5;
    private int numKilledTasks_;
    /**
     * <code>int32 num_killed_tasks = 5;</code>
     * @return The numKilledTasks.
     */
    @java.lang.Override
    public int getNumKilledTasks() {
      return numKilledTasks_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (numTasks_ != 0) {
        output.writeInt32(1, numTasks_);
      }
      if (numActiveTasks_ != 0) {
        output.writeInt32(2, numActiveTasks_);
      }
      if (numCompletedTasks_ != 0) {
        output.writeInt32(3, numCompletedTasks_);
      }
      if (numFailedTasks_ != 0) {
        output.writeInt32(4, numFailedTasks_);
      }
      if (numKilledTasks_ != 0) {
        output.writeInt32(5, numKilledTasks_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (numTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, numTasks_);
      }
      if (numActiveTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, numActiveTasks_);
      }
      if (numCompletedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, numCompletedTasks_);
      }
      if (numFailedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, numFailedTasks_);
      }
      if (numKilledTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(5, numKilledTasks_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary other = (org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary) obj;

      if (getNumTasks()
          != other.getNumTasks()) return false;
      if (getNumActiveTasks()
          != other.getNumActiveTasks()) return false;
      if (getNumCompletedTasks()
          != other.getNumCompletedTasks()) return false;
      if (getNumFailedTasks()
          != other.getNumFailedTasks()) return false;
      if (getNumKilledTasks()
          != other.getNumKilledTasks()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NUM_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumTasks();
      hash = (37 * hash) + NUM_ACTIVE_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumActiveTasks();
      hash = (37 * hash) + NUM_COMPLETED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumCompletedTasks();
      hash = (37 * hash) + NUM_FAILED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumFailedTasks();
      hash = (37 * hash) + NUM_KILLED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumKilledTasks();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SpeculationStageSummary}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SpeculationStageSummary)
        org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.class, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        numTasks_ = 0;

        numActiveTasks_ = 0;

        numCompletedTasks_ = 0;

        numFailedTasks_ = 0;

        numKilledTasks_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary build() {
        org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary result = new org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary(this);
        result.numTasks_ = numTasks_;
        result.numActiveTasks_ = numActiveTasks_;
        result.numCompletedTasks_ = numCompletedTasks_;
        result.numFailedTasks_ = numFailedTasks_;
        result.numKilledTasks_ = numKilledTasks_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance()) return this;
        if (other.getNumTasks() != 0) {
          setNumTasks(other.getNumTasks());
        }
        if (other.getNumActiveTasks() != 0) {
          setNumActiveTasks(other.getNumActiveTasks());
        }
        if (other.getNumCompletedTasks() != 0) {
          setNumCompletedTasks(other.getNumCompletedTasks());
        }
        if (other.getNumFailedTasks() != 0) {
          setNumFailedTasks(other.getNumFailedTasks());
        }
        if (other.getNumKilledTasks() != 0) {
          setNumKilledTasks(other.getNumKilledTasks());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private int numTasks_ ;
      /**
       * <code>int32 num_tasks = 1;</code>
       * @return The numTasks.
       */
      @java.lang.Override
      public int getNumTasks() {
        return numTasks_;
      }
      /**
       * <code>int32 num_tasks = 1;</code>
       * @param value The numTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumTasks(int value) {
        
        numTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_tasks = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumTasks() {
        
        numTasks_ = 0;
        onChanged();
        return this;
      }

      private int numActiveTasks_ ;
      /**
       * <code>int32 num_active_tasks = 2;</code>
       * @return The numActiveTasks.
       */
      @java.lang.Override
      public int getNumActiveTasks() {
        return numActiveTasks_;
      }
      /**
       * <code>int32 num_active_tasks = 2;</code>
       * @param value The numActiveTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumActiveTasks(int value) {
        
        numActiveTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_active_tasks = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumActiveTasks() {
        
        numActiveTasks_ = 0;
        onChanged();
        return this;
      }

      private int numCompletedTasks_ ;
      /**
       * <code>int32 num_completed_tasks = 3;</code>
       * @return The numCompletedTasks.
       */
      @java.lang.Override
      public int getNumCompletedTasks() {
        return numCompletedTasks_;
      }
      /**
       * <code>int32 num_completed_tasks = 3;</code>
       * @param value The numCompletedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumCompletedTasks(int value) {
        
        numCompletedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_completed_tasks = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCompletedTasks() {
        
        numCompletedTasks_ = 0;
        onChanged();
        return this;
      }

      private int numFailedTasks_ ;
      /**
       * <code>int32 num_failed_tasks = 4;</code>
       * @return The numFailedTasks.
       */
      @java.lang.Override
      public int getNumFailedTasks() {
        return numFailedTasks_;
      }
      /**
       * <code>int32 num_failed_tasks = 4;</code>
       * @param value The numFailedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumFailedTasks(int value) {
        
        numFailedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_failed_tasks = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumFailedTasks() {
        
        numFailedTasks_ = 0;
        onChanged();
        return this;
      }

      private int numKilledTasks_ ;
      /**
       * <code>int32 num_killed_tasks = 5;</code>
       * @return The numKilledTasks.
       */
      @java.lang.Override
      public int getNumKilledTasks() {
        return numKilledTasks_;
      }
      /**
       * <code>int32 num_killed_tasks = 5;</code>
       * @param value The numKilledTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumKilledTasks(int value) {
        
        numKilledTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_killed_tasks = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumKilledTasks() {
        
        numKilledTasks_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SpeculationStageSummary)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SpeculationStageSummary)
    private static final org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SpeculationStageSummary>
        PARSER = new com.google.protobuf.AbstractParser<SpeculationStageSummary>() {
      @java.lang.Override
      public SpeculationStageSummary parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SpeculationStageSummary(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SpeculationStageSummary> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SpeculationStageSummary> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SpeculationStageSummaryWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SpeculationStageSummaryWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 stage_id = 1;</code>
     * @return The stageId.
     */
    long getStageId();

    /**
     * <code>int32 stage_attempt_id = 2;</code>
     * @return The stageAttemptId.
     */
    int getStageAttemptId();

    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder getInfoOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SpeculationStageSummaryWrapper}
   */
  public static final class SpeculationStageSummaryWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SpeculationStageSummaryWrapper)
      SpeculationStageSummaryWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SpeculationStageSummaryWrapper.newBuilder() to construct.
    private SpeculationStageSummaryWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SpeculationStageSummaryWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SpeculationStageSummaryWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SpeculationStageSummaryWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              stageId_ = input.readInt64();
              break;
            }
            case 16: {

              stageAttemptId_ = input.readInt32();
              break;
            }
            case 26: {
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper.class, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper.Builder.class);
    }

    public static final int STAGE_ID_FIELD_NUMBER = 1;
    private long stageId_;
    /**
     * <code>int64 stage_id = 1;</code>
     * @return The stageId.
     */
    @java.lang.Override
    public long getStageId() {
      return stageId_;
    }

    public static final int STAGE_ATTEMPT_ID_FIELD_NUMBER = 2;
    private int stageAttemptId_;
    /**
     * <code>int32 stage_attempt_id = 2;</code>
     * @return The stageAttemptId.
     */
    @java.lang.Override
    public int getStageAttemptId() {
      return stageAttemptId_;
    }

    public static final int INFO_FIELD_NUMBER = 3;
    private org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary info_;
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (stageId_ != 0L) {
        output.writeInt64(1, stageId_);
      }
      if (stageAttemptId_ != 0) {
        output.writeInt32(2, stageAttemptId_);
      }
      if (info_ != null) {
        output.writeMessage(3, getInfo());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (stageId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, stageId_);
      }
      if (stageAttemptId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, stageAttemptId_);
      }
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getInfo());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper other = (org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper) obj;

      if (getStageId()
          != other.getStageId()) return false;
      if (getStageAttemptId()
          != other.getStageAttemptId()) return false;
      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STAGE_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getStageId());
      hash = (37 * hash) + STAGE_ATTEMPT_ID_FIELD_NUMBER;
      hash = (53 * hash) + getStageAttemptId();
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SpeculationStageSummaryWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SpeculationStageSummaryWrapper)
        org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper.class, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        stageId_ = 0L;

        stageAttemptId_ = 0;

        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper result = new org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper(this);
        result.stageId_ = stageId_;
        result.stageAttemptId_ = stageAttemptId_;
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper.getDefaultInstance()) return this;
        if (other.getStageId() != 0L) {
          setStageId(other.getStageId());
        }
        if (other.getStageAttemptId() != 0) {
          setStageAttemptId(other.getStageAttemptId());
        }
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long stageId_ ;
      /**
       * <code>int64 stage_id = 1;</code>
       * @return The stageId.
       */
      @java.lang.Override
      public long getStageId() {
        return stageId_;
      }
      /**
       * <code>int64 stage_id = 1;</code>
       * @param value The stageId to set.
       * @return This builder for chaining.
       */
      public Builder setStageId(long value) {
        
        stageId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 stage_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageId() {
        
        stageId_ = 0L;
        onChanged();
        return this;
      }

      private int stageAttemptId_ ;
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @return The stageAttemptId.
       */
      @java.lang.Override
      public int getStageAttemptId() {
        return stageAttemptId_;
      }
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @param value The stageAttemptId to set.
       * @return This builder for chaining.
       */
      public Builder setStageAttemptId(int value) {
        
        stageAttemptId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 stage_attempt_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageAttemptId() {
        
        stageAttemptId_ = 0;
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary info = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SpeculationStageSummaryWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SpeculationStageSummaryWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SpeculationStageSummaryWrapper>
        PARSER = new com.google.protobuf.AbstractParser<SpeculationStageSummaryWrapper>() {
      @java.lang.Override
      public SpeculationStageSummaryWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SpeculationStageSummaryWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SpeculationStageSummaryWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SpeculationStageSummaryWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ProcessSummaryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ProcessSummary)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    boolean hasId();
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    java.lang.String getId();
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    com.google.protobuf.ByteString
        getIdBytes();

    /**
     * <code>string host_port = 2;</code>
     * @return Whether the hostPort field is set.
     */
    boolean hasHostPort();
    /**
     * <code>string host_port = 2;</code>
     * @return The hostPort.
     */
    java.lang.String getHostPort();
    /**
     * <code>string host_port = 2;</code>
     * @return The bytes for hostPort.
     */
    com.google.protobuf.ByteString
        getHostPortBytes();

    /**
     * <code>bool is_active = 3;</code>
     * @return The isActive.
     */
    boolean getIsActive();

    /**
     * <code>int32 total_cores = 4;</code>
     * @return The totalCores.
     */
    int getTotalCores();

    /**
     * <code>int64 add_time = 5;</code>
     * @return The addTime.
     */
    long getAddTime();

    /**
     * <code>int64 remove_time = 6;</code>
     * @return Whether the removeTime field is set.
     */
    boolean hasRemoveTime();
    /**
     * <code>int64 remove_time = 6;</code>
     * @return The removeTime.
     */
    long getRemoveTime();

    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */
    int getProcessLogsCount();
    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */
    boolean containsProcessLogs(
        java.lang.String key);
    /**
     * Use {@link #getProcessLogsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getProcessLogs();
    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getProcessLogsMap();
    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */

    java.lang.String getProcessLogsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */

    java.lang.String getProcessLogsOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ProcessSummary}
   */
  public static final class ProcessSummary extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ProcessSummary)
      ProcessSummaryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ProcessSummary.newBuilder() to construct.
    private ProcessSummary(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ProcessSummary() {
      id_ = "";
      hostPort_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ProcessSummary();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ProcessSummary(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              id_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              hostPort_ = s;
              break;
            }
            case 24: {

              isActive_ = input.readBool();
              break;
            }
            case 32: {

              totalCores_ = input.readInt32();
              break;
            }
            case 40: {

              addTime_ = input.readInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000004;
              removeTime_ = input.readInt64();
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                processLogs_ = com.google.protobuf.MapField.newMapField(
                    ProcessLogsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000008;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              processLogs__ = input.readMessage(
                  ProcessLogsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              processLogs_.getMutableMap().put(
                  processLogs__.getKey(), processLogs__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummary_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 7:
          return internalGetProcessLogs();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummary_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.class, org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object id_;
    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    @java.lang.Override
    public boolean hasId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public java.lang.String getId() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        id_ = s;
        return s;
      }
    }
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getIdBytes() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        id_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int HOST_PORT_FIELD_NUMBER = 2;
    private volatile java.lang.Object hostPort_;
    /**
     * <code>string host_port = 2;</code>
     * @return Whether the hostPort field is set.
     */
    @java.lang.Override
    public boolean hasHostPort() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string host_port = 2;</code>
     * @return The hostPort.
     */
    @java.lang.Override
    public java.lang.String getHostPort() {
      java.lang.Object ref = hostPort_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        hostPort_ = s;
        return s;
      }
    }
    /**
     * <code>string host_port = 2;</code>
     * @return The bytes for hostPort.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getHostPortBytes() {
      java.lang.Object ref = hostPort_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        hostPort_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int IS_ACTIVE_FIELD_NUMBER = 3;
    private boolean isActive_;
    /**
     * <code>bool is_active = 3;</code>
     * @return The isActive.
     */
    @java.lang.Override
    public boolean getIsActive() {
      return isActive_;
    }

    public static final int TOTAL_CORES_FIELD_NUMBER = 4;
    private int totalCores_;
    /**
     * <code>int32 total_cores = 4;</code>
     * @return The totalCores.
     */
    @java.lang.Override
    public int getTotalCores() {
      return totalCores_;
    }

    public static final int ADD_TIME_FIELD_NUMBER = 5;
    private long addTime_;
    /**
     * <code>int64 add_time = 5;</code>
     * @return The addTime.
     */
    @java.lang.Override
    public long getAddTime() {
      return addTime_;
    }

    public static final int REMOVE_TIME_FIELD_NUMBER = 6;
    private long removeTime_;
    /**
     * <code>int64 remove_time = 6;</code>
     * @return Whether the removeTime field is set.
     */
    @java.lang.Override
    public boolean hasRemoveTime() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>int64 remove_time = 6;</code>
     * @return The removeTime.
     */
    @java.lang.Override
    public long getRemoveTime() {
      return removeTime_;
    }

    public static final int PROCESS_LOGS_FIELD_NUMBER = 7;
    private static final class ProcessLogsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummary_ProcessLogsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> processLogs_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetProcessLogs() {
      if (processLogs_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ProcessLogsDefaultEntryHolder.defaultEntry);
      }
      return processLogs_;
    }

    public int getProcessLogsCount() {
      return internalGetProcessLogs().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */

    @java.lang.Override
    public boolean containsProcessLogs(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetProcessLogs().getMap().containsKey(key);
    }
    /**
     * Use {@link #getProcessLogsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getProcessLogs() {
      return getProcessLogsMap();
    }
    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getProcessLogsMap() {
      return internalGetProcessLogs().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */
    @java.lang.Override

    public java.lang.String getProcessLogsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetProcessLogs().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; process_logs = 7;</code>
     */
    @java.lang.Override

    public java.lang.String getProcessLogsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetProcessLogs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, hostPort_);
      }
      if (isActive_ != false) {
        output.writeBool(3, isActive_);
      }
      if (totalCores_ != 0) {
        output.writeInt32(4, totalCores_);
      }
      if (addTime_ != 0L) {
        output.writeInt64(5, addTime_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt64(6, removeTime_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetProcessLogs(),
          ProcessLogsDefaultEntryHolder.defaultEntry,
          7);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, hostPort_);
      }
      if (isActive_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, isActive_);
      }
      if (totalCores_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, totalCores_);
      }
      if (addTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, addTime_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, removeTime_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetProcessLogs().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        processLogs__ = ProcessLogsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(7, processLogs__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ProcessSummary)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ProcessSummary other = (org.apache.spark.status.protobuf.StoreTypes.ProcessSummary) obj;

      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (!getId()
            .equals(other.getId())) return false;
      }
      if (hasHostPort() != other.hasHostPort()) return false;
      if (hasHostPort()) {
        if (!getHostPort()
            .equals(other.getHostPort())) return false;
      }
      if (getIsActive()
          != other.getIsActive()) return false;
      if (getTotalCores()
          != other.getTotalCores()) return false;
      if (getAddTime()
          != other.getAddTime()) return false;
      if (hasRemoveTime() != other.hasRemoveTime()) return false;
      if (hasRemoveTime()) {
        if (getRemoveTime()
            != other.getRemoveTime()) return false;
      }
      if (!internalGetProcessLogs().equals(
          other.internalGetProcessLogs())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId().hashCode();
      }
      if (hasHostPort()) {
        hash = (37 * hash) + HOST_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getHostPort().hashCode();
      }
      hash = (37 * hash) + IS_ACTIVE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsActive());
      hash = (37 * hash) + TOTAL_CORES_FIELD_NUMBER;
      hash = (53 * hash) + getTotalCores();
      hash = (37 * hash) + ADD_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getAddTime());
      if (hasRemoveTime()) {
        hash = (37 * hash) + REMOVE_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getRemoveTime());
      }
      if (!internalGetProcessLogs().getMap().isEmpty()) {
        hash = (37 * hash) + PROCESS_LOGS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetProcessLogs().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ProcessSummary prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ProcessSummary}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ProcessSummary)
        org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummary_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 7:
            return internalGetProcessLogs();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 7:
            return internalGetMutableProcessLogs();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummary_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.class, org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        hostPort_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        isActive_ = false;

        totalCores_ = 0;

        addTime_ = 0L;

        removeTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        internalGetMutableProcessLogs().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummary_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummary getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummary build() {
        org.apache.spark.status.protobuf.StoreTypes.ProcessSummary result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummary buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ProcessSummary result = new org.apache.spark.status.protobuf.StoreTypes.ProcessSummary(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.hostPort_ = hostPort_;
        result.isActive_ = isActive_;
        result.totalCores_ = totalCores_;
        result.addTime_ = addTime_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.removeTime_ = removeTime_;
          to_bitField0_ |= 0x00000004;
        }
        result.processLogs_ = internalGetProcessLogs();
        result.processLogs_.makeImmutable();
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ProcessSummary) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ProcessSummary)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ProcessSummary other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.getDefaultInstance()) return this;
        if (other.hasId()) {
          bitField0_ |= 0x00000001;
          id_ = other.id_;
          onChanged();
        }
        if (other.hasHostPort()) {
          bitField0_ |= 0x00000002;
          hostPort_ = other.hostPort_;
          onChanged();
        }
        if (other.getIsActive() != false) {
          setIsActive(other.getIsActive());
        }
        if (other.getTotalCores() != 0) {
          setTotalCores(other.getTotalCores());
        }
        if (other.getAddTime() != 0L) {
          setAddTime(other.getAddTime());
        }
        if (other.hasRemoveTime()) {
          setRemoveTime(other.getRemoveTime());
        }
        internalGetMutableProcessLogs().mergeFrom(
            other.internalGetProcessLogs());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ProcessSummary parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ProcessSummary) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object id_ = "";
      /**
       * <code>string id = 1;</code>
       * @return Whether the id field is set.
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string id = 1;</code>
       * @return The id.
       */
      public java.lang.String getId() {
        java.lang.Object ref = id_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          id_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @return The bytes for id.
       */
      public com.google.protobuf.ByteString
          getIdBytes() {
        java.lang.Object ref = id_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          id_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = getDefaultInstance().getId();
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @param value The bytes for id to set.
       * @return This builder for chaining.
       */
      public Builder setIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object hostPort_ = "";
      /**
       * <code>string host_port = 2;</code>
       * @return Whether the hostPort field is set.
       */
      public boolean hasHostPort() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string host_port = 2;</code>
       * @return The hostPort.
       */
      public java.lang.String getHostPort() {
        java.lang.Object ref = hostPort_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          hostPort_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string host_port = 2;</code>
       * @return The bytes for hostPort.
       */
      public com.google.protobuf.ByteString
          getHostPortBytes() {
        java.lang.Object ref = hostPort_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          hostPort_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string host_port = 2;</code>
       * @param value The hostPort to set.
       * @return This builder for chaining.
       */
      public Builder setHostPort(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        hostPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string host_port = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearHostPort() {
        bitField0_ = (bitField0_ & ~0x00000002);
        hostPort_ = getDefaultInstance().getHostPort();
        onChanged();
        return this;
      }
      /**
       * <code>string host_port = 2;</code>
       * @param value The bytes for hostPort to set.
       * @return This builder for chaining.
       */
      public Builder setHostPortBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        hostPort_ = value;
        onChanged();
        return this;
      }

      private boolean isActive_ ;
      /**
       * <code>bool is_active = 3;</code>
       * @return The isActive.
       */
      @java.lang.Override
      public boolean getIsActive() {
        return isActive_;
      }
      /**
       * <code>bool is_active = 3;</code>
       * @param value The isActive to set.
       * @return This builder for chaining.
       */
      public Builder setIsActive(boolean value) {
        
        isActive_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool is_active = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsActive() {
        
        isActive_ = false;
        onChanged();
        return this;
      }

      private int totalCores_ ;
      /**
       * <code>int32 total_cores = 4;</code>
       * @return The totalCores.
       */
      @java.lang.Override
      public int getTotalCores() {
        return totalCores_;
      }
      /**
       * <code>int32 total_cores = 4;</code>
       * @param value The totalCores to set.
       * @return This builder for chaining.
       */
      public Builder setTotalCores(int value) {
        
        totalCores_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 total_cores = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalCores() {
        
        totalCores_ = 0;
        onChanged();
        return this;
      }

      private long addTime_ ;
      /**
       * <code>int64 add_time = 5;</code>
       * @return The addTime.
       */
      @java.lang.Override
      public long getAddTime() {
        return addTime_;
      }
      /**
       * <code>int64 add_time = 5;</code>
       * @param value The addTime to set.
       * @return This builder for chaining.
       */
      public Builder setAddTime(long value) {
        
        addTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 add_time = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearAddTime() {
        
        addTime_ = 0L;
        onChanged();
        return this;
      }

      private long removeTime_ ;
      /**
       * <code>int64 remove_time = 6;</code>
       * @return Whether the removeTime field is set.
       */
      @java.lang.Override
      public boolean hasRemoveTime() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>int64 remove_time = 6;</code>
       * @return The removeTime.
       */
      @java.lang.Override
      public long getRemoveTime() {
        return removeTime_;
      }
      /**
       * <code>int64 remove_time = 6;</code>
       * @param value The removeTime to set.
       * @return This builder for chaining.
       */
      public Builder setRemoveTime(long value) {
        bitField0_ |= 0x00000004;
        removeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remove_time = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoveTime() {
        bitField0_ = (bitField0_ & ~0x00000004);
        removeTime_ = 0L;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> processLogs_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetProcessLogs() {
        if (processLogs_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ProcessLogsDefaultEntryHolder.defaultEntry);
        }
        return processLogs_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableProcessLogs() {
        onChanged();;
        if (processLogs_ == null) {
          processLogs_ = com.google.protobuf.MapField.newMapField(
              ProcessLogsDefaultEntryHolder.defaultEntry);
        }
        if (!processLogs_.isMutable()) {
          processLogs_ = processLogs_.copy();
        }
        return processLogs_;
      }

      public int getProcessLogsCount() {
        return internalGetProcessLogs().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; process_logs = 7;</code>
       */

      @java.lang.Override
      public boolean containsProcessLogs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetProcessLogs().getMap().containsKey(key);
      }
      /**
       * Use {@link #getProcessLogsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getProcessLogs() {
        return getProcessLogsMap();
      }
      /**
       * <code>map&lt;string, string&gt; process_logs = 7;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getProcessLogsMap() {
        return internalGetProcessLogs().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; process_logs = 7;</code>
       */
      @java.lang.Override

      public java.lang.String getProcessLogsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetProcessLogs().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; process_logs = 7;</code>
       */
      @java.lang.Override

      public java.lang.String getProcessLogsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetProcessLogs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearProcessLogs() {
        internalGetMutableProcessLogs().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; process_logs = 7;</code>
       */

      public Builder removeProcessLogs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableProcessLogs().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableProcessLogs() {
        return internalGetMutableProcessLogs().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; process_logs = 7;</code>
       */
      public Builder putProcessLogs(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableProcessLogs().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; process_logs = 7;</code>
       */

      public Builder putAllProcessLogs(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableProcessLogs().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ProcessSummary)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ProcessSummary)
    private static final org.apache.spark.status.protobuf.StoreTypes.ProcessSummary DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ProcessSummary();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummary getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ProcessSummary>
        PARSER = new com.google.protobuf.AbstractParser<ProcessSummary>() {
      @java.lang.Override
      public ProcessSummary parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ProcessSummary(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ProcessSummary> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ProcessSummary> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ProcessSummary getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ProcessSummaryWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ProcessSummaryWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.ProcessSummary getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryOrBuilder getInfoOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ProcessSummaryWrapper}
   */
  public static final class ProcessSummaryWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ProcessSummaryWrapper)
      ProcessSummaryWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ProcessSummaryWrapper.newBuilder() to construct.
    private ProcessSummaryWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ProcessSummaryWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ProcessSummaryWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ProcessSummaryWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper.Builder.class);
    }

    public static final int INFO_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.ProcessSummary info_;
    /**
     * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ProcessSummary getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (info_ != null) {
        output.writeMessage(1, getInfo());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getInfo());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper other = (org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper) obj;

      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ProcessSummaryWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ProcessSummaryWrapper)
        org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper result = new org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper(this);
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper.getDefaultInstance()) return this;
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ProcessSummary info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ProcessSummary, org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummary getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.ProcessSummary value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.ProcessSummary value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ProcessSummary info = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ProcessSummary, org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ProcessSummary, org.apache.spark.status.protobuf.StoreTypes.ProcessSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ProcessSummaryWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ProcessSummaryWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ProcessSummaryWrapper>
        PARSER = new com.google.protobuf.AbstractParser<ProcessSummaryWrapper>() {
      @java.lang.Override
      public ProcessSummaryWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ProcessSummaryWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ProcessSummaryWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ProcessSummaryWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ProcessSummaryWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface MemoryMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.MemoryMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 used_on_heap_storage_memory = 1;</code>
     * @return The usedOnHeapStorageMemory.
     */
    long getUsedOnHeapStorageMemory();

    /**
     * <code>int64 used_off_heap_storage_memory = 2;</code>
     * @return The usedOffHeapStorageMemory.
     */
    long getUsedOffHeapStorageMemory();

    /**
     * <code>int64 total_on_heap_storage_memory = 3;</code>
     * @return The totalOnHeapStorageMemory.
     */
    long getTotalOnHeapStorageMemory();

    /**
     * <code>int64 total_off_heap_storage_memory = 4;</code>
     * @return The totalOffHeapStorageMemory.
     */
    long getTotalOffHeapStorageMemory();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.MemoryMetrics}
   */
  public static final class MemoryMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.MemoryMetrics)
      MemoryMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use MemoryMetrics.newBuilder() to construct.
    private MemoryMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private MemoryMetrics() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new MemoryMetrics();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private MemoryMetrics(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              usedOnHeapStorageMemory_ = input.readInt64();
              break;
            }
            case 16: {

              usedOffHeapStorageMemory_ = input.readInt64();
              break;
            }
            case 24: {

              totalOnHeapStorageMemory_ = input.readInt64();
              break;
            }
            case 32: {

              totalOffHeapStorageMemory_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_MemoryMetrics_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_MemoryMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.class, org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.Builder.class);
    }

    public static final int USED_ON_HEAP_STORAGE_MEMORY_FIELD_NUMBER = 1;
    private long usedOnHeapStorageMemory_;
    /**
     * <code>int64 used_on_heap_storage_memory = 1;</code>
     * @return The usedOnHeapStorageMemory.
     */
    @java.lang.Override
    public long getUsedOnHeapStorageMemory() {
      return usedOnHeapStorageMemory_;
    }

    public static final int USED_OFF_HEAP_STORAGE_MEMORY_FIELD_NUMBER = 2;
    private long usedOffHeapStorageMemory_;
    /**
     * <code>int64 used_off_heap_storage_memory = 2;</code>
     * @return The usedOffHeapStorageMemory.
     */
    @java.lang.Override
    public long getUsedOffHeapStorageMemory() {
      return usedOffHeapStorageMemory_;
    }

    public static final int TOTAL_ON_HEAP_STORAGE_MEMORY_FIELD_NUMBER = 3;
    private long totalOnHeapStorageMemory_;
    /**
     * <code>int64 total_on_heap_storage_memory = 3;</code>
     * @return The totalOnHeapStorageMemory.
     */
    @java.lang.Override
    public long getTotalOnHeapStorageMemory() {
      return totalOnHeapStorageMemory_;
    }

    public static final int TOTAL_OFF_HEAP_STORAGE_MEMORY_FIELD_NUMBER = 4;
    private long totalOffHeapStorageMemory_;
    /**
     * <code>int64 total_off_heap_storage_memory = 4;</code>
     * @return The totalOffHeapStorageMemory.
     */
    @java.lang.Override
    public long getTotalOffHeapStorageMemory() {
      return totalOffHeapStorageMemory_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (usedOnHeapStorageMemory_ != 0L) {
        output.writeInt64(1, usedOnHeapStorageMemory_);
      }
      if (usedOffHeapStorageMemory_ != 0L) {
        output.writeInt64(2, usedOffHeapStorageMemory_);
      }
      if (totalOnHeapStorageMemory_ != 0L) {
        output.writeInt64(3, totalOnHeapStorageMemory_);
      }
      if (totalOffHeapStorageMemory_ != 0L) {
        output.writeInt64(4, totalOffHeapStorageMemory_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (usedOnHeapStorageMemory_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, usedOnHeapStorageMemory_);
      }
      if (usedOffHeapStorageMemory_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, usedOffHeapStorageMemory_);
      }
      if (totalOnHeapStorageMemory_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, totalOnHeapStorageMemory_);
      }
      if (totalOffHeapStorageMemory_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, totalOffHeapStorageMemory_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics other = (org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics) obj;

      if (getUsedOnHeapStorageMemory()
          != other.getUsedOnHeapStorageMemory()) return false;
      if (getUsedOffHeapStorageMemory()
          != other.getUsedOffHeapStorageMemory()) return false;
      if (getTotalOnHeapStorageMemory()
          != other.getTotalOnHeapStorageMemory()) return false;
      if (getTotalOffHeapStorageMemory()
          != other.getTotalOffHeapStorageMemory()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + USED_ON_HEAP_STORAGE_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getUsedOnHeapStorageMemory());
      hash = (37 * hash) + USED_OFF_HEAP_STORAGE_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getUsedOffHeapStorageMemory());
      hash = (37 * hash) + TOTAL_ON_HEAP_STORAGE_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTotalOnHeapStorageMemory());
      hash = (37 * hash) + TOTAL_OFF_HEAP_STORAGE_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTotalOffHeapStorageMemory());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.MemoryMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.MemoryMetrics)
        org.apache.spark.status.protobuf.StoreTypes.MemoryMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_MemoryMetrics_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_MemoryMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.class, org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        usedOnHeapStorageMemory_ = 0L;

        usedOffHeapStorageMemory_ = 0L;

        totalOnHeapStorageMemory_ = 0L;

        totalOffHeapStorageMemory_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_MemoryMetrics_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics build() {
        org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics result = new org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics(this);
        result.usedOnHeapStorageMemory_ = usedOnHeapStorageMemory_;
        result.usedOffHeapStorageMemory_ = usedOffHeapStorageMemory_;
        result.totalOnHeapStorageMemory_ = totalOnHeapStorageMemory_;
        result.totalOffHeapStorageMemory_ = totalOffHeapStorageMemory_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.getDefaultInstance()) return this;
        if (other.getUsedOnHeapStorageMemory() != 0L) {
          setUsedOnHeapStorageMemory(other.getUsedOnHeapStorageMemory());
        }
        if (other.getUsedOffHeapStorageMemory() != 0L) {
          setUsedOffHeapStorageMemory(other.getUsedOffHeapStorageMemory());
        }
        if (other.getTotalOnHeapStorageMemory() != 0L) {
          setTotalOnHeapStorageMemory(other.getTotalOnHeapStorageMemory());
        }
        if (other.getTotalOffHeapStorageMemory() != 0L) {
          setTotalOffHeapStorageMemory(other.getTotalOffHeapStorageMemory());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long usedOnHeapStorageMemory_ ;
      /**
       * <code>int64 used_on_heap_storage_memory = 1;</code>
       * @return The usedOnHeapStorageMemory.
       */
      @java.lang.Override
      public long getUsedOnHeapStorageMemory() {
        return usedOnHeapStorageMemory_;
      }
      /**
       * <code>int64 used_on_heap_storage_memory = 1;</code>
       * @param value The usedOnHeapStorageMemory to set.
       * @return This builder for chaining.
       */
      public Builder setUsedOnHeapStorageMemory(long value) {
        
        usedOnHeapStorageMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 used_on_heap_storage_memory = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearUsedOnHeapStorageMemory() {
        
        usedOnHeapStorageMemory_ = 0L;
        onChanged();
        return this;
      }

      private long usedOffHeapStorageMemory_ ;
      /**
       * <code>int64 used_off_heap_storage_memory = 2;</code>
       * @return The usedOffHeapStorageMemory.
       */
      @java.lang.Override
      public long getUsedOffHeapStorageMemory() {
        return usedOffHeapStorageMemory_;
      }
      /**
       * <code>int64 used_off_heap_storage_memory = 2;</code>
       * @param value The usedOffHeapStorageMemory to set.
       * @return This builder for chaining.
       */
      public Builder setUsedOffHeapStorageMemory(long value) {
        
        usedOffHeapStorageMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 used_off_heap_storage_memory = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearUsedOffHeapStorageMemory() {
        
        usedOffHeapStorageMemory_ = 0L;
        onChanged();
        return this;
      }

      private long totalOnHeapStorageMemory_ ;
      /**
       * <code>int64 total_on_heap_storage_memory = 3;</code>
       * @return The totalOnHeapStorageMemory.
       */
      @java.lang.Override
      public long getTotalOnHeapStorageMemory() {
        return totalOnHeapStorageMemory_;
      }
      /**
       * <code>int64 total_on_heap_storage_memory = 3;</code>
       * @param value The totalOnHeapStorageMemory to set.
       * @return This builder for chaining.
       */
      public Builder setTotalOnHeapStorageMemory(long value) {
        
        totalOnHeapStorageMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 total_on_heap_storage_memory = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalOnHeapStorageMemory() {
        
        totalOnHeapStorageMemory_ = 0L;
        onChanged();
        return this;
      }

      private long totalOffHeapStorageMemory_ ;
      /**
       * <code>int64 total_off_heap_storage_memory = 4;</code>
       * @return The totalOffHeapStorageMemory.
       */
      @java.lang.Override
      public long getTotalOffHeapStorageMemory() {
        return totalOffHeapStorageMemory_;
      }
      /**
       * <code>int64 total_off_heap_storage_memory = 4;</code>
       * @param value The totalOffHeapStorageMemory to set.
       * @return This builder for chaining.
       */
      public Builder setTotalOffHeapStorageMemory(long value) {
        
        totalOffHeapStorageMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 total_off_heap_storage_memory = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalOffHeapStorageMemory() {
        
        totalOffHeapStorageMemory_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.MemoryMetrics)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.MemoryMetrics)
    private static final org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<MemoryMetrics>
        PARSER = new com.google.protobuf.AbstractParser<MemoryMetrics>() {
      @java.lang.Override
      public MemoryMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new MemoryMetrics(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<MemoryMetrics> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<MemoryMetrics> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ResourceInformationOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ResourceInformation)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>repeated string addresses = 2;</code>
     * @return A list containing the addresses.
     */
    java.util.List<java.lang.String>
        getAddressesList();
    /**
     * <code>repeated string addresses = 2;</code>
     * @return The count of addresses.
     */
    int getAddressesCount();
    /**
     * <code>repeated string addresses = 2;</code>
     * @param index The index of the element to return.
     * @return The addresses at the given index.
     */
    java.lang.String getAddresses(int index);
    /**
     * <code>repeated string addresses = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the addresses at the given index.
     */
    com.google.protobuf.ByteString
        getAddressesBytes(int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ResourceInformation}
   */
  public static final class ResourceInformation extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ResourceInformation)
      ResourceInformationOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ResourceInformation.newBuilder() to construct.
    private ResourceInformation(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ResourceInformation() {
      name_ = "";
      addresses_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ResourceInformation();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ResourceInformation(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                addresses_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000002;
              }
              addresses_.add(s);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          addresses_ = addresses_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceInformation_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceInformation_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ResourceInformation.class, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation.Builder.class);
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int ADDRESSES_FIELD_NUMBER = 2;
    private com.google.protobuf.LazyStringList addresses_;
    /**
     * <code>repeated string addresses = 2;</code>
     * @return A list containing the addresses.
     */
    public com.google.protobuf.ProtocolStringList
        getAddressesList() {
      return addresses_;
    }
    /**
     * <code>repeated string addresses = 2;</code>
     * @return The count of addresses.
     */
    public int getAddressesCount() {
      return addresses_.size();
    }
    /**
     * <code>repeated string addresses = 2;</code>
     * @param index The index of the element to return.
     * @return The addresses at the given index.
     */
    public java.lang.String getAddresses(int index) {
      return addresses_.get(index);
    }
    /**
     * <code>repeated string addresses = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the addresses at the given index.
     */
    public com.google.protobuf.ByteString
        getAddressesBytes(int index) {
      return addresses_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      for (int i = 0; i < addresses_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, addresses_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < addresses_.size(); i++) {
          dataSize += computeStringSizeNoTag(addresses_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getAddressesList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ResourceInformation)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ResourceInformation other = (org.apache.spark.status.protobuf.StoreTypes.ResourceInformation) obj;

      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (!getAddressesList()
          .equals(other.getAddressesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (getAddressesCount() > 0) {
        hash = (37 * hash) + ADDRESSES_FIELD_NUMBER;
        hash = (53 * hash) + getAddressesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ResourceInformation prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ResourceInformation}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ResourceInformation)
        org.apache.spark.status.protobuf.StoreTypes.ResourceInformationOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceInformation_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceInformation_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ResourceInformation.class, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ResourceInformation.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        addresses_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ResourceInformation_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ResourceInformation.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceInformation build() {
        org.apache.spark.status.protobuf.StoreTypes.ResourceInformation result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ResourceInformation buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ResourceInformation result = new org.apache.spark.status.protobuf.StoreTypes.ResourceInformation(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((bitField0_ & 0x00000002) != 0)) {
          addresses_ = addresses_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.addresses_ = addresses_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ResourceInformation) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ResourceInformation)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ResourceInformation other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ResourceInformation.getDefaultInstance()) return this;
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (!other.addresses_.isEmpty()) {
          if (addresses_.isEmpty()) {
            addresses_ = other.addresses_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureAddressesIsMutable();
            addresses_.addAll(other.addresses_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ResourceInformation parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ResourceInformation) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList addresses_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureAddressesIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          addresses_ = new com.google.protobuf.LazyStringArrayList(addresses_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @return A list containing the addresses.
       */
      public com.google.protobuf.ProtocolStringList
          getAddressesList() {
        return addresses_.getUnmodifiableView();
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @return The count of addresses.
       */
      public int getAddressesCount() {
        return addresses_.size();
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @param index The index of the element to return.
       * @return The addresses at the given index.
       */
      public java.lang.String getAddresses(int index) {
        return addresses_.get(index);
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @param index The index of the value to return.
       * @return The bytes of the addresses at the given index.
       */
      public com.google.protobuf.ByteString
          getAddressesBytes(int index) {
        return addresses_.getByteString(index);
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @param index The index to set the value at.
       * @param value The addresses to set.
       * @return This builder for chaining.
       */
      public Builder setAddresses(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureAddressesIsMutable();
        addresses_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @param value The addresses to add.
       * @return This builder for chaining.
       */
      public Builder addAddresses(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureAddressesIsMutable();
        addresses_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @param values The addresses to add.
       * @return This builder for chaining.
       */
      public Builder addAllAddresses(
          java.lang.Iterable<java.lang.String> values) {
        ensureAddressesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, addresses_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearAddresses() {
        addresses_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string addresses = 2;</code>
       * @param value The bytes of the addresses to add.
       * @return This builder for chaining.
       */
      public Builder addAddressesBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureAddressesIsMutable();
        addresses_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ResourceInformation)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ResourceInformation)
    private static final org.apache.spark.status.protobuf.StoreTypes.ResourceInformation DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ResourceInformation();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ResourceInformation>
        PARSER = new com.google.protobuf.AbstractParser<ResourceInformation>() {
      @java.lang.Override
      public ResourceInformation parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ResourceInformation(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ResourceInformation> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ResourceInformation> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExecutorSummaryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ExecutorSummary)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    boolean hasId();
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    java.lang.String getId();
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    com.google.protobuf.ByteString
        getIdBytes();

    /**
     * <code>string host_port = 2;</code>
     * @return Whether the hostPort field is set.
     */
    boolean hasHostPort();
    /**
     * <code>string host_port = 2;</code>
     * @return The hostPort.
     */
    java.lang.String getHostPort();
    /**
     * <code>string host_port = 2;</code>
     * @return The bytes for hostPort.
     */
    com.google.protobuf.ByteString
        getHostPortBytes();

    /**
     * <code>bool is_active = 3;</code>
     * @return The isActive.
     */
    boolean getIsActive();

    /**
     * <code>int32 rdd_blocks = 4;</code>
     * @return The rddBlocks.
     */
    int getRddBlocks();

    /**
     * <code>int64 memory_used = 5;</code>
     * @return The memoryUsed.
     */
    long getMemoryUsed();

    /**
     * <code>int64 disk_used = 6;</code>
     * @return The diskUsed.
     */
    long getDiskUsed();

    /**
     * <code>int32 total_cores = 7;</code>
     * @return The totalCores.
     */
    int getTotalCores();

    /**
     * <code>int32 max_tasks = 8;</code>
     * @return The maxTasks.
     */
    int getMaxTasks();

    /**
     * <code>int32 active_tasks = 9;</code>
     * @return The activeTasks.
     */
    int getActiveTasks();

    /**
     * <code>int32 failed_tasks = 10;</code>
     * @return The failedTasks.
     */
    int getFailedTasks();

    /**
     * <code>int32 completed_tasks = 11;</code>
     * @return The completedTasks.
     */
    int getCompletedTasks();

    /**
     * <code>int32 total_tasks = 12;</code>
     * @return The totalTasks.
     */
    int getTotalTasks();

    /**
     * <code>int64 total_duration = 13;</code>
     * @return The totalDuration.
     */
    long getTotalDuration();

    /**
     * <code>int64 total_gc_time = 14;</code>
     * @return The totalGcTime.
     */
    long getTotalGcTime();

    /**
     * <code>int64 total_input_bytes = 15;</code>
     * @return The totalInputBytes.
     */
    long getTotalInputBytes();

    /**
     * <code>int64 total_shuffle_read = 16;</code>
     * @return The totalShuffleRead.
     */
    long getTotalShuffleRead();

    /**
     * <code>int64 total_shuffle_write = 17;</code>
     * @return The totalShuffleWrite.
     */
    long getTotalShuffleWrite();

    /**
     * <code>bool is_blacklisted = 18;</code>
     * @return The isBlacklisted.
     */
    boolean getIsBlacklisted();

    /**
     * <code>int64 max_memory = 19;</code>
     * @return The maxMemory.
     */
    long getMaxMemory();

    /**
     * <code>int64 add_time = 20;</code>
     * @return The addTime.
     */
    long getAddTime();

    /**
     * <code>int64 remove_time = 21;</code>
     * @return Whether the removeTime field is set.
     */
    boolean hasRemoveTime();
    /**
     * <code>int64 remove_time = 21;</code>
     * @return The removeTime.
     */
    long getRemoveTime();

    /**
     * <code>string remove_reason = 22;</code>
     * @return Whether the removeReason field is set.
     */
    boolean hasRemoveReason();
    /**
     * <code>string remove_reason = 22;</code>
     * @return The removeReason.
     */
    java.lang.String getRemoveReason();
    /**
     * <code>string remove_reason = 22;</code>
     * @return The bytes for removeReason.
     */
    com.google.protobuf.ByteString
        getRemoveReasonBytes();

    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */
    int getExecutorLogsCount();
    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */
    boolean containsExecutorLogs(
        java.lang.String key);
    /**
     * Use {@link #getExecutorLogsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getExecutorLogs();
    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getExecutorLogsMap();
    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */

    java.lang.String getExecutorLogsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */

    java.lang.String getExecutorLogsOrThrow(
        java.lang.String key);

    /**
     * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
     * @return Whether the memoryMetrics field is set.
     */
    boolean hasMemoryMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
     * @return The memoryMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics getMemoryMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.MemoryMetricsOrBuilder getMemoryMetricsOrBuilder();

    /**
     * <code>repeated int64 blacklisted_in_stages = 25;</code>
     * @return A list containing the blacklistedInStages.
     */
    java.util.List<java.lang.Long> getBlacklistedInStagesList();
    /**
     * <code>repeated int64 blacklisted_in_stages = 25;</code>
     * @return The count of blacklistedInStages.
     */
    int getBlacklistedInStagesCount();
    /**
     * <code>repeated int64 blacklisted_in_stages = 25;</code>
     * @param index The index of the element to return.
     * @return The blacklistedInStages at the given index.
     */
    long getBlacklistedInStages(int index);

    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
     * @return Whether the peakMemoryMetrics field is set.
     */
    boolean hasPeakMemoryMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
     * @return The peakMemoryMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakMemoryMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakMemoryMetricsOrBuilder();

    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */
    int getAttributesCount();
    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */
    boolean containsAttributes(
        java.lang.String key);
    /**
     * Use {@link #getAttributesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getAttributes();
    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getAttributesMap();
    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */

    java.lang.String getAttributesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */

    java.lang.String getAttributesOrThrow(
        java.lang.String key);

    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */
    int getResourcesCount();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */
    boolean containsResources(
        java.lang.String key);
    /**
     * Use {@link #getResourcesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>
    getResources();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */
    java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>
    getResourcesMap();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getResourcesOrDefault(
        java.lang.String key,
        org.apache.spark.status.protobuf.StoreTypes.ResourceInformation defaultValue);
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getResourcesOrThrow(
        java.lang.String key);

    /**
     * <code>int32 resource_profile_id = 29;</code>
     * @return The resourceProfileId.
     */
    int getResourceProfileId();

    /**
     * <code>bool is_excluded = 30;</code>
     * @return The isExcluded.
     */
    boolean getIsExcluded();

    /**
     * <code>repeated int64 excluded_in_stages = 31;</code>
     * @return A list containing the excludedInStages.
     */
    java.util.List<java.lang.Long> getExcludedInStagesList();
    /**
     * <code>repeated int64 excluded_in_stages = 31;</code>
     * @return The count of excludedInStages.
     */
    int getExcludedInStagesCount();
    /**
     * <code>repeated int64 excluded_in_stages = 31;</code>
     * @param index The index of the element to return.
     * @return The excludedInStages at the given index.
     */
    long getExcludedInStages(int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorSummary}
   */
  public static final class ExecutorSummary extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ExecutorSummary)
      ExecutorSummaryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ExecutorSummary.newBuilder() to construct.
    private ExecutorSummary(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ExecutorSummary() {
      id_ = "";
      hostPort_ = "";
      removeReason_ = "";
      blacklistedInStages_ = emptyLongList();
      excludedInStages_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ExecutorSummary();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ExecutorSummary(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              id_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              hostPort_ = s;
              break;
            }
            case 24: {

              isActive_ = input.readBool();
              break;
            }
            case 32: {

              rddBlocks_ = input.readInt32();
              break;
            }
            case 40: {

              memoryUsed_ = input.readInt64();
              break;
            }
            case 48: {

              diskUsed_ = input.readInt64();
              break;
            }
            case 56: {

              totalCores_ = input.readInt32();
              break;
            }
            case 64: {

              maxTasks_ = input.readInt32();
              break;
            }
            case 72: {

              activeTasks_ = input.readInt32();
              break;
            }
            case 80: {

              failedTasks_ = input.readInt32();
              break;
            }
            case 88: {

              completedTasks_ = input.readInt32();
              break;
            }
            case 96: {

              totalTasks_ = input.readInt32();
              break;
            }
            case 104: {

              totalDuration_ = input.readInt64();
              break;
            }
            case 112: {

              totalGcTime_ = input.readInt64();
              break;
            }
            case 120: {

              totalInputBytes_ = input.readInt64();
              break;
            }
            case 128: {

              totalShuffleRead_ = input.readInt64();
              break;
            }
            case 136: {

              totalShuffleWrite_ = input.readInt64();
              break;
            }
            case 144: {

              isBlacklisted_ = input.readBool();
              break;
            }
            case 152: {

              maxMemory_ = input.readInt64();
              break;
            }
            case 160: {

              addTime_ = input.readInt64();
              break;
            }
            case 168: {
              bitField0_ |= 0x00000004;
              removeTime_ = input.readInt64();
              break;
            }
            case 178: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000008;
              removeReason_ = s;
              break;
            }
            case 186: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                executorLogs_ = com.google.protobuf.MapField.newMapField(
                    ExecutorLogsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000010;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              executorLogs__ = input.readMessage(
                  ExecutorLogsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              executorLogs_.getMutableMap().put(
                  executorLogs__.getKey(), executorLogs__.getValue());
              break;
            }
            case 194: {
              org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) != 0)) {
                subBuilder = memoryMetrics_.toBuilder();
              }
              memoryMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(memoryMetrics_);
                memoryMetrics_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 200: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                blacklistedInStages_ = newLongList();
                mutable_bitField0_ |= 0x00000040;
              }
              blacklistedInStages_.addLong(input.readInt64());
              break;
            }
            case 202: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000040) != 0) && input.getBytesUntilLimit() > 0) {
                blacklistedInStages_ = newLongList();
                mutable_bitField0_ |= 0x00000040;
              }
              while (input.getBytesUntilLimit() > 0) {
                blacklistedInStages_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 210: {
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder subBuilder = null;
              if (((bitField0_ & 0x00000020) != 0)) {
                subBuilder = peakMemoryMetrics_.toBuilder();
              }
              peakMemoryMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(peakMemoryMetrics_);
                peakMemoryMetrics_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000020;
              break;
            }
            case 218: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                attributes_ = com.google.protobuf.MapField.newMapField(
                    AttributesDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000100;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              attributes__ = input.readMessage(
                  AttributesDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              attributes_.getMutableMap().put(
                  attributes__.getKey(), attributes__.getValue());
              break;
            }
            case 226: {
              if (!((mutable_bitField0_ & 0x00000200) != 0)) {
                resources_ = com.google.protobuf.MapField.newMapField(
                    ResourcesDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000200;
              }
              com.google.protobuf.MapEntry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>
              resources__ = input.readMessage(
                  ResourcesDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              resources_.getMutableMap().put(
                  resources__.getKey(), resources__.getValue());
              break;
            }
            case 232: {

              resourceProfileId_ = input.readInt32();
              break;
            }
            case 240: {

              isExcluded_ = input.readBool();
              break;
            }
            case 248: {
              if (!((mutable_bitField0_ & 0x00000400) != 0)) {
                excludedInStages_ = newLongList();
                mutable_bitField0_ |= 0x00000400;
              }
              excludedInStages_.addLong(input.readInt64());
              break;
            }
            case 250: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000400) != 0) && input.getBytesUntilLimit() > 0) {
                excludedInStages_ = newLongList();
                mutable_bitField0_ |= 0x00000400;
              }
              while (input.getBytesUntilLimit() > 0) {
                excludedInStages_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          blacklistedInStages_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000400) != 0)) {
          excludedInStages_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 23:
          return internalGetExecutorLogs();
        case 27:
          return internalGetAttributes();
        case 28:
          return internalGetResources();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummary_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object id_;
    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    @java.lang.Override
    public boolean hasId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public java.lang.String getId() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        id_ = s;
        return s;
      }
    }
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getIdBytes() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        id_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int HOST_PORT_FIELD_NUMBER = 2;
    private volatile java.lang.Object hostPort_;
    /**
     * <code>string host_port = 2;</code>
     * @return Whether the hostPort field is set.
     */
    @java.lang.Override
    public boolean hasHostPort() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string host_port = 2;</code>
     * @return The hostPort.
     */
    @java.lang.Override
    public java.lang.String getHostPort() {
      java.lang.Object ref = hostPort_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        hostPort_ = s;
        return s;
      }
    }
    /**
     * <code>string host_port = 2;</code>
     * @return The bytes for hostPort.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getHostPortBytes() {
      java.lang.Object ref = hostPort_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        hostPort_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int IS_ACTIVE_FIELD_NUMBER = 3;
    private boolean isActive_;
    /**
     * <code>bool is_active = 3;</code>
     * @return The isActive.
     */
    @java.lang.Override
    public boolean getIsActive() {
      return isActive_;
    }

    public static final int RDD_BLOCKS_FIELD_NUMBER = 4;
    private int rddBlocks_;
    /**
     * <code>int32 rdd_blocks = 4;</code>
     * @return The rddBlocks.
     */
    @java.lang.Override
    public int getRddBlocks() {
      return rddBlocks_;
    }

    public static final int MEMORY_USED_FIELD_NUMBER = 5;
    private long memoryUsed_;
    /**
     * <code>int64 memory_used = 5;</code>
     * @return The memoryUsed.
     */
    @java.lang.Override
    public long getMemoryUsed() {
      return memoryUsed_;
    }

    public static final int DISK_USED_FIELD_NUMBER = 6;
    private long diskUsed_;
    /**
     * <code>int64 disk_used = 6;</code>
     * @return The diskUsed.
     */
    @java.lang.Override
    public long getDiskUsed() {
      return diskUsed_;
    }

    public static final int TOTAL_CORES_FIELD_NUMBER = 7;
    private int totalCores_;
    /**
     * <code>int32 total_cores = 7;</code>
     * @return The totalCores.
     */
    @java.lang.Override
    public int getTotalCores() {
      return totalCores_;
    }

    public static final int MAX_TASKS_FIELD_NUMBER = 8;
    private int maxTasks_;
    /**
     * <code>int32 max_tasks = 8;</code>
     * @return The maxTasks.
     */
    @java.lang.Override
    public int getMaxTasks() {
      return maxTasks_;
    }

    public static final int ACTIVE_TASKS_FIELD_NUMBER = 9;
    private int activeTasks_;
    /**
     * <code>int32 active_tasks = 9;</code>
     * @return The activeTasks.
     */
    @java.lang.Override
    public int getActiveTasks() {
      return activeTasks_;
    }

    public static final int FAILED_TASKS_FIELD_NUMBER = 10;
    private int failedTasks_;
    /**
     * <code>int32 failed_tasks = 10;</code>
     * @return The failedTasks.
     */
    @java.lang.Override
    public int getFailedTasks() {
      return failedTasks_;
    }

    public static final int COMPLETED_TASKS_FIELD_NUMBER = 11;
    private int completedTasks_;
    /**
     * <code>int32 completed_tasks = 11;</code>
     * @return The completedTasks.
     */
    @java.lang.Override
    public int getCompletedTasks() {
      return completedTasks_;
    }

    public static final int TOTAL_TASKS_FIELD_NUMBER = 12;
    private int totalTasks_;
    /**
     * <code>int32 total_tasks = 12;</code>
     * @return The totalTasks.
     */
    @java.lang.Override
    public int getTotalTasks() {
      return totalTasks_;
    }

    public static final int TOTAL_DURATION_FIELD_NUMBER = 13;
    private long totalDuration_;
    /**
     * <code>int64 total_duration = 13;</code>
     * @return The totalDuration.
     */
    @java.lang.Override
    public long getTotalDuration() {
      return totalDuration_;
    }

    public static final int TOTAL_GC_TIME_FIELD_NUMBER = 14;
    private long totalGcTime_;
    /**
     * <code>int64 total_gc_time = 14;</code>
     * @return The totalGcTime.
     */
    @java.lang.Override
    public long getTotalGcTime() {
      return totalGcTime_;
    }

    public static final int TOTAL_INPUT_BYTES_FIELD_NUMBER = 15;
    private long totalInputBytes_;
    /**
     * <code>int64 total_input_bytes = 15;</code>
     * @return The totalInputBytes.
     */
    @java.lang.Override
    public long getTotalInputBytes() {
      return totalInputBytes_;
    }

    public static final int TOTAL_SHUFFLE_READ_FIELD_NUMBER = 16;
    private long totalShuffleRead_;
    /**
     * <code>int64 total_shuffle_read = 16;</code>
     * @return The totalShuffleRead.
     */
    @java.lang.Override
    public long getTotalShuffleRead() {
      return totalShuffleRead_;
    }

    public static final int TOTAL_SHUFFLE_WRITE_FIELD_NUMBER = 17;
    private long totalShuffleWrite_;
    /**
     * <code>int64 total_shuffle_write = 17;</code>
     * @return The totalShuffleWrite.
     */
    @java.lang.Override
    public long getTotalShuffleWrite() {
      return totalShuffleWrite_;
    }

    public static final int IS_BLACKLISTED_FIELD_NUMBER = 18;
    private boolean isBlacklisted_;
    /**
     * <code>bool is_blacklisted = 18;</code>
     * @return The isBlacklisted.
     */
    @java.lang.Override
    public boolean getIsBlacklisted() {
      return isBlacklisted_;
    }

    public static final int MAX_MEMORY_FIELD_NUMBER = 19;
    private long maxMemory_;
    /**
     * <code>int64 max_memory = 19;</code>
     * @return The maxMemory.
     */
    @java.lang.Override
    public long getMaxMemory() {
      return maxMemory_;
    }

    public static final int ADD_TIME_FIELD_NUMBER = 20;
    private long addTime_;
    /**
     * <code>int64 add_time = 20;</code>
     * @return The addTime.
     */
    @java.lang.Override
    public long getAddTime() {
      return addTime_;
    }

    public static final int REMOVE_TIME_FIELD_NUMBER = 21;
    private long removeTime_;
    /**
     * <code>int64 remove_time = 21;</code>
     * @return Whether the removeTime field is set.
     */
    @java.lang.Override
    public boolean hasRemoveTime() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>int64 remove_time = 21;</code>
     * @return The removeTime.
     */
    @java.lang.Override
    public long getRemoveTime() {
      return removeTime_;
    }

    public static final int REMOVE_REASON_FIELD_NUMBER = 22;
    private volatile java.lang.Object removeReason_;
    /**
     * <code>string remove_reason = 22;</code>
     * @return Whether the removeReason field is set.
     */
    @java.lang.Override
    public boolean hasRemoveReason() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>string remove_reason = 22;</code>
     * @return The removeReason.
     */
    @java.lang.Override
    public java.lang.String getRemoveReason() {
      java.lang.Object ref = removeReason_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        removeReason_ = s;
        return s;
      }
    }
    /**
     * <code>string remove_reason = 22;</code>
     * @return The bytes for removeReason.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getRemoveReasonBytes() {
      java.lang.Object ref = removeReason_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        removeReason_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int EXECUTOR_LOGS_FIELD_NUMBER = 23;
    private static final class ExecutorLogsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ExecutorLogsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> executorLogs_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetExecutorLogs() {
      if (executorLogs_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ExecutorLogsDefaultEntryHolder.defaultEntry);
      }
      return executorLogs_;
    }

    public int getExecutorLogsCount() {
      return internalGetExecutorLogs().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */

    @java.lang.Override
    public boolean containsExecutorLogs(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetExecutorLogs().getMap().containsKey(key);
    }
    /**
     * Use {@link #getExecutorLogsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getExecutorLogs() {
      return getExecutorLogsMap();
    }
    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getExecutorLogsMap() {
      return internalGetExecutorLogs().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */
    @java.lang.Override

    public java.lang.String getExecutorLogsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetExecutorLogs().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; executor_logs = 23;</code>
     */
    @java.lang.Override

    public java.lang.String getExecutorLogsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetExecutorLogs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int MEMORY_METRICS_FIELD_NUMBER = 24;
    private org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics memoryMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
     * @return Whether the memoryMetrics field is set.
     */
    @java.lang.Override
    public boolean hasMemoryMetrics() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
     * @return The memoryMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics getMemoryMetrics() {
      return memoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.getDefaultInstance() : memoryMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.MemoryMetricsOrBuilder getMemoryMetricsOrBuilder() {
      return memoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.getDefaultInstance() : memoryMetrics_;
    }

    public static final int BLACKLISTED_IN_STAGES_FIELD_NUMBER = 25;
    private com.google.protobuf.Internal.LongList blacklistedInStages_;
    /**
     * <code>repeated int64 blacklisted_in_stages = 25;</code>
     * @return A list containing the blacklistedInStages.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getBlacklistedInStagesList() {
      return blacklistedInStages_;
    }
    /**
     * <code>repeated int64 blacklisted_in_stages = 25;</code>
     * @return The count of blacklistedInStages.
     */
    public int getBlacklistedInStagesCount() {
      return blacklistedInStages_.size();
    }
    /**
     * <code>repeated int64 blacklisted_in_stages = 25;</code>
     * @param index The index of the element to return.
     * @return The blacklistedInStages at the given index.
     */
    public long getBlacklistedInStages(int index) {
      return blacklistedInStages_.getLong(index);
    }
    private int blacklistedInStagesMemoizedSerializedSize = -1;

    public static final int PEAK_MEMORY_METRICS_FIELD_NUMBER = 26;
    private org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics peakMemoryMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
     * @return Whether the peakMemoryMetrics field is set.
     */
    @java.lang.Override
    public boolean hasPeakMemoryMetrics() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
     * @return The peakMemoryMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakMemoryMetrics() {
      return peakMemoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakMemoryMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakMemoryMetricsOrBuilder() {
      return peakMemoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakMemoryMetrics_;
    }

    public static final int ATTRIBUTES_FIELD_NUMBER = 27;
    private static final class AttributesDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummary_AttributesEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> attributes_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetAttributes() {
      if (attributes_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            AttributesDefaultEntryHolder.defaultEntry);
      }
      return attributes_;
    }

    public int getAttributesCount() {
      return internalGetAttributes().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */

    @java.lang.Override
    public boolean containsAttributes(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetAttributes().getMap().containsKey(key);
    }
    /**
     * Use {@link #getAttributesMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getAttributes() {
      return getAttributesMap();
    }
    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getAttributesMap() {
      return internalGetAttributes().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */
    @java.lang.Override

    public java.lang.String getAttributesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetAttributes().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; attributes = 27;</code>
     */
    @java.lang.Override

    public java.lang.String getAttributesOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetAttributes().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int RESOURCES_FIELD_NUMBER = 28;
    private static final class ResourcesDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ResourcesEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  org.apache.spark.status.protobuf.StoreTypes.ResourceInformation.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> resources_;
    private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>
    internalGetResources() {
      if (resources_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ResourcesDefaultEntryHolder.defaultEntry);
      }
      return resources_;
    }

    public int getResourcesCount() {
      return internalGetResources().getMap().size();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */

    @java.lang.Override
    public boolean containsResources(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetResources().getMap().containsKey(key);
    }
    /**
     * Use {@link #getResourcesMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> getResources() {
      return getResourcesMap();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> getResourcesMap() {
      return internalGetResources().getMap();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getResourcesOrDefault(
        java.lang.String key,
        org.apache.spark.status.protobuf.StoreTypes.ResourceInformation defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> map =
          internalGetResources().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getResourcesOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> map =
          internalGetResources().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int RESOURCE_PROFILE_ID_FIELD_NUMBER = 29;
    private int resourceProfileId_;
    /**
     * <code>int32 resource_profile_id = 29;</code>
     * @return The resourceProfileId.
     */
    @java.lang.Override
    public int getResourceProfileId() {
      return resourceProfileId_;
    }

    public static final int IS_EXCLUDED_FIELD_NUMBER = 30;
    private boolean isExcluded_;
    /**
     * <code>bool is_excluded = 30;</code>
     * @return The isExcluded.
     */
    @java.lang.Override
    public boolean getIsExcluded() {
      return isExcluded_;
    }

    public static final int EXCLUDED_IN_STAGES_FIELD_NUMBER = 31;
    private com.google.protobuf.Internal.LongList excludedInStages_;
    /**
     * <code>repeated int64 excluded_in_stages = 31;</code>
     * @return A list containing the excludedInStages.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getExcludedInStagesList() {
      return excludedInStages_;
    }
    /**
     * <code>repeated int64 excluded_in_stages = 31;</code>
     * @return The count of excludedInStages.
     */
    public int getExcludedInStagesCount() {
      return excludedInStages_.size();
    }
    /**
     * <code>repeated int64 excluded_in_stages = 31;</code>
     * @param index The index of the element to return.
     * @return The excludedInStages at the given index.
     */
    public long getExcludedInStages(int index) {
      return excludedInStages_.getLong(index);
    }
    private int excludedInStagesMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, hostPort_);
      }
      if (isActive_ != false) {
        output.writeBool(3, isActive_);
      }
      if (rddBlocks_ != 0) {
        output.writeInt32(4, rddBlocks_);
      }
      if (memoryUsed_ != 0L) {
        output.writeInt64(5, memoryUsed_);
      }
      if (diskUsed_ != 0L) {
        output.writeInt64(6, diskUsed_);
      }
      if (totalCores_ != 0) {
        output.writeInt32(7, totalCores_);
      }
      if (maxTasks_ != 0) {
        output.writeInt32(8, maxTasks_);
      }
      if (activeTasks_ != 0) {
        output.writeInt32(9, activeTasks_);
      }
      if (failedTasks_ != 0) {
        output.writeInt32(10, failedTasks_);
      }
      if (completedTasks_ != 0) {
        output.writeInt32(11, completedTasks_);
      }
      if (totalTasks_ != 0) {
        output.writeInt32(12, totalTasks_);
      }
      if (totalDuration_ != 0L) {
        output.writeInt64(13, totalDuration_);
      }
      if (totalGcTime_ != 0L) {
        output.writeInt64(14, totalGcTime_);
      }
      if (totalInputBytes_ != 0L) {
        output.writeInt64(15, totalInputBytes_);
      }
      if (totalShuffleRead_ != 0L) {
        output.writeInt64(16, totalShuffleRead_);
      }
      if (totalShuffleWrite_ != 0L) {
        output.writeInt64(17, totalShuffleWrite_);
      }
      if (isBlacklisted_ != false) {
        output.writeBool(18, isBlacklisted_);
      }
      if (maxMemory_ != 0L) {
        output.writeInt64(19, maxMemory_);
      }
      if (addTime_ != 0L) {
        output.writeInt64(20, addTime_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt64(21, removeTime_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 22, removeReason_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetExecutorLogs(),
          ExecutorLogsDefaultEntryHolder.defaultEntry,
          23);
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeMessage(24, getMemoryMetrics());
      }
      if (getBlacklistedInStagesList().size() > 0) {
        output.writeUInt32NoTag(202);
        output.writeUInt32NoTag(blacklistedInStagesMemoizedSerializedSize);
      }
      for (int i = 0; i < blacklistedInStages_.size(); i++) {
        output.writeInt64NoTag(blacklistedInStages_.getLong(i));
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeMessage(26, getPeakMemoryMetrics());
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetAttributes(),
          AttributesDefaultEntryHolder.defaultEntry,
          27);
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetResources(),
          ResourcesDefaultEntryHolder.defaultEntry,
          28);
      if (resourceProfileId_ != 0) {
        output.writeInt32(29, resourceProfileId_);
      }
      if (isExcluded_ != false) {
        output.writeBool(30, isExcluded_);
      }
      if (getExcludedInStagesList().size() > 0) {
        output.writeUInt32NoTag(250);
        output.writeUInt32NoTag(excludedInStagesMemoizedSerializedSize);
      }
      for (int i = 0; i < excludedInStages_.size(); i++) {
        output.writeInt64NoTag(excludedInStages_.getLong(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, hostPort_);
      }
      if (isActive_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, isActive_);
      }
      if (rddBlocks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, rddBlocks_);
      }
      if (memoryUsed_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, memoryUsed_);
      }
      if (diskUsed_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, diskUsed_);
      }
      if (totalCores_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, totalCores_);
      }
      if (maxTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(8, maxTasks_);
      }
      if (activeTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(9, activeTasks_);
      }
      if (failedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(10, failedTasks_);
      }
      if (completedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(11, completedTasks_);
      }
      if (totalTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(12, totalTasks_);
      }
      if (totalDuration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(13, totalDuration_);
      }
      if (totalGcTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(14, totalGcTime_);
      }
      if (totalInputBytes_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(15, totalInputBytes_);
      }
      if (totalShuffleRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(16, totalShuffleRead_);
      }
      if (totalShuffleWrite_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(17, totalShuffleWrite_);
      }
      if (isBlacklisted_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(18, isBlacklisted_);
      }
      if (maxMemory_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(19, maxMemory_);
      }
      if (addTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(20, addTime_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(21, removeTime_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(22, removeReason_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetExecutorLogs().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        executorLogs__ = ExecutorLogsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(23, executorLogs__);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(24, getMemoryMetrics());
      }
      {
        int dataSize = 0;
        for (int i = 0; i < blacklistedInStages_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(blacklistedInStages_.getLong(i));
        }
        size += dataSize;
        if (!getBlacklistedInStagesList().isEmpty()) {
          size += 2;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        blacklistedInStagesMemoizedSerializedSize = dataSize;
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(26, getPeakMemoryMetrics());
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetAttributes().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        attributes__ = AttributesDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(27, attributes__);
      }
      for (java.util.Map.Entry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> entry
           : internalGetResources().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>
        resources__ = ResourcesDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(28, resources__);
      }
      if (resourceProfileId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(29, resourceProfileId_);
      }
      if (isExcluded_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(30, isExcluded_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < excludedInStages_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(excludedInStages_.getLong(i));
        }
        size += dataSize;
        if (!getExcludedInStagesList().isEmpty()) {
          size += 2;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        excludedInStagesMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary other = (org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary) obj;

      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (!getId()
            .equals(other.getId())) return false;
      }
      if (hasHostPort() != other.hasHostPort()) return false;
      if (hasHostPort()) {
        if (!getHostPort()
            .equals(other.getHostPort())) return false;
      }
      if (getIsActive()
          != other.getIsActive()) return false;
      if (getRddBlocks()
          != other.getRddBlocks()) return false;
      if (getMemoryUsed()
          != other.getMemoryUsed()) return false;
      if (getDiskUsed()
          != other.getDiskUsed()) return false;
      if (getTotalCores()
          != other.getTotalCores()) return false;
      if (getMaxTasks()
          != other.getMaxTasks()) return false;
      if (getActiveTasks()
          != other.getActiveTasks()) return false;
      if (getFailedTasks()
          != other.getFailedTasks()) return false;
      if (getCompletedTasks()
          != other.getCompletedTasks()) return false;
      if (getTotalTasks()
          != other.getTotalTasks()) return false;
      if (getTotalDuration()
          != other.getTotalDuration()) return false;
      if (getTotalGcTime()
          != other.getTotalGcTime()) return false;
      if (getTotalInputBytes()
          != other.getTotalInputBytes()) return false;
      if (getTotalShuffleRead()
          != other.getTotalShuffleRead()) return false;
      if (getTotalShuffleWrite()
          != other.getTotalShuffleWrite()) return false;
      if (getIsBlacklisted()
          != other.getIsBlacklisted()) return false;
      if (getMaxMemory()
          != other.getMaxMemory()) return false;
      if (getAddTime()
          != other.getAddTime()) return false;
      if (hasRemoveTime() != other.hasRemoveTime()) return false;
      if (hasRemoveTime()) {
        if (getRemoveTime()
            != other.getRemoveTime()) return false;
      }
      if (hasRemoveReason() != other.hasRemoveReason()) return false;
      if (hasRemoveReason()) {
        if (!getRemoveReason()
            .equals(other.getRemoveReason())) return false;
      }
      if (!internalGetExecutorLogs().equals(
          other.internalGetExecutorLogs())) return false;
      if (hasMemoryMetrics() != other.hasMemoryMetrics()) return false;
      if (hasMemoryMetrics()) {
        if (!getMemoryMetrics()
            .equals(other.getMemoryMetrics())) return false;
      }
      if (!getBlacklistedInStagesList()
          .equals(other.getBlacklistedInStagesList())) return false;
      if (hasPeakMemoryMetrics() != other.hasPeakMemoryMetrics()) return false;
      if (hasPeakMemoryMetrics()) {
        if (!getPeakMemoryMetrics()
            .equals(other.getPeakMemoryMetrics())) return false;
      }
      if (!internalGetAttributes().equals(
          other.internalGetAttributes())) return false;
      if (!internalGetResources().equals(
          other.internalGetResources())) return false;
      if (getResourceProfileId()
          != other.getResourceProfileId()) return false;
      if (getIsExcluded()
          != other.getIsExcluded()) return false;
      if (!getExcludedInStagesList()
          .equals(other.getExcludedInStagesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId().hashCode();
      }
      if (hasHostPort()) {
        hash = (37 * hash) + HOST_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getHostPort().hashCode();
      }
      hash = (37 * hash) + IS_ACTIVE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsActive());
      hash = (37 * hash) + RDD_BLOCKS_FIELD_NUMBER;
      hash = (53 * hash) + getRddBlocks();
      hash = (37 * hash) + MEMORY_USED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryUsed());
      hash = (37 * hash) + DISK_USED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskUsed());
      hash = (37 * hash) + TOTAL_CORES_FIELD_NUMBER;
      hash = (53 * hash) + getTotalCores();
      hash = (37 * hash) + MAX_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getMaxTasks();
      hash = (37 * hash) + ACTIVE_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getActiveTasks();
      hash = (37 * hash) + FAILED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getFailedTasks();
      hash = (37 * hash) + COMPLETED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getCompletedTasks();
      hash = (37 * hash) + TOTAL_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getTotalTasks();
      hash = (37 * hash) + TOTAL_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTotalDuration());
      hash = (37 * hash) + TOTAL_GC_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTotalGcTime());
      hash = (37 * hash) + TOTAL_INPUT_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTotalInputBytes());
      hash = (37 * hash) + TOTAL_SHUFFLE_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTotalShuffleRead());
      hash = (37 * hash) + TOTAL_SHUFFLE_WRITE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTotalShuffleWrite());
      hash = (37 * hash) + IS_BLACKLISTED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsBlacklisted());
      hash = (37 * hash) + MAX_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMaxMemory());
      hash = (37 * hash) + ADD_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getAddTime());
      if (hasRemoveTime()) {
        hash = (37 * hash) + REMOVE_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getRemoveTime());
      }
      if (hasRemoveReason()) {
        hash = (37 * hash) + REMOVE_REASON_FIELD_NUMBER;
        hash = (53 * hash) + getRemoveReason().hashCode();
      }
      if (!internalGetExecutorLogs().getMap().isEmpty()) {
        hash = (37 * hash) + EXECUTOR_LOGS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetExecutorLogs().hashCode();
      }
      if (hasMemoryMetrics()) {
        hash = (37 * hash) + MEMORY_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getMemoryMetrics().hashCode();
      }
      if (getBlacklistedInStagesCount() > 0) {
        hash = (37 * hash) + BLACKLISTED_IN_STAGES_FIELD_NUMBER;
        hash = (53 * hash) + getBlacklistedInStagesList().hashCode();
      }
      if (hasPeakMemoryMetrics()) {
        hash = (37 * hash) + PEAK_MEMORY_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getPeakMemoryMetrics().hashCode();
      }
      if (!internalGetAttributes().getMap().isEmpty()) {
        hash = (37 * hash) + ATTRIBUTES_FIELD_NUMBER;
        hash = (53 * hash) + internalGetAttributes().hashCode();
      }
      if (!internalGetResources().getMap().isEmpty()) {
        hash = (37 * hash) + RESOURCES_FIELD_NUMBER;
        hash = (53 * hash) + internalGetResources().hashCode();
      }
      hash = (37 * hash) + RESOURCE_PROFILE_ID_FIELD_NUMBER;
      hash = (53 * hash) + getResourceProfileId();
      hash = (37 * hash) + IS_EXCLUDED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsExcluded());
      if (getExcludedInStagesCount() > 0) {
        hash = (37 * hash) + EXCLUDED_IN_STAGES_FIELD_NUMBER;
        hash = (53 * hash) + getExcludedInStagesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorSummary}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ExecutorSummary)
        org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 23:
            return internalGetExecutorLogs();
          case 27:
            return internalGetAttributes();
          case 28:
            return internalGetResources();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 23:
            return internalGetMutableExecutorLogs();
          case 27:
            return internalGetMutableAttributes();
          case 28:
            return internalGetMutableResources();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummary_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getMemoryMetricsFieldBuilder();
          getPeakMemoryMetricsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        hostPort_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        isActive_ = false;

        rddBlocks_ = 0;

        memoryUsed_ = 0L;

        diskUsed_ = 0L;

        totalCores_ = 0;

        maxTasks_ = 0;

        activeTasks_ = 0;

        failedTasks_ = 0;

        completedTasks_ = 0;

        totalTasks_ = 0;

        totalDuration_ = 0L;

        totalGcTime_ = 0L;

        totalInputBytes_ = 0L;

        totalShuffleRead_ = 0L;

        totalShuffleWrite_ = 0L;

        isBlacklisted_ = false;

        maxMemory_ = 0L;

        addTime_ = 0L;

        removeTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        removeReason_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        internalGetMutableExecutorLogs().clear();
        if (memoryMetricsBuilder_ == null) {
          memoryMetrics_ = null;
        } else {
          memoryMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        blacklistedInStages_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000040);
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = null;
        } else {
          peakMemoryMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        internalGetMutableAttributes().clear();
        internalGetMutableResources().clear();
        resourceProfileId_ = 0;

        isExcluded_ = false;

        excludedInStages_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000400);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary build() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary result = new org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.hostPort_ = hostPort_;
        result.isActive_ = isActive_;
        result.rddBlocks_ = rddBlocks_;
        result.memoryUsed_ = memoryUsed_;
        result.diskUsed_ = diskUsed_;
        result.totalCores_ = totalCores_;
        result.maxTasks_ = maxTasks_;
        result.activeTasks_ = activeTasks_;
        result.failedTasks_ = failedTasks_;
        result.completedTasks_ = completedTasks_;
        result.totalTasks_ = totalTasks_;
        result.totalDuration_ = totalDuration_;
        result.totalGcTime_ = totalGcTime_;
        result.totalInputBytes_ = totalInputBytes_;
        result.totalShuffleRead_ = totalShuffleRead_;
        result.totalShuffleWrite_ = totalShuffleWrite_;
        result.isBlacklisted_ = isBlacklisted_;
        result.maxMemory_ = maxMemory_;
        result.addTime_ = addTime_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.removeTime_ = removeTime_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.removeReason_ = removeReason_;
        result.executorLogs_ = internalGetExecutorLogs();
        result.executorLogs_.makeImmutable();
        if (((from_bitField0_ & 0x00000020) != 0)) {
          if (memoryMetricsBuilder_ == null) {
            result.memoryMetrics_ = memoryMetrics_;
          } else {
            result.memoryMetrics_ = memoryMetricsBuilder_.build();
          }
          to_bitField0_ |= 0x00000010;
        }
        if (((bitField0_ & 0x00000040) != 0)) {
          blacklistedInStages_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000040);
        }
        result.blacklistedInStages_ = blacklistedInStages_;
        if (((from_bitField0_ & 0x00000080) != 0)) {
          if (peakMemoryMetricsBuilder_ == null) {
            result.peakMemoryMetrics_ = peakMemoryMetrics_;
          } else {
            result.peakMemoryMetrics_ = peakMemoryMetricsBuilder_.build();
          }
          to_bitField0_ |= 0x00000020;
        }
        result.attributes_ = internalGetAttributes();
        result.attributes_.makeImmutable();
        result.resources_ = internalGetResources();
        result.resources_.makeImmutable();
        result.resourceProfileId_ = resourceProfileId_;
        result.isExcluded_ = isExcluded_;
        if (((bitField0_ & 0x00000400) != 0)) {
          excludedInStages_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000400);
        }
        result.excludedInStages_ = excludedInStages_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.getDefaultInstance()) return this;
        if (other.hasId()) {
          bitField0_ |= 0x00000001;
          id_ = other.id_;
          onChanged();
        }
        if (other.hasHostPort()) {
          bitField0_ |= 0x00000002;
          hostPort_ = other.hostPort_;
          onChanged();
        }
        if (other.getIsActive() != false) {
          setIsActive(other.getIsActive());
        }
        if (other.getRddBlocks() != 0) {
          setRddBlocks(other.getRddBlocks());
        }
        if (other.getMemoryUsed() != 0L) {
          setMemoryUsed(other.getMemoryUsed());
        }
        if (other.getDiskUsed() != 0L) {
          setDiskUsed(other.getDiskUsed());
        }
        if (other.getTotalCores() != 0) {
          setTotalCores(other.getTotalCores());
        }
        if (other.getMaxTasks() != 0) {
          setMaxTasks(other.getMaxTasks());
        }
        if (other.getActiveTasks() != 0) {
          setActiveTasks(other.getActiveTasks());
        }
        if (other.getFailedTasks() != 0) {
          setFailedTasks(other.getFailedTasks());
        }
        if (other.getCompletedTasks() != 0) {
          setCompletedTasks(other.getCompletedTasks());
        }
        if (other.getTotalTasks() != 0) {
          setTotalTasks(other.getTotalTasks());
        }
        if (other.getTotalDuration() != 0L) {
          setTotalDuration(other.getTotalDuration());
        }
        if (other.getTotalGcTime() != 0L) {
          setTotalGcTime(other.getTotalGcTime());
        }
        if (other.getTotalInputBytes() != 0L) {
          setTotalInputBytes(other.getTotalInputBytes());
        }
        if (other.getTotalShuffleRead() != 0L) {
          setTotalShuffleRead(other.getTotalShuffleRead());
        }
        if (other.getTotalShuffleWrite() != 0L) {
          setTotalShuffleWrite(other.getTotalShuffleWrite());
        }
        if (other.getIsBlacklisted() != false) {
          setIsBlacklisted(other.getIsBlacklisted());
        }
        if (other.getMaxMemory() != 0L) {
          setMaxMemory(other.getMaxMemory());
        }
        if (other.getAddTime() != 0L) {
          setAddTime(other.getAddTime());
        }
        if (other.hasRemoveTime()) {
          setRemoveTime(other.getRemoveTime());
        }
        if (other.hasRemoveReason()) {
          bitField0_ |= 0x00000008;
          removeReason_ = other.removeReason_;
          onChanged();
        }
        internalGetMutableExecutorLogs().mergeFrom(
            other.internalGetExecutorLogs());
        if (other.hasMemoryMetrics()) {
          mergeMemoryMetrics(other.getMemoryMetrics());
        }
        if (!other.blacklistedInStages_.isEmpty()) {
          if (blacklistedInStages_.isEmpty()) {
            blacklistedInStages_ = other.blacklistedInStages_;
            bitField0_ = (bitField0_ & ~0x00000040);
          } else {
            ensureBlacklistedInStagesIsMutable();
            blacklistedInStages_.addAll(other.blacklistedInStages_);
          }
          onChanged();
        }
        if (other.hasPeakMemoryMetrics()) {
          mergePeakMemoryMetrics(other.getPeakMemoryMetrics());
        }
        internalGetMutableAttributes().mergeFrom(
            other.internalGetAttributes());
        internalGetMutableResources().mergeFrom(
            other.internalGetResources());
        if (other.getResourceProfileId() != 0) {
          setResourceProfileId(other.getResourceProfileId());
        }
        if (other.getIsExcluded() != false) {
          setIsExcluded(other.getIsExcluded());
        }
        if (!other.excludedInStages_.isEmpty()) {
          if (excludedInStages_.isEmpty()) {
            excludedInStages_ = other.excludedInStages_;
            bitField0_ = (bitField0_ & ~0x00000400);
          } else {
            ensureExcludedInStagesIsMutable();
            excludedInStages_.addAll(other.excludedInStages_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object id_ = "";
      /**
       * <code>string id = 1;</code>
       * @return Whether the id field is set.
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string id = 1;</code>
       * @return The id.
       */
      public java.lang.String getId() {
        java.lang.Object ref = id_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          id_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @return The bytes for id.
       */
      public com.google.protobuf.ByteString
          getIdBytes() {
        java.lang.Object ref = id_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          id_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = getDefaultInstance().getId();
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @param value The bytes for id to set.
       * @return This builder for chaining.
       */
      public Builder setIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object hostPort_ = "";
      /**
       * <code>string host_port = 2;</code>
       * @return Whether the hostPort field is set.
       */
      public boolean hasHostPort() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string host_port = 2;</code>
       * @return The hostPort.
       */
      public java.lang.String getHostPort() {
        java.lang.Object ref = hostPort_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          hostPort_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string host_port = 2;</code>
       * @return The bytes for hostPort.
       */
      public com.google.protobuf.ByteString
          getHostPortBytes() {
        java.lang.Object ref = hostPort_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          hostPort_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string host_port = 2;</code>
       * @param value The hostPort to set.
       * @return This builder for chaining.
       */
      public Builder setHostPort(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        hostPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string host_port = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearHostPort() {
        bitField0_ = (bitField0_ & ~0x00000002);
        hostPort_ = getDefaultInstance().getHostPort();
        onChanged();
        return this;
      }
      /**
       * <code>string host_port = 2;</code>
       * @param value The bytes for hostPort to set.
       * @return This builder for chaining.
       */
      public Builder setHostPortBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        hostPort_ = value;
        onChanged();
        return this;
      }

      private boolean isActive_ ;
      /**
       * <code>bool is_active = 3;</code>
       * @return The isActive.
       */
      @java.lang.Override
      public boolean getIsActive() {
        return isActive_;
      }
      /**
       * <code>bool is_active = 3;</code>
       * @param value The isActive to set.
       * @return This builder for chaining.
       */
      public Builder setIsActive(boolean value) {
        
        isActive_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool is_active = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsActive() {
        
        isActive_ = false;
        onChanged();
        return this;
      }

      private int rddBlocks_ ;
      /**
       * <code>int32 rdd_blocks = 4;</code>
       * @return The rddBlocks.
       */
      @java.lang.Override
      public int getRddBlocks() {
        return rddBlocks_;
      }
      /**
       * <code>int32 rdd_blocks = 4;</code>
       * @param value The rddBlocks to set.
       * @return This builder for chaining.
       */
      public Builder setRddBlocks(int value) {
        
        rddBlocks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 rdd_blocks = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearRddBlocks() {
        
        rddBlocks_ = 0;
        onChanged();
        return this;
      }

      private long memoryUsed_ ;
      /**
       * <code>int64 memory_used = 5;</code>
       * @return The memoryUsed.
       */
      @java.lang.Override
      public long getMemoryUsed() {
        return memoryUsed_;
      }
      /**
       * <code>int64 memory_used = 5;</code>
       * @param value The memoryUsed to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryUsed(long value) {
        
        memoryUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_used = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryUsed() {
        
        memoryUsed_ = 0L;
        onChanged();
        return this;
      }

      private long diskUsed_ ;
      /**
       * <code>int64 disk_used = 6;</code>
       * @return The diskUsed.
       */
      @java.lang.Override
      public long getDiskUsed() {
        return diskUsed_;
      }
      /**
       * <code>int64 disk_used = 6;</code>
       * @param value The diskUsed to set.
       * @return This builder for chaining.
       */
      public Builder setDiskUsed(long value) {
        
        diskUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_used = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskUsed() {
        
        diskUsed_ = 0L;
        onChanged();
        return this;
      }

      private int totalCores_ ;
      /**
       * <code>int32 total_cores = 7;</code>
       * @return The totalCores.
       */
      @java.lang.Override
      public int getTotalCores() {
        return totalCores_;
      }
      /**
       * <code>int32 total_cores = 7;</code>
       * @param value The totalCores to set.
       * @return This builder for chaining.
       */
      public Builder setTotalCores(int value) {
        
        totalCores_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 total_cores = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalCores() {
        
        totalCores_ = 0;
        onChanged();
        return this;
      }

      private int maxTasks_ ;
      /**
       * <code>int32 max_tasks = 8;</code>
       * @return The maxTasks.
       */
      @java.lang.Override
      public int getMaxTasks() {
        return maxTasks_;
      }
      /**
       * <code>int32 max_tasks = 8;</code>
       * @param value The maxTasks to set.
       * @return This builder for chaining.
       */
      public Builder setMaxTasks(int value) {
        
        maxTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 max_tasks = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxTasks() {
        
        maxTasks_ = 0;
        onChanged();
        return this;
      }

      private int activeTasks_ ;
      /**
       * <code>int32 active_tasks = 9;</code>
       * @return The activeTasks.
       */
      @java.lang.Override
      public int getActiveTasks() {
        return activeTasks_;
      }
      /**
       * <code>int32 active_tasks = 9;</code>
       * @param value The activeTasks to set.
       * @return This builder for chaining.
       */
      public Builder setActiveTasks(int value) {
        
        activeTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 active_tasks = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearActiveTasks() {
        
        activeTasks_ = 0;
        onChanged();
        return this;
      }

      private int failedTasks_ ;
      /**
       * <code>int32 failed_tasks = 10;</code>
       * @return The failedTasks.
       */
      @java.lang.Override
      public int getFailedTasks() {
        return failedTasks_;
      }
      /**
       * <code>int32 failed_tasks = 10;</code>
       * @param value The failedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setFailedTasks(int value) {
        
        failedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 failed_tasks = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearFailedTasks() {
        
        failedTasks_ = 0;
        onChanged();
        return this;
      }

      private int completedTasks_ ;
      /**
       * <code>int32 completed_tasks = 11;</code>
       * @return The completedTasks.
       */
      @java.lang.Override
      public int getCompletedTasks() {
        return completedTasks_;
      }
      /**
       * <code>int32 completed_tasks = 11;</code>
       * @param value The completedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setCompletedTasks(int value) {
        
        completedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 completed_tasks = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearCompletedTasks() {
        
        completedTasks_ = 0;
        onChanged();
        return this;
      }

      private int totalTasks_ ;
      /**
       * <code>int32 total_tasks = 12;</code>
       * @return The totalTasks.
       */
      @java.lang.Override
      public int getTotalTasks() {
        return totalTasks_;
      }
      /**
       * <code>int32 total_tasks = 12;</code>
       * @param value The totalTasks to set.
       * @return This builder for chaining.
       */
      public Builder setTotalTasks(int value) {
        
        totalTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 total_tasks = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalTasks() {
        
        totalTasks_ = 0;
        onChanged();
        return this;
      }

      private long totalDuration_ ;
      /**
       * <code>int64 total_duration = 13;</code>
       * @return The totalDuration.
       */
      @java.lang.Override
      public long getTotalDuration() {
        return totalDuration_;
      }
      /**
       * <code>int64 total_duration = 13;</code>
       * @param value The totalDuration to set.
       * @return This builder for chaining.
       */
      public Builder setTotalDuration(long value) {
        
        totalDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 total_duration = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalDuration() {
        
        totalDuration_ = 0L;
        onChanged();
        return this;
      }

      private long totalGcTime_ ;
      /**
       * <code>int64 total_gc_time = 14;</code>
       * @return The totalGcTime.
       */
      @java.lang.Override
      public long getTotalGcTime() {
        return totalGcTime_;
      }
      /**
       * <code>int64 total_gc_time = 14;</code>
       * @param value The totalGcTime to set.
       * @return This builder for chaining.
       */
      public Builder setTotalGcTime(long value) {
        
        totalGcTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 total_gc_time = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalGcTime() {
        
        totalGcTime_ = 0L;
        onChanged();
        return this;
      }

      private long totalInputBytes_ ;
      /**
       * <code>int64 total_input_bytes = 15;</code>
       * @return The totalInputBytes.
       */
      @java.lang.Override
      public long getTotalInputBytes() {
        return totalInputBytes_;
      }
      /**
       * <code>int64 total_input_bytes = 15;</code>
       * @param value The totalInputBytes to set.
       * @return This builder for chaining.
       */
      public Builder setTotalInputBytes(long value) {
        
        totalInputBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 total_input_bytes = 15;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalInputBytes() {
        
        totalInputBytes_ = 0L;
        onChanged();
        return this;
      }

      private long totalShuffleRead_ ;
      /**
       * <code>int64 total_shuffle_read = 16;</code>
       * @return The totalShuffleRead.
       */
      @java.lang.Override
      public long getTotalShuffleRead() {
        return totalShuffleRead_;
      }
      /**
       * <code>int64 total_shuffle_read = 16;</code>
       * @param value The totalShuffleRead to set.
       * @return This builder for chaining.
       */
      public Builder setTotalShuffleRead(long value) {
        
        totalShuffleRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 total_shuffle_read = 16;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalShuffleRead() {
        
        totalShuffleRead_ = 0L;
        onChanged();
        return this;
      }

      private long totalShuffleWrite_ ;
      /**
       * <code>int64 total_shuffle_write = 17;</code>
       * @return The totalShuffleWrite.
       */
      @java.lang.Override
      public long getTotalShuffleWrite() {
        return totalShuffleWrite_;
      }
      /**
       * <code>int64 total_shuffle_write = 17;</code>
       * @param value The totalShuffleWrite to set.
       * @return This builder for chaining.
       */
      public Builder setTotalShuffleWrite(long value) {
        
        totalShuffleWrite_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 total_shuffle_write = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalShuffleWrite() {
        
        totalShuffleWrite_ = 0L;
        onChanged();
        return this;
      }

      private boolean isBlacklisted_ ;
      /**
       * <code>bool is_blacklisted = 18;</code>
       * @return The isBlacklisted.
       */
      @java.lang.Override
      public boolean getIsBlacklisted() {
        return isBlacklisted_;
      }
      /**
       * <code>bool is_blacklisted = 18;</code>
       * @param value The isBlacklisted to set.
       * @return This builder for chaining.
       */
      public Builder setIsBlacklisted(boolean value) {
        
        isBlacklisted_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool is_blacklisted = 18;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsBlacklisted() {
        
        isBlacklisted_ = false;
        onChanged();
        return this;
      }

      private long maxMemory_ ;
      /**
       * <code>int64 max_memory = 19;</code>
       * @return The maxMemory.
       */
      @java.lang.Override
      public long getMaxMemory() {
        return maxMemory_;
      }
      /**
       * <code>int64 max_memory = 19;</code>
       * @param value The maxMemory to set.
       * @return This builder for chaining.
       */
      public Builder setMaxMemory(long value) {
        
        maxMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 max_memory = 19;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxMemory() {
        
        maxMemory_ = 0L;
        onChanged();
        return this;
      }

      private long addTime_ ;
      /**
       * <code>int64 add_time = 20;</code>
       * @return The addTime.
       */
      @java.lang.Override
      public long getAddTime() {
        return addTime_;
      }
      /**
       * <code>int64 add_time = 20;</code>
       * @param value The addTime to set.
       * @return This builder for chaining.
       */
      public Builder setAddTime(long value) {
        
        addTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 add_time = 20;</code>
       * @return This builder for chaining.
       */
      public Builder clearAddTime() {
        
        addTime_ = 0L;
        onChanged();
        return this;
      }

      private long removeTime_ ;
      /**
       * <code>int64 remove_time = 21;</code>
       * @return Whether the removeTime field is set.
       */
      @java.lang.Override
      public boolean hasRemoveTime() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>int64 remove_time = 21;</code>
       * @return The removeTime.
       */
      @java.lang.Override
      public long getRemoveTime() {
        return removeTime_;
      }
      /**
       * <code>int64 remove_time = 21;</code>
       * @param value The removeTime to set.
       * @return This builder for chaining.
       */
      public Builder setRemoveTime(long value) {
        bitField0_ |= 0x00000004;
        removeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remove_time = 21;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoveTime() {
        bitField0_ = (bitField0_ & ~0x00000004);
        removeTime_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object removeReason_ = "";
      /**
       * <code>string remove_reason = 22;</code>
       * @return Whether the removeReason field is set.
       */
      public boolean hasRemoveReason() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>string remove_reason = 22;</code>
       * @return The removeReason.
       */
      public java.lang.String getRemoveReason() {
        java.lang.Object ref = removeReason_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          removeReason_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string remove_reason = 22;</code>
       * @return The bytes for removeReason.
       */
      public com.google.protobuf.ByteString
          getRemoveReasonBytes() {
        java.lang.Object ref = removeReason_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          removeReason_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string remove_reason = 22;</code>
       * @param value The removeReason to set.
       * @return This builder for chaining.
       */
      public Builder setRemoveReason(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        removeReason_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string remove_reason = 22;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoveReason() {
        bitField0_ = (bitField0_ & ~0x00000008);
        removeReason_ = getDefaultInstance().getRemoveReason();
        onChanged();
        return this;
      }
      /**
       * <code>string remove_reason = 22;</code>
       * @param value The bytes for removeReason to set.
       * @return This builder for chaining.
       */
      public Builder setRemoveReasonBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000008;
        removeReason_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> executorLogs_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetExecutorLogs() {
        if (executorLogs_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ExecutorLogsDefaultEntryHolder.defaultEntry);
        }
        return executorLogs_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableExecutorLogs() {
        onChanged();;
        if (executorLogs_ == null) {
          executorLogs_ = com.google.protobuf.MapField.newMapField(
              ExecutorLogsDefaultEntryHolder.defaultEntry);
        }
        if (!executorLogs_.isMutable()) {
          executorLogs_ = executorLogs_.copy();
        }
        return executorLogs_;
      }

      public int getExecutorLogsCount() {
        return internalGetExecutorLogs().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 23;</code>
       */

      @java.lang.Override
      public boolean containsExecutorLogs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetExecutorLogs().getMap().containsKey(key);
      }
      /**
       * Use {@link #getExecutorLogsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getExecutorLogs() {
        return getExecutorLogsMap();
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 23;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getExecutorLogsMap() {
        return internalGetExecutorLogs().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 23;</code>
       */
      @java.lang.Override

      public java.lang.String getExecutorLogsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetExecutorLogs().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 23;</code>
       */
      @java.lang.Override

      public java.lang.String getExecutorLogsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetExecutorLogs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearExecutorLogs() {
        internalGetMutableExecutorLogs().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 23;</code>
       */

      public Builder removeExecutorLogs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableExecutorLogs().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableExecutorLogs() {
        return internalGetMutableExecutorLogs().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 23;</code>
       */
      public Builder putExecutorLogs(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableExecutorLogs().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 23;</code>
       */

      public Builder putAllExecutorLogs(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableExecutorLogs().getMutableMap()
            .putAll(values);
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics memoryMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics, org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.MemoryMetricsOrBuilder> memoryMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       * @return Whether the memoryMetrics field is set.
       */
      public boolean hasMemoryMetrics() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       * @return The memoryMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics getMemoryMetrics() {
        if (memoryMetricsBuilder_ == null) {
          return memoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.getDefaultInstance() : memoryMetrics_;
        } else {
          return memoryMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       */
      public Builder setMemoryMetrics(org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics value) {
        if (memoryMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          memoryMetrics_ = value;
          onChanged();
        } else {
          memoryMetricsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       */
      public Builder setMemoryMetrics(
          org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.Builder builderForValue) {
        if (memoryMetricsBuilder_ == null) {
          memoryMetrics_ = builderForValue.build();
          onChanged();
        } else {
          memoryMetricsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       */
      public Builder mergeMemoryMetrics(org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics value) {
        if (memoryMetricsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0) &&
              memoryMetrics_ != null &&
              memoryMetrics_ != org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.getDefaultInstance()) {
            memoryMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.newBuilder(memoryMetrics_).mergeFrom(value).buildPartial();
          } else {
            memoryMetrics_ = value;
          }
          onChanged();
        } else {
          memoryMetricsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       */
      public Builder clearMemoryMetrics() {
        if (memoryMetricsBuilder_ == null) {
          memoryMetrics_ = null;
          onChanged();
        } else {
          memoryMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.Builder getMemoryMetricsBuilder() {
        bitField0_ |= 0x00000020;
        onChanged();
        return getMemoryMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.MemoryMetricsOrBuilder getMemoryMetricsOrBuilder() {
        if (memoryMetricsBuilder_ != null) {
          return memoryMetricsBuilder_.getMessageOrBuilder();
        } else {
          return memoryMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.getDefaultInstance() : memoryMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.MemoryMetrics memory_metrics = 24;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics, org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.MemoryMetricsOrBuilder> 
          getMemoryMetricsFieldBuilder() {
        if (memoryMetricsBuilder_ == null) {
          memoryMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics, org.apache.spark.status.protobuf.StoreTypes.MemoryMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.MemoryMetricsOrBuilder>(
                  getMemoryMetrics(),
                  getParentForChildren(),
                  isClean());
          memoryMetrics_ = null;
        }
        return memoryMetricsBuilder_;
      }

      private com.google.protobuf.Internal.LongList blacklistedInStages_ = emptyLongList();
      private void ensureBlacklistedInStagesIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          blacklistedInStages_ = mutableCopy(blacklistedInStages_);
          bitField0_ |= 0x00000040;
         }
      }
      /**
       * <code>repeated int64 blacklisted_in_stages = 25;</code>
       * @return A list containing the blacklistedInStages.
       */
      public java.util.List<java.lang.Long>
          getBlacklistedInStagesList() {
        return ((bitField0_ & 0x00000040) != 0) ?
                 java.util.Collections.unmodifiableList(blacklistedInStages_) : blacklistedInStages_;
      }
      /**
       * <code>repeated int64 blacklisted_in_stages = 25;</code>
       * @return The count of blacklistedInStages.
       */
      public int getBlacklistedInStagesCount() {
        return blacklistedInStages_.size();
      }
      /**
       * <code>repeated int64 blacklisted_in_stages = 25;</code>
       * @param index The index of the element to return.
       * @return The blacklistedInStages at the given index.
       */
      public long getBlacklistedInStages(int index) {
        return blacklistedInStages_.getLong(index);
      }
      /**
       * <code>repeated int64 blacklisted_in_stages = 25;</code>
       * @param index The index to set the value at.
       * @param value The blacklistedInStages to set.
       * @return This builder for chaining.
       */
      public Builder setBlacklistedInStages(
          int index, long value) {
        ensureBlacklistedInStagesIsMutable();
        blacklistedInStages_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 blacklisted_in_stages = 25;</code>
       * @param value The blacklistedInStages to add.
       * @return This builder for chaining.
       */
      public Builder addBlacklistedInStages(long value) {
        ensureBlacklistedInStagesIsMutable();
        blacklistedInStages_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 blacklisted_in_stages = 25;</code>
       * @param values The blacklistedInStages to add.
       * @return This builder for chaining.
       */
      public Builder addAllBlacklistedInStages(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureBlacklistedInStagesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, blacklistedInStages_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 blacklisted_in_stages = 25;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlacklistedInStages() {
        blacklistedInStages_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000040);
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics peakMemoryMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> peakMemoryMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       * @return Whether the peakMemoryMetrics field is set.
       */
      public boolean hasPeakMemoryMetrics() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       * @return The peakMemoryMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakMemoryMetrics() {
        if (peakMemoryMetricsBuilder_ == null) {
          return peakMemoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakMemoryMetrics_;
        } else {
          return peakMemoryMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       */
      public Builder setPeakMemoryMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (peakMemoryMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          peakMemoryMetrics_ = value;
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       */
      public Builder setPeakMemoryMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder builderForValue) {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = builderForValue.build();
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       */
      public Builder mergePeakMemoryMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (peakMemoryMetricsBuilder_ == null) {
          if (((bitField0_ & 0x00000080) != 0) &&
              peakMemoryMetrics_ != null &&
              peakMemoryMetrics_ != org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance()) {
            peakMemoryMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.newBuilder(peakMemoryMetrics_).mergeFrom(value).buildPartial();
          } else {
            peakMemoryMetrics_ = value;
          }
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       */
      public Builder clearPeakMemoryMetrics() {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = null;
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder getPeakMemoryMetricsBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getPeakMemoryMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakMemoryMetricsOrBuilder() {
        if (peakMemoryMetricsBuilder_ != null) {
          return peakMemoryMetricsBuilder_.getMessageOrBuilder();
        } else {
          return peakMemoryMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakMemoryMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_memory_metrics = 26;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> 
          getPeakMemoryMetricsFieldBuilder() {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder>(
                  getPeakMemoryMetrics(),
                  getParentForChildren(),
                  isClean());
          peakMemoryMetrics_ = null;
        }
        return peakMemoryMetricsBuilder_;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> attributes_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetAttributes() {
        if (attributes_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              AttributesDefaultEntryHolder.defaultEntry);
        }
        return attributes_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableAttributes() {
        onChanged();;
        if (attributes_ == null) {
          attributes_ = com.google.protobuf.MapField.newMapField(
              AttributesDefaultEntryHolder.defaultEntry);
        }
        if (!attributes_.isMutable()) {
          attributes_ = attributes_.copy();
        }
        return attributes_;
      }

      public int getAttributesCount() {
        return internalGetAttributes().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; attributes = 27;</code>
       */

      @java.lang.Override
      public boolean containsAttributes(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetAttributes().getMap().containsKey(key);
      }
      /**
       * Use {@link #getAttributesMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getAttributes() {
        return getAttributesMap();
      }
      /**
       * <code>map&lt;string, string&gt; attributes = 27;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getAttributesMap() {
        return internalGetAttributes().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; attributes = 27;</code>
       */
      @java.lang.Override

      public java.lang.String getAttributesOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetAttributes().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; attributes = 27;</code>
       */
      @java.lang.Override

      public java.lang.String getAttributesOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetAttributes().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearAttributes() {
        internalGetMutableAttributes().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; attributes = 27;</code>
       */

      public Builder removeAttributes(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableAttributes().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableAttributes() {
        return internalGetMutableAttributes().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; attributes = 27;</code>
       */
      public Builder putAttributes(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableAttributes().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; attributes = 27;</code>
       */

      public Builder putAllAttributes(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableAttributes().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> resources_;
      private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>
      internalGetResources() {
        if (resources_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ResourcesDefaultEntryHolder.defaultEntry);
        }
        return resources_;
      }
      private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>
      internalGetMutableResources() {
        onChanged();;
        if (resources_ == null) {
          resources_ = com.google.protobuf.MapField.newMapField(
              ResourcesDefaultEntryHolder.defaultEntry);
        }
        if (!resources_.isMutable()) {
          resources_ = resources_.copy();
        }
        return resources_;
      }

      public int getResourcesCount() {
        return internalGetResources().getMap().size();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
       */

      @java.lang.Override
      public boolean containsResources(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetResources().getMap().containsKey(key);
      }
      /**
       * Use {@link #getResourcesMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> getResources() {
        return getResourcesMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> getResourcesMap() {
        return internalGetResources().getMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getResourcesOrDefault(
          java.lang.String key,
          org.apache.spark.status.protobuf.StoreTypes.ResourceInformation defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> map =
            internalGetResources().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.ResourceInformation getResourcesOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> map =
            internalGetResources().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearResources() {
        internalGetMutableResources().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
       */

      public Builder removeResources(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableResources().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation>
      getMutableResources() {
        return internalGetMutableResources().getMutableMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
       */
      public Builder putResources(
          java.lang.String key,
          org.apache.spark.status.protobuf.StoreTypes.ResourceInformation value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableResources().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ResourceInformation&gt; resources = 28;</code>
       */

      public Builder putAllResources(
          java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ResourceInformation> values) {
        internalGetMutableResources().getMutableMap()
            .putAll(values);
        return this;
      }

      private int resourceProfileId_ ;
      /**
       * <code>int32 resource_profile_id = 29;</code>
       * @return The resourceProfileId.
       */
      @java.lang.Override
      public int getResourceProfileId() {
        return resourceProfileId_;
      }
      /**
       * <code>int32 resource_profile_id = 29;</code>
       * @param value The resourceProfileId to set.
       * @return This builder for chaining.
       */
      public Builder setResourceProfileId(int value) {
        
        resourceProfileId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 resource_profile_id = 29;</code>
       * @return This builder for chaining.
       */
      public Builder clearResourceProfileId() {
        
        resourceProfileId_ = 0;
        onChanged();
        return this;
      }

      private boolean isExcluded_ ;
      /**
       * <code>bool is_excluded = 30;</code>
       * @return The isExcluded.
       */
      @java.lang.Override
      public boolean getIsExcluded() {
        return isExcluded_;
      }
      /**
       * <code>bool is_excluded = 30;</code>
       * @param value The isExcluded to set.
       * @return This builder for chaining.
       */
      public Builder setIsExcluded(boolean value) {
        
        isExcluded_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool is_excluded = 30;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsExcluded() {
        
        isExcluded_ = false;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList excludedInStages_ = emptyLongList();
      private void ensureExcludedInStagesIsMutable() {
        if (!((bitField0_ & 0x00000400) != 0)) {
          excludedInStages_ = mutableCopy(excludedInStages_);
          bitField0_ |= 0x00000400;
         }
      }
      /**
       * <code>repeated int64 excluded_in_stages = 31;</code>
       * @return A list containing the excludedInStages.
       */
      public java.util.List<java.lang.Long>
          getExcludedInStagesList() {
        return ((bitField0_ & 0x00000400) != 0) ?
                 java.util.Collections.unmodifiableList(excludedInStages_) : excludedInStages_;
      }
      /**
       * <code>repeated int64 excluded_in_stages = 31;</code>
       * @return The count of excludedInStages.
       */
      public int getExcludedInStagesCount() {
        return excludedInStages_.size();
      }
      /**
       * <code>repeated int64 excluded_in_stages = 31;</code>
       * @param index The index of the element to return.
       * @return The excludedInStages at the given index.
       */
      public long getExcludedInStages(int index) {
        return excludedInStages_.getLong(index);
      }
      /**
       * <code>repeated int64 excluded_in_stages = 31;</code>
       * @param index The index to set the value at.
       * @param value The excludedInStages to set.
       * @return This builder for chaining.
       */
      public Builder setExcludedInStages(
          int index, long value) {
        ensureExcludedInStagesIsMutable();
        excludedInStages_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 excluded_in_stages = 31;</code>
       * @param value The excludedInStages to add.
       * @return This builder for chaining.
       */
      public Builder addExcludedInStages(long value) {
        ensureExcludedInStagesIsMutable();
        excludedInStages_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 excluded_in_stages = 31;</code>
       * @param values The excludedInStages to add.
       * @return This builder for chaining.
       */
      public Builder addAllExcludedInStages(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureExcludedInStagesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, excludedInStages_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 excluded_in_stages = 31;</code>
       * @return This builder for chaining.
       */
      public Builder clearExcludedInStages() {
        excludedInStages_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000400);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ExecutorSummary)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ExecutorSummary)
    private static final org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ExecutorSummary>
        PARSER = new com.google.protobuf.AbstractParser<ExecutorSummary>() {
      @java.lang.Override
      public ExecutorSummary parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ExecutorSummary(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ExecutorSummary> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ExecutorSummary> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExecutorSummaryWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ExecutorSummaryWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryOrBuilder getInfoOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorSummaryWrapper}
   */
  public static final class ExecutorSummaryWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ExecutorSummaryWrapper)
      ExecutorSummaryWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ExecutorSummaryWrapper.newBuilder() to construct.
    private ExecutorSummaryWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ExecutorSummaryWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ExecutorSummaryWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ExecutorSummaryWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper.Builder.class);
    }

    public static final int INFO_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary info_;
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (info_ != null) {
        output.writeMessage(1, getInfo());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getInfo());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper other = (org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper) obj;

      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorSummaryWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ExecutorSummaryWrapper)
        org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper result = new org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper(this);
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper.getDefaultInstance()) return this;
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorSummary info = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ExecutorSummaryWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ExecutorSummaryWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ExecutorSummaryWrapper>
        PARSER = new com.google.protobuf.AbstractParser<ExecutorSummaryWrapper>() {
      @java.lang.Override
      public ExecutorSummaryWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ExecutorSummaryWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ExecutorSummaryWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ExecutorSummaryWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorSummaryWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SQLPlanMetricOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SQLPlanMetric)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>int64 accumulator_id = 2;</code>
     * @return The accumulatorId.
     */
    long getAccumulatorId();

    /**
     * <code>string metric_type = 3;</code>
     * @return Whether the metricType field is set.
     */
    boolean hasMetricType();
    /**
     * <code>string metric_type = 3;</code>
     * @return The metricType.
     */
    java.lang.String getMetricType();
    /**
     * <code>string metric_type = 3;</code>
     * @return The bytes for metricType.
     */
    com.google.protobuf.ByteString
        getMetricTypeBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SQLPlanMetric}
   */
  public static final class SQLPlanMetric extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SQLPlanMetric)
      SQLPlanMetricOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SQLPlanMetric.newBuilder() to construct.
    private SQLPlanMetric(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SQLPlanMetric() {
      name_ = "";
      metricType_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SQLPlanMetric();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SQLPlanMetric(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 16: {

              accumulatorId_ = input.readInt64();
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              metricType_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.class, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder.class);
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int ACCUMULATOR_ID_FIELD_NUMBER = 2;
    private long accumulatorId_;
    /**
     * <code>int64 accumulator_id = 2;</code>
     * @return The accumulatorId.
     */
    @java.lang.Override
    public long getAccumulatorId() {
      return accumulatorId_;
    }

    public static final int METRIC_TYPE_FIELD_NUMBER = 3;
    private volatile java.lang.Object metricType_;
    /**
     * <code>string metric_type = 3;</code>
     * @return Whether the metricType field is set.
     */
    @java.lang.Override
    public boolean hasMetricType() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string metric_type = 3;</code>
     * @return The metricType.
     */
    @java.lang.Override
    public java.lang.String getMetricType() {
      java.lang.Object ref = metricType_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        metricType_ = s;
        return s;
      }
    }
    /**
     * <code>string metric_type = 3;</code>
     * @return The bytes for metricType.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getMetricTypeBytes() {
      java.lang.Object ref = metricType_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        metricType_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (accumulatorId_ != 0L) {
        output.writeInt64(2, accumulatorId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, metricType_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (accumulatorId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, accumulatorId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, metricType_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric other = (org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric) obj;

      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (getAccumulatorId()
          != other.getAccumulatorId()) return false;
      if (hasMetricType() != other.hasMetricType()) return false;
      if (hasMetricType()) {
        if (!getMetricType()
            .equals(other.getMetricType())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      hash = (37 * hash) + ACCUMULATOR_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getAccumulatorId());
      if (hasMetricType()) {
        hash = (37 * hash) + METRIC_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + getMetricType().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SQLPlanMetric}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SQLPlanMetric)
        org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.class, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        accumulatorId_ = 0L;

        metricType_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric build() {
        org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric result = new org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        result.accumulatorId_ = accumulatorId_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.metricType_ = metricType_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.getDefaultInstance()) return this;
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.getAccumulatorId() != 0L) {
          setAccumulatorId(other.getAccumulatorId());
        }
        if (other.hasMetricType()) {
          bitField0_ |= 0x00000002;
          metricType_ = other.metricType_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private long accumulatorId_ ;
      /**
       * <code>int64 accumulator_id = 2;</code>
       * @return The accumulatorId.
       */
      @java.lang.Override
      public long getAccumulatorId() {
        return accumulatorId_;
      }
      /**
       * <code>int64 accumulator_id = 2;</code>
       * @param value The accumulatorId to set.
       * @return This builder for chaining.
       */
      public Builder setAccumulatorId(long value) {
        
        accumulatorId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 accumulator_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearAccumulatorId() {
        
        accumulatorId_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object metricType_ = "";
      /**
       * <code>string metric_type = 3;</code>
       * @return Whether the metricType field is set.
       */
      public boolean hasMetricType() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string metric_type = 3;</code>
       * @return The metricType.
       */
      public java.lang.String getMetricType() {
        java.lang.Object ref = metricType_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          metricType_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string metric_type = 3;</code>
       * @return The bytes for metricType.
       */
      public com.google.protobuf.ByteString
          getMetricTypeBytes() {
        java.lang.Object ref = metricType_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          metricType_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string metric_type = 3;</code>
       * @param value The metricType to set.
       * @return This builder for chaining.
       */
      public Builder setMetricType(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        metricType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string metric_type = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearMetricType() {
        bitField0_ = (bitField0_ & ~0x00000002);
        metricType_ = getDefaultInstance().getMetricType();
        onChanged();
        return this;
      }
      /**
       * <code>string metric_type = 3;</code>
       * @param value The bytes for metricType to set.
       * @return This builder for chaining.
       */
      public Builder setMetricTypeBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        metricType_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SQLPlanMetric)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SQLPlanMetric)
    private static final org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SQLPlanMetric>
        PARSER = new com.google.protobuf.AbstractParser<SQLPlanMetric>() {
      @java.lang.Override
      public SQLPlanMetric parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SQLPlanMetric(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SQLPlanMetric> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SQLPlanMetric> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SQLExecutionUIDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SQLExecutionUIData)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 execution_id = 1;</code>
     * @return The executionId.
     */
    long getExecutionId();

    /**
     * <code>int64 root_execution_id = 2;</code>
     * @return The rootExecutionId.
     */
    long getRootExecutionId();

    /**
     * <code>string description = 3;</code>
     * @return Whether the description field is set.
     */
    boolean hasDescription();
    /**
     * <code>string description = 3;</code>
     * @return The description.
     */
    java.lang.String getDescription();
    /**
     * <code>string description = 3;</code>
     * @return The bytes for description.
     */
    com.google.protobuf.ByteString
        getDescriptionBytes();

    /**
     * <code>string details = 4;</code>
     * @return Whether the details field is set.
     */
    boolean hasDetails();
    /**
     * <code>string details = 4;</code>
     * @return The details.
     */
    java.lang.String getDetails();
    /**
     * <code>string details = 4;</code>
     * @return The bytes for details.
     */
    com.google.protobuf.ByteString
        getDetailsBytes();

    /**
     * <code>string physical_plan_description = 5;</code>
     * @return Whether the physicalPlanDescription field is set.
     */
    boolean hasPhysicalPlanDescription();
    /**
     * <code>string physical_plan_description = 5;</code>
     * @return The physicalPlanDescription.
     */
    java.lang.String getPhysicalPlanDescription();
    /**
     * <code>string physical_plan_description = 5;</code>
     * @return The bytes for physicalPlanDescription.
     */
    com.google.protobuf.ByteString
        getPhysicalPlanDescriptionBytes();

    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */
    int getModifiedConfigsCount();
    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */
    boolean containsModifiedConfigs(
        java.lang.String key);
    /**
     * Use {@link #getModifiedConfigsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getModifiedConfigs();
    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getModifiedConfigsMap();
    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */

    java.lang.String getModifiedConfigsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */

    java.lang.String getModifiedConfigsOrThrow(
        java.lang.String key);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> 
        getMetricsList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    int getMetricsCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
        getMetricsOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
        int index);

    /**
     * <code>int64 submission_time = 8;</code>
     * @return The submissionTime.
     */
    long getSubmissionTime();

    /**
     * <code>int64 completion_time = 9;</code>
     * @return Whether the completionTime field is set.
     */
    boolean hasCompletionTime();
    /**
     * <code>int64 completion_time = 9;</code>
     * @return The completionTime.
     */
    long getCompletionTime();

    /**
     * <code>string error_message = 10;</code>
     * @return Whether the errorMessage field is set.
     */
    boolean hasErrorMessage();
    /**
     * <code>string error_message = 10;</code>
     * @return The errorMessage.
     */
    java.lang.String getErrorMessage();
    /**
     * <code>string error_message = 10;</code>
     * @return The bytes for errorMessage.
     */
    com.google.protobuf.ByteString
        getErrorMessageBytes();

    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    int getJobsCount();
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    boolean containsJobs(
        long key);
    /**
     * Use {@link #getJobsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus>
    getJobs();
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus>
    getJobsMap();
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getJobsOrDefault(
        long key,
        org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus defaultValue);
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getJobsOrThrow(
        long key);
    /**
     * Use {@link #getJobsValueMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.Long, java.lang.Integer>
    getJobsValue();
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    java.util.Map<java.lang.Long, java.lang.Integer>
    getJobsValueMap();
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */

    int getJobsValueOrDefault(
        long key,
        int defaultValue);
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */

    int getJobsValueOrThrow(
        long key);

    /**
     * <code>repeated int64 stages = 12;</code>
     * @return A list containing the stages.
     */
    java.util.List<java.lang.Long> getStagesList();
    /**
     * <code>repeated int64 stages = 12;</code>
     * @return The count of stages.
     */
    int getStagesCount();
    /**
     * <code>repeated int64 stages = 12;</code>
     * @param index The index of the element to return.
     * @return The stages at the given index.
     */
    long getStages(int index);

    /**
     * <code>bool metric_values_is_null = 13;</code>
     * @return The metricValuesIsNull.
     */
    boolean getMetricValuesIsNull();

    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */
    int getMetricValuesCount();
    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */
    boolean containsMetricValues(
        long key);
    /**
     * Use {@link #getMetricValuesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.Long, java.lang.String>
    getMetricValues();
    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */
    java.util.Map<java.lang.Long, java.lang.String>
    getMetricValuesMap();
    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */

    java.lang.String getMetricValuesOrDefault(
        long key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */

    java.lang.String getMetricValuesOrThrow(
        long key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SQLExecutionUIData}
   */
  public static final class SQLExecutionUIData extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SQLExecutionUIData)
      SQLExecutionUIDataOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SQLExecutionUIData.newBuilder() to construct.
    private SQLExecutionUIData(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SQLExecutionUIData() {
      description_ = "";
      details_ = "";
      physicalPlanDescription_ = "";
      metrics_ = java.util.Collections.emptyList();
      errorMessage_ = "";
      stages_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SQLExecutionUIData();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SQLExecutionUIData(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              executionId_ = input.readInt64();
              break;
            }
            case 16: {

              rootExecutionId_ = input.readInt64();
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              description_ = s;
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              details_ = s;
              break;
            }
            case 42: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              physicalPlanDescription_ = s;
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                modifiedConfigs_ = com.google.protobuf.MapField.newMapField(
                    ModifiedConfigsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000008;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              modifiedConfigs__ = input.readMessage(
                  ModifiedConfigsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              modifiedConfigs_.getMutableMap().put(
                  modifiedConfigs__.getKey(), modifiedConfigs__.getValue());
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                metrics_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric>();
                mutable_bitField0_ |= 0x00000010;
              }
              metrics_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.parser(), extensionRegistry));
              break;
            }
            case 64: {

              submissionTime_ = input.readInt64();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000008;
              completionTime_ = input.readInt64();
              break;
            }
            case 82: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000010;
              errorMessage_ = s;
              break;
            }
            case 90: {
              if (!((mutable_bitField0_ & 0x00000080) != 0)) {
                jobs_ = com.google.protobuf.MapField.newMapField(
                    JobsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000080;
              }
              com.google.protobuf.MapEntry<java.lang.Long, java.lang.Integer>
              jobs__ = input.readMessage(
                  JobsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              jobs_.getMutableMap().put(
                  jobs__.getKey(), jobs__.getValue());
              break;
            }
            case 96: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                stages_ = newLongList();
                mutable_bitField0_ |= 0x00000100;
              }
              stages_.addLong(input.readInt64());
              break;
            }
            case 98: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000100) != 0) && input.getBytesUntilLimit() > 0) {
                stages_ = newLongList();
                mutable_bitField0_ |= 0x00000100;
              }
              while (input.getBytesUntilLimit() > 0) {
                stages_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 104: {

              metricValuesIsNull_ = input.readBool();
              break;
            }
            case 114: {
              if (!((mutable_bitField0_ & 0x00000200) != 0)) {
                metricValues_ = com.google.protobuf.MapField.newMapField(
                    MetricValuesDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000200;
              }
              com.google.protobuf.MapEntry<java.lang.Long, java.lang.String>
              metricValues__ = input.readMessage(
                  MetricValuesDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              metricValues_.getMutableMap().put(
                  metricValues__.getKey(), metricValues__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          metrics_ = java.util.Collections.unmodifiableList(metrics_);
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          stages_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 6:
          return internalGetModifiedConfigs();
        case 11:
          return internalGetJobs();
        case 14:
          return internalGetMetricValues();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData.class, org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData.Builder.class);
    }

    private int bitField0_;
    public static final int EXECUTION_ID_FIELD_NUMBER = 1;
    private long executionId_;
    /**
     * <code>int64 execution_id = 1;</code>
     * @return The executionId.
     */
    @java.lang.Override
    public long getExecutionId() {
      return executionId_;
    }

    public static final int ROOT_EXECUTION_ID_FIELD_NUMBER = 2;
    private long rootExecutionId_;
    /**
     * <code>int64 root_execution_id = 2;</code>
     * @return The rootExecutionId.
     */
    @java.lang.Override
    public long getRootExecutionId() {
      return rootExecutionId_;
    }

    public static final int DESCRIPTION_FIELD_NUMBER = 3;
    private volatile java.lang.Object description_;
    /**
     * <code>string description = 3;</code>
     * @return Whether the description field is set.
     */
    @java.lang.Override
    public boolean hasDescription() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string description = 3;</code>
     * @return The description.
     */
    @java.lang.Override
    public java.lang.String getDescription() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        description_ = s;
        return s;
      }
    }
    /**
     * <code>string description = 3;</code>
     * @return The bytes for description.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDescriptionBytes() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        description_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DETAILS_FIELD_NUMBER = 4;
    private volatile java.lang.Object details_;
    /**
     * <code>string details = 4;</code>
     * @return Whether the details field is set.
     */
    @java.lang.Override
    public boolean hasDetails() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string details = 4;</code>
     * @return The details.
     */
    @java.lang.Override
    public java.lang.String getDetails() {
      java.lang.Object ref = details_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        details_ = s;
        return s;
      }
    }
    /**
     * <code>string details = 4;</code>
     * @return The bytes for details.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDetailsBytes() {
      java.lang.Object ref = details_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        details_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int PHYSICAL_PLAN_DESCRIPTION_FIELD_NUMBER = 5;
    private volatile java.lang.Object physicalPlanDescription_;
    /**
     * <code>string physical_plan_description = 5;</code>
     * @return Whether the physicalPlanDescription field is set.
     */
    @java.lang.Override
    public boolean hasPhysicalPlanDescription() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string physical_plan_description = 5;</code>
     * @return The physicalPlanDescription.
     */
    @java.lang.Override
    public java.lang.String getPhysicalPlanDescription() {
      java.lang.Object ref = physicalPlanDescription_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        physicalPlanDescription_ = s;
        return s;
      }
    }
    /**
     * <code>string physical_plan_description = 5;</code>
     * @return The bytes for physicalPlanDescription.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getPhysicalPlanDescriptionBytes() {
      java.lang.Object ref = physicalPlanDescription_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        physicalPlanDescription_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int MODIFIED_CONFIGS_FIELD_NUMBER = 6;
    private static final class ModifiedConfigsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_ModifiedConfigsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> modifiedConfigs_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetModifiedConfigs() {
      if (modifiedConfigs_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ModifiedConfigsDefaultEntryHolder.defaultEntry);
      }
      return modifiedConfigs_;
    }

    public int getModifiedConfigsCount() {
      return internalGetModifiedConfigs().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */

    @java.lang.Override
    public boolean containsModifiedConfigs(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetModifiedConfigs().getMap().containsKey(key);
    }
    /**
     * Use {@link #getModifiedConfigsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getModifiedConfigs() {
      return getModifiedConfigsMap();
    }
    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getModifiedConfigsMap() {
      return internalGetModifiedConfigs().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */
    @java.lang.Override

    public java.lang.String getModifiedConfigsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetModifiedConfigs().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; modified_configs = 6;</code>
     */
    @java.lang.Override

    public java.lang.String getModifiedConfigsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetModifiedConfigs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int METRICS_FIELD_NUMBER = 7;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> metrics_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> getMetricsList() {
      return metrics_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
        getMetricsOrBuilderList() {
      return metrics_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    @java.lang.Override
    public int getMetricsCount() {
      return metrics_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index) {
      return metrics_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
        int index) {
      return metrics_.get(index);
    }

    public static final int SUBMISSION_TIME_FIELD_NUMBER = 8;
    private long submissionTime_;
    /**
     * <code>int64 submission_time = 8;</code>
     * @return The submissionTime.
     */
    @java.lang.Override
    public long getSubmissionTime() {
      return submissionTime_;
    }

    public static final int COMPLETION_TIME_FIELD_NUMBER = 9;
    private long completionTime_;
    /**
     * <code>int64 completion_time = 9;</code>
     * @return Whether the completionTime field is set.
     */
    @java.lang.Override
    public boolean hasCompletionTime() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>int64 completion_time = 9;</code>
     * @return The completionTime.
     */
    @java.lang.Override
    public long getCompletionTime() {
      return completionTime_;
    }

    public static final int ERROR_MESSAGE_FIELD_NUMBER = 10;
    private volatile java.lang.Object errorMessage_;
    /**
     * <code>string error_message = 10;</code>
     * @return Whether the errorMessage field is set.
     */
    @java.lang.Override
    public boolean hasErrorMessage() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>string error_message = 10;</code>
     * @return The errorMessage.
     */
    @java.lang.Override
    public java.lang.String getErrorMessage() {
      java.lang.Object ref = errorMessage_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        errorMessage_ = s;
        return s;
      }
    }
    /**
     * <code>string error_message = 10;</code>
     * @return The bytes for errorMessage.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getErrorMessageBytes() {
      java.lang.Object ref = errorMessage_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        errorMessage_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int JOBS_FIELD_NUMBER = 11;
    private static final class JobsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.Long, java.lang.Integer> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.Long, java.lang.Integer>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_JobsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.INT64,
                  0L,
                  com.google.protobuf.WireFormat.FieldType.ENUM,
                  org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.JOB_EXECUTION_STATUS_UNSPECIFIED.getNumber());
    }
    private com.google.protobuf.MapField<
        java.lang.Long, java.lang.Integer> jobs_;
    private com.google.protobuf.MapField<java.lang.Long, java.lang.Integer>
    internalGetJobs() {
      if (jobs_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            JobsDefaultEntryHolder.defaultEntry);
      }
      return jobs_;
    }
    private static final
    com.google.protobuf.Internal.MapAdapter.Converter<
        java.lang.Integer, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus> jobsValueConverter =
            com.google.protobuf.Internal.MapAdapter.newEnumConverter(
                org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.internalGetValueMap(),
                org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus.UNRECOGNIZED);
    private static final java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus>
    internalGetAdaptedJobsMap(
        java.util.Map<java.lang.Long, java.lang.Integer> map) {
      return new com.google.protobuf.Internal.MapAdapter<
          java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus, java.lang.Integer>(
              map, jobsValueConverter);
    }

    public int getJobsCount() {
      return internalGetJobs().getMap().size();
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */

    @java.lang.Override
    public boolean containsJobs(
        long key) {
      
      return internalGetJobs().getMap().containsKey(key);
    }
    /**
     * Use {@link #getJobsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus>
    getJobs() {
      return getJobsMap();
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus>
    getJobsMap() {
      return internalGetAdaptedJobsMap(
          internalGetJobs().getMap());}
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getJobsOrDefault(
        long key,
        org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus defaultValue) {
      
      java.util.Map<java.lang.Long, java.lang.Integer> map =
          internalGetJobs().getMap();
      return map.containsKey(key)
             ? jobsValueConverter.doForward(map.get(key))
             : defaultValue;
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getJobsOrThrow(
        long key) {
      
      java.util.Map<java.lang.Long, java.lang.Integer> map =
          internalGetJobs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return jobsValueConverter.doForward(map.get(key));
    }
    /**
     * Use {@link #getJobsValueMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.Long, java.lang.Integer>
    getJobsValue() {
      return getJobsValueMap();
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.Long, java.lang.Integer>
    getJobsValueMap() {
      return internalGetJobs().getMap();
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    @java.lang.Override

    public int getJobsValueOrDefault(
        long key,
        int defaultValue) {
      
      java.util.Map<java.lang.Long, java.lang.Integer> map =
          internalGetJobs().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
     */
    @java.lang.Override

    public int getJobsValueOrThrow(
        long key) {
      
      java.util.Map<java.lang.Long, java.lang.Integer> map =
          internalGetJobs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int STAGES_FIELD_NUMBER = 12;
    private com.google.protobuf.Internal.LongList stages_;
    /**
     * <code>repeated int64 stages = 12;</code>
     * @return A list containing the stages.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getStagesList() {
      return stages_;
    }
    /**
     * <code>repeated int64 stages = 12;</code>
     * @return The count of stages.
     */
    public int getStagesCount() {
      return stages_.size();
    }
    /**
     * <code>repeated int64 stages = 12;</code>
     * @param index The index of the element to return.
     * @return The stages at the given index.
     */
    public long getStages(int index) {
      return stages_.getLong(index);
    }
    private int stagesMemoizedSerializedSize = -1;

    public static final int METRIC_VALUES_IS_NULL_FIELD_NUMBER = 13;
    private boolean metricValuesIsNull_;
    /**
     * <code>bool metric_values_is_null = 13;</code>
     * @return The metricValuesIsNull.
     */
    @java.lang.Override
    public boolean getMetricValuesIsNull() {
      return metricValuesIsNull_;
    }

    public static final int METRIC_VALUES_FIELD_NUMBER = 14;
    private static final class MetricValuesDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.Long, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.Long, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_MetricValuesEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.INT64,
                  0L,
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.Long, java.lang.String> metricValues_;
    private com.google.protobuf.MapField<java.lang.Long, java.lang.String>
    internalGetMetricValues() {
      if (metricValues_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            MetricValuesDefaultEntryHolder.defaultEntry);
      }
      return metricValues_;
    }

    public int getMetricValuesCount() {
      return internalGetMetricValues().getMap().size();
    }
    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */

    @java.lang.Override
    public boolean containsMetricValues(
        long key) {
      
      return internalGetMetricValues().getMap().containsKey(key);
    }
    /**
     * Use {@link #getMetricValuesMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.Long, java.lang.String> getMetricValues() {
      return getMetricValuesMap();
    }
    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.Long, java.lang.String> getMetricValuesMap() {
      return internalGetMetricValues().getMap();
    }
    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricValuesOrDefault(
        long key,
        java.lang.String defaultValue) {
      
      java.util.Map<java.lang.Long, java.lang.String> map =
          internalGetMetricValues().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;int64, string&gt; metric_values = 14;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricValuesOrThrow(
        long key) {
      
      java.util.Map<java.lang.Long, java.lang.String> map =
          internalGetMetricValues().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (executionId_ != 0L) {
        output.writeInt64(1, executionId_);
      }
      if (rootExecutionId_ != 0L) {
        output.writeInt64(2, rootExecutionId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, description_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, details_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, physicalPlanDescription_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetModifiedConfigs(),
          ModifiedConfigsDefaultEntryHolder.defaultEntry,
          6);
      for (int i = 0; i < metrics_.size(); i++) {
        output.writeMessage(7, metrics_.get(i));
      }
      if (submissionTime_ != 0L) {
        output.writeInt64(8, submissionTime_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(9, completionTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 10, errorMessage_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeLongMapTo(
          output,
          internalGetJobs(),
          JobsDefaultEntryHolder.defaultEntry,
          11);
      if (getStagesList().size() > 0) {
        output.writeUInt32NoTag(98);
        output.writeUInt32NoTag(stagesMemoizedSerializedSize);
      }
      for (int i = 0; i < stages_.size(); i++) {
        output.writeInt64NoTag(stages_.getLong(i));
      }
      if (metricValuesIsNull_ != false) {
        output.writeBool(13, metricValuesIsNull_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeLongMapTo(
          output,
          internalGetMetricValues(),
          MetricValuesDefaultEntryHolder.defaultEntry,
          14);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (executionId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, executionId_);
      }
      if (rootExecutionId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, rootExecutionId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, description_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, details_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, physicalPlanDescription_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetModifiedConfigs().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        modifiedConfigs__ = ModifiedConfigsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(6, modifiedConfigs__);
      }
      for (int i = 0; i < metrics_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, metrics_.get(i));
      }
      if (submissionTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, submissionTime_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(9, completionTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(10, errorMessage_);
      }
      for (java.util.Map.Entry<java.lang.Long, java.lang.Integer> entry
           : internalGetJobs().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.Long, java.lang.Integer>
        jobs__ = JobsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(11, jobs__);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < stages_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(stages_.getLong(i));
        }
        size += dataSize;
        if (!getStagesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        stagesMemoizedSerializedSize = dataSize;
      }
      if (metricValuesIsNull_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(13, metricValuesIsNull_);
      }
      for (java.util.Map.Entry<java.lang.Long, java.lang.String> entry
           : internalGetMetricValues().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.Long, java.lang.String>
        metricValues__ = MetricValuesDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(14, metricValues__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData other = (org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData) obj;

      if (getExecutionId()
          != other.getExecutionId()) return false;
      if (getRootExecutionId()
          != other.getRootExecutionId()) return false;
      if (hasDescription() != other.hasDescription()) return false;
      if (hasDescription()) {
        if (!getDescription()
            .equals(other.getDescription())) return false;
      }
      if (hasDetails() != other.hasDetails()) return false;
      if (hasDetails()) {
        if (!getDetails()
            .equals(other.getDetails())) return false;
      }
      if (hasPhysicalPlanDescription() != other.hasPhysicalPlanDescription()) return false;
      if (hasPhysicalPlanDescription()) {
        if (!getPhysicalPlanDescription()
            .equals(other.getPhysicalPlanDescription())) return false;
      }
      if (!internalGetModifiedConfigs().equals(
          other.internalGetModifiedConfigs())) return false;
      if (!getMetricsList()
          .equals(other.getMetricsList())) return false;
      if (getSubmissionTime()
          != other.getSubmissionTime()) return false;
      if (hasCompletionTime() != other.hasCompletionTime()) return false;
      if (hasCompletionTime()) {
        if (getCompletionTime()
            != other.getCompletionTime()) return false;
      }
      if (hasErrorMessage() != other.hasErrorMessage()) return false;
      if (hasErrorMessage()) {
        if (!getErrorMessage()
            .equals(other.getErrorMessage())) return false;
      }
      if (!internalGetJobs().equals(
          other.internalGetJobs())) return false;
      if (!getStagesList()
          .equals(other.getStagesList())) return false;
      if (getMetricValuesIsNull()
          != other.getMetricValuesIsNull()) return false;
      if (!internalGetMetricValues().equals(
          other.internalGetMetricValues())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + EXECUTION_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutionId());
      hash = (37 * hash) + ROOT_EXECUTION_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRootExecutionId());
      if (hasDescription()) {
        hash = (37 * hash) + DESCRIPTION_FIELD_NUMBER;
        hash = (53 * hash) + getDescription().hashCode();
      }
      if (hasDetails()) {
        hash = (37 * hash) + DETAILS_FIELD_NUMBER;
        hash = (53 * hash) + getDetails().hashCode();
      }
      if (hasPhysicalPlanDescription()) {
        hash = (37 * hash) + PHYSICAL_PLAN_DESCRIPTION_FIELD_NUMBER;
        hash = (53 * hash) + getPhysicalPlanDescription().hashCode();
      }
      if (!internalGetModifiedConfigs().getMap().isEmpty()) {
        hash = (37 * hash) + MODIFIED_CONFIGS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetModifiedConfigs().hashCode();
      }
      if (getMetricsCount() > 0) {
        hash = (37 * hash) + METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getMetricsList().hashCode();
      }
      hash = (37 * hash) + SUBMISSION_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getSubmissionTime());
      if (hasCompletionTime()) {
        hash = (37 * hash) + COMPLETION_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getCompletionTime());
      }
      if (hasErrorMessage()) {
        hash = (37 * hash) + ERROR_MESSAGE_FIELD_NUMBER;
        hash = (53 * hash) + getErrorMessage().hashCode();
      }
      if (!internalGetJobs().getMap().isEmpty()) {
        hash = (37 * hash) + JOBS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetJobs().hashCode();
      }
      if (getStagesCount() > 0) {
        hash = (37 * hash) + STAGES_FIELD_NUMBER;
        hash = (53 * hash) + getStagesList().hashCode();
      }
      hash = (37 * hash) + METRIC_VALUES_IS_NULL_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getMetricValuesIsNull());
      if (!internalGetMetricValues().getMap().isEmpty()) {
        hash = (37 * hash) + METRIC_VALUES_FIELD_NUMBER;
        hash = (53 * hash) + internalGetMetricValues().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SQLExecutionUIData}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SQLExecutionUIData)
        org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 6:
            return internalGetModifiedConfigs();
          case 11:
            return internalGetJobs();
          case 14:
            return internalGetMetricValues();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 6:
            return internalGetMutableModifiedConfigs();
          case 11:
            return internalGetMutableJobs();
          case 14:
            return internalGetMutableMetricValues();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData.class, org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getMetricsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        executionId_ = 0L;

        rootExecutionId_ = 0L;

        description_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        details_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        physicalPlanDescription_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        internalGetMutableModifiedConfigs().clear();
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          metricsBuilder_.clear();
        }
        submissionTime_ = 0L;

        completionTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        errorMessage_ = "";
        bitField0_ = (bitField0_ & ~0x00000040);
        internalGetMutableJobs().clear();
        stages_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000100);
        metricValuesIsNull_ = false;

        internalGetMutableMetricValues().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData build() {
        org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData result = new org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.executionId_ = executionId_;
        result.rootExecutionId_ = rootExecutionId_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.description_ = description_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.details_ = details_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.physicalPlanDescription_ = physicalPlanDescription_;
        result.modifiedConfigs_ = internalGetModifiedConfigs();
        result.modifiedConfigs_.makeImmutable();
        if (metricsBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0)) {
            metrics_ = java.util.Collections.unmodifiableList(metrics_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.metrics_ = metrics_;
        } else {
          result.metrics_ = metricsBuilder_.build();
        }
        result.submissionTime_ = submissionTime_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.completionTime_ = completionTime_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.errorMessage_ = errorMessage_;
        result.jobs_ = internalGetJobs();
        result.jobs_.makeImmutable();
        if (((bitField0_ & 0x00000100) != 0)) {
          stages_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000100);
        }
        result.stages_ = stages_;
        result.metricValuesIsNull_ = metricValuesIsNull_;
        result.metricValues_ = internalGetMetricValues();
        result.metricValues_.makeImmutable();
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData.getDefaultInstance()) return this;
        if (other.getExecutionId() != 0L) {
          setExecutionId(other.getExecutionId());
        }
        if (other.getRootExecutionId() != 0L) {
          setRootExecutionId(other.getRootExecutionId());
        }
        if (other.hasDescription()) {
          bitField0_ |= 0x00000001;
          description_ = other.description_;
          onChanged();
        }
        if (other.hasDetails()) {
          bitField0_ |= 0x00000002;
          details_ = other.details_;
          onChanged();
        }
        if (other.hasPhysicalPlanDescription()) {
          bitField0_ |= 0x00000004;
          physicalPlanDescription_ = other.physicalPlanDescription_;
          onChanged();
        }
        internalGetMutableModifiedConfigs().mergeFrom(
            other.internalGetModifiedConfigs());
        if (metricsBuilder_ == null) {
          if (!other.metrics_.isEmpty()) {
            if (metrics_.isEmpty()) {
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureMetricsIsMutable();
              metrics_.addAll(other.metrics_);
            }
            onChanged();
          }
        } else {
          if (!other.metrics_.isEmpty()) {
            if (metricsBuilder_.isEmpty()) {
              metricsBuilder_.dispose();
              metricsBuilder_ = null;
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000010);
              metricsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getMetricsFieldBuilder() : null;
            } else {
              metricsBuilder_.addAllMessages(other.metrics_);
            }
          }
        }
        if (other.getSubmissionTime() != 0L) {
          setSubmissionTime(other.getSubmissionTime());
        }
        if (other.hasCompletionTime()) {
          setCompletionTime(other.getCompletionTime());
        }
        if (other.hasErrorMessage()) {
          bitField0_ |= 0x00000040;
          errorMessage_ = other.errorMessage_;
          onChanged();
        }
        internalGetMutableJobs().mergeFrom(
            other.internalGetJobs());
        if (!other.stages_.isEmpty()) {
          if (stages_.isEmpty()) {
            stages_ = other.stages_;
            bitField0_ = (bitField0_ & ~0x00000100);
          } else {
            ensureStagesIsMutable();
            stages_.addAll(other.stages_);
          }
          onChanged();
        }
        if (other.getMetricValuesIsNull() != false) {
          setMetricValuesIsNull(other.getMetricValuesIsNull());
        }
        internalGetMutableMetricValues().mergeFrom(
            other.internalGetMetricValues());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long executionId_ ;
      /**
       * <code>int64 execution_id = 1;</code>
       * @return The executionId.
       */
      @java.lang.Override
      public long getExecutionId() {
        return executionId_;
      }
      /**
       * <code>int64 execution_id = 1;</code>
       * @param value The executionId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutionId(long value) {
        
        executionId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 execution_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutionId() {
        
        executionId_ = 0L;
        onChanged();
        return this;
      }

      private long rootExecutionId_ ;
      /**
       * <code>int64 root_execution_id = 2;</code>
       * @return The rootExecutionId.
       */
      @java.lang.Override
      public long getRootExecutionId() {
        return rootExecutionId_;
      }
      /**
       * <code>int64 root_execution_id = 2;</code>
       * @param value The rootExecutionId to set.
       * @return This builder for chaining.
       */
      public Builder setRootExecutionId(long value) {
        
        rootExecutionId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 root_execution_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearRootExecutionId() {
        
        rootExecutionId_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object description_ = "";
      /**
       * <code>string description = 3;</code>
       * @return Whether the description field is set.
       */
      public boolean hasDescription() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string description = 3;</code>
       * @return The description.
       */
      public java.lang.String getDescription() {
        java.lang.Object ref = description_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          description_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string description = 3;</code>
       * @return The bytes for description.
       */
      public com.google.protobuf.ByteString
          getDescriptionBytes() {
        java.lang.Object ref = description_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          description_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string description = 3;</code>
       * @param value The description to set.
       * @return This builder for chaining.
       */
      public Builder setDescription(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        description_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string description = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDescription() {
        bitField0_ = (bitField0_ & ~0x00000001);
        description_ = getDefaultInstance().getDescription();
        onChanged();
        return this;
      }
      /**
       * <code>string description = 3;</code>
       * @param value The bytes for description to set.
       * @return This builder for chaining.
       */
      public Builder setDescriptionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        description_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object details_ = "";
      /**
       * <code>string details = 4;</code>
       * @return Whether the details field is set.
       */
      public boolean hasDetails() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string details = 4;</code>
       * @return The details.
       */
      public java.lang.String getDetails() {
        java.lang.Object ref = details_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          details_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string details = 4;</code>
       * @return The bytes for details.
       */
      public com.google.protobuf.ByteString
          getDetailsBytes() {
        java.lang.Object ref = details_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          details_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string details = 4;</code>
       * @param value The details to set.
       * @return This builder for chaining.
       */
      public Builder setDetails(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        details_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string details = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearDetails() {
        bitField0_ = (bitField0_ & ~0x00000002);
        details_ = getDefaultInstance().getDetails();
        onChanged();
        return this;
      }
      /**
       * <code>string details = 4;</code>
       * @param value The bytes for details to set.
       * @return This builder for chaining.
       */
      public Builder setDetailsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        details_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object physicalPlanDescription_ = "";
      /**
       * <code>string physical_plan_description = 5;</code>
       * @return Whether the physicalPlanDescription field is set.
       */
      public boolean hasPhysicalPlanDescription() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string physical_plan_description = 5;</code>
       * @return The physicalPlanDescription.
       */
      public java.lang.String getPhysicalPlanDescription() {
        java.lang.Object ref = physicalPlanDescription_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          physicalPlanDescription_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string physical_plan_description = 5;</code>
       * @return The bytes for physicalPlanDescription.
       */
      public com.google.protobuf.ByteString
          getPhysicalPlanDescriptionBytes() {
        java.lang.Object ref = physicalPlanDescription_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          physicalPlanDescription_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string physical_plan_description = 5;</code>
       * @param value The physicalPlanDescription to set.
       * @return This builder for chaining.
       */
      public Builder setPhysicalPlanDescription(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        physicalPlanDescription_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string physical_plan_description = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearPhysicalPlanDescription() {
        bitField0_ = (bitField0_ & ~0x00000004);
        physicalPlanDescription_ = getDefaultInstance().getPhysicalPlanDescription();
        onChanged();
        return this;
      }
      /**
       * <code>string physical_plan_description = 5;</code>
       * @param value The bytes for physicalPlanDescription to set.
       * @return This builder for chaining.
       */
      public Builder setPhysicalPlanDescriptionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        physicalPlanDescription_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> modifiedConfigs_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetModifiedConfigs() {
        if (modifiedConfigs_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ModifiedConfigsDefaultEntryHolder.defaultEntry);
        }
        return modifiedConfigs_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableModifiedConfigs() {
        onChanged();;
        if (modifiedConfigs_ == null) {
          modifiedConfigs_ = com.google.protobuf.MapField.newMapField(
              ModifiedConfigsDefaultEntryHolder.defaultEntry);
        }
        if (!modifiedConfigs_.isMutable()) {
          modifiedConfigs_ = modifiedConfigs_.copy();
        }
        return modifiedConfigs_;
      }

      public int getModifiedConfigsCount() {
        return internalGetModifiedConfigs().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; modified_configs = 6;</code>
       */

      @java.lang.Override
      public boolean containsModifiedConfigs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetModifiedConfigs().getMap().containsKey(key);
      }
      /**
       * Use {@link #getModifiedConfigsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getModifiedConfigs() {
        return getModifiedConfigsMap();
      }
      /**
       * <code>map&lt;string, string&gt; modified_configs = 6;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getModifiedConfigsMap() {
        return internalGetModifiedConfigs().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; modified_configs = 6;</code>
       */
      @java.lang.Override

      public java.lang.String getModifiedConfigsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetModifiedConfigs().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; modified_configs = 6;</code>
       */
      @java.lang.Override

      public java.lang.String getModifiedConfigsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetModifiedConfigs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearModifiedConfigs() {
        internalGetMutableModifiedConfigs().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; modified_configs = 6;</code>
       */

      public Builder removeModifiedConfigs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableModifiedConfigs().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableModifiedConfigs() {
        return internalGetMutableModifiedConfigs().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; modified_configs = 6;</code>
       */
      public Builder putModifiedConfigs(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableModifiedConfigs().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; modified_configs = 6;</code>
       */

      public Builder putAllModifiedConfigs(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableModifiedConfigs().getMutableMap()
            .putAll(values);
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> metrics_ =
        java.util.Collections.emptyList();
      private void ensureMetricsIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          metrics_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric>(metrics_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> metricsBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> getMetricsList() {
        if (metricsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(metrics_);
        } else {
          return metricsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public int getMetricsCount() {
        if (metricsBuilder_ == null) {
          return metrics_.size();
        } else {
          return metricsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);
        } else {
          return metricsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder setMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.set(index, value);
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder setMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.set(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder addMetrics(org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder addMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(index, value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder addMetrics(
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder addMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder addAllMetrics(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> values) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, metrics_);
          onChanged();
        } else {
          metricsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder clearMetrics() {
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          metricsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public Builder removeMetrics(int index) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.remove(index);
          onChanged();
        } else {
          metricsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder getMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
          int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);  } else {
          return metricsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
           getMetricsOrBuilderList() {
        if (metricsBuilder_ != null) {
          return metricsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(metrics_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder addMetricsBuilder() {
        return getMetricsFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder addMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 7;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder> 
           getMetricsBuilderList() {
        return getMetricsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
          getMetricsFieldBuilder() {
        if (metricsBuilder_ == null) {
          metricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder>(
                  metrics_,
                  ((bitField0_ & 0x00000010) != 0),
                  getParentForChildren(),
                  isClean());
          metrics_ = null;
        }
        return metricsBuilder_;
      }

      private long submissionTime_ ;
      /**
       * <code>int64 submission_time = 8;</code>
       * @return The submissionTime.
       */
      @java.lang.Override
      public long getSubmissionTime() {
        return submissionTime_;
      }
      /**
       * <code>int64 submission_time = 8;</code>
       * @param value The submissionTime to set.
       * @return This builder for chaining.
       */
      public Builder setSubmissionTime(long value) {
        
        submissionTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 submission_time = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearSubmissionTime() {
        
        submissionTime_ = 0L;
        onChanged();
        return this;
      }

      private long completionTime_ ;
      /**
       * <code>int64 completion_time = 9;</code>
       * @return Whether the completionTime field is set.
       */
      @java.lang.Override
      public boolean hasCompletionTime() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>int64 completion_time = 9;</code>
       * @return The completionTime.
       */
      @java.lang.Override
      public long getCompletionTime() {
        return completionTime_;
      }
      /**
       * <code>int64 completion_time = 9;</code>
       * @param value The completionTime to set.
       * @return This builder for chaining.
       */
      public Builder setCompletionTime(long value) {
        bitField0_ |= 0x00000020;
        completionTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 completion_time = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearCompletionTime() {
        bitField0_ = (bitField0_ & ~0x00000020);
        completionTime_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object errorMessage_ = "";
      /**
       * <code>string error_message = 10;</code>
       * @return Whether the errorMessage field is set.
       */
      public boolean hasErrorMessage() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>string error_message = 10;</code>
       * @return The errorMessage.
       */
      public java.lang.String getErrorMessage() {
        java.lang.Object ref = errorMessage_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          errorMessage_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string error_message = 10;</code>
       * @return The bytes for errorMessage.
       */
      public com.google.protobuf.ByteString
          getErrorMessageBytes() {
        java.lang.Object ref = errorMessage_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          errorMessage_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string error_message = 10;</code>
       * @param value The errorMessage to set.
       * @return This builder for chaining.
       */
      public Builder setErrorMessage(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        errorMessage_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string error_message = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearErrorMessage() {
        bitField0_ = (bitField0_ & ~0x00000040);
        errorMessage_ = getDefaultInstance().getErrorMessage();
        onChanged();
        return this;
      }
      /**
       * <code>string error_message = 10;</code>
       * @param value The bytes for errorMessage to set.
       * @return This builder for chaining.
       */
      public Builder setErrorMessageBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000040;
        errorMessage_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.Long, java.lang.Integer> jobs_;
      private com.google.protobuf.MapField<java.lang.Long, java.lang.Integer>
      internalGetJobs() {
        if (jobs_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              JobsDefaultEntryHolder.defaultEntry);
        }
        return jobs_;
      }
      private com.google.protobuf.MapField<java.lang.Long, java.lang.Integer>
      internalGetMutableJobs() {
        onChanged();;
        if (jobs_ == null) {
          jobs_ = com.google.protobuf.MapField.newMapField(
              JobsDefaultEntryHolder.defaultEntry);
        }
        if (!jobs_.isMutable()) {
          jobs_ = jobs_.copy();
        }
        return jobs_;
      }

      public int getJobsCount() {
        return internalGetJobs().getMap().size();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */

      @java.lang.Override
      public boolean containsJobs(
          long key) {
        
        return internalGetJobs().getMap().containsKey(key);
      }
      /**
       * Use {@link #getJobsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus>
      getJobs() {
        return getJobsMap();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus>
      getJobsMap() {
        return internalGetAdaptedJobsMap(
            internalGetJobs().getMap());}
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getJobsOrDefault(
          long key,
          org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus defaultValue) {
        
        java.util.Map<java.lang.Long, java.lang.Integer> map =
            internalGetJobs().getMap();
        return map.containsKey(key)
               ? jobsValueConverter.doForward(map.get(key))
               : defaultValue;
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus getJobsOrThrow(
          long key) {
        
        java.util.Map<java.lang.Long, java.lang.Integer> map =
            internalGetJobs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return jobsValueConverter.doForward(map.get(key));
      }
      /**
       * Use {@link #getJobsValueMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, java.lang.Integer>
      getJobsValue() {
        return getJobsValueMap();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.Long, java.lang.Integer>
      getJobsValueMap() {
        return internalGetJobs().getMap();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      @java.lang.Override

      public int getJobsValueOrDefault(
          long key,
          int defaultValue) {
        
        java.util.Map<java.lang.Long, java.lang.Integer> map =
            internalGetJobs().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      @java.lang.Override

      public int getJobsValueOrThrow(
          long key) {
        
        java.util.Map<java.lang.Long, java.lang.Integer> map =
            internalGetJobs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearJobs() {
        internalGetMutableJobs().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */

      public Builder removeJobs(
          long key) {
        
        internalGetMutableJobs().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus>
      getMutableJobs() {
        return internalGetAdaptedJobsMap(
             internalGetMutableJobs().getMutableMap());
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      public Builder putJobs(
          long key,
          org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus value) {
        
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableJobs().getMutableMap()
            .put(key, jobsValueConverter.doBackward(value));
        return this;
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      public Builder putAllJobs(
          java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.JobExecutionStatus> values) {
        internalGetAdaptedJobsMap(
            internalGetMutableJobs().getMutableMap())
                .putAll(values);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, java.lang.Integer>
      getMutableJobsValue() {
        return internalGetMutableJobs().getMutableMap();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      public Builder putJobsValue(
          long key,
          int value) {
        
        internalGetMutableJobs().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.JobExecutionStatus&gt; jobs = 11;</code>
       */
      public Builder putAllJobsValue(
          java.util.Map<java.lang.Long, java.lang.Integer> values) {
        internalGetMutableJobs().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.Internal.LongList stages_ = emptyLongList();
      private void ensureStagesIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          stages_ = mutableCopy(stages_);
          bitField0_ |= 0x00000100;
         }
      }
      /**
       * <code>repeated int64 stages = 12;</code>
       * @return A list containing the stages.
       */
      public java.util.List<java.lang.Long>
          getStagesList() {
        return ((bitField0_ & 0x00000100) != 0) ?
                 java.util.Collections.unmodifiableList(stages_) : stages_;
      }
      /**
       * <code>repeated int64 stages = 12;</code>
       * @return The count of stages.
       */
      public int getStagesCount() {
        return stages_.size();
      }
      /**
       * <code>repeated int64 stages = 12;</code>
       * @param index The index of the element to return.
       * @return The stages at the given index.
       */
      public long getStages(int index) {
        return stages_.getLong(index);
      }
      /**
       * <code>repeated int64 stages = 12;</code>
       * @param index The index to set the value at.
       * @param value The stages to set.
       * @return This builder for chaining.
       */
      public Builder setStages(
          int index, long value) {
        ensureStagesIsMutable();
        stages_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stages = 12;</code>
       * @param value The stages to add.
       * @return This builder for chaining.
       */
      public Builder addStages(long value) {
        ensureStagesIsMutable();
        stages_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stages = 12;</code>
       * @param values The stages to add.
       * @return This builder for chaining.
       */
      public Builder addAllStages(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureStagesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, stages_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stages = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearStages() {
        stages_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000100);
        onChanged();
        return this;
      }

      private boolean metricValuesIsNull_ ;
      /**
       * <code>bool metric_values_is_null = 13;</code>
       * @return The metricValuesIsNull.
       */
      @java.lang.Override
      public boolean getMetricValuesIsNull() {
        return metricValuesIsNull_;
      }
      /**
       * <code>bool metric_values_is_null = 13;</code>
       * @param value The metricValuesIsNull to set.
       * @return This builder for chaining.
       */
      public Builder setMetricValuesIsNull(boolean value) {
        
        metricValuesIsNull_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool metric_values_is_null = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearMetricValuesIsNull() {
        
        metricValuesIsNull_ = false;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.Long, java.lang.String> metricValues_;
      private com.google.protobuf.MapField<java.lang.Long, java.lang.String>
      internalGetMetricValues() {
        if (metricValues_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              MetricValuesDefaultEntryHolder.defaultEntry);
        }
        return metricValues_;
      }
      private com.google.protobuf.MapField<java.lang.Long, java.lang.String>
      internalGetMutableMetricValues() {
        onChanged();;
        if (metricValues_ == null) {
          metricValues_ = com.google.protobuf.MapField.newMapField(
              MetricValuesDefaultEntryHolder.defaultEntry);
        }
        if (!metricValues_.isMutable()) {
          metricValues_ = metricValues_.copy();
        }
        return metricValues_;
      }

      public int getMetricValuesCount() {
        return internalGetMetricValues().getMap().size();
      }
      /**
       * <code>map&lt;int64, string&gt; metric_values = 14;</code>
       */

      @java.lang.Override
      public boolean containsMetricValues(
          long key) {
        
        return internalGetMetricValues().getMap().containsKey(key);
      }
      /**
       * Use {@link #getMetricValuesMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, java.lang.String> getMetricValues() {
        return getMetricValuesMap();
      }
      /**
       * <code>map&lt;int64, string&gt; metric_values = 14;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.Long, java.lang.String> getMetricValuesMap() {
        return internalGetMetricValues().getMap();
      }
      /**
       * <code>map&lt;int64, string&gt; metric_values = 14;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricValuesOrDefault(
          long key,
          java.lang.String defaultValue) {
        
        java.util.Map<java.lang.Long, java.lang.String> map =
            internalGetMetricValues().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;int64, string&gt; metric_values = 14;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricValuesOrThrow(
          long key) {
        
        java.util.Map<java.lang.Long, java.lang.String> map =
            internalGetMetricValues().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearMetricValues() {
        internalGetMutableMetricValues().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;int64, string&gt; metric_values = 14;</code>
       */

      public Builder removeMetricValues(
          long key) {
        
        internalGetMutableMetricValues().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, java.lang.String>
      getMutableMetricValues() {
        return internalGetMutableMetricValues().getMutableMap();
      }
      /**
       * <code>map&lt;int64, string&gt; metric_values = 14;</code>
       */
      public Builder putMetricValues(
          long key,
          java.lang.String value) {
        
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetricValues().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;int64, string&gt; metric_values = 14;</code>
       */

      public Builder putAllMetricValues(
          java.util.Map<java.lang.Long, java.lang.String> values) {
        internalGetMutableMetricValues().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SQLExecutionUIData)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SQLExecutionUIData)
    private static final org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SQLExecutionUIData>
        PARSER = new com.google.protobuf.AbstractParser<SQLExecutionUIData>() {
      @java.lang.Override
      public SQLExecutionUIData parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SQLExecutionUIData(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SQLExecutionUIData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SQLExecutionUIData> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SparkPlanGraphNodeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SparkPlanGraphNode)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 id = 1;</code>
     * @return The id.
     */
    long getId();

    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>string desc = 3;</code>
     * @return Whether the desc field is set.
     */
    boolean hasDesc();
    /**
     * <code>string desc = 3;</code>
     * @return The desc.
     */
    java.lang.String getDesc();
    /**
     * <code>string desc = 3;</code>
     * @return The bytes for desc.
     */
    com.google.protobuf.ByteString
        getDescBytes();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> 
        getMetricsList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    int getMetricsCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
        getMetricsOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphNode}
   */
  public static final class SparkPlanGraphNode extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SparkPlanGraphNode)
      SparkPlanGraphNodeOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkPlanGraphNode.newBuilder() to construct.
    private SparkPlanGraphNode(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkPlanGraphNode() {
      name_ = "";
      desc_ = "";
      metrics_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SparkPlanGraphNode();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SparkPlanGraphNode(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              id_ = input.readInt64();
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              desc_ = s;
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                metrics_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric>();
                mutable_bitField0_ |= 0x00000004;
              }
              metrics_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          metrics_ = java.util.Collections.unmodifiableList(metrics_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private long id_;
    /**
     * <code>int64 id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public long getId() {
      return id_;
    }

    public static final int NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DESC_FIELD_NUMBER = 3;
    private volatile java.lang.Object desc_;
    /**
     * <code>string desc = 3;</code>
     * @return Whether the desc field is set.
     */
    @java.lang.Override
    public boolean hasDesc() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string desc = 3;</code>
     * @return The desc.
     */
    @java.lang.Override
    public java.lang.String getDesc() {
      java.lang.Object ref = desc_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        desc_ = s;
        return s;
      }
    }
    /**
     * <code>string desc = 3;</code>
     * @return The bytes for desc.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDescBytes() {
      java.lang.Object ref = desc_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        desc_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int METRICS_FIELD_NUMBER = 4;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> metrics_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> getMetricsList() {
      return metrics_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
        getMetricsOrBuilderList() {
      return metrics_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    @java.lang.Override
    public int getMetricsCount() {
      return metrics_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index) {
      return metrics_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
        int index) {
      return metrics_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (id_ != 0L) {
        output.writeInt64(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, desc_);
      }
      for (int i = 0; i < metrics_.size(); i++) {
        output.writeMessage(4, metrics_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (id_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, desc_);
      }
      for (int i = 0; i < metrics_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, metrics_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode other = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) obj;

      if (getId()
          != other.getId()) return false;
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasDesc() != other.hasDesc()) return false;
      if (hasDesc()) {
        if (!getDesc()
            .equals(other.getDesc())) return false;
      }
      if (!getMetricsList()
          .equals(other.getMetricsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getId());
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasDesc()) {
        hash = (37 * hash) + DESC_FIELD_NUMBER;
        hash = (53 * hash) + getDesc().hashCode();
      }
      if (getMetricsCount() > 0) {
        hash = (37 * hash) + METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getMetricsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphNode}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SparkPlanGraphNode)
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getMetricsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0L;

        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        desc_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          metricsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode build() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode result = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.desc_ = desc_;
        if (metricsBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            metrics_ = java.util.Collections.unmodifiableList(metrics_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.metrics_ = metrics_;
        } else {
          result.metrics_ = metricsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance()) return this;
        if (other.getId() != 0L) {
          setId(other.getId());
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasDesc()) {
          bitField0_ |= 0x00000002;
          desc_ = other.desc_;
          onChanged();
        }
        if (metricsBuilder_ == null) {
          if (!other.metrics_.isEmpty()) {
            if (metrics_.isEmpty()) {
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureMetricsIsMutable();
              metrics_.addAll(other.metrics_);
            }
            onChanged();
          }
        } else {
          if (!other.metrics_.isEmpty()) {
            if (metricsBuilder_.isEmpty()) {
              metricsBuilder_.dispose();
              metricsBuilder_ = null;
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000004);
              metricsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getMetricsFieldBuilder() : null;
            } else {
              metricsBuilder_.addAllMessages(other.metrics_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long id_ ;
      /**
       * <code>int64 id = 1;</code>
       * @return The id.
       */
      @java.lang.Override
      public long getId() {
        return id_;
      }
      /**
       * <code>int64 id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(long value) {
        
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        
        id_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 2;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 2;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object desc_ = "";
      /**
       * <code>string desc = 3;</code>
       * @return Whether the desc field is set.
       */
      public boolean hasDesc() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string desc = 3;</code>
       * @return The desc.
       */
      public java.lang.String getDesc() {
        java.lang.Object ref = desc_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          desc_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string desc = 3;</code>
       * @return The bytes for desc.
       */
      public com.google.protobuf.ByteString
          getDescBytes() {
        java.lang.Object ref = desc_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          desc_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string desc = 3;</code>
       * @param value The desc to set.
       * @return This builder for chaining.
       */
      public Builder setDesc(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        desc_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string desc = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDesc() {
        bitField0_ = (bitField0_ & ~0x00000002);
        desc_ = getDefaultInstance().getDesc();
        onChanged();
        return this;
      }
      /**
       * <code>string desc = 3;</code>
       * @param value The bytes for desc to set.
       * @return This builder for chaining.
       */
      public Builder setDescBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        desc_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> metrics_ =
        java.util.Collections.emptyList();
      private void ensureMetricsIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          metrics_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric>(metrics_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> metricsBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> getMetricsList() {
        if (metricsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(metrics_);
        } else {
          return metricsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public int getMetricsCount() {
        if (metricsBuilder_ == null) {
          return metrics_.size();
        } else {
          return metricsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);
        } else {
          return metricsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder setMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.set(index, value);
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder setMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.set(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder addMetrics(org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder addMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(index, value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder addMetrics(
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder addMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder addAllMetrics(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> values) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, metrics_);
          onChanged();
        } else {
          metricsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder clearMetrics() {
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          metricsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public Builder removeMetrics(int index) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.remove(index);
          onChanged();
        } else {
          metricsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder getMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
          int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);  } else {
          return metricsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
           getMetricsOrBuilderList() {
        if (metricsBuilder_ != null) {
          return metricsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(metrics_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder addMetricsBuilder() {
        return getMetricsFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder addMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder> 
           getMetricsBuilderList() {
        return getMetricsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
          getMetricsFieldBuilder() {
        if (metricsBuilder_ == null) {
          metricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder>(
                  metrics_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          metrics_ = null;
        }
        return metricsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SparkPlanGraphNode)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SparkPlanGraphNode)
    private static final org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkPlanGraphNode>
        PARSER = new com.google.protobuf.AbstractParser<SparkPlanGraphNode>() {
      @java.lang.Override
      public SparkPlanGraphNode parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SparkPlanGraphNode(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SparkPlanGraphNode> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkPlanGraphNode> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SparkPlanGraphClusterWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 id = 1;</code>
     * @return The id.
     */
    long getId();

    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>string desc = 3;</code>
     * @return Whether the desc field is set.
     */
    boolean hasDesc();
    /**
     * <code>string desc = 3;</code>
     * @return The desc.
     */
    java.lang.String getDesc();
    /**
     * <code>string desc = 3;</code>
     * @return The bytes for desc.
     */
    com.google.protobuf.ByteString
        getDescBytes();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> 
        getNodesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getNodes(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    int getNodesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> 
        getNodesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder getNodesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> 
        getMetricsList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    int getMetricsCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
        getMetricsOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper}
   */
  public static final class SparkPlanGraphClusterWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper)
      SparkPlanGraphClusterWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkPlanGraphClusterWrapper.newBuilder() to construct.
    private SparkPlanGraphClusterWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkPlanGraphClusterWrapper() {
      name_ = "";
      desc_ = "";
      nodes_ = java.util.Collections.emptyList();
      metrics_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SparkPlanGraphClusterWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SparkPlanGraphClusterWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              id_ = input.readInt64();
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              desc_ = s;
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                nodes_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper>();
                mutable_bitField0_ |= 0x00000004;
              }
              nodes_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.parser(), extensionRegistry));
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                metrics_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric>();
                mutable_bitField0_ |= 0x00000008;
              }
              metrics_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          nodes_ = java.util.Collections.unmodifiableList(nodes_);
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          metrics_ = java.util.Collections.unmodifiableList(metrics_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private long id_;
    /**
     * <code>int64 id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public long getId() {
      return id_;
    }

    public static final int NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DESC_FIELD_NUMBER = 3;
    private volatile java.lang.Object desc_;
    /**
     * <code>string desc = 3;</code>
     * @return Whether the desc field is set.
     */
    @java.lang.Override
    public boolean hasDesc() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string desc = 3;</code>
     * @return The desc.
     */
    @java.lang.Override
    public java.lang.String getDesc() {
      java.lang.Object ref = desc_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        desc_ = s;
        return s;
      }
    }
    /**
     * <code>string desc = 3;</code>
     * @return The bytes for desc.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDescBytes() {
      java.lang.Object ref = desc_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        desc_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int NODES_FIELD_NUMBER = 4;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> nodes_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> getNodesList() {
      return nodes_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> 
        getNodesOrBuilderList() {
      return nodes_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    @java.lang.Override
    public int getNodesCount() {
      return nodes_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getNodes(int index) {
      return nodes_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder getNodesOrBuilder(
        int index) {
      return nodes_.get(index);
    }

    public static final int METRICS_FIELD_NUMBER = 5;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> metrics_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> getMetricsList() {
      return metrics_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
        getMetricsOrBuilderList() {
      return metrics_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    @java.lang.Override
    public int getMetricsCount() {
      return metrics_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index) {
      return metrics_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
        int index) {
      return metrics_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (id_ != 0L) {
        output.writeInt64(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, desc_);
      }
      for (int i = 0; i < nodes_.size(); i++) {
        output.writeMessage(4, nodes_.get(i));
      }
      for (int i = 0; i < metrics_.size(); i++) {
        output.writeMessage(5, metrics_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (id_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, desc_);
      }
      for (int i = 0; i < nodes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, nodes_.get(i));
      }
      for (int i = 0; i < metrics_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, metrics_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper other = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) obj;

      if (getId()
          != other.getId()) return false;
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasDesc() != other.hasDesc()) return false;
      if (hasDesc()) {
        if (!getDesc()
            .equals(other.getDesc())) return false;
      }
      if (!getNodesList()
          .equals(other.getNodesList())) return false;
      if (!getMetricsList()
          .equals(other.getMetricsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getId());
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasDesc()) {
        hash = (37 * hash) + DESC_FIELD_NUMBER;
        hash = (53 * hash) + getDesc().hashCode();
      }
      if (getNodesCount() > 0) {
        hash = (37 * hash) + NODES_FIELD_NUMBER;
        hash = (53 * hash) + getNodesList().hashCode();
      }
      if (getMetricsCount() > 0) {
        hash = (37 * hash) + METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getMetricsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper)
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodesFieldBuilder();
          getMetricsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0L;

        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        desc_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (nodesBuilder_ == null) {
          nodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          nodesBuilder_.clear();
        }
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          metricsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper result = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.desc_ = desc_;
        if (nodesBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            nodes_ = java.util.Collections.unmodifiableList(nodes_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.nodes_ = nodes_;
        } else {
          result.nodes_ = nodesBuilder_.build();
        }
        if (metricsBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0)) {
            metrics_ = java.util.Collections.unmodifiableList(metrics_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.metrics_ = metrics_;
        } else {
          result.metrics_ = metricsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance()) return this;
        if (other.getId() != 0L) {
          setId(other.getId());
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasDesc()) {
          bitField0_ |= 0x00000002;
          desc_ = other.desc_;
          onChanged();
        }
        if (nodesBuilder_ == null) {
          if (!other.nodes_.isEmpty()) {
            if (nodes_.isEmpty()) {
              nodes_ = other.nodes_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureNodesIsMutable();
              nodes_.addAll(other.nodes_);
            }
            onChanged();
          }
        } else {
          if (!other.nodes_.isEmpty()) {
            if (nodesBuilder_.isEmpty()) {
              nodesBuilder_.dispose();
              nodesBuilder_ = null;
              nodes_ = other.nodes_;
              bitField0_ = (bitField0_ & ~0x00000004);
              nodesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNodesFieldBuilder() : null;
            } else {
              nodesBuilder_.addAllMessages(other.nodes_);
            }
          }
        }
        if (metricsBuilder_ == null) {
          if (!other.metrics_.isEmpty()) {
            if (metrics_.isEmpty()) {
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureMetricsIsMutable();
              metrics_.addAll(other.metrics_);
            }
            onChanged();
          }
        } else {
          if (!other.metrics_.isEmpty()) {
            if (metricsBuilder_.isEmpty()) {
              metricsBuilder_.dispose();
              metricsBuilder_ = null;
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000008);
              metricsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getMetricsFieldBuilder() : null;
            } else {
              metricsBuilder_.addAllMessages(other.metrics_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long id_ ;
      /**
       * <code>int64 id = 1;</code>
       * @return The id.
       */
      @java.lang.Override
      public long getId() {
        return id_;
      }
      /**
       * <code>int64 id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(long value) {
        
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        
        id_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 2;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 2;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object desc_ = "";
      /**
       * <code>string desc = 3;</code>
       * @return Whether the desc field is set.
       */
      public boolean hasDesc() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string desc = 3;</code>
       * @return The desc.
       */
      public java.lang.String getDesc() {
        java.lang.Object ref = desc_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          desc_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string desc = 3;</code>
       * @return The bytes for desc.
       */
      public com.google.protobuf.ByteString
          getDescBytes() {
        java.lang.Object ref = desc_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          desc_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string desc = 3;</code>
       * @param value The desc to set.
       * @return This builder for chaining.
       */
      public Builder setDesc(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        desc_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string desc = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDesc() {
        bitField0_ = (bitField0_ & ~0x00000002);
        desc_ = getDefaultInstance().getDesc();
        onChanged();
        return this;
      }
      /**
       * <code>string desc = 3;</code>
       * @param value The bytes for desc to set.
       * @return This builder for chaining.
       */
      public Builder setDescBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        desc_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> nodes_ =
        java.util.Collections.emptyList();
      private void ensureNodesIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          nodes_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper>(nodes_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> nodesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> getNodesList() {
        if (nodesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodes_);
        } else {
          return nodesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public int getNodesCount() {
        if (nodesBuilder_ == null) {
          return nodes_.size();
        } else {
          return nodesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getNodes(int index) {
        if (nodesBuilder_ == null) {
          return nodes_.get(index);
        } else {
          return nodesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder setNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper value) {
        if (nodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesIsMutable();
          nodes_.set(index, value);
          onChanged();
        } else {
          nodesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder setNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder builderForValue) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          nodes_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder addNodes(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper value) {
        if (nodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesIsMutable();
          nodes_.add(value);
          onChanged();
        } else {
          nodesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder addNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper value) {
        if (nodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesIsMutable();
          nodes_.add(index, value);
          onChanged();
        } else {
          nodesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder addNodes(
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder builderForValue) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          nodes_.add(builderForValue.build());
          onChanged();
        } else {
          nodesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder addNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder builderForValue) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          nodes_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder addAllNodes(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> values) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, nodes_);
          onChanged();
        } else {
          nodesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder clearNodes() {
        if (nodesBuilder_ == null) {
          nodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          nodesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public Builder removeNodes(int index) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          nodes_.remove(index);
          onChanged();
        } else {
          nodesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder getNodesBuilder(
          int index) {
        return getNodesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder getNodesOrBuilder(
          int index) {
        if (nodesBuilder_ == null) {
          return nodes_.get(index);  } else {
          return nodesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> 
           getNodesOrBuilderList() {
        if (nodesBuilder_ != null) {
          return nodesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodes_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder addNodesBuilder() {
        return getNodesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder addNodesBuilder(
          int index) {
        return getNodesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder> 
           getNodesBuilderList() {
        return getNodesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> 
          getNodesFieldBuilder() {
        if (nodesBuilder_ == null) {
          nodesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder>(
                  nodes_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          nodes_ = null;
        }
        return nodesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> metrics_ =
        java.util.Collections.emptyList();
      private void ensureMetricsIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          metrics_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric>(metrics_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> metricsBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> getMetricsList() {
        if (metricsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(metrics_);
        } else {
          return metricsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public int getMetricsCount() {
        if (metricsBuilder_ == null) {
          return metrics_.size();
        } else {
          return metricsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric getMetrics(int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);
        } else {
          return metricsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder setMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.set(index, value);
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder setMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.set(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder addMetrics(org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder addMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(index, value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder addMetrics(
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder addMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder addAllMetrics(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric> values) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, metrics_);
          onChanged();
        } else {
          metricsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder clearMetrics() {
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          metricsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public Builder removeMetrics(int index) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.remove(index);
          onChanged();
        } else {
          metricsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder getMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder getMetricsOrBuilder(
          int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);  } else {
          return metricsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
           getMetricsOrBuilderList() {
        if (metricsBuilder_ != null) {
          return metricsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(metrics_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder addMetricsBuilder() {
        return getMetricsFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder addMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SQLPlanMetric metrics = 5;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder> 
           getMetricsBuilderList() {
        return getMetricsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder> 
          getMetricsFieldBuilder() {
        if (metricsBuilder_ == null) {
          metricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetric.Builder, org.apache.spark.status.protobuf.StoreTypes.SQLPlanMetricOrBuilder>(
                  metrics_,
                  ((bitField0_ & 0x00000008) != 0),
                  getParentForChildren(),
                  isClean());
          metrics_ = null;
        }
        return metricsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkPlanGraphClusterWrapper>
        PARSER = new com.google.protobuf.AbstractParser<SparkPlanGraphClusterWrapper>() {
      @java.lang.Override
      public SparkPlanGraphClusterWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SparkPlanGraphClusterWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SparkPlanGraphClusterWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkPlanGraphClusterWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SparkPlanGraphNodeWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
     * @return Whether the node field is set.
     */
    boolean hasNode();
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
     * @return The node.
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode getNode();
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeOrBuilder getNodeOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
     * @return Whether the cluster field is set.
     */
    boolean hasCluster();
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
     * @return The cluster.
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper getCluster();
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapperOrBuilder getClusterOrBuilder();

    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.WrapperCase getWrapperCase();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper}
   */
  public static final class SparkPlanGraphNodeWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper)
      SparkPlanGraphNodeWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkPlanGraphNodeWrapper.newBuilder() to construct.
    private SparkPlanGraphNodeWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkPlanGraphNodeWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SparkPlanGraphNodeWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SparkPlanGraphNodeWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.Builder subBuilder = null;
              if (wrapperCase_ == 1) {
                subBuilder = ((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_).toBuilder();
              }
              wrapper_ =
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_);
                wrapper_ = subBuilder.buildPartial();
              }
              wrapperCase_ = 1;
              break;
            }
            case 18: {
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.Builder subBuilder = null;
              if (wrapperCase_ == 2) {
                subBuilder = ((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_).toBuilder();
              }
              wrapper_ =
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_);
                wrapper_ = subBuilder.buildPartial();
              }
              wrapperCase_ = 2;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder.class);
    }

    private int wrapperCase_ = 0;
    private java.lang.Object wrapper_;
    public enum WrapperCase
        implements com.google.protobuf.Internal.EnumLite,
            com.google.protobuf.AbstractMessage.InternalOneOfEnum {
      NODE(1),
      CLUSTER(2),
      WRAPPER_NOT_SET(0);
      private final int value;
      private WrapperCase(int value) {
        this.value = value;
      }
      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static WrapperCase valueOf(int value) {
        return forNumber(value);
      }

      public static WrapperCase forNumber(int value) {
        switch (value) {
          case 1: return NODE;
          case 2: return CLUSTER;
          case 0: return WRAPPER_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public WrapperCase
    getWrapperCase() {
      return WrapperCase.forNumber(
          wrapperCase_);
    }

    public static final int NODE_FIELD_NUMBER = 1;
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
     * @return Whether the node field is set.
     */
    @java.lang.Override
    public boolean hasNode() {
      return wrapperCase_ == 1;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
     * @return The node.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode getNode() {
      if (wrapperCase_ == 1) {
         return (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_;
      }
      return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeOrBuilder getNodeOrBuilder() {
      if (wrapperCase_ == 1) {
         return (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_;
      }
      return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance();
    }

    public static final int CLUSTER_FIELD_NUMBER = 2;
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
     * @return Whether the cluster field is set.
     */
    @java.lang.Override
    public boolean hasCluster() {
      return wrapperCase_ == 2;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
     * @return The cluster.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper getCluster() {
      if (wrapperCase_ == 2) {
         return (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_;
      }
      return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapperOrBuilder getClusterOrBuilder() {
      if (wrapperCase_ == 2) {
         return (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_;
      }
      return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (wrapperCase_ == 1) {
        output.writeMessage(1, (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_);
      }
      if (wrapperCase_ == 2) {
        output.writeMessage(2, (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (wrapperCase_ == 1) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_);
      }
      if (wrapperCase_ == 2) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper other = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper) obj;

      if (!getWrapperCase().equals(other.getWrapperCase())) return false;
      switch (wrapperCase_) {
        case 1:
          if (!getNode()
              .equals(other.getNode())) return false;
          break;
        case 2:
          if (!getCluster()
              .equals(other.getCluster())) return false;
          break;
        case 0:
        default:
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      switch (wrapperCase_) {
        case 1:
          hash = (37 * hash) + NODE_FIELD_NUMBER;
          hash = (53 * hash) + getNode().hashCode();
          break;
        case 2:
          hash = (37 * hash) + CLUSTER_FIELD_NUMBER;
          hash = (53 * hash) + getCluster().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper)
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        wrapperCase_ = 0;
        wrapper_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper result = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper(this);
        if (wrapperCase_ == 1) {
          if (nodeBuilder_ == null) {
            result.wrapper_ = wrapper_;
          } else {
            result.wrapper_ = nodeBuilder_.build();
          }
        }
        if (wrapperCase_ == 2) {
          if (clusterBuilder_ == null) {
            result.wrapper_ = wrapper_;
          } else {
            result.wrapper_ = clusterBuilder_.build();
          }
        }
        result.wrapperCase_ = wrapperCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.getDefaultInstance()) return this;
        switch (other.getWrapperCase()) {
          case NODE: {
            mergeNode(other.getNode());
            break;
          }
          case CLUSTER: {
            mergeCluster(other.getCluster());
            break;
          }
          case WRAPPER_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int wrapperCase_ = 0;
      private java.lang.Object wrapper_;
      public WrapperCase
          getWrapperCase() {
        return WrapperCase.forNumber(
            wrapperCase_);
      }

      public Builder clearWrapper() {
        wrapperCase_ = 0;
        wrapper_ = null;
        onChanged();
        return this;
      }


      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeOrBuilder> nodeBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       * @return Whether the node field is set.
       */
      @java.lang.Override
      public boolean hasNode() {
        return wrapperCase_ == 1;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       * @return The node.
       */
      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode getNode() {
        if (nodeBuilder_ == null) {
          if (wrapperCase_ == 1) {
            return (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_;
          }
          return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance();
        } else {
          if (wrapperCase_ == 1) {
            return nodeBuilder_.getMessage();
          }
          return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       */
      public Builder setNode(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode value) {
        if (nodeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          wrapper_ = value;
          onChanged();
        } else {
          nodeBuilder_.setMessage(value);
        }
        wrapperCase_ = 1;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       */
      public Builder setNode(
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.Builder builderForValue) {
        if (nodeBuilder_ == null) {
          wrapper_ = builderForValue.build();
          onChanged();
        } else {
          nodeBuilder_.setMessage(builderForValue.build());
        }
        wrapperCase_ = 1;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       */
      public Builder mergeNode(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode value) {
        if (nodeBuilder_ == null) {
          if (wrapperCase_ == 1 &&
              wrapper_ != org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance()) {
            wrapper_ = org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.newBuilder((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_)
                .mergeFrom(value).buildPartial();
          } else {
            wrapper_ = value;
          }
          onChanged();
        } else {
          if (wrapperCase_ == 1) {
            nodeBuilder_.mergeFrom(value);
          }
          nodeBuilder_.setMessage(value);
        }
        wrapperCase_ = 1;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       */
      public Builder clearNode() {
        if (nodeBuilder_ == null) {
          if (wrapperCase_ == 1) {
            wrapperCase_ = 0;
            wrapper_ = null;
            onChanged();
          }
        } else {
          if (wrapperCase_ == 1) {
            wrapperCase_ = 0;
            wrapper_ = null;
          }
          nodeBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.Builder getNodeBuilder() {
        return getNodeFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       */
      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeOrBuilder getNodeOrBuilder() {
        if ((wrapperCase_ == 1) && (nodeBuilder_ != null)) {
          return nodeBuilder_.getMessageOrBuilder();
        } else {
          if (wrapperCase_ == 1) {
            return (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_;
          }
          return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphNode node = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeOrBuilder> 
          getNodeFieldBuilder() {
        if (nodeBuilder_ == null) {
          if (!(wrapperCase_ == 1)) {
            wrapper_ = org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.getDefaultInstance();
          }
          nodeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeOrBuilder>(
                  (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNode) wrapper_,
                  getParentForChildren(),
                  isClean());
          wrapper_ = null;
        }
        wrapperCase_ = 1;
        onChanged();;
        return nodeBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapperOrBuilder> clusterBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       * @return Whether the cluster field is set.
       */
      @java.lang.Override
      public boolean hasCluster() {
        return wrapperCase_ == 2;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       * @return The cluster.
       */
      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper getCluster() {
        if (clusterBuilder_ == null) {
          if (wrapperCase_ == 2) {
            return (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_;
          }
          return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance();
        } else {
          if (wrapperCase_ == 2) {
            return clusterBuilder_.getMessage();
          }
          return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       */
      public Builder setCluster(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper value) {
        if (clusterBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          wrapper_ = value;
          onChanged();
        } else {
          clusterBuilder_.setMessage(value);
        }
        wrapperCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       */
      public Builder setCluster(
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.Builder builderForValue) {
        if (clusterBuilder_ == null) {
          wrapper_ = builderForValue.build();
          onChanged();
        } else {
          clusterBuilder_.setMessage(builderForValue.build());
        }
        wrapperCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       */
      public Builder mergeCluster(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper value) {
        if (clusterBuilder_ == null) {
          if (wrapperCase_ == 2 &&
              wrapper_ != org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance()) {
            wrapper_ = org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.newBuilder((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_)
                .mergeFrom(value).buildPartial();
          } else {
            wrapper_ = value;
          }
          onChanged();
        } else {
          if (wrapperCase_ == 2) {
            clusterBuilder_.mergeFrom(value);
          }
          clusterBuilder_.setMessage(value);
        }
        wrapperCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       */
      public Builder clearCluster() {
        if (clusterBuilder_ == null) {
          if (wrapperCase_ == 2) {
            wrapperCase_ = 0;
            wrapper_ = null;
            onChanged();
          }
        } else {
          if (wrapperCase_ == 2) {
            wrapperCase_ = 0;
            wrapper_ = null;
          }
          clusterBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.Builder getClusterBuilder() {
        return getClusterFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       */
      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapperOrBuilder getClusterOrBuilder() {
        if ((wrapperCase_ == 2) && (clusterBuilder_ != null)) {
          return clusterBuilder_.getMessageOrBuilder();
        } else {
          if (wrapperCase_ == 2) {
            return (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_;
          }
          return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SparkPlanGraphClusterWrapper cluster = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapperOrBuilder> 
          getClusterFieldBuilder() {
        if (clusterBuilder_ == null) {
          if (!(wrapperCase_ == 2)) {
            wrapper_ = org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.getDefaultInstance();
          }
          clusterBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapperOrBuilder>(
                  (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphClusterWrapper) wrapper_,
                  getParentForChildren(),
                  isClean());
          wrapper_ = null;
        }
        wrapperCase_ = 2;
        onChanged();;
        return clusterBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkPlanGraphNodeWrapper>
        PARSER = new com.google.protobuf.AbstractParser<SparkPlanGraphNodeWrapper>() {
      @java.lang.Override
      public SparkPlanGraphNodeWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SparkPlanGraphNodeWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SparkPlanGraphNodeWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkPlanGraphNodeWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SparkPlanGraphEdgeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SparkPlanGraphEdge)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 from_id = 1;</code>
     * @return The fromId.
     */
    long getFromId();

    /**
     * <code>int64 to_id = 2;</code>
     * @return The toId.
     */
    long getToId();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphEdge}
   */
  public static final class SparkPlanGraphEdge extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SparkPlanGraphEdge)
      SparkPlanGraphEdgeOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkPlanGraphEdge.newBuilder() to construct.
    private SparkPlanGraphEdge(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkPlanGraphEdge() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SparkPlanGraphEdge();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SparkPlanGraphEdge(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              fromId_ = input.readInt64();
              break;
            }
            case 16: {

              toId_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder.class);
    }

    public static final int FROM_ID_FIELD_NUMBER = 1;
    private long fromId_;
    /**
     * <code>int64 from_id = 1;</code>
     * @return The fromId.
     */
    @java.lang.Override
    public long getFromId() {
      return fromId_;
    }

    public static final int TO_ID_FIELD_NUMBER = 2;
    private long toId_;
    /**
     * <code>int64 to_id = 2;</code>
     * @return The toId.
     */
    @java.lang.Override
    public long getToId() {
      return toId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (fromId_ != 0L) {
        output.writeInt64(1, fromId_);
      }
      if (toId_ != 0L) {
        output.writeInt64(2, toId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (fromId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, fromId_);
      }
      if (toId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, toId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge other = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge) obj;

      if (getFromId()
          != other.getFromId()) return false;
      if (getToId()
          != other.getToId()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + FROM_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getFromId());
      hash = (37 * hash) + TO_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getToId());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphEdge}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SparkPlanGraphEdge)
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        fromId_ = 0L;

        toId_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge build() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge result = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge(this);
        result.fromId_ = fromId_;
        result.toId_ = toId_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.getDefaultInstance()) return this;
        if (other.getFromId() != 0L) {
          setFromId(other.getFromId());
        }
        if (other.getToId() != 0L) {
          setToId(other.getToId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long fromId_ ;
      /**
       * <code>int64 from_id = 1;</code>
       * @return The fromId.
       */
      @java.lang.Override
      public long getFromId() {
        return fromId_;
      }
      /**
       * <code>int64 from_id = 1;</code>
       * @param value The fromId to set.
       * @return This builder for chaining.
       */
      public Builder setFromId(long value) {
        
        fromId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 from_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearFromId() {
        
        fromId_ = 0L;
        onChanged();
        return this;
      }

      private long toId_ ;
      /**
       * <code>int64 to_id = 2;</code>
       * @return The toId.
       */
      @java.lang.Override
      public long getToId() {
        return toId_;
      }
      /**
       * <code>int64 to_id = 2;</code>
       * @param value The toId to set.
       * @return This builder for chaining.
       */
      public Builder setToId(long value) {
        
        toId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 to_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearToId() {
        
        toId_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SparkPlanGraphEdge)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SparkPlanGraphEdge)
    private static final org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkPlanGraphEdge>
        PARSER = new com.google.protobuf.AbstractParser<SparkPlanGraphEdge>() {
      @java.lang.Override
      public SparkPlanGraphEdge parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SparkPlanGraphEdge(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SparkPlanGraphEdge> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkPlanGraphEdge> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SparkPlanGraphWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SparkPlanGraphWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 execution_id = 1;</code>
     * @return The executionId.
     */
    long getExecutionId();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> 
        getNodesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getNodes(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    int getNodesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> 
        getNodesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder getNodesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge> 
        getEdgesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge getEdges(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    int getEdgesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder> 
        getEdgesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder getEdgesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphWrapper}
   */
  public static final class SparkPlanGraphWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SparkPlanGraphWrapper)
      SparkPlanGraphWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkPlanGraphWrapper.newBuilder() to construct.
    private SparkPlanGraphWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkPlanGraphWrapper() {
      nodes_ = java.util.Collections.emptyList();
      edges_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SparkPlanGraphWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SparkPlanGraphWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              executionId_ = input.readInt64();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                nodes_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper>();
                mutable_bitField0_ |= 0x00000001;
              }
              nodes_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.parser(), extensionRegistry));
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                edges_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge>();
                mutable_bitField0_ |= 0x00000002;
              }
              edges_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          nodes_ = java.util.Collections.unmodifiableList(nodes_);
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          edges_ = java.util.Collections.unmodifiableList(edges_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper.Builder.class);
    }

    public static final int EXECUTION_ID_FIELD_NUMBER = 1;
    private long executionId_;
    /**
     * <code>int64 execution_id = 1;</code>
     * @return The executionId.
     */
    @java.lang.Override
    public long getExecutionId() {
      return executionId_;
    }

    public static final int NODES_FIELD_NUMBER = 2;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> nodes_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> getNodesList() {
      return nodes_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> 
        getNodesOrBuilderList() {
      return nodes_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    @java.lang.Override
    public int getNodesCount() {
      return nodes_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getNodes(int index) {
      return nodes_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder getNodesOrBuilder(
        int index) {
      return nodes_.get(index);
    }

    public static final int EDGES_FIELD_NUMBER = 3;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge> edges_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge> getEdgesList() {
      return edges_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder> 
        getEdgesOrBuilderList() {
      return edges_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    @java.lang.Override
    public int getEdgesCount() {
      return edges_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge getEdges(int index) {
      return edges_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder getEdgesOrBuilder(
        int index) {
      return edges_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (executionId_ != 0L) {
        output.writeInt64(1, executionId_);
      }
      for (int i = 0; i < nodes_.size(); i++) {
        output.writeMessage(2, nodes_.get(i));
      }
      for (int i = 0; i < edges_.size(); i++) {
        output.writeMessage(3, edges_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (executionId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, executionId_);
      }
      for (int i = 0; i < nodes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, nodes_.get(i));
      }
      for (int i = 0; i < edges_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, edges_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper other = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper) obj;

      if (getExecutionId()
          != other.getExecutionId()) return false;
      if (!getNodesList()
          .equals(other.getNodesList())) return false;
      if (!getEdgesList()
          .equals(other.getEdgesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + EXECUTION_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutionId());
      if (getNodesCount() > 0) {
        hash = (37 * hash) + NODES_FIELD_NUMBER;
        hash = (53 * hash) + getNodesList().hashCode();
      }
      if (getEdgesCount() > 0) {
        hash = (37 * hash) + EDGES_FIELD_NUMBER;
        hash = (53 * hash) + getEdgesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SparkPlanGraphWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SparkPlanGraphWrapper)
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper.class, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodesFieldBuilder();
          getEdgesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        executionId_ = 0L;

        if (nodesBuilder_ == null) {
          nodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          nodesBuilder_.clear();
        }
        if (edgesBuilder_ == null) {
          edges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          edgesBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper result = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper(this);
        int from_bitField0_ = bitField0_;
        result.executionId_ = executionId_;
        if (nodesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            nodes_ = java.util.Collections.unmodifiableList(nodes_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.nodes_ = nodes_;
        } else {
          result.nodes_ = nodesBuilder_.build();
        }
        if (edgesBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            edges_ = java.util.Collections.unmodifiableList(edges_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.edges_ = edges_;
        } else {
          result.edges_ = edgesBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper.getDefaultInstance()) return this;
        if (other.getExecutionId() != 0L) {
          setExecutionId(other.getExecutionId());
        }
        if (nodesBuilder_ == null) {
          if (!other.nodes_.isEmpty()) {
            if (nodes_.isEmpty()) {
              nodes_ = other.nodes_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNodesIsMutable();
              nodes_.addAll(other.nodes_);
            }
            onChanged();
          }
        } else {
          if (!other.nodes_.isEmpty()) {
            if (nodesBuilder_.isEmpty()) {
              nodesBuilder_.dispose();
              nodesBuilder_ = null;
              nodes_ = other.nodes_;
              bitField0_ = (bitField0_ & ~0x00000001);
              nodesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNodesFieldBuilder() : null;
            } else {
              nodesBuilder_.addAllMessages(other.nodes_);
            }
          }
        }
        if (edgesBuilder_ == null) {
          if (!other.edges_.isEmpty()) {
            if (edges_.isEmpty()) {
              edges_ = other.edges_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureEdgesIsMutable();
              edges_.addAll(other.edges_);
            }
            onChanged();
          }
        } else {
          if (!other.edges_.isEmpty()) {
            if (edgesBuilder_.isEmpty()) {
              edgesBuilder_.dispose();
              edgesBuilder_ = null;
              edges_ = other.edges_;
              bitField0_ = (bitField0_ & ~0x00000002);
              edgesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getEdgesFieldBuilder() : null;
            } else {
              edgesBuilder_.addAllMessages(other.edges_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long executionId_ ;
      /**
       * <code>int64 execution_id = 1;</code>
       * @return The executionId.
       */
      @java.lang.Override
      public long getExecutionId() {
        return executionId_;
      }
      /**
       * <code>int64 execution_id = 1;</code>
       * @param value The executionId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutionId(long value) {
        
        executionId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 execution_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutionId() {
        
        executionId_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> nodes_ =
        java.util.Collections.emptyList();
      private void ensureNodesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          nodes_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper>(nodes_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> nodesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> getNodesList() {
        if (nodesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodes_);
        } else {
          return nodesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public int getNodesCount() {
        if (nodesBuilder_ == null) {
          return nodes_.size();
        } else {
          return nodesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper getNodes(int index) {
        if (nodesBuilder_ == null) {
          return nodes_.get(index);
        } else {
          return nodesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder setNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper value) {
        if (nodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesIsMutable();
          nodes_.set(index, value);
          onChanged();
        } else {
          nodesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder setNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder builderForValue) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          nodes_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder addNodes(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper value) {
        if (nodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesIsMutable();
          nodes_.add(value);
          onChanged();
        } else {
          nodesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder addNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper value) {
        if (nodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesIsMutable();
          nodes_.add(index, value);
          onChanged();
        } else {
          nodesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder addNodes(
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder builderForValue) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          nodes_.add(builderForValue.build());
          onChanged();
        } else {
          nodesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder addNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder builderForValue) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          nodes_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder addAllNodes(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper> values) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, nodes_);
          onChanged();
        } else {
          nodesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder clearNodes() {
        if (nodesBuilder_ == null) {
          nodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          nodesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public Builder removeNodes(int index) {
        if (nodesBuilder_ == null) {
          ensureNodesIsMutable();
          nodes_.remove(index);
          onChanged();
        } else {
          nodesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder getNodesBuilder(
          int index) {
        return getNodesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder getNodesOrBuilder(
          int index) {
        if (nodesBuilder_ == null) {
          return nodes_.get(index);  } else {
          return nodesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> 
           getNodesOrBuilderList() {
        if (nodesBuilder_ != null) {
          return nodesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodes_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder addNodesBuilder() {
        return getNodesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder addNodesBuilder(
          int index) {
        return getNodesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphNodeWrapper nodes = 2;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder> 
           getNodesBuilderList() {
        return getNodesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder> 
          getNodesFieldBuilder() {
        if (nodesBuilder_ == null) {
          nodesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphNodeWrapperOrBuilder>(
                  nodes_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          nodes_ = null;
        }
        return nodesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge> edges_ =
        java.util.Collections.emptyList();
      private void ensureEdgesIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          edges_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge>(edges_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder> edgesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge> getEdgesList() {
        if (edgesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(edges_);
        } else {
          return edgesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public int getEdgesCount() {
        if (edgesBuilder_ == null) {
          return edges_.size();
        } else {
          return edgesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge getEdges(int index) {
        if (edgesBuilder_ == null) {
          return edges_.get(index);
        } else {
          return edgesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder setEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge value) {
        if (edgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEdgesIsMutable();
          edges_.set(index, value);
          onChanged();
        } else {
          edgesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder setEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder builderForValue) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          edges_.set(index, builderForValue.build());
          onChanged();
        } else {
          edgesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder addEdges(org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge value) {
        if (edgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEdgesIsMutable();
          edges_.add(value);
          onChanged();
        } else {
          edgesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder addEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge value) {
        if (edgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEdgesIsMutable();
          edges_.add(index, value);
          onChanged();
        } else {
          edgesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder addEdges(
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder builderForValue) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          edges_.add(builderForValue.build());
          onChanged();
        } else {
          edgesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder addEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder builderForValue) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          edges_.add(index, builderForValue.build());
          onChanged();
        } else {
          edgesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder addAllEdges(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge> values) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, edges_);
          onChanged();
        } else {
          edgesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder clearEdges() {
        if (edgesBuilder_ == null) {
          edges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          edgesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public Builder removeEdges(int index) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          edges_.remove(index);
          onChanged();
        } else {
          edgesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder getEdgesBuilder(
          int index) {
        return getEdgesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder getEdgesOrBuilder(
          int index) {
        if (edgesBuilder_ == null) {
          return edges_.get(index);  } else {
          return edgesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder> 
           getEdgesOrBuilderList() {
        if (edgesBuilder_ != null) {
          return edgesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(edges_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder addEdgesBuilder() {
        return getEdgesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder addEdgesBuilder(
          int index) {
        return getEdgesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SparkPlanGraphEdge edges = 3;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder> 
           getEdgesBuilderList() {
        return getEdgesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder> 
          getEdgesFieldBuilder() {
        if (edgesBuilder_ == null) {
          edgesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphEdgeOrBuilder>(
                  edges_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          edges_ = null;
        }
        return edgesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SparkPlanGraphWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SparkPlanGraphWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkPlanGraphWrapper>
        PARSER = new com.google.protobuf.AbstractParser<SparkPlanGraphWrapper>() {
      @java.lang.Override
      public SparkPlanGraphWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SparkPlanGraphWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SparkPlanGraphWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkPlanGraphWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SparkPlanGraphWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RDDOperationEdgeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RDDOperationEdge)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int32 from_id = 1;</code>
     * @return The fromId.
     */
    int getFromId();

    /**
     * <code>int32 to_id = 2;</code>
     * @return The toId.
     */
    int getToId();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RDDOperationEdge}
   */
  public static final class RDDOperationEdge extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RDDOperationEdge)
      RDDOperationEdgeOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RDDOperationEdge.newBuilder() to construct.
    private RDDOperationEdge(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RDDOperationEdge() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RDDOperationEdge();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RDDOperationEdge(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              fromId_ = input.readInt32();
              break;
            }
            case 16: {

              toId_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.class, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder.class);
    }

    public static final int FROM_ID_FIELD_NUMBER = 1;
    private int fromId_;
    /**
     * <code>int32 from_id = 1;</code>
     * @return The fromId.
     */
    @java.lang.Override
    public int getFromId() {
      return fromId_;
    }

    public static final int TO_ID_FIELD_NUMBER = 2;
    private int toId_;
    /**
     * <code>int32 to_id = 2;</code>
     * @return The toId.
     */
    @java.lang.Override
    public int getToId() {
      return toId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (fromId_ != 0) {
        output.writeInt32(1, fromId_);
      }
      if (toId_ != 0) {
        output.writeInt32(2, toId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (fromId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, fromId_);
      }
      if (toId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, toId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge other = (org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge) obj;

      if (getFromId()
          != other.getFromId()) return false;
      if (getToId()
          != other.getToId()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + FROM_ID_FIELD_NUMBER;
      hash = (53 * hash) + getFromId();
      hash = (37 * hash) + TO_ID_FIELD_NUMBER;
      hash = (53 * hash) + getToId();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RDDOperationEdge}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RDDOperationEdge)
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.class, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        fromId_ = 0;

        toId_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge build() {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge result = new org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge(this);
        result.fromId_ = fromId_;
        result.toId_ = toId_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.getDefaultInstance()) return this;
        if (other.getFromId() != 0) {
          setFromId(other.getFromId());
        }
        if (other.getToId() != 0) {
          setToId(other.getToId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private int fromId_ ;
      /**
       * <code>int32 from_id = 1;</code>
       * @return The fromId.
       */
      @java.lang.Override
      public int getFromId() {
        return fromId_;
      }
      /**
       * <code>int32 from_id = 1;</code>
       * @param value The fromId to set.
       * @return This builder for chaining.
       */
      public Builder setFromId(int value) {
        
        fromId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 from_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearFromId() {
        
        fromId_ = 0;
        onChanged();
        return this;
      }

      private int toId_ ;
      /**
       * <code>int32 to_id = 2;</code>
       * @return The toId.
       */
      @java.lang.Override
      public int getToId() {
        return toId_;
      }
      /**
       * <code>int32 to_id = 2;</code>
       * @param value The toId to set.
       * @return This builder for chaining.
       */
      public Builder setToId(int value) {
        
        toId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 to_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearToId() {
        
        toId_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RDDOperationEdge)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RDDOperationEdge)
    private static final org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RDDOperationEdge>
        PARSER = new com.google.protobuf.AbstractParser<RDDOperationEdge>() {
      @java.lang.Override
      public RDDOperationEdge parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RDDOperationEdge(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RDDOperationEdge> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RDDOperationEdge> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RDDOperationNodeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RDDOperationNode)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int32 id = 1;</code>
     * @return The id.
     */
    int getId();

    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>bool cached = 3;</code>
     * @return The cached.
     */
    boolean getCached();

    /**
     * <code>bool barrier = 4;</code>
     * @return The barrier.
     */
    boolean getBarrier();

    /**
     * <code>string callsite = 5;</code>
     * @return Whether the callsite field is set.
     */
    boolean hasCallsite();
    /**
     * <code>string callsite = 5;</code>
     * @return The callsite.
     */
    java.lang.String getCallsite();
    /**
     * <code>string callsite = 5;</code>
     * @return The bytes for callsite.
     */
    com.google.protobuf.ByteString
        getCallsiteBytes();

    /**
     * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
     * @return The enum numeric value on the wire for outputDeterministicLevel.
     */
    int getOutputDeterministicLevelValue();
    /**
     * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
     * @return The outputDeterministicLevel.
     */
    org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel getOutputDeterministicLevel();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RDDOperationNode}
   */
  public static final class RDDOperationNode extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RDDOperationNode)
      RDDOperationNodeOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RDDOperationNode.newBuilder() to construct.
    private RDDOperationNode(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RDDOperationNode() {
      name_ = "";
      callsite_ = "";
      outputDeterministicLevel_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RDDOperationNode();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RDDOperationNode(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              id_ = input.readInt32();
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 24: {

              cached_ = input.readBool();
              break;
            }
            case 32: {

              barrier_ = input.readBool();
              break;
            }
            case 42: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              callsite_ = s;
              break;
            }
            case 48: {
              int rawValue = input.readEnum();

              outputDeterministicLevel_ = rawValue;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationNode_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationNode_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.class, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private int id_;
    /**
     * <code>int32 id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public int getId() {
      return id_;
    }

    public static final int NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int CACHED_FIELD_NUMBER = 3;
    private boolean cached_;
    /**
     * <code>bool cached = 3;</code>
     * @return The cached.
     */
    @java.lang.Override
    public boolean getCached() {
      return cached_;
    }

    public static final int BARRIER_FIELD_NUMBER = 4;
    private boolean barrier_;
    /**
     * <code>bool barrier = 4;</code>
     * @return The barrier.
     */
    @java.lang.Override
    public boolean getBarrier() {
      return barrier_;
    }

    public static final int CALLSITE_FIELD_NUMBER = 5;
    private volatile java.lang.Object callsite_;
    /**
     * <code>string callsite = 5;</code>
     * @return Whether the callsite field is set.
     */
    @java.lang.Override
    public boolean hasCallsite() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string callsite = 5;</code>
     * @return The callsite.
     */
    @java.lang.Override
    public java.lang.String getCallsite() {
      java.lang.Object ref = callsite_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        callsite_ = s;
        return s;
      }
    }
    /**
     * <code>string callsite = 5;</code>
     * @return The bytes for callsite.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getCallsiteBytes() {
      java.lang.Object ref = callsite_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        callsite_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int OUTPUT_DETERMINISTIC_LEVEL_FIELD_NUMBER = 6;
    private int outputDeterministicLevel_;
    /**
     * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
     * @return The enum numeric value on the wire for outputDeterministicLevel.
     */
    @java.lang.Override public int getOutputDeterministicLevelValue() {
      return outputDeterministicLevel_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
     * @return The outputDeterministicLevel.
     */
    @java.lang.Override public org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel getOutputDeterministicLevel() {
      @SuppressWarnings("deprecation")
      org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel result = org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel.valueOf(outputDeterministicLevel_);
      return result == null ? org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel.UNRECOGNIZED : result;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (id_ != 0) {
        output.writeInt32(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, name_);
      }
      if (cached_ != false) {
        output.writeBool(3, cached_);
      }
      if (barrier_ != false) {
        output.writeBool(4, barrier_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, callsite_);
      }
      if (outputDeterministicLevel_ != org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel.DETERMINISTIC_LEVEL_UNSPECIFIED.getNumber()) {
        output.writeEnum(6, outputDeterministicLevel_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (id_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, id_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, name_);
      }
      if (cached_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, cached_);
      }
      if (barrier_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(4, barrier_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, callsite_);
      }
      if (outputDeterministicLevel_ != org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel.DETERMINISTIC_LEVEL_UNSPECIFIED.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(6, outputDeterministicLevel_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode other = (org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode) obj;

      if (getId()
          != other.getId()) return false;
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (getCached()
          != other.getCached()) return false;
      if (getBarrier()
          != other.getBarrier()) return false;
      if (hasCallsite() != other.hasCallsite()) return false;
      if (hasCallsite()) {
        if (!getCallsite()
            .equals(other.getCallsite())) return false;
      }
      if (outputDeterministicLevel_ != other.outputDeterministicLevel_) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + getId();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      hash = (37 * hash) + CACHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getCached());
      hash = (37 * hash) + BARRIER_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getBarrier());
      if (hasCallsite()) {
        hash = (37 * hash) + CALLSITE_FIELD_NUMBER;
        hash = (53 * hash) + getCallsite().hashCode();
      }
      hash = (37 * hash) + OUTPUT_DETERMINISTIC_LEVEL_FIELD_NUMBER;
      hash = (53 * hash) + outputDeterministicLevel_;
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RDDOperationNode}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RDDOperationNode)
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationNode_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationNode_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.class, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0;

        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        cached_ = false;

        barrier_ = false;

        callsite_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        outputDeterministicLevel_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationNode_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode build() {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode result = new org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        result.cached_ = cached_;
        result.barrier_ = barrier_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.callsite_ = callsite_;
        result.outputDeterministicLevel_ = outputDeterministicLevel_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.getDefaultInstance()) return this;
        if (other.getId() != 0) {
          setId(other.getId());
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.getCached() != false) {
          setCached(other.getCached());
        }
        if (other.getBarrier() != false) {
          setBarrier(other.getBarrier());
        }
        if (other.hasCallsite()) {
          bitField0_ |= 0x00000002;
          callsite_ = other.callsite_;
          onChanged();
        }
        if (other.outputDeterministicLevel_ != 0) {
          setOutputDeterministicLevelValue(other.getOutputDeterministicLevelValue());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int id_ ;
      /**
       * <code>int32 id = 1;</code>
       * @return The id.
       */
      @java.lang.Override
      public int getId() {
        return id_;
      }
      /**
       * <code>int32 id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(int value) {
        
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        
        id_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 2;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 2;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private boolean cached_ ;
      /**
       * <code>bool cached = 3;</code>
       * @return The cached.
       */
      @java.lang.Override
      public boolean getCached() {
        return cached_;
      }
      /**
       * <code>bool cached = 3;</code>
       * @param value The cached to set.
       * @return This builder for chaining.
       */
      public Builder setCached(boolean value) {
        
        cached_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool cached = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearCached() {
        
        cached_ = false;
        onChanged();
        return this;
      }

      private boolean barrier_ ;
      /**
       * <code>bool barrier = 4;</code>
       * @return The barrier.
       */
      @java.lang.Override
      public boolean getBarrier() {
        return barrier_;
      }
      /**
       * <code>bool barrier = 4;</code>
       * @param value The barrier to set.
       * @return This builder for chaining.
       */
      public Builder setBarrier(boolean value) {
        
        barrier_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool barrier = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearBarrier() {
        
        barrier_ = false;
        onChanged();
        return this;
      }

      private java.lang.Object callsite_ = "";
      /**
       * <code>string callsite = 5;</code>
       * @return Whether the callsite field is set.
       */
      public boolean hasCallsite() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string callsite = 5;</code>
       * @return The callsite.
       */
      public java.lang.String getCallsite() {
        java.lang.Object ref = callsite_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          callsite_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string callsite = 5;</code>
       * @return The bytes for callsite.
       */
      public com.google.protobuf.ByteString
          getCallsiteBytes() {
        java.lang.Object ref = callsite_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          callsite_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string callsite = 5;</code>
       * @param value The callsite to set.
       * @return This builder for chaining.
       */
      public Builder setCallsite(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        callsite_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string callsite = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearCallsite() {
        bitField0_ = (bitField0_ & ~0x00000002);
        callsite_ = getDefaultInstance().getCallsite();
        onChanged();
        return this;
      }
      /**
       * <code>string callsite = 5;</code>
       * @param value The bytes for callsite to set.
       * @return This builder for chaining.
       */
      public Builder setCallsiteBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        callsite_ = value;
        onChanged();
        return this;
      }

      private int outputDeterministicLevel_ = 0;
      /**
       * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
       * @return The enum numeric value on the wire for outputDeterministicLevel.
       */
      @java.lang.Override public int getOutputDeterministicLevelValue() {
        return outputDeterministicLevel_;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
       * @param value The enum numeric value on the wire for outputDeterministicLevel to set.
       * @return This builder for chaining.
       */
      public Builder setOutputDeterministicLevelValue(int value) {
        
        outputDeterministicLevel_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
       * @return The outputDeterministicLevel.
       */
      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel getOutputDeterministicLevel() {
        @SuppressWarnings("deprecation")
        org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel result = org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel.valueOf(outputDeterministicLevel_);
        return result == null ? org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel.UNRECOGNIZED : result;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
       * @param value The outputDeterministicLevel to set.
       * @return This builder for chaining.
       */
      public Builder setOutputDeterministicLevel(org.apache.spark.status.protobuf.StoreTypes.DeterministicLevel value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        outputDeterministicLevel_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.DeterministicLevel output_deterministic_level = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputDeterministicLevel() {
        
        outputDeterministicLevel_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RDDOperationNode)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RDDOperationNode)
    private static final org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RDDOperationNode>
        PARSER = new com.google.protobuf.AbstractParser<RDDOperationNode>() {
      @java.lang.Override
      public RDDOperationNode parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RDDOperationNode(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RDDOperationNode> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RDDOperationNode> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RDDOperationClusterWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RDDOperationClusterWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    boolean hasId();
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    java.lang.String getId();
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    com.google.protobuf.ByteString
        getIdBytes();

    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode> 
        getChildNodesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode getChildNodes(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    int getChildNodesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder> 
        getChildNodesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder getChildNodesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper> 
        getChildClustersList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getChildClusters(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    int getChildClustersCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder> 
        getChildClustersOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder getChildClustersOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RDDOperationClusterWrapper}
   */
  public static final class RDDOperationClusterWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RDDOperationClusterWrapper)
      RDDOperationClusterWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RDDOperationClusterWrapper.newBuilder() to construct.
    private RDDOperationClusterWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RDDOperationClusterWrapper() {
      id_ = "";
      name_ = "";
      childNodes_ = java.util.Collections.emptyList();
      childClusters_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RDDOperationClusterWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RDDOperationClusterWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              id_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              name_ = s;
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                childNodes_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode>();
                mutable_bitField0_ |= 0x00000004;
              }
              childNodes_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.parser(), extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                childClusters_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper>();
                mutable_bitField0_ |= 0x00000008;
              }
              childClusters_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          childNodes_ = java.util.Collections.unmodifiableList(childNodes_);
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          childClusters_ = java.util.Collections.unmodifiableList(childClusters_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.class, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object id_;
    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    @java.lang.Override
    public boolean hasId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public java.lang.String getId() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        id_ = s;
        return s;
      }
    }
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getIdBytes() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        id_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 2;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string name = 2;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 2;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int CHILD_NODES_FIELD_NUMBER = 3;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode> childNodes_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode> getChildNodesList() {
      return childNodes_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder> 
        getChildNodesOrBuilderList() {
      return childNodes_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    @java.lang.Override
    public int getChildNodesCount() {
      return childNodes_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode getChildNodes(int index) {
      return childNodes_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder getChildNodesOrBuilder(
        int index) {
      return childNodes_.get(index);
    }

    public static final int CHILD_CLUSTERS_FIELD_NUMBER = 4;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper> childClusters_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper> getChildClustersList() {
      return childClusters_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder> 
        getChildClustersOrBuilderList() {
      return childClusters_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    @java.lang.Override
    public int getChildClustersCount() {
      return childClusters_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getChildClusters(int index) {
      return childClusters_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder getChildClustersOrBuilder(
        int index) {
      return childClusters_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, name_);
      }
      for (int i = 0; i < childNodes_.size(); i++) {
        output.writeMessage(3, childNodes_.get(i));
      }
      for (int i = 0; i < childClusters_.size(); i++) {
        output.writeMessage(4, childClusters_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, name_);
      }
      for (int i = 0; i < childNodes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, childNodes_.get(i));
      }
      for (int i = 0; i < childClusters_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, childClusters_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper other = (org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper) obj;

      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (!getId()
            .equals(other.getId())) return false;
      }
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (!getChildNodesList()
          .equals(other.getChildNodesList())) return false;
      if (!getChildClustersList()
          .equals(other.getChildClustersList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId().hashCode();
      }
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (getChildNodesCount() > 0) {
        hash = (37 * hash) + CHILD_NODES_FIELD_NUMBER;
        hash = (53 * hash) + getChildNodesList().hashCode();
      }
      if (getChildClustersCount() > 0) {
        hash = (37 * hash) + CHILD_CLUSTERS_FIELD_NUMBER;
        hash = (53 * hash) + getChildClustersList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RDDOperationClusterWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RDDOperationClusterWrapper)
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.class, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getChildNodesFieldBuilder();
          getChildClustersFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (childNodesBuilder_ == null) {
          childNodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          childNodesBuilder_.clear();
        }
        if (childClustersBuilder_ == null) {
          childClusters_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          childClustersBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper result = new org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.name_ = name_;
        if (childNodesBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            childNodes_ = java.util.Collections.unmodifiableList(childNodes_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.childNodes_ = childNodes_;
        } else {
          result.childNodes_ = childNodesBuilder_.build();
        }
        if (childClustersBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0)) {
            childClusters_ = java.util.Collections.unmodifiableList(childClusters_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.childClusters_ = childClusters_;
        } else {
          result.childClusters_ = childClustersBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.getDefaultInstance()) return this;
        if (other.hasId()) {
          bitField0_ |= 0x00000001;
          id_ = other.id_;
          onChanged();
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000002;
          name_ = other.name_;
          onChanged();
        }
        if (childNodesBuilder_ == null) {
          if (!other.childNodes_.isEmpty()) {
            if (childNodes_.isEmpty()) {
              childNodes_ = other.childNodes_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureChildNodesIsMutable();
              childNodes_.addAll(other.childNodes_);
            }
            onChanged();
          }
        } else {
          if (!other.childNodes_.isEmpty()) {
            if (childNodesBuilder_.isEmpty()) {
              childNodesBuilder_.dispose();
              childNodesBuilder_ = null;
              childNodes_ = other.childNodes_;
              bitField0_ = (bitField0_ & ~0x00000004);
              childNodesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getChildNodesFieldBuilder() : null;
            } else {
              childNodesBuilder_.addAllMessages(other.childNodes_);
            }
          }
        }
        if (childClustersBuilder_ == null) {
          if (!other.childClusters_.isEmpty()) {
            if (childClusters_.isEmpty()) {
              childClusters_ = other.childClusters_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureChildClustersIsMutable();
              childClusters_.addAll(other.childClusters_);
            }
            onChanged();
          }
        } else {
          if (!other.childClusters_.isEmpty()) {
            if (childClustersBuilder_.isEmpty()) {
              childClustersBuilder_.dispose();
              childClustersBuilder_ = null;
              childClusters_ = other.childClusters_;
              bitField0_ = (bitField0_ & ~0x00000008);
              childClustersBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getChildClustersFieldBuilder() : null;
            } else {
              childClustersBuilder_.addAllMessages(other.childClusters_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object id_ = "";
      /**
       * <code>string id = 1;</code>
       * @return Whether the id field is set.
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string id = 1;</code>
       * @return The id.
       */
      public java.lang.String getId() {
        java.lang.Object ref = id_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          id_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @return The bytes for id.
       */
      public com.google.protobuf.ByteString
          getIdBytes() {
        java.lang.Object ref = id_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          id_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = getDefaultInstance().getId();
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @param value The bytes for id to set.
       * @return This builder for chaining.
       */
      public Builder setIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 2;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string name = 2;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 2;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 2;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        name_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode> childNodes_ =
        java.util.Collections.emptyList();
      private void ensureChildNodesIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          childNodes_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode>(childNodes_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder> childNodesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode> getChildNodesList() {
        if (childNodesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(childNodes_);
        } else {
          return childNodesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public int getChildNodesCount() {
        if (childNodesBuilder_ == null) {
          return childNodes_.size();
        } else {
          return childNodesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode getChildNodes(int index) {
        if (childNodesBuilder_ == null) {
          return childNodes_.get(index);
        } else {
          return childNodesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder setChildNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode value) {
        if (childNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureChildNodesIsMutable();
          childNodes_.set(index, value);
          onChanged();
        } else {
          childNodesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder setChildNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder builderForValue) {
        if (childNodesBuilder_ == null) {
          ensureChildNodesIsMutable();
          childNodes_.set(index, builderForValue.build());
          onChanged();
        } else {
          childNodesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder addChildNodes(org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode value) {
        if (childNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureChildNodesIsMutable();
          childNodes_.add(value);
          onChanged();
        } else {
          childNodesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder addChildNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode value) {
        if (childNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureChildNodesIsMutable();
          childNodes_.add(index, value);
          onChanged();
        } else {
          childNodesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder addChildNodes(
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder builderForValue) {
        if (childNodesBuilder_ == null) {
          ensureChildNodesIsMutable();
          childNodes_.add(builderForValue.build());
          onChanged();
        } else {
          childNodesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder addChildNodes(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder builderForValue) {
        if (childNodesBuilder_ == null) {
          ensureChildNodesIsMutable();
          childNodes_.add(index, builderForValue.build());
          onChanged();
        } else {
          childNodesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder addAllChildNodes(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode> values) {
        if (childNodesBuilder_ == null) {
          ensureChildNodesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, childNodes_);
          onChanged();
        } else {
          childNodesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder clearChildNodes() {
        if (childNodesBuilder_ == null) {
          childNodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          childNodesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public Builder removeChildNodes(int index) {
        if (childNodesBuilder_ == null) {
          ensureChildNodesIsMutable();
          childNodes_.remove(index);
          onChanged();
        } else {
          childNodesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder getChildNodesBuilder(
          int index) {
        return getChildNodesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder getChildNodesOrBuilder(
          int index) {
        if (childNodesBuilder_ == null) {
          return childNodes_.get(index);  } else {
          return childNodesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder> 
           getChildNodesOrBuilderList() {
        if (childNodesBuilder_ != null) {
          return childNodesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(childNodes_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder addChildNodesBuilder() {
        return getChildNodesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder addChildNodesBuilder(
          int index) {
        return getChildNodesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationNode child_nodes = 3;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder> 
           getChildNodesBuilderList() {
        return getChildNodesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder> 
          getChildNodesFieldBuilder() {
        if (childNodesBuilder_ == null) {
          childNodesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNode.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationNodeOrBuilder>(
                  childNodes_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          childNodes_ = null;
        }
        return childNodesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper> childClusters_ =
        java.util.Collections.emptyList();
      private void ensureChildClustersIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          childClusters_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper>(childClusters_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder> childClustersBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper> getChildClustersList() {
        if (childClustersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(childClusters_);
        } else {
          return childClustersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public int getChildClustersCount() {
        if (childClustersBuilder_ == null) {
          return childClusters_.size();
        } else {
          return childClustersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getChildClusters(int index) {
        if (childClustersBuilder_ == null) {
          return childClusters_.get(index);
        } else {
          return childClustersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder setChildClusters(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper value) {
        if (childClustersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureChildClustersIsMutable();
          childClusters_.set(index, value);
          onChanged();
        } else {
          childClustersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder setChildClusters(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder builderForValue) {
        if (childClustersBuilder_ == null) {
          ensureChildClustersIsMutable();
          childClusters_.set(index, builderForValue.build());
          onChanged();
        } else {
          childClustersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder addChildClusters(org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper value) {
        if (childClustersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureChildClustersIsMutable();
          childClusters_.add(value);
          onChanged();
        } else {
          childClustersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder addChildClusters(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper value) {
        if (childClustersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureChildClustersIsMutable();
          childClusters_.add(index, value);
          onChanged();
        } else {
          childClustersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder addChildClusters(
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder builderForValue) {
        if (childClustersBuilder_ == null) {
          ensureChildClustersIsMutable();
          childClusters_.add(builderForValue.build());
          onChanged();
        } else {
          childClustersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder addChildClusters(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder builderForValue) {
        if (childClustersBuilder_ == null) {
          ensureChildClustersIsMutable();
          childClusters_.add(index, builderForValue.build());
          onChanged();
        } else {
          childClustersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder addAllChildClusters(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper> values) {
        if (childClustersBuilder_ == null) {
          ensureChildClustersIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, childClusters_);
          onChanged();
        } else {
          childClustersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder clearChildClusters() {
        if (childClustersBuilder_ == null) {
          childClusters_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          childClustersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public Builder removeChildClusters(int index) {
        if (childClustersBuilder_ == null) {
          ensureChildClustersIsMutable();
          childClusters_.remove(index);
          onChanged();
        } else {
          childClustersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder getChildClustersBuilder(
          int index) {
        return getChildClustersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder getChildClustersOrBuilder(
          int index) {
        if (childClustersBuilder_ == null) {
          return childClusters_.get(index);  } else {
          return childClustersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder> 
           getChildClustersOrBuilderList() {
        if (childClustersBuilder_ != null) {
          return childClustersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(childClusters_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder addChildClustersBuilder() {
        return getChildClustersFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder addChildClustersBuilder(
          int index) {
        return getChildClustersFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationClusterWrapper child_clusters = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder> 
           getChildClustersBuilderList() {
        return getChildClustersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder> 
          getChildClustersFieldBuilder() {
        if (childClustersBuilder_ == null) {
          childClustersBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder>(
                  childClusters_,
                  ((bitField0_ & 0x00000008) != 0),
                  getParentForChildren(),
                  isClean());
          childClusters_ = null;
        }
        return childClustersBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RDDOperationClusterWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RDDOperationClusterWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RDDOperationClusterWrapper>
        PARSER = new com.google.protobuf.AbstractParser<RDDOperationClusterWrapper>() {
      @java.lang.Override
      public RDDOperationClusterWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RDDOperationClusterWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RDDOperationClusterWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RDDOperationClusterWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RDDOperationGraphWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.RDDOperationGraphWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 stage_id = 1;</code>
     * @return The stageId.
     */
    long getStageId();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> 
        getEdgesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getEdges(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    int getEdgesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
        getEdgesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getEdgesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> 
        getOutgoingEdgesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getOutgoingEdges(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    int getOutgoingEdgesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
        getOutgoingEdgesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getOutgoingEdgesOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> 
        getIncomingEdgesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getIncomingEdges(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    int getIncomingEdgesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
        getIncomingEdgesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getIncomingEdgesOrBuilder(
        int index);

    /**
     * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
     * @return Whether the rootCluster field is set.
     */
    boolean hasRootCluster();
    /**
     * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
     * @return The rootCluster.
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getRootCluster();
    /**
     * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder getRootClusterOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.RDDOperationGraphWrapper}
   */
  public static final class RDDOperationGraphWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.RDDOperationGraphWrapper)
      RDDOperationGraphWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RDDOperationGraphWrapper.newBuilder() to construct.
    private RDDOperationGraphWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RDDOperationGraphWrapper() {
      edges_ = java.util.Collections.emptyList();
      outgoingEdges_ = java.util.Collections.emptyList();
      incomingEdges_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RDDOperationGraphWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RDDOperationGraphWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              stageId_ = input.readInt64();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                edges_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge>();
                mutable_bitField0_ |= 0x00000001;
              }
              edges_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.parser(), extensionRegistry));
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                outgoingEdges_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge>();
                mutable_bitField0_ |= 0x00000002;
              }
              outgoingEdges_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.parser(), extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                incomingEdges_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge>();
                mutable_bitField0_ |= 0x00000004;
              }
              incomingEdges_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.parser(), extensionRegistry));
              break;
            }
            case 42: {
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder subBuilder = null;
              if (rootCluster_ != null) {
                subBuilder = rootCluster_.toBuilder();
              }
              rootCluster_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(rootCluster_);
                rootCluster_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          edges_ = java.util.Collections.unmodifiableList(edges_);
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          outgoingEdges_ = java.util.Collections.unmodifiableList(outgoingEdges_);
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          incomingEdges_ = java.util.Collections.unmodifiableList(incomingEdges_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper.class, org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper.Builder.class);
    }

    public static final int STAGE_ID_FIELD_NUMBER = 1;
    private long stageId_;
    /**
     * <code>int64 stage_id = 1;</code>
     * @return The stageId.
     */
    @java.lang.Override
    public long getStageId() {
      return stageId_;
    }

    public static final int EDGES_FIELD_NUMBER = 2;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> edges_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> getEdgesList() {
      return edges_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
        getEdgesOrBuilderList() {
      return edges_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    @java.lang.Override
    public int getEdgesCount() {
      return edges_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getEdges(int index) {
      return edges_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getEdgesOrBuilder(
        int index) {
      return edges_.get(index);
    }

    public static final int OUTGOING_EDGES_FIELD_NUMBER = 3;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> outgoingEdges_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> getOutgoingEdgesList() {
      return outgoingEdges_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
        getOutgoingEdgesOrBuilderList() {
      return outgoingEdges_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    @java.lang.Override
    public int getOutgoingEdgesCount() {
      return outgoingEdges_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getOutgoingEdges(int index) {
      return outgoingEdges_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getOutgoingEdgesOrBuilder(
        int index) {
      return outgoingEdges_.get(index);
    }

    public static final int INCOMING_EDGES_FIELD_NUMBER = 4;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> incomingEdges_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> getIncomingEdgesList() {
      return incomingEdges_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
        getIncomingEdgesOrBuilderList() {
      return incomingEdges_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    @java.lang.Override
    public int getIncomingEdgesCount() {
      return incomingEdges_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getIncomingEdges(int index) {
      return incomingEdges_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getIncomingEdgesOrBuilder(
        int index) {
      return incomingEdges_.get(index);
    }

    public static final int ROOT_CLUSTER_FIELD_NUMBER = 5;
    private org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper rootCluster_;
    /**
     * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
     * @return Whether the rootCluster field is set.
     */
    @java.lang.Override
    public boolean hasRootCluster() {
      return rootCluster_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
     * @return The rootCluster.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getRootCluster() {
      return rootCluster_ == null ? org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.getDefaultInstance() : rootCluster_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder getRootClusterOrBuilder() {
      return getRootCluster();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (stageId_ != 0L) {
        output.writeInt64(1, stageId_);
      }
      for (int i = 0; i < edges_.size(); i++) {
        output.writeMessage(2, edges_.get(i));
      }
      for (int i = 0; i < outgoingEdges_.size(); i++) {
        output.writeMessage(3, outgoingEdges_.get(i));
      }
      for (int i = 0; i < incomingEdges_.size(); i++) {
        output.writeMessage(4, incomingEdges_.get(i));
      }
      if (rootCluster_ != null) {
        output.writeMessage(5, getRootCluster());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (stageId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, stageId_);
      }
      for (int i = 0; i < edges_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, edges_.get(i));
      }
      for (int i = 0; i < outgoingEdges_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, outgoingEdges_.get(i));
      }
      for (int i = 0; i < incomingEdges_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, incomingEdges_.get(i));
      }
      if (rootCluster_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getRootCluster());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper other = (org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper) obj;

      if (getStageId()
          != other.getStageId()) return false;
      if (!getEdgesList()
          .equals(other.getEdgesList())) return false;
      if (!getOutgoingEdgesList()
          .equals(other.getOutgoingEdgesList())) return false;
      if (!getIncomingEdgesList()
          .equals(other.getIncomingEdgesList())) return false;
      if (hasRootCluster() != other.hasRootCluster()) return false;
      if (hasRootCluster()) {
        if (!getRootCluster()
            .equals(other.getRootCluster())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STAGE_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getStageId());
      if (getEdgesCount() > 0) {
        hash = (37 * hash) + EDGES_FIELD_NUMBER;
        hash = (53 * hash) + getEdgesList().hashCode();
      }
      if (getOutgoingEdgesCount() > 0) {
        hash = (37 * hash) + OUTGOING_EDGES_FIELD_NUMBER;
        hash = (53 * hash) + getOutgoingEdgesList().hashCode();
      }
      if (getIncomingEdgesCount() > 0) {
        hash = (37 * hash) + INCOMING_EDGES_FIELD_NUMBER;
        hash = (53 * hash) + getIncomingEdgesList().hashCode();
      }
      if (hasRootCluster()) {
        hash = (37 * hash) + ROOT_CLUSTER_FIELD_NUMBER;
        hash = (53 * hash) + getRootCluster().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.RDDOperationGraphWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.RDDOperationGraphWrapper)
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper.class, org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getEdgesFieldBuilder();
          getOutgoingEdgesFieldBuilder();
          getIncomingEdgesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        stageId_ = 0L;

        if (edgesBuilder_ == null) {
          edges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          edgesBuilder_.clear();
        }
        if (outgoingEdgesBuilder_ == null) {
          outgoingEdges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          outgoingEdgesBuilder_.clear();
        }
        if (incomingEdgesBuilder_ == null) {
          incomingEdges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          incomingEdgesBuilder_.clear();
        }
        if (rootClusterBuilder_ == null) {
          rootCluster_ = null;
        } else {
          rootCluster_ = null;
          rootClusterBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper result = new org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper(this);
        int from_bitField0_ = bitField0_;
        result.stageId_ = stageId_;
        if (edgesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            edges_ = java.util.Collections.unmodifiableList(edges_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.edges_ = edges_;
        } else {
          result.edges_ = edgesBuilder_.build();
        }
        if (outgoingEdgesBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            outgoingEdges_ = java.util.Collections.unmodifiableList(outgoingEdges_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.outgoingEdges_ = outgoingEdges_;
        } else {
          result.outgoingEdges_ = outgoingEdgesBuilder_.build();
        }
        if (incomingEdgesBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            incomingEdges_ = java.util.Collections.unmodifiableList(incomingEdges_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.incomingEdges_ = incomingEdges_;
        } else {
          result.incomingEdges_ = incomingEdgesBuilder_.build();
        }
        if (rootClusterBuilder_ == null) {
          result.rootCluster_ = rootCluster_;
        } else {
          result.rootCluster_ = rootClusterBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper.getDefaultInstance()) return this;
        if (other.getStageId() != 0L) {
          setStageId(other.getStageId());
        }
        if (edgesBuilder_ == null) {
          if (!other.edges_.isEmpty()) {
            if (edges_.isEmpty()) {
              edges_ = other.edges_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureEdgesIsMutable();
              edges_.addAll(other.edges_);
            }
            onChanged();
          }
        } else {
          if (!other.edges_.isEmpty()) {
            if (edgesBuilder_.isEmpty()) {
              edgesBuilder_.dispose();
              edgesBuilder_ = null;
              edges_ = other.edges_;
              bitField0_ = (bitField0_ & ~0x00000001);
              edgesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getEdgesFieldBuilder() : null;
            } else {
              edgesBuilder_.addAllMessages(other.edges_);
            }
          }
        }
        if (outgoingEdgesBuilder_ == null) {
          if (!other.outgoingEdges_.isEmpty()) {
            if (outgoingEdges_.isEmpty()) {
              outgoingEdges_ = other.outgoingEdges_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureOutgoingEdgesIsMutable();
              outgoingEdges_.addAll(other.outgoingEdges_);
            }
            onChanged();
          }
        } else {
          if (!other.outgoingEdges_.isEmpty()) {
            if (outgoingEdgesBuilder_.isEmpty()) {
              outgoingEdgesBuilder_.dispose();
              outgoingEdgesBuilder_ = null;
              outgoingEdges_ = other.outgoingEdges_;
              bitField0_ = (bitField0_ & ~0x00000002);
              outgoingEdgesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getOutgoingEdgesFieldBuilder() : null;
            } else {
              outgoingEdgesBuilder_.addAllMessages(other.outgoingEdges_);
            }
          }
        }
        if (incomingEdgesBuilder_ == null) {
          if (!other.incomingEdges_.isEmpty()) {
            if (incomingEdges_.isEmpty()) {
              incomingEdges_ = other.incomingEdges_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureIncomingEdgesIsMutable();
              incomingEdges_.addAll(other.incomingEdges_);
            }
            onChanged();
          }
        } else {
          if (!other.incomingEdges_.isEmpty()) {
            if (incomingEdgesBuilder_.isEmpty()) {
              incomingEdgesBuilder_.dispose();
              incomingEdgesBuilder_ = null;
              incomingEdges_ = other.incomingEdges_;
              bitField0_ = (bitField0_ & ~0x00000004);
              incomingEdgesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getIncomingEdgesFieldBuilder() : null;
            } else {
              incomingEdgesBuilder_.addAllMessages(other.incomingEdges_);
            }
          }
        }
        if (other.hasRootCluster()) {
          mergeRootCluster(other.getRootCluster());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long stageId_ ;
      /**
       * <code>int64 stage_id = 1;</code>
       * @return The stageId.
       */
      @java.lang.Override
      public long getStageId() {
        return stageId_;
      }
      /**
       * <code>int64 stage_id = 1;</code>
       * @param value The stageId to set.
       * @return This builder for chaining.
       */
      public Builder setStageId(long value) {
        
        stageId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 stage_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageId() {
        
        stageId_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> edges_ =
        java.util.Collections.emptyList();
      private void ensureEdgesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          edges_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge>(edges_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> edgesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> getEdgesList() {
        if (edgesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(edges_);
        } else {
          return edgesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public int getEdgesCount() {
        if (edgesBuilder_ == null) {
          return edges_.size();
        } else {
          return edgesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getEdges(int index) {
        if (edgesBuilder_ == null) {
          return edges_.get(index);
        } else {
          return edgesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder setEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (edgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEdgesIsMutable();
          edges_.set(index, value);
          onChanged();
        } else {
          edgesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder setEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          edges_.set(index, builderForValue.build());
          onChanged();
        } else {
          edgesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder addEdges(org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (edgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEdgesIsMutable();
          edges_.add(value);
          onChanged();
        } else {
          edgesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder addEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (edgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEdgesIsMutable();
          edges_.add(index, value);
          onChanged();
        } else {
          edgesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder addEdges(
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          edges_.add(builderForValue.build());
          onChanged();
        } else {
          edgesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder addEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          edges_.add(index, builderForValue.build());
          onChanged();
        } else {
          edgesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder addAllEdges(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> values) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, edges_);
          onChanged();
        } else {
          edgesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder clearEdges() {
        if (edgesBuilder_ == null) {
          edges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          edgesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public Builder removeEdges(int index) {
        if (edgesBuilder_ == null) {
          ensureEdgesIsMutable();
          edges_.remove(index);
          onChanged();
        } else {
          edgesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder getEdgesBuilder(
          int index) {
        return getEdgesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getEdgesOrBuilder(
          int index) {
        if (edgesBuilder_ == null) {
          return edges_.get(index);  } else {
          return edgesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
           getEdgesOrBuilderList() {
        if (edgesBuilder_ != null) {
          return edgesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(edges_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder addEdgesBuilder() {
        return getEdgesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder addEdgesBuilder(
          int index) {
        return getEdgesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge edges = 2;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder> 
           getEdgesBuilderList() {
        return getEdgesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
          getEdgesFieldBuilder() {
        if (edgesBuilder_ == null) {
          edgesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder>(
                  edges_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          edges_ = null;
        }
        return edgesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> outgoingEdges_ =
        java.util.Collections.emptyList();
      private void ensureOutgoingEdgesIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          outgoingEdges_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge>(outgoingEdges_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> outgoingEdgesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> getOutgoingEdgesList() {
        if (outgoingEdgesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(outgoingEdges_);
        } else {
          return outgoingEdgesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public int getOutgoingEdgesCount() {
        if (outgoingEdgesBuilder_ == null) {
          return outgoingEdges_.size();
        } else {
          return outgoingEdgesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getOutgoingEdges(int index) {
        if (outgoingEdgesBuilder_ == null) {
          return outgoingEdges_.get(index);
        } else {
          return outgoingEdgesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder setOutgoingEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (outgoingEdgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutgoingEdgesIsMutable();
          outgoingEdges_.set(index, value);
          onChanged();
        } else {
          outgoingEdgesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder setOutgoingEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (outgoingEdgesBuilder_ == null) {
          ensureOutgoingEdgesIsMutable();
          outgoingEdges_.set(index, builderForValue.build());
          onChanged();
        } else {
          outgoingEdgesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder addOutgoingEdges(org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (outgoingEdgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutgoingEdgesIsMutable();
          outgoingEdges_.add(value);
          onChanged();
        } else {
          outgoingEdgesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder addOutgoingEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (outgoingEdgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutgoingEdgesIsMutable();
          outgoingEdges_.add(index, value);
          onChanged();
        } else {
          outgoingEdgesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder addOutgoingEdges(
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (outgoingEdgesBuilder_ == null) {
          ensureOutgoingEdgesIsMutable();
          outgoingEdges_.add(builderForValue.build());
          onChanged();
        } else {
          outgoingEdgesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder addOutgoingEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (outgoingEdgesBuilder_ == null) {
          ensureOutgoingEdgesIsMutable();
          outgoingEdges_.add(index, builderForValue.build());
          onChanged();
        } else {
          outgoingEdgesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder addAllOutgoingEdges(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> values) {
        if (outgoingEdgesBuilder_ == null) {
          ensureOutgoingEdgesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, outgoingEdges_);
          onChanged();
        } else {
          outgoingEdgesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder clearOutgoingEdges() {
        if (outgoingEdgesBuilder_ == null) {
          outgoingEdges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          outgoingEdgesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public Builder removeOutgoingEdges(int index) {
        if (outgoingEdgesBuilder_ == null) {
          ensureOutgoingEdgesIsMutable();
          outgoingEdges_.remove(index);
          onChanged();
        } else {
          outgoingEdgesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder getOutgoingEdgesBuilder(
          int index) {
        return getOutgoingEdgesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getOutgoingEdgesOrBuilder(
          int index) {
        if (outgoingEdgesBuilder_ == null) {
          return outgoingEdges_.get(index);  } else {
          return outgoingEdgesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
           getOutgoingEdgesOrBuilderList() {
        if (outgoingEdgesBuilder_ != null) {
          return outgoingEdgesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(outgoingEdges_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder addOutgoingEdgesBuilder() {
        return getOutgoingEdgesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder addOutgoingEdgesBuilder(
          int index) {
        return getOutgoingEdgesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge outgoing_edges = 3;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder> 
           getOutgoingEdgesBuilderList() {
        return getOutgoingEdgesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
          getOutgoingEdgesFieldBuilder() {
        if (outgoingEdgesBuilder_ == null) {
          outgoingEdgesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder>(
                  outgoingEdges_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          outgoingEdges_ = null;
        }
        return outgoingEdgesBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> incomingEdges_ =
        java.util.Collections.emptyList();
      private void ensureIncomingEdgesIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          incomingEdges_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge>(incomingEdges_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> incomingEdgesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> getIncomingEdgesList() {
        if (incomingEdgesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(incomingEdges_);
        } else {
          return incomingEdgesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public int getIncomingEdgesCount() {
        if (incomingEdgesBuilder_ == null) {
          return incomingEdges_.size();
        } else {
          return incomingEdgesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge getIncomingEdges(int index) {
        if (incomingEdgesBuilder_ == null) {
          return incomingEdges_.get(index);
        } else {
          return incomingEdgesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder setIncomingEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (incomingEdgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncomingEdgesIsMutable();
          incomingEdges_.set(index, value);
          onChanged();
        } else {
          incomingEdgesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder setIncomingEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (incomingEdgesBuilder_ == null) {
          ensureIncomingEdgesIsMutable();
          incomingEdges_.set(index, builderForValue.build());
          onChanged();
        } else {
          incomingEdgesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder addIncomingEdges(org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (incomingEdgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncomingEdgesIsMutable();
          incomingEdges_.add(value);
          onChanged();
        } else {
          incomingEdgesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder addIncomingEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge value) {
        if (incomingEdgesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncomingEdgesIsMutable();
          incomingEdges_.add(index, value);
          onChanged();
        } else {
          incomingEdgesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder addIncomingEdges(
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (incomingEdgesBuilder_ == null) {
          ensureIncomingEdgesIsMutable();
          incomingEdges_.add(builderForValue.build());
          onChanged();
        } else {
          incomingEdgesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder addIncomingEdges(
          int index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder builderForValue) {
        if (incomingEdgesBuilder_ == null) {
          ensureIncomingEdgesIsMutable();
          incomingEdges_.add(index, builderForValue.build());
          onChanged();
        } else {
          incomingEdgesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder addAllIncomingEdges(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge> values) {
        if (incomingEdgesBuilder_ == null) {
          ensureIncomingEdgesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, incomingEdges_);
          onChanged();
        } else {
          incomingEdgesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder clearIncomingEdges() {
        if (incomingEdgesBuilder_ == null) {
          incomingEdges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          incomingEdgesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public Builder removeIncomingEdges(int index) {
        if (incomingEdgesBuilder_ == null) {
          ensureIncomingEdgesIsMutable();
          incomingEdges_.remove(index);
          onChanged();
        } else {
          incomingEdgesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder getIncomingEdgesBuilder(
          int index) {
        return getIncomingEdgesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder getIncomingEdgesOrBuilder(
          int index) {
        if (incomingEdgesBuilder_ == null) {
          return incomingEdges_.get(index);  } else {
          return incomingEdgesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
           getIncomingEdgesOrBuilderList() {
        if (incomingEdgesBuilder_ != null) {
          return incomingEdgesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(incomingEdges_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder addIncomingEdgesBuilder() {
        return getIncomingEdgesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder addIncomingEdgesBuilder(
          int index) {
        return getIncomingEdgesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.RDDOperationEdge incoming_edges = 4;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder> 
           getIncomingEdgesBuilderList() {
        return getIncomingEdgesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder> 
          getIncomingEdgesFieldBuilder() {
        if (incomingEdgesBuilder_ == null) {
          incomingEdgesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdge.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationEdgeOrBuilder>(
                  incomingEdges_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          incomingEdges_ = null;
        }
        return incomingEdgesBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper rootCluster_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder> rootClusterBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       * @return Whether the rootCluster field is set.
       */
      public boolean hasRootCluster() {
        return rootClusterBuilder_ != null || rootCluster_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       * @return The rootCluster.
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper getRootCluster() {
        if (rootClusterBuilder_ == null) {
          return rootCluster_ == null ? org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.getDefaultInstance() : rootCluster_;
        } else {
          return rootClusterBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       */
      public Builder setRootCluster(org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper value) {
        if (rootClusterBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          rootCluster_ = value;
          onChanged();
        } else {
          rootClusterBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       */
      public Builder setRootCluster(
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder builderForValue) {
        if (rootClusterBuilder_ == null) {
          rootCluster_ = builderForValue.build();
          onChanged();
        } else {
          rootClusterBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       */
      public Builder mergeRootCluster(org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper value) {
        if (rootClusterBuilder_ == null) {
          if (rootCluster_ != null) {
            rootCluster_ =
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.newBuilder(rootCluster_).mergeFrom(value).buildPartial();
          } else {
            rootCluster_ = value;
          }
          onChanged();
        } else {
          rootClusterBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       */
      public Builder clearRootCluster() {
        if (rootClusterBuilder_ == null) {
          rootCluster_ = null;
          onChanged();
        } else {
          rootCluster_ = null;
          rootClusterBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder getRootClusterBuilder() {
        
        onChanged();
        return getRootClusterFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder getRootClusterOrBuilder() {
        if (rootClusterBuilder_ != null) {
          return rootClusterBuilder_.getMessageOrBuilder();
        } else {
          return rootCluster_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.getDefaultInstance() : rootCluster_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.RDDOperationClusterWrapper root_cluster = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder> 
          getRootClusterFieldBuilder() {
        if (rootClusterBuilder_ == null) {
          rootClusterBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapper.Builder, org.apache.spark.status.protobuf.StoreTypes.RDDOperationClusterWrapperOrBuilder>(
                  getRootCluster(),
                  getParentForChildren(),
                  isClean());
          rootCluster_ = null;
        }
        return rootClusterBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.RDDOperationGraphWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.RDDOperationGraphWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<RDDOperationGraphWrapper>
        PARSER = new com.google.protobuf.AbstractParser<RDDOperationGraphWrapper>() {
      @java.lang.Override
      public RDDOperationGraphWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RDDOperationGraphWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RDDOperationGraphWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RDDOperationGraphWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.RDDOperationGraphWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StreamingQueryDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.StreamingQueryData)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>string id = 2;</code>
     * @return Whether the id field is set.
     */
    boolean hasId();
    /**
     * <code>string id = 2;</code>
     * @return The id.
     */
    java.lang.String getId();
    /**
     * <code>string id = 2;</code>
     * @return The bytes for id.
     */
    com.google.protobuf.ByteString
        getIdBytes();

    /**
     * <code>string run_id = 3;</code>
     * @return Whether the runId field is set.
     */
    boolean hasRunId();
    /**
     * <code>string run_id = 3;</code>
     * @return The runId.
     */
    java.lang.String getRunId();
    /**
     * <code>string run_id = 3;</code>
     * @return The bytes for runId.
     */
    com.google.protobuf.ByteString
        getRunIdBytes();

    /**
     * <code>bool is_active = 4;</code>
     * @return The isActive.
     */
    boolean getIsActive();

    /**
     * <code>string exception = 5;</code>
     * @return Whether the exception field is set.
     */
    boolean hasException();
    /**
     * <code>string exception = 5;</code>
     * @return The exception.
     */
    java.lang.String getException();
    /**
     * <code>string exception = 5;</code>
     * @return The bytes for exception.
     */
    com.google.protobuf.ByteString
        getExceptionBytes();

    /**
     * <code>int64 start_timestamp = 6;</code>
     * @return The startTimestamp.
     */
    long getStartTimestamp();

    /**
     * <code>int64 end_timestamp = 7;</code>
     * @return Whether the endTimestamp field is set.
     */
    boolean hasEndTimestamp();
    /**
     * <code>int64 end_timestamp = 7;</code>
     * @return The endTimestamp.
     */
    long getEndTimestamp();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.StreamingQueryData}
   */
  public static final class StreamingQueryData extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.StreamingQueryData)
      StreamingQueryDataOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StreamingQueryData.newBuilder() to construct.
    private StreamingQueryData(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StreamingQueryData() {
      name_ = "";
      id_ = "";
      runId_ = "";
      exception_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StreamingQueryData();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StreamingQueryData(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              id_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              runId_ = s;
              break;
            }
            case 32: {

              isActive_ = input.readBool();
              break;
            }
            case 42: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000008;
              exception_ = s;
              break;
            }
            case 48: {

              startTimestamp_ = input.readInt64();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000010;
              endTimestamp_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryData_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData.class, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData.Builder.class);
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int ID_FIELD_NUMBER = 2;
    private volatile java.lang.Object id_;
    /**
     * <code>string id = 2;</code>
     * @return Whether the id field is set.
     */
    @java.lang.Override
    public boolean hasId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string id = 2;</code>
     * @return The id.
     */
    @java.lang.Override
    public java.lang.String getId() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        id_ = s;
        return s;
      }
    }
    /**
     * <code>string id = 2;</code>
     * @return The bytes for id.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getIdBytes() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        id_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int RUN_ID_FIELD_NUMBER = 3;
    private volatile java.lang.Object runId_;
    /**
     * <code>string run_id = 3;</code>
     * @return Whether the runId field is set.
     */
    @java.lang.Override
    public boolean hasRunId() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string run_id = 3;</code>
     * @return The runId.
     */
    @java.lang.Override
    public java.lang.String getRunId() {
      java.lang.Object ref = runId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        runId_ = s;
        return s;
      }
    }
    /**
     * <code>string run_id = 3;</code>
     * @return The bytes for runId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getRunIdBytes() {
      java.lang.Object ref = runId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        runId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int IS_ACTIVE_FIELD_NUMBER = 4;
    private boolean isActive_;
    /**
     * <code>bool is_active = 4;</code>
     * @return The isActive.
     */
    @java.lang.Override
    public boolean getIsActive() {
      return isActive_;
    }

    public static final int EXCEPTION_FIELD_NUMBER = 5;
    private volatile java.lang.Object exception_;
    /**
     * <code>string exception = 5;</code>
     * @return Whether the exception field is set.
     */
    @java.lang.Override
    public boolean hasException() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>string exception = 5;</code>
     * @return The exception.
     */
    @java.lang.Override
    public java.lang.String getException() {
      java.lang.Object ref = exception_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        exception_ = s;
        return s;
      }
    }
    /**
     * <code>string exception = 5;</code>
     * @return The bytes for exception.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getExceptionBytes() {
      java.lang.Object ref = exception_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        exception_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int START_TIMESTAMP_FIELD_NUMBER = 6;
    private long startTimestamp_;
    /**
     * <code>int64 start_timestamp = 6;</code>
     * @return The startTimestamp.
     */
    @java.lang.Override
    public long getStartTimestamp() {
      return startTimestamp_;
    }

    public static final int END_TIMESTAMP_FIELD_NUMBER = 7;
    private long endTimestamp_;
    /**
     * <code>int64 end_timestamp = 7;</code>
     * @return Whether the endTimestamp field is set.
     */
    @java.lang.Override
    public boolean hasEndTimestamp() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>int64 end_timestamp = 7;</code>
     * @return The endTimestamp.
     */
    @java.lang.Override
    public long getEndTimestamp() {
      return endTimestamp_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, id_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, runId_);
      }
      if (isActive_ != false) {
        output.writeBool(4, isActive_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, exception_);
      }
      if (startTimestamp_ != 0L) {
        output.writeInt64(6, startTimestamp_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt64(7, endTimestamp_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, id_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, runId_);
      }
      if (isActive_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(4, isActive_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, exception_);
      }
      if (startTimestamp_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, startTimestamp_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, endTimestamp_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData other = (org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData) obj;

      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (!getId()
            .equals(other.getId())) return false;
      }
      if (hasRunId() != other.hasRunId()) return false;
      if (hasRunId()) {
        if (!getRunId()
            .equals(other.getRunId())) return false;
      }
      if (getIsActive()
          != other.getIsActive()) return false;
      if (hasException() != other.hasException()) return false;
      if (hasException()) {
        if (!getException()
            .equals(other.getException())) return false;
      }
      if (getStartTimestamp()
          != other.getStartTimestamp()) return false;
      if (hasEndTimestamp() != other.hasEndTimestamp()) return false;
      if (hasEndTimestamp()) {
        if (getEndTimestamp()
            != other.getEndTimestamp()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId().hashCode();
      }
      if (hasRunId()) {
        hash = (37 * hash) + RUN_ID_FIELD_NUMBER;
        hash = (53 * hash) + getRunId().hashCode();
      }
      hash = (37 * hash) + IS_ACTIVE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsActive());
      if (hasException()) {
        hash = (37 * hash) + EXCEPTION_FIELD_NUMBER;
        hash = (53 * hash) + getException().hashCode();
      }
      hash = (37 * hash) + START_TIMESTAMP_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getStartTimestamp());
      if (hasEndTimestamp()) {
        hash = (37 * hash) + END_TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getEndTimestamp());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.StreamingQueryData}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.StreamingQueryData)
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryData_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData.class, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        runId_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        isActive_ = false;

        exception_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        startTimestamp_ = 0L;

        endTimestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryData_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData build() {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData result = new org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.runId_ = runId_;
        result.isActive_ = isActive_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.exception_ = exception_;
        result.startTimestamp_ = startTimestamp_;
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.endTimestamp_ = endTimestamp_;
          to_bitField0_ |= 0x00000010;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData.getDefaultInstance()) return this;
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasId()) {
          bitField0_ |= 0x00000002;
          id_ = other.id_;
          onChanged();
        }
        if (other.hasRunId()) {
          bitField0_ |= 0x00000004;
          runId_ = other.runId_;
          onChanged();
        }
        if (other.getIsActive() != false) {
          setIsActive(other.getIsActive());
        }
        if (other.hasException()) {
          bitField0_ |= 0x00000008;
          exception_ = other.exception_;
          onChanged();
        }
        if (other.getStartTimestamp() != 0L) {
          setStartTimestamp(other.getStartTimestamp());
        }
        if (other.hasEndTimestamp()) {
          setEndTimestamp(other.getEndTimestamp());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object id_ = "";
      /**
       * <code>string id = 2;</code>
       * @return Whether the id field is set.
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string id = 2;</code>
       * @return The id.
       */
      public java.lang.String getId() {
        java.lang.Object ref = id_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          id_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string id = 2;</code>
       * @return The bytes for id.
       */
      public com.google.protobuf.ByteString
          getIdBytes() {
        java.lang.Object ref = id_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          id_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string id = 2;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        id_ = getDefaultInstance().getId();
        onChanged();
        return this;
      }
      /**
       * <code>string id = 2;</code>
       * @param value The bytes for id to set.
       * @return This builder for chaining.
       */
      public Builder setIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        id_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object runId_ = "";
      /**
       * <code>string run_id = 3;</code>
       * @return Whether the runId field is set.
       */
      public boolean hasRunId() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string run_id = 3;</code>
       * @return The runId.
       */
      public java.lang.String getRunId() {
        java.lang.Object ref = runId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          runId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string run_id = 3;</code>
       * @return The bytes for runId.
       */
      public com.google.protobuf.ByteString
          getRunIdBytes() {
        java.lang.Object ref = runId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          runId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string run_id = 3;</code>
       * @param value The runId to set.
       * @return This builder for chaining.
       */
      public Builder setRunId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        runId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string run_id = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearRunId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        runId_ = getDefaultInstance().getRunId();
        onChanged();
        return this;
      }
      /**
       * <code>string run_id = 3;</code>
       * @param value The bytes for runId to set.
       * @return This builder for chaining.
       */
      public Builder setRunIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        runId_ = value;
        onChanged();
        return this;
      }

      private boolean isActive_ ;
      /**
       * <code>bool is_active = 4;</code>
       * @return The isActive.
       */
      @java.lang.Override
      public boolean getIsActive() {
        return isActive_;
      }
      /**
       * <code>bool is_active = 4;</code>
       * @param value The isActive to set.
       * @return This builder for chaining.
       */
      public Builder setIsActive(boolean value) {
        
        isActive_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool is_active = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsActive() {
        
        isActive_ = false;
        onChanged();
        return this;
      }

      private java.lang.Object exception_ = "";
      /**
       * <code>string exception = 5;</code>
       * @return Whether the exception field is set.
       */
      public boolean hasException() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>string exception = 5;</code>
       * @return The exception.
       */
      public java.lang.String getException() {
        java.lang.Object ref = exception_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          exception_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string exception = 5;</code>
       * @return The bytes for exception.
       */
      public com.google.protobuf.ByteString
          getExceptionBytes() {
        java.lang.Object ref = exception_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          exception_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string exception = 5;</code>
       * @param value The exception to set.
       * @return This builder for chaining.
       */
      public Builder setException(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        exception_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string exception = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearException() {
        bitField0_ = (bitField0_ & ~0x00000008);
        exception_ = getDefaultInstance().getException();
        onChanged();
        return this;
      }
      /**
       * <code>string exception = 5;</code>
       * @param value The bytes for exception to set.
       * @return This builder for chaining.
       */
      public Builder setExceptionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000008;
        exception_ = value;
        onChanged();
        return this;
      }

      private long startTimestamp_ ;
      /**
       * <code>int64 start_timestamp = 6;</code>
       * @return The startTimestamp.
       */
      @java.lang.Override
      public long getStartTimestamp() {
        return startTimestamp_;
      }
      /**
       * <code>int64 start_timestamp = 6;</code>
       * @param value The startTimestamp to set.
       * @return This builder for chaining.
       */
      public Builder setStartTimestamp(long value) {
        
        startTimestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 start_timestamp = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearStartTimestamp() {
        
        startTimestamp_ = 0L;
        onChanged();
        return this;
      }

      private long endTimestamp_ ;
      /**
       * <code>int64 end_timestamp = 7;</code>
       * @return Whether the endTimestamp field is set.
       */
      @java.lang.Override
      public boolean hasEndTimestamp() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>int64 end_timestamp = 7;</code>
       * @return The endTimestamp.
       */
      @java.lang.Override
      public long getEndTimestamp() {
        return endTimestamp_;
      }
      /**
       * <code>int64 end_timestamp = 7;</code>
       * @param value The endTimestamp to set.
       * @return This builder for chaining.
       */
      public Builder setEndTimestamp(long value) {
        bitField0_ |= 0x00000010;
        endTimestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 end_timestamp = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearEndTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000010);
        endTimestamp_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.StreamingQueryData)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.StreamingQueryData)
    private static final org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StreamingQueryData>
        PARSER = new com.google.protobuf.AbstractParser<StreamingQueryData>() {
      @java.lang.Override
      public StreamingQueryData parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StreamingQueryData(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StreamingQueryData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StreamingQueryData> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StageDataWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.StageDataWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
     * @return Whether the info field is set.
     */
    boolean hasInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
     * @return The info.
     */
    org.apache.spark.status.protobuf.StoreTypes.StageData getInfo();
    /**
     * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.StageDataOrBuilder getInfoOrBuilder();

    /**
     * <code>repeated int64 job_ids = 2;</code>
     * @return A list containing the jobIds.
     */
    java.util.List<java.lang.Long> getJobIdsList();
    /**
     * <code>repeated int64 job_ids = 2;</code>
     * @return The count of jobIds.
     */
    int getJobIdsCount();
    /**
     * <code>repeated int64 job_ids = 2;</code>
     * @param index The index of the element to return.
     * @return The jobIds at the given index.
     */
    long getJobIds(int index);

    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */
    int getLocalityCount();
    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */
    boolean containsLocality(
        java.lang.String key);
    /**
     * Use {@link #getLocalityMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.Long>
    getLocality();
    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */
    java.util.Map<java.lang.String, java.lang.Long>
    getLocalityMap();
    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */

    long getLocalityOrDefault(
        java.lang.String key,
        long defaultValue);
    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */

    long getLocalityOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.StageDataWrapper}
   */
  public static final class StageDataWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.StageDataWrapper)
      StageDataWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StageDataWrapper.newBuilder() to construct.
    private StageDataWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StageDataWrapper() {
      jobIds_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StageDataWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StageDataWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.StageData.Builder subBuilder = null;
              if (info_ != null) {
                subBuilder = info_.toBuilder();
              }
              info_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.StageData.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(info_);
                info_ = subBuilder.buildPartial();
              }

              break;
            }
            case 16: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                jobIds_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              jobIds_.addLong(input.readInt64());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                jobIds_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                jobIds_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                locality_ = com.google.protobuf.MapField.newMapField(
                    LocalityDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000002;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.Long>
              locality__ = input.readMessage(
                  LocalityDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              locality_.getMutableMap().put(
                  locality__.getKey(), locality__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          jobIds_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageDataWrapper_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 3:
          return internalGetLocality();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageDataWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper.class, org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper.Builder.class);
    }

    public static final int INFO_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.StageData info_;
    /**
     * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
     * @return Whether the info field is set.
     */
    @java.lang.Override
    public boolean hasInfo() {
      return info_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
     * @return The info.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StageData getInfo() {
      return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.StageData.getDefaultInstance() : info_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StageDataOrBuilder getInfoOrBuilder() {
      return getInfo();
    }

    public static final int JOB_IDS_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.LongList jobIds_;
    /**
     * <code>repeated int64 job_ids = 2;</code>
     * @return A list containing the jobIds.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getJobIdsList() {
      return jobIds_;
    }
    /**
     * <code>repeated int64 job_ids = 2;</code>
     * @return The count of jobIds.
     */
    public int getJobIdsCount() {
      return jobIds_.size();
    }
    /**
     * <code>repeated int64 job_ids = 2;</code>
     * @param index The index of the element to return.
     * @return The jobIds at the given index.
     */
    public long getJobIds(int index) {
      return jobIds_.getLong(index);
    }
    private int jobIdsMemoizedSerializedSize = -1;

    public static final int LOCALITY_FIELD_NUMBER = 3;
    private static final class LocalityDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.Long> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.Long>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageDataWrapper_LocalityEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.INT64,
                  0L);
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.Long> locality_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
    internalGetLocality() {
      if (locality_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            LocalityDefaultEntryHolder.defaultEntry);
      }
      return locality_;
    }

    public int getLocalityCount() {
      return internalGetLocality().getMap().size();
    }
    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */

    @java.lang.Override
    public boolean containsLocality(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetLocality().getMap().containsKey(key);
    }
    /**
     * Use {@link #getLocalityMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.Long> getLocality() {
      return getLocalityMap();
    }
    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.Long> getLocalityMap() {
      return internalGetLocality().getMap();
    }
    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */
    @java.lang.Override

    public long getLocalityOrDefault(
        java.lang.String key,
        long defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Long> map =
          internalGetLocality().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, int64&gt; locality = 3;</code>
     */
    @java.lang.Override

    public long getLocalityOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Long> map =
          internalGetLocality().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (info_ != null) {
        output.writeMessage(1, getInfo());
      }
      if (getJobIdsList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(jobIdsMemoizedSerializedSize);
      }
      for (int i = 0; i < jobIds_.size(); i++) {
        output.writeInt64NoTag(jobIds_.getLong(i));
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetLocality(),
          LocalityDefaultEntryHolder.defaultEntry,
          3);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (info_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getInfo());
      }
      {
        int dataSize = 0;
        for (int i = 0; i < jobIds_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(jobIds_.getLong(i));
        }
        size += dataSize;
        if (!getJobIdsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        jobIdsMemoizedSerializedSize = dataSize;
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.Long> entry
           : internalGetLocality().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.Long>
        locality__ = LocalityDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(3, locality__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper other = (org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper) obj;

      if (hasInfo() != other.hasInfo()) return false;
      if (hasInfo()) {
        if (!getInfo()
            .equals(other.getInfo())) return false;
      }
      if (!getJobIdsList()
          .equals(other.getJobIdsList())) return false;
      if (!internalGetLocality().equals(
          other.internalGetLocality())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasInfo()) {
        hash = (37 * hash) + INFO_FIELD_NUMBER;
        hash = (53 * hash) + getInfo().hashCode();
      }
      if (getJobIdsCount() > 0) {
        hash = (37 * hash) + JOB_IDS_FIELD_NUMBER;
        hash = (53 * hash) + getJobIdsList().hashCode();
      }
      if (!internalGetLocality().getMap().isEmpty()) {
        hash = (37 * hash) + LOCALITY_FIELD_NUMBER;
        hash = (53 * hash) + internalGetLocality().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.StageDataWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.StageDataWrapper)
        org.apache.spark.status.protobuf.StoreTypes.StageDataWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageDataWrapper_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetLocality();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetMutableLocality();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageDataWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper.class, org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (infoBuilder_ == null) {
          info_ = null;
        } else {
          info_ = null;
          infoBuilder_ = null;
        }
        jobIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        internalGetMutableLocality().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageDataWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper result = new org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper(this);
        int from_bitField0_ = bitField0_;
        if (infoBuilder_ == null) {
          result.info_ = info_;
        } else {
          result.info_ = infoBuilder_.build();
        }
        if (((bitField0_ & 0x00000001) != 0)) {
          jobIds_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.jobIds_ = jobIds_;
        result.locality_ = internalGetLocality();
        result.locality_.makeImmutable();
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper.getDefaultInstance()) return this;
        if (other.hasInfo()) {
          mergeInfo(other.getInfo());
        }
        if (!other.jobIds_.isEmpty()) {
          if (jobIds_.isEmpty()) {
            jobIds_ = other.jobIds_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureJobIdsIsMutable();
            jobIds_.addAll(other.jobIds_);
          }
          onChanged();
        }
        internalGetMutableLocality().mergeFrom(
            other.internalGetLocality());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.spark.status.protobuf.StoreTypes.StageData info_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.StageData, org.apache.spark.status.protobuf.StoreTypes.StageData.Builder, org.apache.spark.status.protobuf.StoreTypes.StageDataOrBuilder> infoBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       * @return Whether the info field is set.
       */
      public boolean hasInfo() {
        return infoBuilder_ != null || info_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       * @return The info.
       */
      public org.apache.spark.status.protobuf.StoreTypes.StageData getInfo() {
        if (infoBuilder_ == null) {
          return info_ == null ? org.apache.spark.status.protobuf.StoreTypes.StageData.getDefaultInstance() : info_;
        } else {
          return infoBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       */
      public Builder setInfo(org.apache.spark.status.protobuf.StoreTypes.StageData value) {
        if (infoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          info_ = value;
          onChanged();
        } else {
          infoBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       */
      public Builder setInfo(
          org.apache.spark.status.protobuf.StoreTypes.StageData.Builder builderForValue) {
        if (infoBuilder_ == null) {
          info_ = builderForValue.build();
          onChanged();
        } else {
          infoBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       */
      public Builder mergeInfo(org.apache.spark.status.protobuf.StoreTypes.StageData value) {
        if (infoBuilder_ == null) {
          if (info_ != null) {
            info_ =
              org.apache.spark.status.protobuf.StoreTypes.StageData.newBuilder(info_).mergeFrom(value).buildPartial();
          } else {
            info_ = value;
          }
          onChanged();
        } else {
          infoBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       */
      public Builder clearInfo() {
        if (infoBuilder_ == null) {
          info_ = null;
          onChanged();
        } else {
          info_ = null;
          infoBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StageData.Builder getInfoBuilder() {
        
        onChanged();
        return getInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StageDataOrBuilder getInfoOrBuilder() {
        if (infoBuilder_ != null) {
          return infoBuilder_.getMessageOrBuilder();
        } else {
          return info_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.StageData.getDefaultInstance() : info_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageData info = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.StageData, org.apache.spark.status.protobuf.StoreTypes.StageData.Builder, org.apache.spark.status.protobuf.StoreTypes.StageDataOrBuilder> 
          getInfoFieldBuilder() {
        if (infoBuilder_ == null) {
          infoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.StageData, org.apache.spark.status.protobuf.StoreTypes.StageData.Builder, org.apache.spark.status.protobuf.StoreTypes.StageDataOrBuilder>(
                  getInfo(),
                  getParentForChildren(),
                  isClean());
          info_ = null;
        }
        return infoBuilder_;
      }

      private com.google.protobuf.Internal.LongList jobIds_ = emptyLongList();
      private void ensureJobIdsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          jobIds_ = mutableCopy(jobIds_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated int64 job_ids = 2;</code>
       * @return A list containing the jobIds.
       */
      public java.util.List<java.lang.Long>
          getJobIdsList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(jobIds_) : jobIds_;
      }
      /**
       * <code>repeated int64 job_ids = 2;</code>
       * @return The count of jobIds.
       */
      public int getJobIdsCount() {
        return jobIds_.size();
      }
      /**
       * <code>repeated int64 job_ids = 2;</code>
       * @param index The index of the element to return.
       * @return The jobIds at the given index.
       */
      public long getJobIds(int index) {
        return jobIds_.getLong(index);
      }
      /**
       * <code>repeated int64 job_ids = 2;</code>
       * @param index The index to set the value at.
       * @param value The jobIds to set.
       * @return This builder for chaining.
       */
      public Builder setJobIds(
          int index, long value) {
        ensureJobIdsIsMutable();
        jobIds_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 job_ids = 2;</code>
       * @param value The jobIds to add.
       * @return This builder for chaining.
       */
      public Builder addJobIds(long value) {
        ensureJobIdsIsMutable();
        jobIds_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 job_ids = 2;</code>
       * @param values The jobIds to add.
       * @return This builder for chaining.
       */
      public Builder addAllJobIds(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureJobIdsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, jobIds_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 job_ids = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearJobIds() {
        jobIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.Long> locality_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
      internalGetLocality() {
        if (locality_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              LocalityDefaultEntryHolder.defaultEntry);
        }
        return locality_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
      internalGetMutableLocality() {
        onChanged();;
        if (locality_ == null) {
          locality_ = com.google.protobuf.MapField.newMapField(
              LocalityDefaultEntryHolder.defaultEntry);
        }
        if (!locality_.isMutable()) {
          locality_ = locality_.copy();
        }
        return locality_;
      }

      public int getLocalityCount() {
        return internalGetLocality().getMap().size();
      }
      /**
       * <code>map&lt;string, int64&gt; locality = 3;</code>
       */

      @java.lang.Override
      public boolean containsLocality(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetLocality().getMap().containsKey(key);
      }
      /**
       * Use {@link #getLocalityMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Long> getLocality() {
        return getLocalityMap();
      }
      /**
       * <code>map&lt;string, int64&gt; locality = 3;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.Long> getLocalityMap() {
        return internalGetLocality().getMap();
      }
      /**
       * <code>map&lt;string, int64&gt; locality = 3;</code>
       */
      @java.lang.Override

      public long getLocalityOrDefault(
          java.lang.String key,
          long defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Long> map =
            internalGetLocality().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, int64&gt; locality = 3;</code>
       */
      @java.lang.Override

      public long getLocalityOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Long> map =
            internalGetLocality().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearLocality() {
        internalGetMutableLocality().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, int64&gt; locality = 3;</code>
       */

      public Builder removeLocality(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableLocality().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Long>
      getMutableLocality() {
        return internalGetMutableLocality().getMutableMap();
      }
      /**
       * <code>map&lt;string, int64&gt; locality = 3;</code>
       */
      public Builder putLocality(
          java.lang.String key,
          long value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        
        internalGetMutableLocality().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, int64&gt; locality = 3;</code>
       */

      public Builder putAllLocality(
          java.util.Map<java.lang.String, java.lang.Long> values) {
        internalGetMutableLocality().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.StageDataWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.StageDataWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StageDataWrapper>
        PARSER = new com.google.protobuf.AbstractParser<StageDataWrapper>() {
      @java.lang.Override
      public StageDataWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StageDataWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StageDataWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StageDataWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StageDataWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.TaskData)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 task_id = 1;</code>
     * @return The taskId.
     */
    long getTaskId();

    /**
     * <code>int32 index = 2;</code>
     * @return The index.
     */
    int getIndex();

    /**
     * <code>int32 attempt = 3;</code>
     * @return The attempt.
     */
    int getAttempt();

    /**
     * <code>int32 partition_id = 4;</code>
     * @return The partitionId.
     */
    int getPartitionId();

    /**
     * <code>int64 launch_time = 5;</code>
     * @return The launchTime.
     */
    long getLaunchTime();

    /**
     * <code>int64 result_fetch_start = 6;</code>
     * @return Whether the resultFetchStart field is set.
     */
    boolean hasResultFetchStart();
    /**
     * <code>int64 result_fetch_start = 6;</code>
     * @return The resultFetchStart.
     */
    long getResultFetchStart();

    /**
     * <code>int64 duration = 7;</code>
     * @return Whether the duration field is set.
     */
    boolean hasDuration();
    /**
     * <code>int64 duration = 7;</code>
     * @return The duration.
     */
    long getDuration();

    /**
     * <code>string executor_id = 8;</code>
     * @return Whether the executorId field is set.
     */
    boolean hasExecutorId();
    /**
     * <code>string executor_id = 8;</code>
     * @return The executorId.
     */
    java.lang.String getExecutorId();
    /**
     * <code>string executor_id = 8;</code>
     * @return The bytes for executorId.
     */
    com.google.protobuf.ByteString
        getExecutorIdBytes();

    /**
     * <code>string host = 9;</code>
     * @return Whether the host field is set.
     */
    boolean hasHost();
    /**
     * <code>string host = 9;</code>
     * @return The host.
     */
    java.lang.String getHost();
    /**
     * <code>string host = 9;</code>
     * @return The bytes for host.
     */
    com.google.protobuf.ByteString
        getHostBytes();

    /**
     * <code>string status = 10;</code>
     * @return Whether the status field is set.
     */
    boolean hasStatus();
    /**
     * <code>string status = 10;</code>
     * @return The status.
     */
    java.lang.String getStatus();
    /**
     * <code>string status = 10;</code>
     * @return The bytes for status.
     */
    com.google.protobuf.ByteString
        getStatusBytes();

    /**
     * <code>string task_locality = 11;</code>
     * @return Whether the taskLocality field is set.
     */
    boolean hasTaskLocality();
    /**
     * <code>string task_locality = 11;</code>
     * @return The taskLocality.
     */
    java.lang.String getTaskLocality();
    /**
     * <code>string task_locality = 11;</code>
     * @return The bytes for taskLocality.
     */
    com.google.protobuf.ByteString
        getTaskLocalityBytes();

    /**
     * <code>bool speculative = 12;</code>
     * @return The speculative.
     */
    boolean getSpeculative();

    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> 
        getAccumulatorUpdatesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    int getAccumulatorUpdatesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
        getAccumulatorUpdatesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
        int index);

    /**
     * <code>string error_message = 14;</code>
     * @return Whether the errorMessage field is set.
     */
    boolean hasErrorMessage();
    /**
     * <code>string error_message = 14;</code>
     * @return The errorMessage.
     */
    java.lang.String getErrorMessage();
    /**
     * <code>string error_message = 14;</code>
     * @return The bytes for errorMessage.
     */
    com.google.protobuf.ByteString
        getErrorMessageBytes();

    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
     * @return Whether the taskMetrics field is set.
     */
    boolean hasTaskMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
     * @return The taskMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.TaskMetrics getTaskMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.TaskMetricsOrBuilder getTaskMetricsOrBuilder();

    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */
    int getExecutorLogsCount();
    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */
    boolean containsExecutorLogs(
        java.lang.String key);
    /**
     * Use {@link #getExecutorLogsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getExecutorLogs();
    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getExecutorLogsMap();
    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */

    java.lang.String getExecutorLogsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */

    java.lang.String getExecutorLogsOrThrow(
        java.lang.String key);

    /**
     * <code>int64 scheduler_delay = 17;</code>
     * @return The schedulerDelay.
     */
    long getSchedulerDelay();

    /**
     * <code>int64 getting_result_time = 18;</code>
     * @return The gettingResultTime.
     */
    long getGettingResultTime();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.TaskData}
   */
  public static final class TaskData extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.TaskData)
      TaskDataOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskData.newBuilder() to construct.
    private TaskData(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskData() {
      executorId_ = "";
      host_ = "";
      status_ = "";
      taskLocality_ = "";
      accumulatorUpdates_ = java.util.Collections.emptyList();
      errorMessage_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new TaskData();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskData(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              taskId_ = input.readInt64();
              break;
            }
            case 16: {

              index_ = input.readInt32();
              break;
            }
            case 24: {

              attempt_ = input.readInt32();
              break;
            }
            case 32: {

              partitionId_ = input.readInt32();
              break;
            }
            case 40: {

              launchTime_ = input.readInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000001;
              resultFetchStart_ = input.readInt64();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000002;
              duration_ = input.readInt64();
              break;
            }
            case 66: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              executorId_ = s;
              break;
            }
            case 74: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000008;
              host_ = s;
              break;
            }
            case 82: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000010;
              status_ = s;
              break;
            }
            case 90: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000020;
              taskLocality_ = s;
              break;
            }
            case 96: {

              speculative_ = input.readBool();
              break;
            }
            case 106: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                accumulatorUpdates_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo>();
                mutable_bitField0_ |= 0x00000040;
              }
              accumulatorUpdates_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.parser(), extensionRegistry));
              break;
            }
            case 114: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000040;
              errorMessage_ = s;
              break;
            }
            case 122: {
              org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.Builder subBuilder = null;
              if (((bitField0_ & 0x00000080) != 0)) {
                subBuilder = taskMetrics_.toBuilder();
              }
              taskMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(taskMetrics_);
                taskMetrics_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000080;
              break;
            }
            case 130: {
              if (!((mutable_bitField0_ & 0x00000200) != 0)) {
                executorLogs_ = com.google.protobuf.MapField.newMapField(
                    ExecutorLogsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000200;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              executorLogs__ = input.readMessage(
                  ExecutorLogsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              executorLogs_.getMutableMap().put(
                  executorLogs__.getKey(), executorLogs__.getValue());
              break;
            }
            case 136: {

              schedulerDelay_ = input.readInt64();
              break;
            }
            case 144: {

              gettingResultTime_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          accumulatorUpdates_ = java.util.Collections.unmodifiableList(accumulatorUpdates_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskData_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 16:
          return internalGetExecutorLogs();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.TaskData.class, org.apache.spark.status.protobuf.StoreTypes.TaskData.Builder.class);
    }

    private int bitField0_;
    public static final int TASK_ID_FIELD_NUMBER = 1;
    private long taskId_;
    /**
     * <code>int64 task_id = 1;</code>
     * @return The taskId.
     */
    @java.lang.Override
    public long getTaskId() {
      return taskId_;
    }

    public static final int INDEX_FIELD_NUMBER = 2;
    private int index_;
    /**
     * <code>int32 index = 2;</code>
     * @return The index.
     */
    @java.lang.Override
    public int getIndex() {
      return index_;
    }

    public static final int ATTEMPT_FIELD_NUMBER = 3;
    private int attempt_;
    /**
     * <code>int32 attempt = 3;</code>
     * @return The attempt.
     */
    @java.lang.Override
    public int getAttempt() {
      return attempt_;
    }

    public static final int PARTITION_ID_FIELD_NUMBER = 4;
    private int partitionId_;
    /**
     * <code>int32 partition_id = 4;</code>
     * @return The partitionId.
     */
    @java.lang.Override
    public int getPartitionId() {
      return partitionId_;
    }

    public static final int LAUNCH_TIME_FIELD_NUMBER = 5;
    private long launchTime_;
    /**
     * <code>int64 launch_time = 5;</code>
     * @return The launchTime.
     */
    @java.lang.Override
    public long getLaunchTime() {
      return launchTime_;
    }

    public static final int RESULT_FETCH_START_FIELD_NUMBER = 6;
    private long resultFetchStart_;
    /**
     * <code>int64 result_fetch_start = 6;</code>
     * @return Whether the resultFetchStart field is set.
     */
    @java.lang.Override
    public boolean hasResultFetchStart() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>int64 result_fetch_start = 6;</code>
     * @return The resultFetchStart.
     */
    @java.lang.Override
    public long getResultFetchStart() {
      return resultFetchStart_;
    }

    public static final int DURATION_FIELD_NUMBER = 7;
    private long duration_;
    /**
     * <code>int64 duration = 7;</code>
     * @return Whether the duration field is set.
     */
    @java.lang.Override
    public boolean hasDuration() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>int64 duration = 7;</code>
     * @return The duration.
     */
    @java.lang.Override
    public long getDuration() {
      return duration_;
    }

    public static final int EXECUTOR_ID_FIELD_NUMBER = 8;
    private volatile java.lang.Object executorId_;
    /**
     * <code>string executor_id = 8;</code>
     * @return Whether the executorId field is set.
     */
    @java.lang.Override
    public boolean hasExecutorId() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string executor_id = 8;</code>
     * @return The executorId.
     */
    @java.lang.Override
    public java.lang.String getExecutorId() {
      java.lang.Object ref = executorId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        executorId_ = s;
        return s;
      }
    }
    /**
     * <code>string executor_id = 8;</code>
     * @return The bytes for executorId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getExecutorIdBytes() {
      java.lang.Object ref = executorId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        executorId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int HOST_FIELD_NUMBER = 9;
    private volatile java.lang.Object host_;
    /**
     * <code>string host = 9;</code>
     * @return Whether the host field is set.
     */
    @java.lang.Override
    public boolean hasHost() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>string host = 9;</code>
     * @return The host.
     */
    @java.lang.Override
    public java.lang.String getHost() {
      java.lang.Object ref = host_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        host_ = s;
        return s;
      }
    }
    /**
     * <code>string host = 9;</code>
     * @return The bytes for host.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getHostBytes() {
      java.lang.Object ref = host_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        host_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int STATUS_FIELD_NUMBER = 10;
    private volatile java.lang.Object status_;
    /**
     * <code>string status = 10;</code>
     * @return Whether the status field is set.
     */
    @java.lang.Override
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>string status = 10;</code>
     * @return The status.
     */
    @java.lang.Override
    public java.lang.String getStatus() {
      java.lang.Object ref = status_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        status_ = s;
        return s;
      }
    }
    /**
     * <code>string status = 10;</code>
     * @return The bytes for status.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStatusBytes() {
      java.lang.Object ref = status_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        status_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TASK_LOCALITY_FIELD_NUMBER = 11;
    private volatile java.lang.Object taskLocality_;
    /**
     * <code>string task_locality = 11;</code>
     * @return Whether the taskLocality field is set.
     */
    @java.lang.Override
    public boolean hasTaskLocality() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>string task_locality = 11;</code>
     * @return The taskLocality.
     */
    @java.lang.Override
    public java.lang.String getTaskLocality() {
      java.lang.Object ref = taskLocality_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        taskLocality_ = s;
        return s;
      }
    }
    /**
     * <code>string task_locality = 11;</code>
     * @return The bytes for taskLocality.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getTaskLocalityBytes() {
      java.lang.Object ref = taskLocality_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        taskLocality_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int SPECULATIVE_FIELD_NUMBER = 12;
    private boolean speculative_;
    /**
     * <code>bool speculative = 12;</code>
     * @return The speculative.
     */
    @java.lang.Override
    public boolean getSpeculative() {
      return speculative_;
    }

    public static final int ACCUMULATOR_UPDATES_FIELD_NUMBER = 13;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> accumulatorUpdates_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> getAccumulatorUpdatesList() {
      return accumulatorUpdates_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
        getAccumulatorUpdatesOrBuilderList() {
      return accumulatorUpdates_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public int getAccumulatorUpdatesCount() {
      return accumulatorUpdates_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index) {
      return accumulatorUpdates_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
        int index) {
      return accumulatorUpdates_.get(index);
    }

    public static final int ERROR_MESSAGE_FIELD_NUMBER = 14;
    private volatile java.lang.Object errorMessage_;
    /**
     * <code>string error_message = 14;</code>
     * @return Whether the errorMessage field is set.
     */
    @java.lang.Override
    public boolean hasErrorMessage() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>string error_message = 14;</code>
     * @return The errorMessage.
     */
    @java.lang.Override
    public java.lang.String getErrorMessage() {
      java.lang.Object ref = errorMessage_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        errorMessage_ = s;
        return s;
      }
    }
    /**
     * <code>string error_message = 14;</code>
     * @return The bytes for errorMessage.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getErrorMessageBytes() {
      java.lang.Object ref = errorMessage_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        errorMessage_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TASK_METRICS_FIELD_NUMBER = 15;
    private org.apache.spark.status.protobuf.StoreTypes.TaskMetrics taskMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
     * @return Whether the taskMetrics field is set.
     */
    @java.lang.Override
    public boolean hasTaskMetrics() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
     * @return The taskMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskMetrics getTaskMetrics() {
      return taskMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.getDefaultInstance() : taskMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskMetricsOrBuilder getTaskMetricsOrBuilder() {
      return taskMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.getDefaultInstance() : taskMetrics_;
    }

    public static final int EXECUTOR_LOGS_FIELD_NUMBER = 16;
    private static final class ExecutorLogsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskData_ExecutorLogsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> executorLogs_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetExecutorLogs() {
      if (executorLogs_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ExecutorLogsDefaultEntryHolder.defaultEntry);
      }
      return executorLogs_;
    }

    public int getExecutorLogsCount() {
      return internalGetExecutorLogs().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */

    @java.lang.Override
    public boolean containsExecutorLogs(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetExecutorLogs().getMap().containsKey(key);
    }
    /**
     * Use {@link #getExecutorLogsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getExecutorLogs() {
      return getExecutorLogsMap();
    }
    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getExecutorLogsMap() {
      return internalGetExecutorLogs().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */
    @java.lang.Override

    public java.lang.String getExecutorLogsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetExecutorLogs().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; executor_logs = 16;</code>
     */
    @java.lang.Override

    public java.lang.String getExecutorLogsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetExecutorLogs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int SCHEDULER_DELAY_FIELD_NUMBER = 17;
    private long schedulerDelay_;
    /**
     * <code>int64 scheduler_delay = 17;</code>
     * @return The schedulerDelay.
     */
    @java.lang.Override
    public long getSchedulerDelay() {
      return schedulerDelay_;
    }

    public static final int GETTING_RESULT_TIME_FIELD_NUMBER = 18;
    private long gettingResultTime_;
    /**
     * <code>int64 getting_result_time = 18;</code>
     * @return The gettingResultTime.
     */
    @java.lang.Override
    public long getGettingResultTime() {
      return gettingResultTime_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (taskId_ != 0L) {
        output.writeInt64(1, taskId_);
      }
      if (index_ != 0) {
        output.writeInt32(2, index_);
      }
      if (attempt_ != 0) {
        output.writeInt32(3, attempt_);
      }
      if (partitionId_ != 0) {
        output.writeInt32(4, partitionId_);
      }
      if (launchTime_ != 0L) {
        output.writeInt64(5, launchTime_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(6, resultFetchStart_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(7, duration_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, executorId_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 9, host_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 10, status_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 11, taskLocality_);
      }
      if (speculative_ != false) {
        output.writeBool(12, speculative_);
      }
      for (int i = 0; i < accumulatorUpdates_.size(); i++) {
        output.writeMessage(13, accumulatorUpdates_.get(i));
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 14, errorMessage_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeMessage(15, getTaskMetrics());
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetExecutorLogs(),
          ExecutorLogsDefaultEntryHolder.defaultEntry,
          16);
      if (schedulerDelay_ != 0L) {
        output.writeInt64(17, schedulerDelay_);
      }
      if (gettingResultTime_ != 0L) {
        output.writeInt64(18, gettingResultTime_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (taskId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, taskId_);
      }
      if (index_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, index_);
      }
      if (attempt_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, attempt_);
      }
      if (partitionId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, partitionId_);
      }
      if (launchTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, launchTime_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, resultFetchStart_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, duration_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, executorId_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(9, host_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(10, status_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(11, taskLocality_);
      }
      if (speculative_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(12, speculative_);
      }
      for (int i = 0; i < accumulatorUpdates_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(13, accumulatorUpdates_.get(i));
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(14, errorMessage_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(15, getTaskMetrics());
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetExecutorLogs().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        executorLogs__ = ExecutorLogsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(16, executorLogs__);
      }
      if (schedulerDelay_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(17, schedulerDelay_);
      }
      if (gettingResultTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(18, gettingResultTime_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.TaskData)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.TaskData other = (org.apache.spark.status.protobuf.StoreTypes.TaskData) obj;

      if (getTaskId()
          != other.getTaskId()) return false;
      if (getIndex()
          != other.getIndex()) return false;
      if (getAttempt()
          != other.getAttempt()) return false;
      if (getPartitionId()
          != other.getPartitionId()) return false;
      if (getLaunchTime()
          != other.getLaunchTime()) return false;
      if (hasResultFetchStart() != other.hasResultFetchStart()) return false;
      if (hasResultFetchStart()) {
        if (getResultFetchStart()
            != other.getResultFetchStart()) return false;
      }
      if (hasDuration() != other.hasDuration()) return false;
      if (hasDuration()) {
        if (getDuration()
            != other.getDuration()) return false;
      }
      if (hasExecutorId() != other.hasExecutorId()) return false;
      if (hasExecutorId()) {
        if (!getExecutorId()
            .equals(other.getExecutorId())) return false;
      }
      if (hasHost() != other.hasHost()) return false;
      if (hasHost()) {
        if (!getHost()
            .equals(other.getHost())) return false;
      }
      if (hasStatus() != other.hasStatus()) return false;
      if (hasStatus()) {
        if (!getStatus()
            .equals(other.getStatus())) return false;
      }
      if (hasTaskLocality() != other.hasTaskLocality()) return false;
      if (hasTaskLocality()) {
        if (!getTaskLocality()
            .equals(other.getTaskLocality())) return false;
      }
      if (getSpeculative()
          != other.getSpeculative()) return false;
      if (!getAccumulatorUpdatesList()
          .equals(other.getAccumulatorUpdatesList())) return false;
      if (hasErrorMessage() != other.hasErrorMessage()) return false;
      if (hasErrorMessage()) {
        if (!getErrorMessage()
            .equals(other.getErrorMessage())) return false;
      }
      if (hasTaskMetrics() != other.hasTaskMetrics()) return false;
      if (hasTaskMetrics()) {
        if (!getTaskMetrics()
            .equals(other.getTaskMetrics())) return false;
      }
      if (!internalGetExecutorLogs().equals(
          other.internalGetExecutorLogs())) return false;
      if (getSchedulerDelay()
          != other.getSchedulerDelay()) return false;
      if (getGettingResultTime()
          != other.getGettingResultTime()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + TASK_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTaskId());
      hash = (37 * hash) + INDEX_FIELD_NUMBER;
      hash = (53 * hash) + getIndex();
      hash = (37 * hash) + ATTEMPT_FIELD_NUMBER;
      hash = (53 * hash) + getAttempt();
      hash = (37 * hash) + PARTITION_ID_FIELD_NUMBER;
      hash = (53 * hash) + getPartitionId();
      hash = (37 * hash) + LAUNCH_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getLaunchTime());
      if (hasResultFetchStart()) {
        hash = (37 * hash) + RESULT_FETCH_START_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getResultFetchStart());
      }
      if (hasDuration()) {
        hash = (37 * hash) + DURATION_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getDuration());
      }
      if (hasExecutorId()) {
        hash = (37 * hash) + EXECUTOR_ID_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorId().hashCode();
      }
      if (hasHost()) {
        hash = (37 * hash) + HOST_FIELD_NUMBER;
        hash = (53 * hash) + getHost().hashCode();
      }
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getStatus().hashCode();
      }
      if (hasTaskLocality()) {
        hash = (37 * hash) + TASK_LOCALITY_FIELD_NUMBER;
        hash = (53 * hash) + getTaskLocality().hashCode();
      }
      hash = (37 * hash) + SPECULATIVE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getSpeculative());
      if (getAccumulatorUpdatesCount() > 0) {
        hash = (37 * hash) + ACCUMULATOR_UPDATES_FIELD_NUMBER;
        hash = (53 * hash) + getAccumulatorUpdatesList().hashCode();
      }
      if (hasErrorMessage()) {
        hash = (37 * hash) + ERROR_MESSAGE_FIELD_NUMBER;
        hash = (53 * hash) + getErrorMessage().hashCode();
      }
      if (hasTaskMetrics()) {
        hash = (37 * hash) + TASK_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getTaskMetrics().hashCode();
      }
      if (!internalGetExecutorLogs().getMap().isEmpty()) {
        hash = (37 * hash) + EXECUTOR_LOGS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetExecutorLogs().hashCode();
      }
      hash = (37 * hash) + SCHEDULER_DELAY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getSchedulerDelay());
      hash = (37 * hash) + GETTING_RESULT_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getGettingResultTime());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.TaskData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.TaskData}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.TaskData)
        org.apache.spark.status.protobuf.StoreTypes.TaskDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskData_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 16:
            return internalGetExecutorLogs();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 16:
            return internalGetMutableExecutorLogs();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.TaskData.class, org.apache.spark.status.protobuf.StoreTypes.TaskData.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.TaskData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAccumulatorUpdatesFieldBuilder();
          getTaskMetricsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        taskId_ = 0L;

        index_ = 0;

        attempt_ = 0;

        partitionId_ = 0;

        launchTime_ = 0L;

        resultFetchStart_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        duration_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        executorId_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        host_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        status_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        taskLocality_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        speculative_ = false;

        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdates_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
        } else {
          accumulatorUpdatesBuilder_.clear();
        }
        errorMessage_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        if (taskMetricsBuilder_ == null) {
          taskMetrics_ = null;
        } else {
          taskMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        internalGetMutableExecutorLogs().clear();
        schedulerDelay_ = 0L;

        gettingResultTime_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskData_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskData getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.TaskData.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskData build() {
        org.apache.spark.status.protobuf.StoreTypes.TaskData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskData buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.TaskData result = new org.apache.spark.status.protobuf.StoreTypes.TaskData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.taskId_ = taskId_;
        result.index_ = index_;
        result.attempt_ = attempt_;
        result.partitionId_ = partitionId_;
        result.launchTime_ = launchTime_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.resultFetchStart_ = resultFetchStart_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.duration_ = duration_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.executorId_ = executorId_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.host_ = host_;
        if (((from_bitField0_ & 0x00000010) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.status_ = status_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000020;
        }
        result.taskLocality_ = taskLocality_;
        result.speculative_ = speculative_;
        if (accumulatorUpdatesBuilder_ == null) {
          if (((bitField0_ & 0x00000040) != 0)) {
            accumulatorUpdates_ = java.util.Collections.unmodifiableList(accumulatorUpdates_);
            bitField0_ = (bitField0_ & ~0x00000040);
          }
          result.accumulatorUpdates_ = accumulatorUpdates_;
        } else {
          result.accumulatorUpdates_ = accumulatorUpdatesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          to_bitField0_ |= 0x00000040;
        }
        result.errorMessage_ = errorMessage_;
        if (((from_bitField0_ & 0x00000100) != 0)) {
          if (taskMetricsBuilder_ == null) {
            result.taskMetrics_ = taskMetrics_;
          } else {
            result.taskMetrics_ = taskMetricsBuilder_.build();
          }
          to_bitField0_ |= 0x00000080;
        }
        result.executorLogs_ = internalGetExecutorLogs();
        result.executorLogs_.makeImmutable();
        result.schedulerDelay_ = schedulerDelay_;
        result.gettingResultTime_ = gettingResultTime_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.TaskData) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.TaskData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.TaskData other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.TaskData.getDefaultInstance()) return this;
        if (other.getTaskId() != 0L) {
          setTaskId(other.getTaskId());
        }
        if (other.getIndex() != 0) {
          setIndex(other.getIndex());
        }
        if (other.getAttempt() != 0) {
          setAttempt(other.getAttempt());
        }
        if (other.getPartitionId() != 0) {
          setPartitionId(other.getPartitionId());
        }
        if (other.getLaunchTime() != 0L) {
          setLaunchTime(other.getLaunchTime());
        }
        if (other.hasResultFetchStart()) {
          setResultFetchStart(other.getResultFetchStart());
        }
        if (other.hasDuration()) {
          setDuration(other.getDuration());
        }
        if (other.hasExecutorId()) {
          bitField0_ |= 0x00000004;
          executorId_ = other.executorId_;
          onChanged();
        }
        if (other.hasHost()) {
          bitField0_ |= 0x00000008;
          host_ = other.host_;
          onChanged();
        }
        if (other.hasStatus()) {
          bitField0_ |= 0x00000010;
          status_ = other.status_;
          onChanged();
        }
        if (other.hasTaskLocality()) {
          bitField0_ |= 0x00000020;
          taskLocality_ = other.taskLocality_;
          onChanged();
        }
        if (other.getSpeculative() != false) {
          setSpeculative(other.getSpeculative());
        }
        if (accumulatorUpdatesBuilder_ == null) {
          if (!other.accumulatorUpdates_.isEmpty()) {
            if (accumulatorUpdates_.isEmpty()) {
              accumulatorUpdates_ = other.accumulatorUpdates_;
              bitField0_ = (bitField0_ & ~0x00000040);
            } else {
              ensureAccumulatorUpdatesIsMutable();
              accumulatorUpdates_.addAll(other.accumulatorUpdates_);
            }
            onChanged();
          }
        } else {
          if (!other.accumulatorUpdates_.isEmpty()) {
            if (accumulatorUpdatesBuilder_.isEmpty()) {
              accumulatorUpdatesBuilder_.dispose();
              accumulatorUpdatesBuilder_ = null;
              accumulatorUpdates_ = other.accumulatorUpdates_;
              bitField0_ = (bitField0_ & ~0x00000040);
              accumulatorUpdatesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getAccumulatorUpdatesFieldBuilder() : null;
            } else {
              accumulatorUpdatesBuilder_.addAllMessages(other.accumulatorUpdates_);
            }
          }
        }
        if (other.hasErrorMessage()) {
          bitField0_ |= 0x00000080;
          errorMessage_ = other.errorMessage_;
          onChanged();
        }
        if (other.hasTaskMetrics()) {
          mergeTaskMetrics(other.getTaskMetrics());
        }
        internalGetMutableExecutorLogs().mergeFrom(
            other.internalGetExecutorLogs());
        if (other.getSchedulerDelay() != 0L) {
          setSchedulerDelay(other.getSchedulerDelay());
        }
        if (other.getGettingResultTime() != 0L) {
          setGettingResultTime(other.getGettingResultTime());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.TaskData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.TaskData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long taskId_ ;
      /**
       * <code>int64 task_id = 1;</code>
       * @return The taskId.
       */
      @java.lang.Override
      public long getTaskId() {
        return taskId_;
      }
      /**
       * <code>int64 task_id = 1;</code>
       * @param value The taskId to set.
       * @return This builder for chaining.
       */
      public Builder setTaskId(long value) {
        
        taskId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 task_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTaskId() {
        
        taskId_ = 0L;
        onChanged();
        return this;
      }

      private int index_ ;
      /**
       * <code>int32 index = 2;</code>
       * @return The index.
       */
      @java.lang.Override
      public int getIndex() {
        return index_;
      }
      /**
       * <code>int32 index = 2;</code>
       * @param value The index to set.
       * @return This builder for chaining.
       */
      public Builder setIndex(int value) {
        
        index_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 index = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearIndex() {
        
        index_ = 0;
        onChanged();
        return this;
      }

      private int attempt_ ;
      /**
       * <code>int32 attempt = 3;</code>
       * @return The attempt.
       */
      @java.lang.Override
      public int getAttempt() {
        return attempt_;
      }
      /**
       * <code>int32 attempt = 3;</code>
       * @param value The attempt to set.
       * @return This builder for chaining.
       */
      public Builder setAttempt(int value) {
        
        attempt_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 attempt = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearAttempt() {
        
        attempt_ = 0;
        onChanged();
        return this;
      }

      private int partitionId_ ;
      /**
       * <code>int32 partition_id = 4;</code>
       * @return The partitionId.
       */
      @java.lang.Override
      public int getPartitionId() {
        return partitionId_;
      }
      /**
       * <code>int32 partition_id = 4;</code>
       * @param value The partitionId to set.
       * @return This builder for chaining.
       */
      public Builder setPartitionId(int value) {
        
        partitionId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 partition_id = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearPartitionId() {
        
        partitionId_ = 0;
        onChanged();
        return this;
      }

      private long launchTime_ ;
      /**
       * <code>int64 launch_time = 5;</code>
       * @return The launchTime.
       */
      @java.lang.Override
      public long getLaunchTime() {
        return launchTime_;
      }
      /**
       * <code>int64 launch_time = 5;</code>
       * @param value The launchTime to set.
       * @return This builder for chaining.
       */
      public Builder setLaunchTime(long value) {
        
        launchTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 launch_time = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearLaunchTime() {
        
        launchTime_ = 0L;
        onChanged();
        return this;
      }

      private long resultFetchStart_ ;
      /**
       * <code>int64 result_fetch_start = 6;</code>
       * @return Whether the resultFetchStart field is set.
       */
      @java.lang.Override
      public boolean hasResultFetchStart() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>int64 result_fetch_start = 6;</code>
       * @return The resultFetchStart.
       */
      @java.lang.Override
      public long getResultFetchStart() {
        return resultFetchStart_;
      }
      /**
       * <code>int64 result_fetch_start = 6;</code>
       * @param value The resultFetchStart to set.
       * @return This builder for chaining.
       */
      public Builder setResultFetchStart(long value) {
        bitField0_ |= 0x00000001;
        resultFetchStart_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 result_fetch_start = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultFetchStart() {
        bitField0_ = (bitField0_ & ~0x00000001);
        resultFetchStart_ = 0L;
        onChanged();
        return this;
      }

      private long duration_ ;
      /**
       * <code>int64 duration = 7;</code>
       * @return Whether the duration field is set.
       */
      @java.lang.Override
      public boolean hasDuration() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>int64 duration = 7;</code>
       * @return The duration.
       */
      @java.lang.Override
      public long getDuration() {
        return duration_;
      }
      /**
       * <code>int64 duration = 7;</code>
       * @param value The duration to set.
       * @return This builder for chaining.
       */
      public Builder setDuration(long value) {
        bitField0_ |= 0x00000002;
        duration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 duration = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearDuration() {
        bitField0_ = (bitField0_ & ~0x00000002);
        duration_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object executorId_ = "";
      /**
       * <code>string executor_id = 8;</code>
       * @return Whether the executorId field is set.
       */
      public boolean hasExecutorId() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string executor_id = 8;</code>
       * @return The executorId.
       */
      public java.lang.String getExecutorId() {
        java.lang.Object ref = executorId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          executorId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string executor_id = 8;</code>
       * @return The bytes for executorId.
       */
      public com.google.protobuf.ByteString
          getExecutorIdBytes() {
        java.lang.Object ref = executorId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          executorId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string executor_id = 8;</code>
       * @param value The executorId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        executorId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string executor_id = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        executorId_ = getDefaultInstance().getExecutorId();
        onChanged();
        return this;
      }
      /**
       * <code>string executor_id = 8;</code>
       * @param value The bytes for executorId to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        executorId_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object host_ = "";
      /**
       * <code>string host = 9;</code>
       * @return Whether the host field is set.
       */
      public boolean hasHost() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>string host = 9;</code>
       * @return The host.
       */
      public java.lang.String getHost() {
        java.lang.Object ref = host_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          host_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string host = 9;</code>
       * @return The bytes for host.
       */
      public com.google.protobuf.ByteString
          getHostBytes() {
        java.lang.Object ref = host_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          host_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string host = 9;</code>
       * @param value The host to set.
       * @return This builder for chaining.
       */
      public Builder setHost(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        host_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string host = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearHost() {
        bitField0_ = (bitField0_ & ~0x00000008);
        host_ = getDefaultInstance().getHost();
        onChanged();
        return this;
      }
      /**
       * <code>string host = 9;</code>
       * @param value The bytes for host to set.
       * @return This builder for chaining.
       */
      public Builder setHostBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000008;
        host_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object status_ = "";
      /**
       * <code>string status = 10;</code>
       * @return Whether the status field is set.
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>string status = 10;</code>
       * @return The status.
       */
      public java.lang.String getStatus() {
        java.lang.Object ref = status_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          status_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string status = 10;</code>
       * @return The bytes for status.
       */
      public com.google.protobuf.ByteString
          getStatusBytes() {
        java.lang.Object ref = status_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          status_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string status = 10;</code>
       * @param value The status to set.
       * @return This builder for chaining.
       */
      public Builder setStatus(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string status = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000010);
        status_ = getDefaultInstance().getStatus();
        onChanged();
        return this;
      }
      /**
       * <code>string status = 10;</code>
       * @param value The bytes for status to set.
       * @return This builder for chaining.
       */
      public Builder setStatusBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000010;
        status_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object taskLocality_ = "";
      /**
       * <code>string task_locality = 11;</code>
       * @return Whether the taskLocality field is set.
       */
      public boolean hasTaskLocality() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>string task_locality = 11;</code>
       * @return The taskLocality.
       */
      public java.lang.String getTaskLocality() {
        java.lang.Object ref = taskLocality_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          taskLocality_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string task_locality = 11;</code>
       * @return The bytes for taskLocality.
       */
      public com.google.protobuf.ByteString
          getTaskLocalityBytes() {
        java.lang.Object ref = taskLocality_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          taskLocality_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string task_locality = 11;</code>
       * @param value The taskLocality to set.
       * @return This builder for chaining.
       */
      public Builder setTaskLocality(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        taskLocality_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string task_locality = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearTaskLocality() {
        bitField0_ = (bitField0_ & ~0x00000020);
        taskLocality_ = getDefaultInstance().getTaskLocality();
        onChanged();
        return this;
      }
      /**
       * <code>string task_locality = 11;</code>
       * @param value The bytes for taskLocality to set.
       * @return This builder for chaining.
       */
      public Builder setTaskLocalityBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000020;
        taskLocality_ = value;
        onChanged();
        return this;
      }

      private boolean speculative_ ;
      /**
       * <code>bool speculative = 12;</code>
       * @return The speculative.
       */
      @java.lang.Override
      public boolean getSpeculative() {
        return speculative_;
      }
      /**
       * <code>bool speculative = 12;</code>
       * @param value The speculative to set.
       * @return This builder for chaining.
       */
      public Builder setSpeculative(boolean value) {
        
        speculative_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool speculative = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearSpeculative() {
        
        speculative_ = false;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> accumulatorUpdates_ =
        java.util.Collections.emptyList();
      private void ensureAccumulatorUpdatesIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          accumulatorUpdates_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo>(accumulatorUpdates_);
          bitField0_ |= 0x00000040;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> accumulatorUpdatesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> getAccumulatorUpdatesList() {
        if (accumulatorUpdatesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(accumulatorUpdates_);
        } else {
          return accumulatorUpdatesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public int getAccumulatorUpdatesCount() {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.size();
        } else {
          return accumulatorUpdatesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.get(index);
        } else {
          return accumulatorUpdatesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder setAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.set(index, value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder setAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.set(index, builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAccumulatorUpdates(org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(index, value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAccumulatorUpdates(
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(index, builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder addAllAccumulatorUpdates(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> values) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, accumulatorUpdates_);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder clearAccumulatorUpdates() {
        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdates_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public Builder removeAccumulatorUpdates(int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.remove(index);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder getAccumulatorUpdatesBuilder(
          int index) {
        return getAccumulatorUpdatesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
          int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.get(index);  } else {
          return accumulatorUpdatesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
           getAccumulatorUpdatesOrBuilderList() {
        if (accumulatorUpdatesBuilder_ != null) {
          return accumulatorUpdatesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(accumulatorUpdates_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder addAccumulatorUpdatesBuilder() {
        return getAccumulatorUpdatesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder addAccumulatorUpdatesBuilder(
          int index) {
        return getAccumulatorUpdatesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 13;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder> 
           getAccumulatorUpdatesBuilderList() {
        return getAccumulatorUpdatesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
          getAccumulatorUpdatesFieldBuilder() {
        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdatesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder>(
                  accumulatorUpdates_,
                  ((bitField0_ & 0x00000040) != 0),
                  getParentForChildren(),
                  isClean());
          accumulatorUpdates_ = null;
        }
        return accumulatorUpdatesBuilder_;
      }

      private java.lang.Object errorMessage_ = "";
      /**
       * <code>string error_message = 14;</code>
       * @return Whether the errorMessage field is set.
       */
      public boolean hasErrorMessage() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>string error_message = 14;</code>
       * @return The errorMessage.
       */
      public java.lang.String getErrorMessage() {
        java.lang.Object ref = errorMessage_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          errorMessage_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string error_message = 14;</code>
       * @return The bytes for errorMessage.
       */
      public com.google.protobuf.ByteString
          getErrorMessageBytes() {
        java.lang.Object ref = errorMessage_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          errorMessage_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string error_message = 14;</code>
       * @param value The errorMessage to set.
       * @return This builder for chaining.
       */
      public Builder setErrorMessage(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        errorMessage_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string error_message = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearErrorMessage() {
        bitField0_ = (bitField0_ & ~0x00000080);
        errorMessage_ = getDefaultInstance().getErrorMessage();
        onChanged();
        return this;
      }
      /**
       * <code>string error_message = 14;</code>
       * @param value The bytes for errorMessage to set.
       * @return This builder for chaining.
       */
      public Builder setErrorMessageBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000080;
        errorMessage_ = value;
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.TaskMetrics taskMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.TaskMetrics, org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.TaskMetricsOrBuilder> taskMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       * @return Whether the taskMetrics field is set.
       */
      public boolean hasTaskMetrics() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       * @return The taskMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetrics getTaskMetrics() {
        if (taskMetricsBuilder_ == null) {
          return taskMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.getDefaultInstance() : taskMetrics_;
        } else {
          return taskMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       */
      public Builder setTaskMetrics(org.apache.spark.status.protobuf.StoreTypes.TaskMetrics value) {
        if (taskMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          taskMetrics_ = value;
          onChanged();
        } else {
          taskMetricsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       */
      public Builder setTaskMetrics(
          org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.Builder builderForValue) {
        if (taskMetricsBuilder_ == null) {
          taskMetrics_ = builderForValue.build();
          onChanged();
        } else {
          taskMetricsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       */
      public Builder mergeTaskMetrics(org.apache.spark.status.protobuf.StoreTypes.TaskMetrics value) {
        if (taskMetricsBuilder_ == null) {
          if (((bitField0_ & 0x00000100) != 0) &&
              taskMetrics_ != null &&
              taskMetrics_ != org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.getDefaultInstance()) {
            taskMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.newBuilder(taskMetrics_).mergeFrom(value).buildPartial();
          } else {
            taskMetrics_ = value;
          }
          onChanged();
        } else {
          taskMetricsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       */
      public Builder clearTaskMetrics() {
        if (taskMetricsBuilder_ == null) {
          taskMetrics_ = null;
          onChanged();
        } else {
          taskMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.Builder getTaskMetricsBuilder() {
        bitField0_ |= 0x00000100;
        onChanged();
        return getTaskMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetricsOrBuilder getTaskMetricsOrBuilder() {
        if (taskMetricsBuilder_ != null) {
          return taskMetricsBuilder_.getMessageOrBuilder();
        } else {
          return taskMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.getDefaultInstance() : taskMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetrics task_metrics = 15;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.TaskMetrics, org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.TaskMetricsOrBuilder> 
          getTaskMetricsFieldBuilder() {
        if (taskMetricsBuilder_ == null) {
          taskMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.TaskMetrics, org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.TaskMetricsOrBuilder>(
                  getTaskMetrics(),
                  getParentForChildren(),
                  isClean());
          taskMetrics_ = null;
        }
        return taskMetricsBuilder_;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> executorLogs_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetExecutorLogs() {
        if (executorLogs_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ExecutorLogsDefaultEntryHolder.defaultEntry);
        }
        return executorLogs_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableExecutorLogs() {
        onChanged();;
        if (executorLogs_ == null) {
          executorLogs_ = com.google.protobuf.MapField.newMapField(
              ExecutorLogsDefaultEntryHolder.defaultEntry);
        }
        if (!executorLogs_.isMutable()) {
          executorLogs_ = executorLogs_.copy();
        }
        return executorLogs_;
      }

      public int getExecutorLogsCount() {
        return internalGetExecutorLogs().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 16;</code>
       */

      @java.lang.Override
      public boolean containsExecutorLogs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetExecutorLogs().getMap().containsKey(key);
      }
      /**
       * Use {@link #getExecutorLogsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getExecutorLogs() {
        return getExecutorLogsMap();
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 16;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getExecutorLogsMap() {
        return internalGetExecutorLogs().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 16;</code>
       */
      @java.lang.Override

      public java.lang.String getExecutorLogsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetExecutorLogs().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 16;</code>
       */
      @java.lang.Override

      public java.lang.String getExecutorLogsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetExecutorLogs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearExecutorLogs() {
        internalGetMutableExecutorLogs().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 16;</code>
       */

      public Builder removeExecutorLogs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableExecutorLogs().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableExecutorLogs() {
        return internalGetMutableExecutorLogs().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 16;</code>
       */
      public Builder putExecutorLogs(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableExecutorLogs().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; executor_logs = 16;</code>
       */

      public Builder putAllExecutorLogs(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableExecutorLogs().getMutableMap()
            .putAll(values);
        return this;
      }

      private long schedulerDelay_ ;
      /**
       * <code>int64 scheduler_delay = 17;</code>
       * @return The schedulerDelay.
       */
      @java.lang.Override
      public long getSchedulerDelay() {
        return schedulerDelay_;
      }
      /**
       * <code>int64 scheduler_delay = 17;</code>
       * @param value The schedulerDelay to set.
       * @return This builder for chaining.
       */
      public Builder setSchedulerDelay(long value) {
        
        schedulerDelay_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 scheduler_delay = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearSchedulerDelay() {
        
        schedulerDelay_ = 0L;
        onChanged();
        return this;
      }

      private long gettingResultTime_ ;
      /**
       * <code>int64 getting_result_time = 18;</code>
       * @return The gettingResultTime.
       */
      @java.lang.Override
      public long getGettingResultTime() {
        return gettingResultTime_;
      }
      /**
       * <code>int64 getting_result_time = 18;</code>
       * @param value The gettingResultTime to set.
       * @return This builder for chaining.
       */
      public Builder setGettingResultTime(long value) {
        
        gettingResultTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 getting_result_time = 18;</code>
       * @return This builder for chaining.
       */
      public Builder clearGettingResultTime() {
        
        gettingResultTime_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.TaskData)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.TaskData)
    private static final org.apache.spark.status.protobuf.StoreTypes.TaskData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.TaskData();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<TaskData>
        PARSER = new com.google.protobuf.AbstractParser<TaskData>() {
      @java.lang.Override
      public TaskData parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TaskData(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<TaskData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<TaskData> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StageDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.StageData)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
     * @return The enum numeric value on the wire for status.
     */
    int getStatusValue();
    /**
     * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
     * @return The status.
     */
    org.apache.spark.status.protobuf.StoreTypes.StageStatus getStatus();

    /**
     * <code>int64 stage_id = 2;</code>
     * @return The stageId.
     */
    long getStageId();

    /**
     * <code>int32 attempt_id = 3;</code>
     * @return The attemptId.
     */
    int getAttemptId();

    /**
     * <code>int32 num_tasks = 4;</code>
     * @return The numTasks.
     */
    int getNumTasks();

    /**
     * <code>int32 num_active_tasks = 5;</code>
     * @return The numActiveTasks.
     */
    int getNumActiveTasks();

    /**
     * <code>int32 num_complete_tasks = 6;</code>
     * @return The numCompleteTasks.
     */
    int getNumCompleteTasks();

    /**
     * <code>int32 num_failed_tasks = 7;</code>
     * @return The numFailedTasks.
     */
    int getNumFailedTasks();

    /**
     * <code>int32 num_killed_tasks = 8;</code>
     * @return The numKilledTasks.
     */
    int getNumKilledTasks();

    /**
     * <code>int32 num_completed_indices = 9;</code>
     * @return The numCompletedIndices.
     */
    int getNumCompletedIndices();

    /**
     * <code>int64 submission_time = 10;</code>
     * @return Whether the submissionTime field is set.
     */
    boolean hasSubmissionTime();
    /**
     * <code>int64 submission_time = 10;</code>
     * @return The submissionTime.
     */
    long getSubmissionTime();

    /**
     * <code>int64 first_task_launched_time = 11;</code>
     * @return Whether the firstTaskLaunchedTime field is set.
     */
    boolean hasFirstTaskLaunchedTime();
    /**
     * <code>int64 first_task_launched_time = 11;</code>
     * @return The firstTaskLaunchedTime.
     */
    long getFirstTaskLaunchedTime();

    /**
     * <code>int64 completion_time = 12;</code>
     * @return Whether the completionTime field is set.
     */
    boolean hasCompletionTime();
    /**
     * <code>int64 completion_time = 12;</code>
     * @return The completionTime.
     */
    long getCompletionTime();

    /**
     * <code>string failure_reason = 13;</code>
     * @return Whether the failureReason field is set.
     */
    boolean hasFailureReason();
    /**
     * <code>string failure_reason = 13;</code>
     * @return The failureReason.
     */
    java.lang.String getFailureReason();
    /**
     * <code>string failure_reason = 13;</code>
     * @return The bytes for failureReason.
     */
    com.google.protobuf.ByteString
        getFailureReasonBytes();

    /**
     * <code>int64 executor_deserialize_time = 14;</code>
     * @return The executorDeserializeTime.
     */
    long getExecutorDeserializeTime();

    /**
     * <code>int64 executor_deserialize_cpu_time = 15;</code>
     * @return The executorDeserializeCpuTime.
     */
    long getExecutorDeserializeCpuTime();

    /**
     * <code>int64 executor_run_time = 16;</code>
     * @return The executorRunTime.
     */
    long getExecutorRunTime();

    /**
     * <code>int64 executor_cpu_time = 17;</code>
     * @return The executorCpuTime.
     */
    long getExecutorCpuTime();

    /**
     * <code>int64 result_size = 18;</code>
     * @return The resultSize.
     */
    long getResultSize();

    /**
     * <code>int64 jvm_gc_time = 19;</code>
     * @return The jvmGcTime.
     */
    long getJvmGcTime();

    /**
     * <code>int64 result_serialization_time = 20;</code>
     * @return The resultSerializationTime.
     */
    long getResultSerializationTime();

    /**
     * <code>int64 memory_bytes_spilled = 21;</code>
     * @return The memoryBytesSpilled.
     */
    long getMemoryBytesSpilled();

    /**
     * <code>int64 disk_bytes_spilled = 22;</code>
     * @return The diskBytesSpilled.
     */
    long getDiskBytesSpilled();

    /**
     * <code>int64 peak_execution_memory = 23;</code>
     * @return The peakExecutionMemory.
     */
    long getPeakExecutionMemory();

    /**
     * <code>int64 input_bytes = 24;</code>
     * @return The inputBytes.
     */
    long getInputBytes();

    /**
     * <code>int64 input_records = 25;</code>
     * @return The inputRecords.
     */
    long getInputRecords();

    /**
     * <code>int64 output_bytes = 26;</code>
     * @return The outputBytes.
     */
    long getOutputBytes();

    /**
     * <code>int64 output_records = 27;</code>
     * @return The outputRecords.
     */
    long getOutputRecords();

    /**
     * <code>int64 shuffle_remote_blocks_fetched = 28;</code>
     * @return The shuffleRemoteBlocksFetched.
     */
    long getShuffleRemoteBlocksFetched();

    /**
     * <code>int64 shuffle_local_blocks_fetched = 29;</code>
     * @return The shuffleLocalBlocksFetched.
     */
    long getShuffleLocalBlocksFetched();

    /**
     * <code>int64 shuffle_fetch_wait_time = 30;</code>
     * @return The shuffleFetchWaitTime.
     */
    long getShuffleFetchWaitTime();

    /**
     * <code>int64 shuffle_remote_bytes_read = 31;</code>
     * @return The shuffleRemoteBytesRead.
     */
    long getShuffleRemoteBytesRead();

    /**
     * <code>int64 shuffle_remote_bytes_read_to_disk = 32;</code>
     * @return The shuffleRemoteBytesReadToDisk.
     */
    long getShuffleRemoteBytesReadToDisk();

    /**
     * <code>int64 shuffle_local_bytes_read = 33;</code>
     * @return The shuffleLocalBytesRead.
     */
    long getShuffleLocalBytesRead();

    /**
     * <code>int64 shuffle_read_bytes = 34;</code>
     * @return The shuffleReadBytes.
     */
    long getShuffleReadBytes();

    /**
     * <code>int64 shuffle_read_records = 35;</code>
     * @return The shuffleReadRecords.
     */
    long getShuffleReadRecords();

    /**
     * <code>int64 shuffle_write_bytes = 36;</code>
     * @return The shuffleWriteBytes.
     */
    long getShuffleWriteBytes();

    /**
     * <code>int64 shuffle_write_time = 37;</code>
     * @return The shuffleWriteTime.
     */
    long getShuffleWriteTime();

    /**
     * <code>int64 shuffle_write_records = 38;</code>
     * @return The shuffleWriteRecords.
     */
    long getShuffleWriteRecords();

    /**
     * <code>string name = 39;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 39;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 39;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>string description = 40;</code>
     * @return Whether the description field is set.
     */
    boolean hasDescription();
    /**
     * <code>string description = 40;</code>
     * @return The description.
     */
    java.lang.String getDescription();
    /**
     * <code>string description = 40;</code>
     * @return The bytes for description.
     */
    com.google.protobuf.ByteString
        getDescriptionBytes();

    /**
     * <code>string details = 41;</code>
     * @return Whether the details field is set.
     */
    boolean hasDetails();
    /**
     * <code>string details = 41;</code>
     * @return The details.
     */
    java.lang.String getDetails();
    /**
     * <code>string details = 41;</code>
     * @return The bytes for details.
     */
    com.google.protobuf.ByteString
        getDetailsBytes();

    /**
     * <code>string scheduling_pool = 42;</code>
     * @return Whether the schedulingPool field is set.
     */
    boolean hasSchedulingPool();
    /**
     * <code>string scheduling_pool = 42;</code>
     * @return The schedulingPool.
     */
    java.lang.String getSchedulingPool();
    /**
     * <code>string scheduling_pool = 42;</code>
     * @return The bytes for schedulingPool.
     */
    com.google.protobuf.ByteString
        getSchedulingPoolBytes();

    /**
     * <code>repeated int64 rdd_ids = 43;</code>
     * @return A list containing the rddIds.
     */
    java.util.List<java.lang.Long> getRddIdsList();
    /**
     * <code>repeated int64 rdd_ids = 43;</code>
     * @return The count of rddIds.
     */
    int getRddIdsCount();
    /**
     * <code>repeated int64 rdd_ids = 43;</code>
     * @param index The index of the element to return.
     * @return The rddIds at the given index.
     */
    long getRddIds(int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> 
        getAccumulatorUpdatesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    int getAccumulatorUpdatesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
        getAccumulatorUpdatesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
        int index);

    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */
    int getTasksCount();
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */
    boolean containsTasks(
        long key);
    /**
     * Use {@link #getTasksMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>
    getTasks();
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */
    java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>
    getTasksMap();
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.TaskData getTasksOrDefault(
        long key,
        org.apache.spark.status.protobuf.StoreTypes.TaskData defaultValue);
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.TaskData getTasksOrThrow(
        long key);

    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */
    int getExecutorSummaryCount();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */
    boolean containsExecutorSummary(
        java.lang.String key);
    /**
     * Use {@link #getExecutorSummaryMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>
    getExecutorSummary();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */
    java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>
    getExecutorSummaryMap();
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getExecutorSummaryOrDefault(
        java.lang.String key,
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary defaultValue);
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */

    org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getExecutorSummaryOrThrow(
        java.lang.String key);

    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
     * @return Whether the speculationSummary field is set.
     */
    boolean hasSpeculationSummary();
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
     * @return The speculationSummary.
     */
    org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getSpeculationSummary();
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder getSpeculationSummaryOrBuilder();

    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */
    int getKilledTasksSummaryCount();
    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */
    boolean containsKilledTasksSummary(
        java.lang.String key);
    /**
     * Use {@link #getKilledTasksSummaryMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.Integer>
    getKilledTasksSummary();
    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */
    java.util.Map<java.lang.String, java.lang.Integer>
    getKilledTasksSummaryMap();
    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */

    int getKilledTasksSummaryOrDefault(
        java.lang.String key,
        int defaultValue);
    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */

    int getKilledTasksSummaryOrThrow(
        java.lang.String key);

    /**
     * <code>int32 resource_profile_id = 49;</code>
     * @return The resourceProfileId.
     */
    int getResourceProfileId();

    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
     * @return Whether the peakExecutorMetrics field is set.
     */
    boolean hasPeakExecutorMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
     * @return The peakExecutorMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakExecutorMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakExecutorMetricsOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
     * @return Whether the taskMetricsDistributions field is set.
     */
    boolean hasTaskMetricsDistributions();
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
     * @return The taskMetricsDistributions.
     */
    org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions getTaskMetricsDistributions();
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributionsOrBuilder getTaskMetricsDistributionsOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
     * @return Whether the executorMetricsDistributions field is set.
     */
    boolean hasExecutorMetricsDistributions();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
     * @return The executorMetricsDistributions.
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions getExecutorMetricsDistributions();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributionsOrBuilder getExecutorMetricsDistributionsOrBuilder();

    /**
     * <code>int64 shuffle_corrupt_merged_block_chunks = 53;</code>
     * @return The shuffleCorruptMergedBlockChunks.
     */
    long getShuffleCorruptMergedBlockChunks();

    /**
     * <code>int64 shuffle_merged_fetch_fallback_count = 54;</code>
     * @return The shuffleMergedFetchFallbackCount.
     */
    long getShuffleMergedFetchFallbackCount();

    /**
     * <code>int64 shuffle_merged_remote_blocks_fetched = 55;</code>
     * @return The shuffleMergedRemoteBlocksFetched.
     */
    long getShuffleMergedRemoteBlocksFetched();

    /**
     * <code>int64 shuffle_merged_local_blocks_fetched = 56;</code>
     * @return The shuffleMergedLocalBlocksFetched.
     */
    long getShuffleMergedLocalBlocksFetched();

    /**
     * <code>int64 shuffle_merged_remote_chunks_fetched = 57;</code>
     * @return The shuffleMergedRemoteChunksFetched.
     */
    long getShuffleMergedRemoteChunksFetched();

    /**
     * <code>int64 shuffle_merged_local_chunks_fetched = 58;</code>
     * @return The shuffleMergedLocalChunksFetched.
     */
    long getShuffleMergedLocalChunksFetched();

    /**
     * <code>int64 shuffle_merged_remote_bytes_read = 59;</code>
     * @return The shuffleMergedRemoteBytesRead.
     */
    long getShuffleMergedRemoteBytesRead();

    /**
     * <code>int64 shuffle_merged_local_bytes_read = 60;</code>
     * @return The shuffleMergedLocalBytesRead.
     */
    long getShuffleMergedLocalBytesRead();

    /**
     * <code>int64 shuffle_remote_reqs_duration = 61;</code>
     * @return The shuffleRemoteReqsDuration.
     */
    long getShuffleRemoteReqsDuration();

    /**
     * <code>int64 shuffle_merged_remote_reqs_duration = 62;</code>
     * @return The shuffleMergedRemoteReqsDuration.
     */
    long getShuffleMergedRemoteReqsDuration();

    /**
     * <code>bool is_shuffle_push_enabled = 63;</code>
     * @return The isShufflePushEnabled.
     */
    boolean getIsShufflePushEnabled();

    /**
     * <code>int32 shuffle_mergers_count = 64;</code>
     * @return The shuffleMergersCount.
     */
    int getShuffleMergersCount();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.StageData}
   */
  public static final class StageData extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.StageData)
      StageDataOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StageData.newBuilder() to construct.
    private StageData(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StageData() {
      status_ = 0;
      failureReason_ = "";
      name_ = "";
      description_ = "";
      details_ = "";
      schedulingPool_ = "";
      rddIds_ = emptyLongList();
      accumulatorUpdates_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StageData();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StageData(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();

              status_ = rawValue;
              break;
            }
            case 16: {

              stageId_ = input.readInt64();
              break;
            }
            case 24: {

              attemptId_ = input.readInt32();
              break;
            }
            case 32: {

              numTasks_ = input.readInt32();
              break;
            }
            case 40: {

              numActiveTasks_ = input.readInt32();
              break;
            }
            case 48: {

              numCompleteTasks_ = input.readInt32();
              break;
            }
            case 56: {

              numFailedTasks_ = input.readInt32();
              break;
            }
            case 64: {

              numKilledTasks_ = input.readInt32();
              break;
            }
            case 72: {

              numCompletedIndices_ = input.readInt32();
              break;
            }
            case 80: {
              bitField0_ |= 0x00000001;
              submissionTime_ = input.readInt64();
              break;
            }
            case 88: {
              bitField0_ |= 0x00000002;
              firstTaskLaunchedTime_ = input.readInt64();
              break;
            }
            case 96: {
              bitField0_ |= 0x00000004;
              completionTime_ = input.readInt64();
              break;
            }
            case 106: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000008;
              failureReason_ = s;
              break;
            }
            case 112: {

              executorDeserializeTime_ = input.readInt64();
              break;
            }
            case 120: {

              executorDeserializeCpuTime_ = input.readInt64();
              break;
            }
            case 128: {

              executorRunTime_ = input.readInt64();
              break;
            }
            case 136: {

              executorCpuTime_ = input.readInt64();
              break;
            }
            case 144: {

              resultSize_ = input.readInt64();
              break;
            }
            case 152: {

              jvmGcTime_ = input.readInt64();
              break;
            }
            case 160: {

              resultSerializationTime_ = input.readInt64();
              break;
            }
            case 168: {

              memoryBytesSpilled_ = input.readInt64();
              break;
            }
            case 176: {

              diskBytesSpilled_ = input.readInt64();
              break;
            }
            case 184: {

              peakExecutionMemory_ = input.readInt64();
              break;
            }
            case 192: {

              inputBytes_ = input.readInt64();
              break;
            }
            case 200: {

              inputRecords_ = input.readInt64();
              break;
            }
            case 208: {

              outputBytes_ = input.readInt64();
              break;
            }
            case 216: {

              outputRecords_ = input.readInt64();
              break;
            }
            case 224: {

              shuffleRemoteBlocksFetched_ = input.readInt64();
              break;
            }
            case 232: {

              shuffleLocalBlocksFetched_ = input.readInt64();
              break;
            }
            case 240: {

              shuffleFetchWaitTime_ = input.readInt64();
              break;
            }
            case 248: {

              shuffleRemoteBytesRead_ = input.readInt64();
              break;
            }
            case 256: {

              shuffleRemoteBytesReadToDisk_ = input.readInt64();
              break;
            }
            case 264: {

              shuffleLocalBytesRead_ = input.readInt64();
              break;
            }
            case 272: {

              shuffleReadBytes_ = input.readInt64();
              break;
            }
            case 280: {

              shuffleReadRecords_ = input.readInt64();
              break;
            }
            case 288: {

              shuffleWriteBytes_ = input.readInt64();
              break;
            }
            case 296: {

              shuffleWriteTime_ = input.readInt64();
              break;
            }
            case 304: {

              shuffleWriteRecords_ = input.readInt64();
              break;
            }
            case 314: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000010;
              name_ = s;
              break;
            }
            case 322: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000020;
              description_ = s;
              break;
            }
            case 330: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000040;
              details_ = s;
              break;
            }
            case 338: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000080;
              schedulingPool_ = s;
              break;
            }
            case 344: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                rddIds_ = newLongList();
                mutable_bitField0_ |= 0x00000100;
              }
              rddIds_.addLong(input.readInt64());
              break;
            }
            case 346: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000100) != 0) && input.getBytesUntilLimit() > 0) {
                rddIds_ = newLongList();
                mutable_bitField0_ |= 0x00000100;
              }
              while (input.getBytesUntilLimit() > 0) {
                rddIds_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 354: {
              if (!((mutable_bitField0_ & 0x00000200) != 0)) {
                accumulatorUpdates_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo>();
                mutable_bitField0_ |= 0x00000200;
              }
              accumulatorUpdates_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.parser(), extensionRegistry));
              break;
            }
            case 362: {
              if (!((mutable_bitField0_ & 0x00000400) != 0)) {
                tasks_ = com.google.protobuf.MapField.newMapField(
                    TasksDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000400;
              }
              com.google.protobuf.MapEntry<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>
              tasks__ = input.readMessage(
                  TasksDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              tasks_.getMutableMap().put(
                  tasks__.getKey(), tasks__.getValue());
              break;
            }
            case 370: {
              if (!((mutable_bitField0_ & 0x00000800) != 0)) {
                executorSummary_ = com.google.protobuf.MapField.newMapField(
                    ExecutorSummaryDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000800;
              }
              com.google.protobuf.MapEntry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>
              executorSummary__ = input.readMessage(
                  ExecutorSummaryDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              executorSummary_.getMutableMap().put(
                  executorSummary__.getKey(), executorSummary__.getValue());
              break;
            }
            case 378: {
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder subBuilder = null;
              if (((bitField0_ & 0x00000100) != 0)) {
                subBuilder = speculationSummary_.toBuilder();
              }
              speculationSummary_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(speculationSummary_);
                speculationSummary_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000100;
              break;
            }
            case 386: {
              if (!((mutable_bitField0_ & 0x00002000) != 0)) {
                killedTasksSummary_ = com.google.protobuf.MapField.newMapField(
                    KilledTasksSummaryDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00002000;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.Integer>
              killedTasksSummary__ = input.readMessage(
                  KilledTasksSummaryDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              killedTasksSummary_.getMutableMap().put(
                  killedTasksSummary__.getKey(), killedTasksSummary__.getValue());
              break;
            }
            case 392: {

              resourceProfileId_ = input.readInt32();
              break;
            }
            case 402: {
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder subBuilder = null;
              if (((bitField0_ & 0x00000200) != 0)) {
                subBuilder = peakExecutorMetrics_.toBuilder();
              }
              peakExecutorMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(peakExecutorMetrics_);
                peakExecutorMetrics_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000200;
              break;
            }
            case 410: {
              org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.Builder subBuilder = null;
              if (((bitField0_ & 0x00000400) != 0)) {
                subBuilder = taskMetricsDistributions_.toBuilder();
              }
              taskMetricsDistributions_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(taskMetricsDistributions_);
                taskMetricsDistributions_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000400;
              break;
            }
            case 418: {
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.Builder subBuilder = null;
              if (((bitField0_ & 0x00000800) != 0)) {
                subBuilder = executorMetricsDistributions_.toBuilder();
              }
              executorMetricsDistributions_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(executorMetricsDistributions_);
                executorMetricsDistributions_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000800;
              break;
            }
            case 424: {

              shuffleCorruptMergedBlockChunks_ = input.readInt64();
              break;
            }
            case 432: {

              shuffleMergedFetchFallbackCount_ = input.readInt64();
              break;
            }
            case 440: {

              shuffleMergedRemoteBlocksFetched_ = input.readInt64();
              break;
            }
            case 448: {

              shuffleMergedLocalBlocksFetched_ = input.readInt64();
              break;
            }
            case 456: {

              shuffleMergedRemoteChunksFetched_ = input.readInt64();
              break;
            }
            case 464: {

              shuffleMergedLocalChunksFetched_ = input.readInt64();
              break;
            }
            case 472: {

              shuffleMergedRemoteBytesRead_ = input.readInt64();
              break;
            }
            case 480: {

              shuffleMergedLocalBytesRead_ = input.readInt64();
              break;
            }
            case 488: {

              shuffleRemoteReqsDuration_ = input.readInt64();
              break;
            }
            case 496: {

              shuffleMergedRemoteReqsDuration_ = input.readInt64();
              break;
            }
            case 504: {

              isShufflePushEnabled_ = input.readBool();
              break;
            }
            case 512: {

              shuffleMergersCount_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          rddIds_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000200) != 0)) {
          accumulatorUpdates_ = java.util.Collections.unmodifiableList(accumulatorUpdates_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageData_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 45:
          return internalGetTasks();
        case 46:
          return internalGetExecutorSummary();
        case 48:
          return internalGetKilledTasksSummary();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.StageData.class, org.apache.spark.status.protobuf.StoreTypes.StageData.Builder.class);
    }

    private int bitField0_;
    public static final int STATUS_FIELD_NUMBER = 1;
    private int status_;
    /**
     * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
     * @return The enum numeric value on the wire for status.
     */
    @java.lang.Override public int getStatusValue() {
      return status_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
     * @return The status.
     */
    @java.lang.Override public org.apache.spark.status.protobuf.StoreTypes.StageStatus getStatus() {
      @SuppressWarnings("deprecation")
      org.apache.spark.status.protobuf.StoreTypes.StageStatus result = org.apache.spark.status.protobuf.StoreTypes.StageStatus.valueOf(status_);
      return result == null ? org.apache.spark.status.protobuf.StoreTypes.StageStatus.UNRECOGNIZED : result;
    }

    public static final int STAGE_ID_FIELD_NUMBER = 2;
    private long stageId_;
    /**
     * <code>int64 stage_id = 2;</code>
     * @return The stageId.
     */
    @java.lang.Override
    public long getStageId() {
      return stageId_;
    }

    public static final int ATTEMPT_ID_FIELD_NUMBER = 3;
    private int attemptId_;
    /**
     * <code>int32 attempt_id = 3;</code>
     * @return The attemptId.
     */
    @java.lang.Override
    public int getAttemptId() {
      return attemptId_;
    }

    public static final int NUM_TASKS_FIELD_NUMBER = 4;
    private int numTasks_;
    /**
     * <code>int32 num_tasks = 4;</code>
     * @return The numTasks.
     */
    @java.lang.Override
    public int getNumTasks() {
      return numTasks_;
    }

    public static final int NUM_ACTIVE_TASKS_FIELD_NUMBER = 5;
    private int numActiveTasks_;
    /**
     * <code>int32 num_active_tasks = 5;</code>
     * @return The numActiveTasks.
     */
    @java.lang.Override
    public int getNumActiveTasks() {
      return numActiveTasks_;
    }

    public static final int NUM_COMPLETE_TASKS_FIELD_NUMBER = 6;
    private int numCompleteTasks_;
    /**
     * <code>int32 num_complete_tasks = 6;</code>
     * @return The numCompleteTasks.
     */
    @java.lang.Override
    public int getNumCompleteTasks() {
      return numCompleteTasks_;
    }

    public static final int NUM_FAILED_TASKS_FIELD_NUMBER = 7;
    private int numFailedTasks_;
    /**
     * <code>int32 num_failed_tasks = 7;</code>
     * @return The numFailedTasks.
     */
    @java.lang.Override
    public int getNumFailedTasks() {
      return numFailedTasks_;
    }

    public static final int NUM_KILLED_TASKS_FIELD_NUMBER = 8;
    private int numKilledTasks_;
    /**
     * <code>int32 num_killed_tasks = 8;</code>
     * @return The numKilledTasks.
     */
    @java.lang.Override
    public int getNumKilledTasks() {
      return numKilledTasks_;
    }

    public static final int NUM_COMPLETED_INDICES_FIELD_NUMBER = 9;
    private int numCompletedIndices_;
    /**
     * <code>int32 num_completed_indices = 9;</code>
     * @return The numCompletedIndices.
     */
    @java.lang.Override
    public int getNumCompletedIndices() {
      return numCompletedIndices_;
    }

    public static final int SUBMISSION_TIME_FIELD_NUMBER = 10;
    private long submissionTime_;
    /**
     * <code>int64 submission_time = 10;</code>
     * @return Whether the submissionTime field is set.
     */
    @java.lang.Override
    public boolean hasSubmissionTime() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>int64 submission_time = 10;</code>
     * @return The submissionTime.
     */
    @java.lang.Override
    public long getSubmissionTime() {
      return submissionTime_;
    }

    public static final int FIRST_TASK_LAUNCHED_TIME_FIELD_NUMBER = 11;
    private long firstTaskLaunchedTime_;
    /**
     * <code>int64 first_task_launched_time = 11;</code>
     * @return Whether the firstTaskLaunchedTime field is set.
     */
    @java.lang.Override
    public boolean hasFirstTaskLaunchedTime() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>int64 first_task_launched_time = 11;</code>
     * @return The firstTaskLaunchedTime.
     */
    @java.lang.Override
    public long getFirstTaskLaunchedTime() {
      return firstTaskLaunchedTime_;
    }

    public static final int COMPLETION_TIME_FIELD_NUMBER = 12;
    private long completionTime_;
    /**
     * <code>int64 completion_time = 12;</code>
     * @return Whether the completionTime field is set.
     */
    @java.lang.Override
    public boolean hasCompletionTime() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>int64 completion_time = 12;</code>
     * @return The completionTime.
     */
    @java.lang.Override
    public long getCompletionTime() {
      return completionTime_;
    }

    public static final int FAILURE_REASON_FIELD_NUMBER = 13;
    private volatile java.lang.Object failureReason_;
    /**
     * <code>string failure_reason = 13;</code>
     * @return Whether the failureReason field is set.
     */
    @java.lang.Override
    public boolean hasFailureReason() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>string failure_reason = 13;</code>
     * @return The failureReason.
     */
    @java.lang.Override
    public java.lang.String getFailureReason() {
      java.lang.Object ref = failureReason_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        failureReason_ = s;
        return s;
      }
    }
    /**
     * <code>string failure_reason = 13;</code>
     * @return The bytes for failureReason.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getFailureReasonBytes() {
      java.lang.Object ref = failureReason_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        failureReason_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER = 14;
    private long executorDeserializeTime_;
    /**
     * <code>int64 executor_deserialize_time = 14;</code>
     * @return The executorDeserializeTime.
     */
    @java.lang.Override
    public long getExecutorDeserializeTime() {
      return executorDeserializeTime_;
    }

    public static final int EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER = 15;
    private long executorDeserializeCpuTime_;
    /**
     * <code>int64 executor_deserialize_cpu_time = 15;</code>
     * @return The executorDeserializeCpuTime.
     */
    @java.lang.Override
    public long getExecutorDeserializeCpuTime() {
      return executorDeserializeCpuTime_;
    }

    public static final int EXECUTOR_RUN_TIME_FIELD_NUMBER = 16;
    private long executorRunTime_;
    /**
     * <code>int64 executor_run_time = 16;</code>
     * @return The executorRunTime.
     */
    @java.lang.Override
    public long getExecutorRunTime() {
      return executorRunTime_;
    }

    public static final int EXECUTOR_CPU_TIME_FIELD_NUMBER = 17;
    private long executorCpuTime_;
    /**
     * <code>int64 executor_cpu_time = 17;</code>
     * @return The executorCpuTime.
     */
    @java.lang.Override
    public long getExecutorCpuTime() {
      return executorCpuTime_;
    }

    public static final int RESULT_SIZE_FIELD_NUMBER = 18;
    private long resultSize_;
    /**
     * <code>int64 result_size = 18;</code>
     * @return The resultSize.
     */
    @java.lang.Override
    public long getResultSize() {
      return resultSize_;
    }

    public static final int JVM_GC_TIME_FIELD_NUMBER = 19;
    private long jvmGcTime_;
    /**
     * <code>int64 jvm_gc_time = 19;</code>
     * @return The jvmGcTime.
     */
    @java.lang.Override
    public long getJvmGcTime() {
      return jvmGcTime_;
    }

    public static final int RESULT_SERIALIZATION_TIME_FIELD_NUMBER = 20;
    private long resultSerializationTime_;
    /**
     * <code>int64 result_serialization_time = 20;</code>
     * @return The resultSerializationTime.
     */
    @java.lang.Override
    public long getResultSerializationTime() {
      return resultSerializationTime_;
    }

    public static final int MEMORY_BYTES_SPILLED_FIELD_NUMBER = 21;
    private long memoryBytesSpilled_;
    /**
     * <code>int64 memory_bytes_spilled = 21;</code>
     * @return The memoryBytesSpilled.
     */
    @java.lang.Override
    public long getMemoryBytesSpilled() {
      return memoryBytesSpilled_;
    }

    public static final int DISK_BYTES_SPILLED_FIELD_NUMBER = 22;
    private long diskBytesSpilled_;
    /**
     * <code>int64 disk_bytes_spilled = 22;</code>
     * @return The diskBytesSpilled.
     */
    @java.lang.Override
    public long getDiskBytesSpilled() {
      return diskBytesSpilled_;
    }

    public static final int PEAK_EXECUTION_MEMORY_FIELD_NUMBER = 23;
    private long peakExecutionMemory_;
    /**
     * <code>int64 peak_execution_memory = 23;</code>
     * @return The peakExecutionMemory.
     */
    @java.lang.Override
    public long getPeakExecutionMemory() {
      return peakExecutionMemory_;
    }

    public static final int INPUT_BYTES_FIELD_NUMBER = 24;
    private long inputBytes_;
    /**
     * <code>int64 input_bytes = 24;</code>
     * @return The inputBytes.
     */
    @java.lang.Override
    public long getInputBytes() {
      return inputBytes_;
    }

    public static final int INPUT_RECORDS_FIELD_NUMBER = 25;
    private long inputRecords_;
    /**
     * <code>int64 input_records = 25;</code>
     * @return The inputRecords.
     */
    @java.lang.Override
    public long getInputRecords() {
      return inputRecords_;
    }

    public static final int OUTPUT_BYTES_FIELD_NUMBER = 26;
    private long outputBytes_;
    /**
     * <code>int64 output_bytes = 26;</code>
     * @return The outputBytes.
     */
    @java.lang.Override
    public long getOutputBytes() {
      return outputBytes_;
    }

    public static final int OUTPUT_RECORDS_FIELD_NUMBER = 27;
    private long outputRecords_;
    /**
     * <code>int64 output_records = 27;</code>
     * @return The outputRecords.
     */
    @java.lang.Override
    public long getOutputRecords() {
      return outputRecords_;
    }

    public static final int SHUFFLE_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER = 28;
    private long shuffleRemoteBlocksFetched_;
    /**
     * <code>int64 shuffle_remote_blocks_fetched = 28;</code>
     * @return The shuffleRemoteBlocksFetched.
     */
    @java.lang.Override
    public long getShuffleRemoteBlocksFetched() {
      return shuffleRemoteBlocksFetched_;
    }

    public static final int SHUFFLE_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER = 29;
    private long shuffleLocalBlocksFetched_;
    /**
     * <code>int64 shuffle_local_blocks_fetched = 29;</code>
     * @return The shuffleLocalBlocksFetched.
     */
    @java.lang.Override
    public long getShuffleLocalBlocksFetched() {
      return shuffleLocalBlocksFetched_;
    }

    public static final int SHUFFLE_FETCH_WAIT_TIME_FIELD_NUMBER = 30;
    private long shuffleFetchWaitTime_;
    /**
     * <code>int64 shuffle_fetch_wait_time = 30;</code>
     * @return The shuffleFetchWaitTime.
     */
    @java.lang.Override
    public long getShuffleFetchWaitTime() {
      return shuffleFetchWaitTime_;
    }

    public static final int SHUFFLE_REMOTE_BYTES_READ_FIELD_NUMBER = 31;
    private long shuffleRemoteBytesRead_;
    /**
     * <code>int64 shuffle_remote_bytes_read = 31;</code>
     * @return The shuffleRemoteBytesRead.
     */
    @java.lang.Override
    public long getShuffleRemoteBytesRead() {
      return shuffleRemoteBytesRead_;
    }

    public static final int SHUFFLE_REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER = 32;
    private long shuffleRemoteBytesReadToDisk_;
    /**
     * <code>int64 shuffle_remote_bytes_read_to_disk = 32;</code>
     * @return The shuffleRemoteBytesReadToDisk.
     */
    @java.lang.Override
    public long getShuffleRemoteBytesReadToDisk() {
      return shuffleRemoteBytesReadToDisk_;
    }

    public static final int SHUFFLE_LOCAL_BYTES_READ_FIELD_NUMBER = 33;
    private long shuffleLocalBytesRead_;
    /**
     * <code>int64 shuffle_local_bytes_read = 33;</code>
     * @return The shuffleLocalBytesRead.
     */
    @java.lang.Override
    public long getShuffleLocalBytesRead() {
      return shuffleLocalBytesRead_;
    }

    public static final int SHUFFLE_READ_BYTES_FIELD_NUMBER = 34;
    private long shuffleReadBytes_;
    /**
     * <code>int64 shuffle_read_bytes = 34;</code>
     * @return The shuffleReadBytes.
     */
    @java.lang.Override
    public long getShuffleReadBytes() {
      return shuffleReadBytes_;
    }

    public static final int SHUFFLE_READ_RECORDS_FIELD_NUMBER = 35;
    private long shuffleReadRecords_;
    /**
     * <code>int64 shuffle_read_records = 35;</code>
     * @return The shuffleReadRecords.
     */
    @java.lang.Override
    public long getShuffleReadRecords() {
      return shuffleReadRecords_;
    }

    public static final int SHUFFLE_WRITE_BYTES_FIELD_NUMBER = 36;
    private long shuffleWriteBytes_;
    /**
     * <code>int64 shuffle_write_bytes = 36;</code>
     * @return The shuffleWriteBytes.
     */
    @java.lang.Override
    public long getShuffleWriteBytes() {
      return shuffleWriteBytes_;
    }

    public static final int SHUFFLE_WRITE_TIME_FIELD_NUMBER = 37;
    private long shuffleWriteTime_;
    /**
     * <code>int64 shuffle_write_time = 37;</code>
     * @return The shuffleWriteTime.
     */
    @java.lang.Override
    public long getShuffleWriteTime() {
      return shuffleWriteTime_;
    }

    public static final int SHUFFLE_WRITE_RECORDS_FIELD_NUMBER = 38;
    private long shuffleWriteRecords_;
    /**
     * <code>int64 shuffle_write_records = 38;</code>
     * @return The shuffleWriteRecords.
     */
    @java.lang.Override
    public long getShuffleWriteRecords() {
      return shuffleWriteRecords_;
    }

    public static final int NAME_FIELD_NUMBER = 39;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 39;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>string name = 39;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 39;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DESCRIPTION_FIELD_NUMBER = 40;
    private volatile java.lang.Object description_;
    /**
     * <code>string description = 40;</code>
     * @return Whether the description field is set.
     */
    @java.lang.Override
    public boolean hasDescription() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>string description = 40;</code>
     * @return The description.
     */
    @java.lang.Override
    public java.lang.String getDescription() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        description_ = s;
        return s;
      }
    }
    /**
     * <code>string description = 40;</code>
     * @return The bytes for description.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDescriptionBytes() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        description_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DETAILS_FIELD_NUMBER = 41;
    private volatile java.lang.Object details_;
    /**
     * <code>string details = 41;</code>
     * @return Whether the details field is set.
     */
    @java.lang.Override
    public boolean hasDetails() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>string details = 41;</code>
     * @return The details.
     */
    @java.lang.Override
    public java.lang.String getDetails() {
      java.lang.Object ref = details_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        details_ = s;
        return s;
      }
    }
    /**
     * <code>string details = 41;</code>
     * @return The bytes for details.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDetailsBytes() {
      java.lang.Object ref = details_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        details_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int SCHEDULING_POOL_FIELD_NUMBER = 42;
    private volatile java.lang.Object schedulingPool_;
    /**
     * <code>string scheduling_pool = 42;</code>
     * @return Whether the schedulingPool field is set.
     */
    @java.lang.Override
    public boolean hasSchedulingPool() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>string scheduling_pool = 42;</code>
     * @return The schedulingPool.
     */
    @java.lang.Override
    public java.lang.String getSchedulingPool() {
      java.lang.Object ref = schedulingPool_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        schedulingPool_ = s;
        return s;
      }
    }
    /**
     * <code>string scheduling_pool = 42;</code>
     * @return The bytes for schedulingPool.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getSchedulingPoolBytes() {
      java.lang.Object ref = schedulingPool_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        schedulingPool_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int RDD_IDS_FIELD_NUMBER = 43;
    private com.google.protobuf.Internal.LongList rddIds_;
    /**
     * <code>repeated int64 rdd_ids = 43;</code>
     * @return A list containing the rddIds.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getRddIdsList() {
      return rddIds_;
    }
    /**
     * <code>repeated int64 rdd_ids = 43;</code>
     * @return The count of rddIds.
     */
    public int getRddIdsCount() {
      return rddIds_.size();
    }
    /**
     * <code>repeated int64 rdd_ids = 43;</code>
     * @param index The index of the element to return.
     * @return The rddIds at the given index.
     */
    public long getRddIds(int index) {
      return rddIds_.getLong(index);
    }
    private int rddIdsMemoizedSerializedSize = -1;

    public static final int ACCUMULATOR_UPDATES_FIELD_NUMBER = 44;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> accumulatorUpdates_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> getAccumulatorUpdatesList() {
      return accumulatorUpdates_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
        getAccumulatorUpdatesOrBuilderList() {
      return accumulatorUpdates_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    @java.lang.Override
    public int getAccumulatorUpdatesCount() {
      return accumulatorUpdates_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index) {
      return accumulatorUpdates_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
        int index) {
      return accumulatorUpdates_.get(index);
    }

    public static final int TASKS_FIELD_NUMBER = 45;
    private static final class TasksDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageData_TasksEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.INT64,
                  0L,
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  org.apache.spark.status.protobuf.StoreTypes.TaskData.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> tasks_;
    private com.google.protobuf.MapField<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>
    internalGetTasks() {
      if (tasks_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            TasksDefaultEntryHolder.defaultEntry);
      }
      return tasks_;
    }

    public int getTasksCount() {
      return internalGetTasks().getMap().size();
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */

    @java.lang.Override
    public boolean containsTasks(
        long key) {
      
      return internalGetTasks().getMap().containsKey(key);
    }
    /**
     * Use {@link #getTasksMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> getTasks() {
      return getTasksMap();
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> getTasksMap() {
      return internalGetTasks().getMap();
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.TaskData getTasksOrDefault(
        long key,
        org.apache.spark.status.protobuf.StoreTypes.TaskData defaultValue) {
      
      java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> map =
          internalGetTasks().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.TaskData getTasksOrThrow(
        long key) {
      
      java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> map =
          internalGetTasks().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int EXECUTOR_SUMMARY_FIELD_NUMBER = 46;
    private static final class ExecutorSummaryDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageData_ExecutorSummaryEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> executorSummary_;
    private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>
    internalGetExecutorSummary() {
      if (executorSummary_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ExecutorSummaryDefaultEntryHolder.defaultEntry);
      }
      return executorSummary_;
    }

    public int getExecutorSummaryCount() {
      return internalGetExecutorSummary().getMap().size();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */

    @java.lang.Override
    public boolean containsExecutorSummary(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetExecutorSummary().getMap().containsKey(key);
    }
    /**
     * Use {@link #getExecutorSummaryMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> getExecutorSummary() {
      return getExecutorSummaryMap();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> getExecutorSummaryMap() {
      return internalGetExecutorSummary().getMap();
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getExecutorSummaryOrDefault(
        java.lang.String key,
        org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> map =
          internalGetExecutorSummary().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
     */
    @java.lang.Override

    public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getExecutorSummaryOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> map =
          internalGetExecutorSummary().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int SPECULATION_SUMMARY_FIELD_NUMBER = 47;
    private org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary speculationSummary_;
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
     * @return Whether the speculationSummary field is set.
     */
    @java.lang.Override
    public boolean hasSpeculationSummary() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
     * @return The speculationSummary.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getSpeculationSummary() {
      return speculationSummary_ == null ? org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance() : speculationSummary_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder getSpeculationSummaryOrBuilder() {
      return speculationSummary_ == null ? org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance() : speculationSummary_;
    }

    public static final int KILLED_TASKS_SUMMARY_FIELD_NUMBER = 48;
    private static final class KilledTasksSummaryDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.Integer> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.Integer>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageData_KilledTasksSummaryEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.INT32,
                  0);
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.Integer> killedTasksSummary_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
    internalGetKilledTasksSummary() {
      if (killedTasksSummary_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            KilledTasksSummaryDefaultEntryHolder.defaultEntry);
      }
      return killedTasksSummary_;
    }

    public int getKilledTasksSummaryCount() {
      return internalGetKilledTasksSummary().getMap().size();
    }
    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */

    @java.lang.Override
    public boolean containsKilledTasksSummary(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetKilledTasksSummary().getMap().containsKey(key);
    }
    /**
     * Use {@link #getKilledTasksSummaryMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.Integer> getKilledTasksSummary() {
      return getKilledTasksSummaryMap();
    }
    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.Integer> getKilledTasksSummaryMap() {
      return internalGetKilledTasksSummary().getMap();
    }
    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */
    @java.lang.Override

    public int getKilledTasksSummaryOrDefault(
        java.lang.String key,
        int defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Integer> map =
          internalGetKilledTasksSummary().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
     */
    @java.lang.Override

    public int getKilledTasksSummaryOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Integer> map =
          internalGetKilledTasksSummary().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int RESOURCE_PROFILE_ID_FIELD_NUMBER = 49;
    private int resourceProfileId_;
    /**
     * <code>int32 resource_profile_id = 49;</code>
     * @return The resourceProfileId.
     */
    @java.lang.Override
    public int getResourceProfileId() {
      return resourceProfileId_;
    }

    public static final int PEAK_EXECUTOR_METRICS_FIELD_NUMBER = 50;
    private org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics peakExecutorMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
     * @return Whether the peakExecutorMetrics field is set.
     */
    @java.lang.Override
    public boolean hasPeakExecutorMetrics() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
     * @return The peakExecutorMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakExecutorMetrics() {
      return peakExecutorMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakExecutorMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakExecutorMetricsOrBuilder() {
      return peakExecutorMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakExecutorMetrics_;
    }

    public static final int TASK_METRICS_DISTRIBUTIONS_FIELD_NUMBER = 51;
    private org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions taskMetricsDistributions_;
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
     * @return Whether the taskMetricsDistributions field is set.
     */
    @java.lang.Override
    public boolean hasTaskMetricsDistributions() {
      return ((bitField0_ & 0x00000400) != 0);
    }
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
     * @return The taskMetricsDistributions.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions getTaskMetricsDistributions() {
      return taskMetricsDistributions_ == null ? org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.getDefaultInstance() : taskMetricsDistributions_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributionsOrBuilder getTaskMetricsDistributionsOrBuilder() {
      return taskMetricsDistributions_ == null ? org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.getDefaultInstance() : taskMetricsDistributions_;
    }

    public static final int EXECUTOR_METRICS_DISTRIBUTIONS_FIELD_NUMBER = 52;
    private org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions executorMetricsDistributions_;
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
     * @return Whether the executorMetricsDistributions field is set.
     */
    @java.lang.Override
    public boolean hasExecutorMetricsDistributions() {
      return ((bitField0_ & 0x00000800) != 0);
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
     * @return The executorMetricsDistributions.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions getExecutorMetricsDistributions() {
      return executorMetricsDistributions_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.getDefaultInstance() : executorMetricsDistributions_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributionsOrBuilder getExecutorMetricsDistributionsOrBuilder() {
      return executorMetricsDistributions_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.getDefaultInstance() : executorMetricsDistributions_;
    }

    public static final int SHUFFLE_CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER = 53;
    private long shuffleCorruptMergedBlockChunks_;
    /**
     * <code>int64 shuffle_corrupt_merged_block_chunks = 53;</code>
     * @return The shuffleCorruptMergedBlockChunks.
     */
    @java.lang.Override
    public long getShuffleCorruptMergedBlockChunks() {
      return shuffleCorruptMergedBlockChunks_;
    }

    public static final int SHUFFLE_MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER = 54;
    private long shuffleMergedFetchFallbackCount_;
    /**
     * <code>int64 shuffle_merged_fetch_fallback_count = 54;</code>
     * @return The shuffleMergedFetchFallbackCount.
     */
    @java.lang.Override
    public long getShuffleMergedFetchFallbackCount() {
      return shuffleMergedFetchFallbackCount_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER = 55;
    private long shuffleMergedRemoteBlocksFetched_;
    /**
     * <code>int64 shuffle_merged_remote_blocks_fetched = 55;</code>
     * @return The shuffleMergedRemoteBlocksFetched.
     */
    @java.lang.Override
    public long getShuffleMergedRemoteBlocksFetched() {
      return shuffleMergedRemoteBlocksFetched_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER = 56;
    private long shuffleMergedLocalBlocksFetched_;
    /**
     * <code>int64 shuffle_merged_local_blocks_fetched = 56;</code>
     * @return The shuffleMergedLocalBlocksFetched.
     */
    @java.lang.Override
    public long getShuffleMergedLocalBlocksFetched() {
      return shuffleMergedLocalBlocksFetched_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_CHUNKS_FETCHED_FIELD_NUMBER = 57;
    private long shuffleMergedRemoteChunksFetched_;
    /**
     * <code>int64 shuffle_merged_remote_chunks_fetched = 57;</code>
     * @return The shuffleMergedRemoteChunksFetched.
     */
    @java.lang.Override
    public long getShuffleMergedRemoteChunksFetched() {
      return shuffleMergedRemoteChunksFetched_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_CHUNKS_FETCHED_FIELD_NUMBER = 58;
    private long shuffleMergedLocalChunksFetched_;
    /**
     * <code>int64 shuffle_merged_local_chunks_fetched = 58;</code>
     * @return The shuffleMergedLocalChunksFetched.
     */
    @java.lang.Override
    public long getShuffleMergedLocalChunksFetched() {
      return shuffleMergedLocalChunksFetched_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_BYTES_READ_FIELD_NUMBER = 59;
    private long shuffleMergedRemoteBytesRead_;
    /**
     * <code>int64 shuffle_merged_remote_bytes_read = 59;</code>
     * @return The shuffleMergedRemoteBytesRead.
     */
    @java.lang.Override
    public long getShuffleMergedRemoteBytesRead() {
      return shuffleMergedRemoteBytesRead_;
    }

    public static final int SHUFFLE_MERGED_LOCAL_BYTES_READ_FIELD_NUMBER = 60;
    private long shuffleMergedLocalBytesRead_;
    /**
     * <code>int64 shuffle_merged_local_bytes_read = 60;</code>
     * @return The shuffleMergedLocalBytesRead.
     */
    @java.lang.Override
    public long getShuffleMergedLocalBytesRead() {
      return shuffleMergedLocalBytesRead_;
    }

    public static final int SHUFFLE_REMOTE_REQS_DURATION_FIELD_NUMBER = 61;
    private long shuffleRemoteReqsDuration_;
    /**
     * <code>int64 shuffle_remote_reqs_duration = 61;</code>
     * @return The shuffleRemoteReqsDuration.
     */
    @java.lang.Override
    public long getShuffleRemoteReqsDuration() {
      return shuffleRemoteReqsDuration_;
    }

    public static final int SHUFFLE_MERGED_REMOTE_REQS_DURATION_FIELD_NUMBER = 62;
    private long shuffleMergedRemoteReqsDuration_;
    /**
     * <code>int64 shuffle_merged_remote_reqs_duration = 62;</code>
     * @return The shuffleMergedRemoteReqsDuration.
     */
    @java.lang.Override
    public long getShuffleMergedRemoteReqsDuration() {
      return shuffleMergedRemoteReqsDuration_;
    }

    public static final int IS_SHUFFLE_PUSH_ENABLED_FIELD_NUMBER = 63;
    private boolean isShufflePushEnabled_;
    /**
     * <code>bool is_shuffle_push_enabled = 63;</code>
     * @return The isShufflePushEnabled.
     */
    @java.lang.Override
    public boolean getIsShufflePushEnabled() {
      return isShufflePushEnabled_;
    }

    public static final int SHUFFLE_MERGERS_COUNT_FIELD_NUMBER = 64;
    private int shuffleMergersCount_;
    /**
     * <code>int32 shuffle_mergers_count = 64;</code>
     * @return The shuffleMergersCount.
     */
    @java.lang.Override
    public int getShuffleMergersCount() {
      return shuffleMergersCount_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (status_ != org.apache.spark.status.protobuf.StoreTypes.StageStatus.STAGE_STATUS_UNSPECIFIED.getNumber()) {
        output.writeEnum(1, status_);
      }
      if (stageId_ != 0L) {
        output.writeInt64(2, stageId_);
      }
      if (attemptId_ != 0) {
        output.writeInt32(3, attemptId_);
      }
      if (numTasks_ != 0) {
        output.writeInt32(4, numTasks_);
      }
      if (numActiveTasks_ != 0) {
        output.writeInt32(5, numActiveTasks_);
      }
      if (numCompleteTasks_ != 0) {
        output.writeInt32(6, numCompleteTasks_);
      }
      if (numFailedTasks_ != 0) {
        output.writeInt32(7, numFailedTasks_);
      }
      if (numKilledTasks_ != 0) {
        output.writeInt32(8, numKilledTasks_);
      }
      if (numCompletedIndices_ != 0) {
        output.writeInt32(9, numCompletedIndices_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(10, submissionTime_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(11, firstTaskLaunchedTime_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt64(12, completionTime_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 13, failureReason_);
      }
      if (executorDeserializeTime_ != 0L) {
        output.writeInt64(14, executorDeserializeTime_);
      }
      if (executorDeserializeCpuTime_ != 0L) {
        output.writeInt64(15, executorDeserializeCpuTime_);
      }
      if (executorRunTime_ != 0L) {
        output.writeInt64(16, executorRunTime_);
      }
      if (executorCpuTime_ != 0L) {
        output.writeInt64(17, executorCpuTime_);
      }
      if (resultSize_ != 0L) {
        output.writeInt64(18, resultSize_);
      }
      if (jvmGcTime_ != 0L) {
        output.writeInt64(19, jvmGcTime_);
      }
      if (resultSerializationTime_ != 0L) {
        output.writeInt64(20, resultSerializationTime_);
      }
      if (memoryBytesSpilled_ != 0L) {
        output.writeInt64(21, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0L) {
        output.writeInt64(22, diskBytesSpilled_);
      }
      if (peakExecutionMemory_ != 0L) {
        output.writeInt64(23, peakExecutionMemory_);
      }
      if (inputBytes_ != 0L) {
        output.writeInt64(24, inputBytes_);
      }
      if (inputRecords_ != 0L) {
        output.writeInt64(25, inputRecords_);
      }
      if (outputBytes_ != 0L) {
        output.writeInt64(26, outputBytes_);
      }
      if (outputRecords_ != 0L) {
        output.writeInt64(27, outputRecords_);
      }
      if (shuffleRemoteBlocksFetched_ != 0L) {
        output.writeInt64(28, shuffleRemoteBlocksFetched_);
      }
      if (shuffleLocalBlocksFetched_ != 0L) {
        output.writeInt64(29, shuffleLocalBlocksFetched_);
      }
      if (shuffleFetchWaitTime_ != 0L) {
        output.writeInt64(30, shuffleFetchWaitTime_);
      }
      if (shuffleRemoteBytesRead_ != 0L) {
        output.writeInt64(31, shuffleRemoteBytesRead_);
      }
      if (shuffleRemoteBytesReadToDisk_ != 0L) {
        output.writeInt64(32, shuffleRemoteBytesReadToDisk_);
      }
      if (shuffleLocalBytesRead_ != 0L) {
        output.writeInt64(33, shuffleLocalBytesRead_);
      }
      if (shuffleReadBytes_ != 0L) {
        output.writeInt64(34, shuffleReadBytes_);
      }
      if (shuffleReadRecords_ != 0L) {
        output.writeInt64(35, shuffleReadRecords_);
      }
      if (shuffleWriteBytes_ != 0L) {
        output.writeInt64(36, shuffleWriteBytes_);
      }
      if (shuffleWriteTime_ != 0L) {
        output.writeInt64(37, shuffleWriteTime_);
      }
      if (shuffleWriteRecords_ != 0L) {
        output.writeInt64(38, shuffleWriteRecords_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 39, name_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 40, description_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 41, details_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 42, schedulingPool_);
      }
      if (getRddIdsList().size() > 0) {
        output.writeUInt32NoTag(346);
        output.writeUInt32NoTag(rddIdsMemoizedSerializedSize);
      }
      for (int i = 0; i < rddIds_.size(); i++) {
        output.writeInt64NoTag(rddIds_.getLong(i));
      }
      for (int i = 0; i < accumulatorUpdates_.size(); i++) {
        output.writeMessage(44, accumulatorUpdates_.get(i));
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeLongMapTo(
          output,
          internalGetTasks(),
          TasksDefaultEntryHolder.defaultEntry,
          45);
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetExecutorSummary(),
          ExecutorSummaryDefaultEntryHolder.defaultEntry,
          46);
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeMessage(47, getSpeculationSummary());
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetKilledTasksSummary(),
          KilledTasksSummaryDefaultEntryHolder.defaultEntry,
          48);
      if (resourceProfileId_ != 0) {
        output.writeInt32(49, resourceProfileId_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        output.writeMessage(50, getPeakExecutorMetrics());
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        output.writeMessage(51, getTaskMetricsDistributions());
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        output.writeMessage(52, getExecutorMetricsDistributions());
      }
      if (shuffleCorruptMergedBlockChunks_ != 0L) {
        output.writeInt64(53, shuffleCorruptMergedBlockChunks_);
      }
      if (shuffleMergedFetchFallbackCount_ != 0L) {
        output.writeInt64(54, shuffleMergedFetchFallbackCount_);
      }
      if (shuffleMergedRemoteBlocksFetched_ != 0L) {
        output.writeInt64(55, shuffleMergedRemoteBlocksFetched_);
      }
      if (shuffleMergedLocalBlocksFetched_ != 0L) {
        output.writeInt64(56, shuffleMergedLocalBlocksFetched_);
      }
      if (shuffleMergedRemoteChunksFetched_ != 0L) {
        output.writeInt64(57, shuffleMergedRemoteChunksFetched_);
      }
      if (shuffleMergedLocalChunksFetched_ != 0L) {
        output.writeInt64(58, shuffleMergedLocalChunksFetched_);
      }
      if (shuffleMergedRemoteBytesRead_ != 0L) {
        output.writeInt64(59, shuffleMergedRemoteBytesRead_);
      }
      if (shuffleMergedLocalBytesRead_ != 0L) {
        output.writeInt64(60, shuffleMergedLocalBytesRead_);
      }
      if (shuffleRemoteReqsDuration_ != 0L) {
        output.writeInt64(61, shuffleRemoteReqsDuration_);
      }
      if (shuffleMergedRemoteReqsDuration_ != 0L) {
        output.writeInt64(62, shuffleMergedRemoteReqsDuration_);
      }
      if (isShufflePushEnabled_ != false) {
        output.writeBool(63, isShufflePushEnabled_);
      }
      if (shuffleMergersCount_ != 0) {
        output.writeInt32(64, shuffleMergersCount_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (status_ != org.apache.spark.status.protobuf.StoreTypes.StageStatus.STAGE_STATUS_UNSPECIFIED.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, status_);
      }
      if (stageId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, stageId_);
      }
      if (attemptId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, attemptId_);
      }
      if (numTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, numTasks_);
      }
      if (numActiveTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(5, numActiveTasks_);
      }
      if (numCompleteTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(6, numCompleteTasks_);
      }
      if (numFailedTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, numFailedTasks_);
      }
      if (numKilledTasks_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(8, numKilledTasks_);
      }
      if (numCompletedIndices_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(9, numCompletedIndices_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(10, submissionTime_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(11, firstTaskLaunchedTime_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(12, completionTime_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(13, failureReason_);
      }
      if (executorDeserializeTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(14, executorDeserializeTime_);
      }
      if (executorDeserializeCpuTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(15, executorDeserializeCpuTime_);
      }
      if (executorRunTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(16, executorRunTime_);
      }
      if (executorCpuTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(17, executorCpuTime_);
      }
      if (resultSize_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(18, resultSize_);
      }
      if (jvmGcTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(19, jvmGcTime_);
      }
      if (resultSerializationTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(20, resultSerializationTime_);
      }
      if (memoryBytesSpilled_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(21, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(22, diskBytesSpilled_);
      }
      if (peakExecutionMemory_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(23, peakExecutionMemory_);
      }
      if (inputBytes_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(24, inputBytes_);
      }
      if (inputRecords_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(25, inputRecords_);
      }
      if (outputBytes_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(26, outputBytes_);
      }
      if (outputRecords_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(27, outputRecords_);
      }
      if (shuffleRemoteBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(28, shuffleRemoteBlocksFetched_);
      }
      if (shuffleLocalBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(29, shuffleLocalBlocksFetched_);
      }
      if (shuffleFetchWaitTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(30, shuffleFetchWaitTime_);
      }
      if (shuffleRemoteBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(31, shuffleRemoteBytesRead_);
      }
      if (shuffleRemoteBytesReadToDisk_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(32, shuffleRemoteBytesReadToDisk_);
      }
      if (shuffleLocalBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(33, shuffleLocalBytesRead_);
      }
      if (shuffleReadBytes_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(34, shuffleReadBytes_);
      }
      if (shuffleReadRecords_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(35, shuffleReadRecords_);
      }
      if (shuffleWriteBytes_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(36, shuffleWriteBytes_);
      }
      if (shuffleWriteTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(37, shuffleWriteTime_);
      }
      if (shuffleWriteRecords_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(38, shuffleWriteRecords_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(39, name_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(40, description_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(41, details_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(42, schedulingPool_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < rddIds_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(rddIds_.getLong(i));
        }
        size += dataSize;
        if (!getRddIdsList().isEmpty()) {
          size += 2;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        rddIdsMemoizedSerializedSize = dataSize;
      }
      for (int i = 0; i < accumulatorUpdates_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(44, accumulatorUpdates_.get(i));
      }
      for (java.util.Map.Entry<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> entry
           : internalGetTasks().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>
        tasks__ = TasksDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(45, tasks__);
      }
      for (java.util.Map.Entry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> entry
           : internalGetExecutorSummary().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>
        executorSummary__ = ExecutorSummaryDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(46, executorSummary__);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(47, getSpeculationSummary());
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.Integer> entry
           : internalGetKilledTasksSummary().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.Integer>
        killedTasksSummary__ = KilledTasksSummaryDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(48, killedTasksSummary__);
      }
      if (resourceProfileId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(49, resourceProfileId_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(50, getPeakExecutorMetrics());
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(51, getTaskMetricsDistributions());
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(52, getExecutorMetricsDistributions());
      }
      if (shuffleCorruptMergedBlockChunks_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(53, shuffleCorruptMergedBlockChunks_);
      }
      if (shuffleMergedFetchFallbackCount_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(54, shuffleMergedFetchFallbackCount_);
      }
      if (shuffleMergedRemoteBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(55, shuffleMergedRemoteBlocksFetched_);
      }
      if (shuffleMergedLocalBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(56, shuffleMergedLocalBlocksFetched_);
      }
      if (shuffleMergedRemoteChunksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(57, shuffleMergedRemoteChunksFetched_);
      }
      if (shuffleMergedLocalChunksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(58, shuffleMergedLocalChunksFetched_);
      }
      if (shuffleMergedRemoteBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(59, shuffleMergedRemoteBytesRead_);
      }
      if (shuffleMergedLocalBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(60, shuffleMergedLocalBytesRead_);
      }
      if (shuffleRemoteReqsDuration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(61, shuffleRemoteReqsDuration_);
      }
      if (shuffleMergedRemoteReqsDuration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(62, shuffleMergedRemoteReqsDuration_);
      }
      if (isShufflePushEnabled_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(63, isShufflePushEnabled_);
      }
      if (shuffleMergersCount_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(64, shuffleMergersCount_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.StageData)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.StageData other = (org.apache.spark.status.protobuf.StoreTypes.StageData) obj;

      if (status_ != other.status_) return false;
      if (getStageId()
          != other.getStageId()) return false;
      if (getAttemptId()
          != other.getAttemptId()) return false;
      if (getNumTasks()
          != other.getNumTasks()) return false;
      if (getNumActiveTasks()
          != other.getNumActiveTasks()) return false;
      if (getNumCompleteTasks()
          != other.getNumCompleteTasks()) return false;
      if (getNumFailedTasks()
          != other.getNumFailedTasks()) return false;
      if (getNumKilledTasks()
          != other.getNumKilledTasks()) return false;
      if (getNumCompletedIndices()
          != other.getNumCompletedIndices()) return false;
      if (hasSubmissionTime() != other.hasSubmissionTime()) return false;
      if (hasSubmissionTime()) {
        if (getSubmissionTime()
            != other.getSubmissionTime()) return false;
      }
      if (hasFirstTaskLaunchedTime() != other.hasFirstTaskLaunchedTime()) return false;
      if (hasFirstTaskLaunchedTime()) {
        if (getFirstTaskLaunchedTime()
            != other.getFirstTaskLaunchedTime()) return false;
      }
      if (hasCompletionTime() != other.hasCompletionTime()) return false;
      if (hasCompletionTime()) {
        if (getCompletionTime()
            != other.getCompletionTime()) return false;
      }
      if (hasFailureReason() != other.hasFailureReason()) return false;
      if (hasFailureReason()) {
        if (!getFailureReason()
            .equals(other.getFailureReason())) return false;
      }
      if (getExecutorDeserializeTime()
          != other.getExecutorDeserializeTime()) return false;
      if (getExecutorDeserializeCpuTime()
          != other.getExecutorDeserializeCpuTime()) return false;
      if (getExecutorRunTime()
          != other.getExecutorRunTime()) return false;
      if (getExecutorCpuTime()
          != other.getExecutorCpuTime()) return false;
      if (getResultSize()
          != other.getResultSize()) return false;
      if (getJvmGcTime()
          != other.getJvmGcTime()) return false;
      if (getResultSerializationTime()
          != other.getResultSerializationTime()) return false;
      if (getMemoryBytesSpilled()
          != other.getMemoryBytesSpilled()) return false;
      if (getDiskBytesSpilled()
          != other.getDiskBytesSpilled()) return false;
      if (getPeakExecutionMemory()
          != other.getPeakExecutionMemory()) return false;
      if (getInputBytes()
          != other.getInputBytes()) return false;
      if (getInputRecords()
          != other.getInputRecords()) return false;
      if (getOutputBytes()
          != other.getOutputBytes()) return false;
      if (getOutputRecords()
          != other.getOutputRecords()) return false;
      if (getShuffleRemoteBlocksFetched()
          != other.getShuffleRemoteBlocksFetched()) return false;
      if (getShuffleLocalBlocksFetched()
          != other.getShuffleLocalBlocksFetched()) return false;
      if (getShuffleFetchWaitTime()
          != other.getShuffleFetchWaitTime()) return false;
      if (getShuffleRemoteBytesRead()
          != other.getShuffleRemoteBytesRead()) return false;
      if (getShuffleRemoteBytesReadToDisk()
          != other.getShuffleRemoteBytesReadToDisk()) return false;
      if (getShuffleLocalBytesRead()
          != other.getShuffleLocalBytesRead()) return false;
      if (getShuffleReadBytes()
          != other.getShuffleReadBytes()) return false;
      if (getShuffleReadRecords()
          != other.getShuffleReadRecords()) return false;
      if (getShuffleWriteBytes()
          != other.getShuffleWriteBytes()) return false;
      if (getShuffleWriteTime()
          != other.getShuffleWriteTime()) return false;
      if (getShuffleWriteRecords()
          != other.getShuffleWriteRecords()) return false;
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasDescription() != other.hasDescription()) return false;
      if (hasDescription()) {
        if (!getDescription()
            .equals(other.getDescription())) return false;
      }
      if (hasDetails() != other.hasDetails()) return false;
      if (hasDetails()) {
        if (!getDetails()
            .equals(other.getDetails())) return false;
      }
      if (hasSchedulingPool() != other.hasSchedulingPool()) return false;
      if (hasSchedulingPool()) {
        if (!getSchedulingPool()
            .equals(other.getSchedulingPool())) return false;
      }
      if (!getRddIdsList()
          .equals(other.getRddIdsList())) return false;
      if (!getAccumulatorUpdatesList()
          .equals(other.getAccumulatorUpdatesList())) return false;
      if (!internalGetTasks().equals(
          other.internalGetTasks())) return false;
      if (!internalGetExecutorSummary().equals(
          other.internalGetExecutorSummary())) return false;
      if (hasSpeculationSummary() != other.hasSpeculationSummary()) return false;
      if (hasSpeculationSummary()) {
        if (!getSpeculationSummary()
            .equals(other.getSpeculationSummary())) return false;
      }
      if (!internalGetKilledTasksSummary().equals(
          other.internalGetKilledTasksSummary())) return false;
      if (getResourceProfileId()
          != other.getResourceProfileId()) return false;
      if (hasPeakExecutorMetrics() != other.hasPeakExecutorMetrics()) return false;
      if (hasPeakExecutorMetrics()) {
        if (!getPeakExecutorMetrics()
            .equals(other.getPeakExecutorMetrics())) return false;
      }
      if (hasTaskMetricsDistributions() != other.hasTaskMetricsDistributions()) return false;
      if (hasTaskMetricsDistributions()) {
        if (!getTaskMetricsDistributions()
            .equals(other.getTaskMetricsDistributions())) return false;
      }
      if (hasExecutorMetricsDistributions() != other.hasExecutorMetricsDistributions()) return false;
      if (hasExecutorMetricsDistributions()) {
        if (!getExecutorMetricsDistributions()
            .equals(other.getExecutorMetricsDistributions())) return false;
      }
      if (getShuffleCorruptMergedBlockChunks()
          != other.getShuffleCorruptMergedBlockChunks()) return false;
      if (getShuffleMergedFetchFallbackCount()
          != other.getShuffleMergedFetchFallbackCount()) return false;
      if (getShuffleMergedRemoteBlocksFetched()
          != other.getShuffleMergedRemoteBlocksFetched()) return false;
      if (getShuffleMergedLocalBlocksFetched()
          != other.getShuffleMergedLocalBlocksFetched()) return false;
      if (getShuffleMergedRemoteChunksFetched()
          != other.getShuffleMergedRemoteChunksFetched()) return false;
      if (getShuffleMergedLocalChunksFetched()
          != other.getShuffleMergedLocalChunksFetched()) return false;
      if (getShuffleMergedRemoteBytesRead()
          != other.getShuffleMergedRemoteBytesRead()) return false;
      if (getShuffleMergedLocalBytesRead()
          != other.getShuffleMergedLocalBytesRead()) return false;
      if (getShuffleRemoteReqsDuration()
          != other.getShuffleRemoteReqsDuration()) return false;
      if (getShuffleMergedRemoteReqsDuration()
          != other.getShuffleMergedRemoteReqsDuration()) return false;
      if (getIsShufflePushEnabled()
          != other.getIsShufflePushEnabled()) return false;
      if (getShuffleMergersCount()
          != other.getShuffleMergersCount()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STATUS_FIELD_NUMBER;
      hash = (53 * hash) + status_;
      hash = (37 * hash) + STAGE_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getStageId());
      hash = (37 * hash) + ATTEMPT_ID_FIELD_NUMBER;
      hash = (53 * hash) + getAttemptId();
      hash = (37 * hash) + NUM_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumTasks();
      hash = (37 * hash) + NUM_ACTIVE_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumActiveTasks();
      hash = (37 * hash) + NUM_COMPLETE_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumCompleteTasks();
      hash = (37 * hash) + NUM_FAILED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumFailedTasks();
      hash = (37 * hash) + NUM_KILLED_TASKS_FIELD_NUMBER;
      hash = (53 * hash) + getNumKilledTasks();
      hash = (37 * hash) + NUM_COMPLETED_INDICES_FIELD_NUMBER;
      hash = (53 * hash) + getNumCompletedIndices();
      if (hasSubmissionTime()) {
        hash = (37 * hash) + SUBMISSION_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getSubmissionTime());
      }
      if (hasFirstTaskLaunchedTime()) {
        hash = (37 * hash) + FIRST_TASK_LAUNCHED_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getFirstTaskLaunchedTime());
      }
      if (hasCompletionTime()) {
        hash = (37 * hash) + COMPLETION_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getCompletionTime());
      }
      if (hasFailureReason()) {
        hash = (37 * hash) + FAILURE_REASON_FIELD_NUMBER;
        hash = (53 * hash) + getFailureReason().hashCode();
      }
      hash = (37 * hash) + EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorDeserializeTime());
      hash = (37 * hash) + EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorDeserializeCpuTime());
      hash = (37 * hash) + EXECUTOR_RUN_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorRunTime());
      hash = (37 * hash) + EXECUTOR_CPU_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorCpuTime());
      hash = (37 * hash) + RESULT_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getResultSize());
      hash = (37 * hash) + JVM_GC_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getJvmGcTime());
      hash = (37 * hash) + RESULT_SERIALIZATION_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getResultSerializationTime());
      hash = (37 * hash) + MEMORY_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryBytesSpilled());
      hash = (37 * hash) + DISK_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskBytesSpilled());
      hash = (37 * hash) + PEAK_EXECUTION_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getPeakExecutionMemory());
      hash = (37 * hash) + INPUT_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getInputBytes());
      hash = (37 * hash) + INPUT_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getInputRecords());
      hash = (37 * hash) + OUTPUT_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getOutputBytes());
      hash = (37 * hash) + OUTPUT_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getOutputRecords());
      hash = (37 * hash) + SHUFFLE_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRemoteBlocksFetched());
      hash = (37 * hash) + SHUFFLE_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleLocalBlocksFetched());
      hash = (37 * hash) + SHUFFLE_FETCH_WAIT_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleFetchWaitTime());
      hash = (37 * hash) + SHUFFLE_REMOTE_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRemoteBytesRead());
      hash = (37 * hash) + SHUFFLE_REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRemoteBytesReadToDisk());
      hash = (37 * hash) + SHUFFLE_LOCAL_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleLocalBytesRead());
      hash = (37 * hash) + SHUFFLE_READ_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleReadBytes());
      hash = (37 * hash) + SHUFFLE_READ_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleReadRecords());
      hash = (37 * hash) + SHUFFLE_WRITE_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleWriteBytes());
      hash = (37 * hash) + SHUFFLE_WRITE_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleWriteTime());
      hash = (37 * hash) + SHUFFLE_WRITE_RECORDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleWriteRecords());
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasDescription()) {
        hash = (37 * hash) + DESCRIPTION_FIELD_NUMBER;
        hash = (53 * hash) + getDescription().hashCode();
      }
      if (hasDetails()) {
        hash = (37 * hash) + DETAILS_FIELD_NUMBER;
        hash = (53 * hash) + getDetails().hashCode();
      }
      if (hasSchedulingPool()) {
        hash = (37 * hash) + SCHEDULING_POOL_FIELD_NUMBER;
        hash = (53 * hash) + getSchedulingPool().hashCode();
      }
      if (getRddIdsCount() > 0) {
        hash = (37 * hash) + RDD_IDS_FIELD_NUMBER;
        hash = (53 * hash) + getRddIdsList().hashCode();
      }
      if (getAccumulatorUpdatesCount() > 0) {
        hash = (37 * hash) + ACCUMULATOR_UPDATES_FIELD_NUMBER;
        hash = (53 * hash) + getAccumulatorUpdatesList().hashCode();
      }
      if (!internalGetTasks().getMap().isEmpty()) {
        hash = (37 * hash) + TASKS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetTasks().hashCode();
      }
      if (!internalGetExecutorSummary().getMap().isEmpty()) {
        hash = (37 * hash) + EXECUTOR_SUMMARY_FIELD_NUMBER;
        hash = (53 * hash) + internalGetExecutorSummary().hashCode();
      }
      if (hasSpeculationSummary()) {
        hash = (37 * hash) + SPECULATION_SUMMARY_FIELD_NUMBER;
        hash = (53 * hash) + getSpeculationSummary().hashCode();
      }
      if (!internalGetKilledTasksSummary().getMap().isEmpty()) {
        hash = (37 * hash) + KILLED_TASKS_SUMMARY_FIELD_NUMBER;
        hash = (53 * hash) + internalGetKilledTasksSummary().hashCode();
      }
      hash = (37 * hash) + RESOURCE_PROFILE_ID_FIELD_NUMBER;
      hash = (53 * hash) + getResourceProfileId();
      if (hasPeakExecutorMetrics()) {
        hash = (37 * hash) + PEAK_EXECUTOR_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getPeakExecutorMetrics().hashCode();
      }
      if (hasTaskMetricsDistributions()) {
        hash = (37 * hash) + TASK_METRICS_DISTRIBUTIONS_FIELD_NUMBER;
        hash = (53 * hash) + getTaskMetricsDistributions().hashCode();
      }
      if (hasExecutorMetricsDistributions()) {
        hash = (37 * hash) + EXECUTOR_METRICS_DISTRIBUTIONS_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorMetricsDistributions().hashCode();
      }
      hash = (37 * hash) + SHUFFLE_CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleCorruptMergedBlockChunks());
      hash = (37 * hash) + SHUFFLE_MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedFetchFallbackCount());
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedRemoteBlocksFetched());
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedLocalBlocksFetched());
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_CHUNKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedRemoteChunksFetched());
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_CHUNKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedLocalChunksFetched());
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedRemoteBytesRead());
      hash = (37 * hash) + SHUFFLE_MERGED_LOCAL_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedLocalBytesRead());
      hash = (37 * hash) + SHUFFLE_REMOTE_REQS_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleRemoteReqsDuration());
      hash = (37 * hash) + SHUFFLE_MERGED_REMOTE_REQS_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getShuffleMergedRemoteReqsDuration());
      hash = (37 * hash) + IS_SHUFFLE_PUSH_ENABLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsShufflePushEnabled());
      hash = (37 * hash) + SHUFFLE_MERGERS_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + getShuffleMergersCount();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StageData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.StageData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.StageData}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.StageData)
        org.apache.spark.status.protobuf.StoreTypes.StageDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageData_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 45:
            return internalGetTasks();
          case 46:
            return internalGetExecutorSummary();
          case 48:
            return internalGetKilledTasksSummary();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 45:
            return internalGetMutableTasks();
          case 46:
            return internalGetMutableExecutorSummary();
          case 48:
            return internalGetMutableKilledTasksSummary();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.StageData.class, org.apache.spark.status.protobuf.StoreTypes.StageData.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.StageData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAccumulatorUpdatesFieldBuilder();
          getSpeculationSummaryFieldBuilder();
          getPeakExecutorMetricsFieldBuilder();
          getTaskMetricsDistributionsFieldBuilder();
          getExecutorMetricsDistributionsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        status_ = 0;

        stageId_ = 0L;

        attemptId_ = 0;

        numTasks_ = 0;

        numActiveTasks_ = 0;

        numCompleteTasks_ = 0;

        numFailedTasks_ = 0;

        numKilledTasks_ = 0;

        numCompletedIndices_ = 0;

        submissionTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        firstTaskLaunchedTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        completionTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        failureReason_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        executorDeserializeTime_ = 0L;

        executorDeserializeCpuTime_ = 0L;

        executorRunTime_ = 0L;

        executorCpuTime_ = 0L;

        resultSize_ = 0L;

        jvmGcTime_ = 0L;

        resultSerializationTime_ = 0L;

        memoryBytesSpilled_ = 0L;

        diskBytesSpilled_ = 0L;

        peakExecutionMemory_ = 0L;

        inputBytes_ = 0L;

        inputRecords_ = 0L;

        outputBytes_ = 0L;

        outputRecords_ = 0L;

        shuffleRemoteBlocksFetched_ = 0L;

        shuffleLocalBlocksFetched_ = 0L;

        shuffleFetchWaitTime_ = 0L;

        shuffleRemoteBytesRead_ = 0L;

        shuffleRemoteBytesReadToDisk_ = 0L;

        shuffleLocalBytesRead_ = 0L;

        shuffleReadBytes_ = 0L;

        shuffleReadRecords_ = 0L;

        shuffleWriteBytes_ = 0L;

        shuffleWriteTime_ = 0L;

        shuffleWriteRecords_ = 0L;

        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        description_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        details_ = "";
        bitField0_ = (bitField0_ & ~0x00000040);
        schedulingPool_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        rddIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000100);
        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdates_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
        } else {
          accumulatorUpdatesBuilder_.clear();
        }
        internalGetMutableTasks().clear();
        internalGetMutableExecutorSummary().clear();
        if (speculationSummaryBuilder_ == null) {
          speculationSummary_ = null;
        } else {
          speculationSummaryBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00001000);
        internalGetMutableKilledTasksSummary().clear();
        resourceProfileId_ = 0;

        if (peakExecutorMetricsBuilder_ == null) {
          peakExecutorMetrics_ = null;
        } else {
          peakExecutorMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00004000);
        if (taskMetricsDistributionsBuilder_ == null) {
          taskMetricsDistributions_ = null;
        } else {
          taskMetricsDistributionsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00008000);
        if (executorMetricsDistributionsBuilder_ == null) {
          executorMetricsDistributions_ = null;
        } else {
          executorMetricsDistributionsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00010000);
        shuffleCorruptMergedBlockChunks_ = 0L;

        shuffleMergedFetchFallbackCount_ = 0L;

        shuffleMergedRemoteBlocksFetched_ = 0L;

        shuffleMergedLocalBlocksFetched_ = 0L;

        shuffleMergedRemoteChunksFetched_ = 0L;

        shuffleMergedLocalChunksFetched_ = 0L;

        shuffleMergedRemoteBytesRead_ = 0L;

        shuffleMergedLocalBytesRead_ = 0L;

        shuffleRemoteReqsDuration_ = 0L;

        shuffleMergedRemoteReqsDuration_ = 0L;

        isShufflePushEnabled_ = false;

        shuffleMergersCount_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StageData_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StageData getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.StageData.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StageData build() {
        org.apache.spark.status.protobuf.StoreTypes.StageData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StageData buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.StageData result = new org.apache.spark.status.protobuf.StoreTypes.StageData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.status_ = status_;
        result.stageId_ = stageId_;
        result.attemptId_ = attemptId_;
        result.numTasks_ = numTasks_;
        result.numActiveTasks_ = numActiveTasks_;
        result.numCompleteTasks_ = numCompleteTasks_;
        result.numFailedTasks_ = numFailedTasks_;
        result.numKilledTasks_ = numKilledTasks_;
        result.numCompletedIndices_ = numCompletedIndices_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.submissionTime_ = submissionTime_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.firstTaskLaunchedTime_ = firstTaskLaunchedTime_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.completionTime_ = completionTime_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.failureReason_ = failureReason_;
        result.executorDeserializeTime_ = executorDeserializeTime_;
        result.executorDeserializeCpuTime_ = executorDeserializeCpuTime_;
        result.executorRunTime_ = executorRunTime_;
        result.executorCpuTime_ = executorCpuTime_;
        result.resultSize_ = resultSize_;
        result.jvmGcTime_ = jvmGcTime_;
        result.resultSerializationTime_ = resultSerializationTime_;
        result.memoryBytesSpilled_ = memoryBytesSpilled_;
        result.diskBytesSpilled_ = diskBytesSpilled_;
        result.peakExecutionMemory_ = peakExecutionMemory_;
        result.inputBytes_ = inputBytes_;
        result.inputRecords_ = inputRecords_;
        result.outputBytes_ = outputBytes_;
        result.outputRecords_ = outputRecords_;
        result.shuffleRemoteBlocksFetched_ = shuffleRemoteBlocksFetched_;
        result.shuffleLocalBlocksFetched_ = shuffleLocalBlocksFetched_;
        result.shuffleFetchWaitTime_ = shuffleFetchWaitTime_;
        result.shuffleRemoteBytesRead_ = shuffleRemoteBytesRead_;
        result.shuffleRemoteBytesReadToDisk_ = shuffleRemoteBytesReadToDisk_;
        result.shuffleLocalBytesRead_ = shuffleLocalBytesRead_;
        result.shuffleReadBytes_ = shuffleReadBytes_;
        result.shuffleReadRecords_ = shuffleReadRecords_;
        result.shuffleWriteBytes_ = shuffleWriteBytes_;
        result.shuffleWriteTime_ = shuffleWriteTime_;
        result.shuffleWriteRecords_ = shuffleWriteRecords_;
        if (((from_bitField0_ & 0x00000010) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000020;
        }
        result.description_ = description_;
        if (((from_bitField0_ & 0x00000040) != 0)) {
          to_bitField0_ |= 0x00000040;
        }
        result.details_ = details_;
        if (((from_bitField0_ & 0x00000080) != 0)) {
          to_bitField0_ |= 0x00000080;
        }
        result.schedulingPool_ = schedulingPool_;
        if (((bitField0_ & 0x00000100) != 0)) {
          rddIds_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000100);
        }
        result.rddIds_ = rddIds_;
        if (accumulatorUpdatesBuilder_ == null) {
          if (((bitField0_ & 0x00000200) != 0)) {
            accumulatorUpdates_ = java.util.Collections.unmodifiableList(accumulatorUpdates_);
            bitField0_ = (bitField0_ & ~0x00000200);
          }
          result.accumulatorUpdates_ = accumulatorUpdates_;
        } else {
          result.accumulatorUpdates_ = accumulatorUpdatesBuilder_.build();
        }
        result.tasks_ = internalGetTasks();
        result.tasks_.makeImmutable();
        result.executorSummary_ = internalGetExecutorSummary();
        result.executorSummary_.makeImmutable();
        if (((from_bitField0_ & 0x00001000) != 0)) {
          if (speculationSummaryBuilder_ == null) {
            result.speculationSummary_ = speculationSummary_;
          } else {
            result.speculationSummary_ = speculationSummaryBuilder_.build();
          }
          to_bitField0_ |= 0x00000100;
        }
        result.killedTasksSummary_ = internalGetKilledTasksSummary();
        result.killedTasksSummary_.makeImmutable();
        result.resourceProfileId_ = resourceProfileId_;
        if (((from_bitField0_ & 0x00004000) != 0)) {
          if (peakExecutorMetricsBuilder_ == null) {
            result.peakExecutorMetrics_ = peakExecutorMetrics_;
          } else {
            result.peakExecutorMetrics_ = peakExecutorMetricsBuilder_.build();
          }
          to_bitField0_ |= 0x00000200;
        }
        if (((from_bitField0_ & 0x00008000) != 0)) {
          if (taskMetricsDistributionsBuilder_ == null) {
            result.taskMetricsDistributions_ = taskMetricsDistributions_;
          } else {
            result.taskMetricsDistributions_ = taskMetricsDistributionsBuilder_.build();
          }
          to_bitField0_ |= 0x00000400;
        }
        if (((from_bitField0_ & 0x00010000) != 0)) {
          if (executorMetricsDistributionsBuilder_ == null) {
            result.executorMetricsDistributions_ = executorMetricsDistributions_;
          } else {
            result.executorMetricsDistributions_ = executorMetricsDistributionsBuilder_.build();
          }
          to_bitField0_ |= 0x00000800;
        }
        result.shuffleCorruptMergedBlockChunks_ = shuffleCorruptMergedBlockChunks_;
        result.shuffleMergedFetchFallbackCount_ = shuffleMergedFetchFallbackCount_;
        result.shuffleMergedRemoteBlocksFetched_ = shuffleMergedRemoteBlocksFetched_;
        result.shuffleMergedLocalBlocksFetched_ = shuffleMergedLocalBlocksFetched_;
        result.shuffleMergedRemoteChunksFetched_ = shuffleMergedRemoteChunksFetched_;
        result.shuffleMergedLocalChunksFetched_ = shuffleMergedLocalChunksFetched_;
        result.shuffleMergedRemoteBytesRead_ = shuffleMergedRemoteBytesRead_;
        result.shuffleMergedLocalBytesRead_ = shuffleMergedLocalBytesRead_;
        result.shuffleRemoteReqsDuration_ = shuffleRemoteReqsDuration_;
        result.shuffleMergedRemoteReqsDuration_ = shuffleMergedRemoteReqsDuration_;
        result.isShufflePushEnabled_ = isShufflePushEnabled_;
        result.shuffleMergersCount_ = shuffleMergersCount_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.StageData) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.StageData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.StageData other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.StageData.getDefaultInstance()) return this;
        if (other.status_ != 0) {
          setStatusValue(other.getStatusValue());
        }
        if (other.getStageId() != 0L) {
          setStageId(other.getStageId());
        }
        if (other.getAttemptId() != 0) {
          setAttemptId(other.getAttemptId());
        }
        if (other.getNumTasks() != 0) {
          setNumTasks(other.getNumTasks());
        }
        if (other.getNumActiveTasks() != 0) {
          setNumActiveTasks(other.getNumActiveTasks());
        }
        if (other.getNumCompleteTasks() != 0) {
          setNumCompleteTasks(other.getNumCompleteTasks());
        }
        if (other.getNumFailedTasks() != 0) {
          setNumFailedTasks(other.getNumFailedTasks());
        }
        if (other.getNumKilledTasks() != 0) {
          setNumKilledTasks(other.getNumKilledTasks());
        }
        if (other.getNumCompletedIndices() != 0) {
          setNumCompletedIndices(other.getNumCompletedIndices());
        }
        if (other.hasSubmissionTime()) {
          setSubmissionTime(other.getSubmissionTime());
        }
        if (other.hasFirstTaskLaunchedTime()) {
          setFirstTaskLaunchedTime(other.getFirstTaskLaunchedTime());
        }
        if (other.hasCompletionTime()) {
          setCompletionTime(other.getCompletionTime());
        }
        if (other.hasFailureReason()) {
          bitField0_ |= 0x00000008;
          failureReason_ = other.failureReason_;
          onChanged();
        }
        if (other.getExecutorDeserializeTime() != 0L) {
          setExecutorDeserializeTime(other.getExecutorDeserializeTime());
        }
        if (other.getExecutorDeserializeCpuTime() != 0L) {
          setExecutorDeserializeCpuTime(other.getExecutorDeserializeCpuTime());
        }
        if (other.getExecutorRunTime() != 0L) {
          setExecutorRunTime(other.getExecutorRunTime());
        }
        if (other.getExecutorCpuTime() != 0L) {
          setExecutorCpuTime(other.getExecutorCpuTime());
        }
        if (other.getResultSize() != 0L) {
          setResultSize(other.getResultSize());
        }
        if (other.getJvmGcTime() != 0L) {
          setJvmGcTime(other.getJvmGcTime());
        }
        if (other.getResultSerializationTime() != 0L) {
          setResultSerializationTime(other.getResultSerializationTime());
        }
        if (other.getMemoryBytesSpilled() != 0L) {
          setMemoryBytesSpilled(other.getMemoryBytesSpilled());
        }
        if (other.getDiskBytesSpilled() != 0L) {
          setDiskBytesSpilled(other.getDiskBytesSpilled());
        }
        if (other.getPeakExecutionMemory() != 0L) {
          setPeakExecutionMemory(other.getPeakExecutionMemory());
        }
        if (other.getInputBytes() != 0L) {
          setInputBytes(other.getInputBytes());
        }
        if (other.getInputRecords() != 0L) {
          setInputRecords(other.getInputRecords());
        }
        if (other.getOutputBytes() != 0L) {
          setOutputBytes(other.getOutputBytes());
        }
        if (other.getOutputRecords() != 0L) {
          setOutputRecords(other.getOutputRecords());
        }
        if (other.getShuffleRemoteBlocksFetched() != 0L) {
          setShuffleRemoteBlocksFetched(other.getShuffleRemoteBlocksFetched());
        }
        if (other.getShuffleLocalBlocksFetched() != 0L) {
          setShuffleLocalBlocksFetched(other.getShuffleLocalBlocksFetched());
        }
        if (other.getShuffleFetchWaitTime() != 0L) {
          setShuffleFetchWaitTime(other.getShuffleFetchWaitTime());
        }
        if (other.getShuffleRemoteBytesRead() != 0L) {
          setShuffleRemoteBytesRead(other.getShuffleRemoteBytesRead());
        }
        if (other.getShuffleRemoteBytesReadToDisk() != 0L) {
          setShuffleRemoteBytesReadToDisk(other.getShuffleRemoteBytesReadToDisk());
        }
        if (other.getShuffleLocalBytesRead() != 0L) {
          setShuffleLocalBytesRead(other.getShuffleLocalBytesRead());
        }
        if (other.getShuffleReadBytes() != 0L) {
          setShuffleReadBytes(other.getShuffleReadBytes());
        }
        if (other.getShuffleReadRecords() != 0L) {
          setShuffleReadRecords(other.getShuffleReadRecords());
        }
        if (other.getShuffleWriteBytes() != 0L) {
          setShuffleWriteBytes(other.getShuffleWriteBytes());
        }
        if (other.getShuffleWriteTime() != 0L) {
          setShuffleWriteTime(other.getShuffleWriteTime());
        }
        if (other.getShuffleWriteRecords() != 0L) {
          setShuffleWriteRecords(other.getShuffleWriteRecords());
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000010;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasDescription()) {
          bitField0_ |= 0x00000020;
          description_ = other.description_;
          onChanged();
        }
        if (other.hasDetails()) {
          bitField0_ |= 0x00000040;
          details_ = other.details_;
          onChanged();
        }
        if (other.hasSchedulingPool()) {
          bitField0_ |= 0x00000080;
          schedulingPool_ = other.schedulingPool_;
          onChanged();
        }
        if (!other.rddIds_.isEmpty()) {
          if (rddIds_.isEmpty()) {
            rddIds_ = other.rddIds_;
            bitField0_ = (bitField0_ & ~0x00000100);
          } else {
            ensureRddIdsIsMutable();
            rddIds_.addAll(other.rddIds_);
          }
          onChanged();
        }
        if (accumulatorUpdatesBuilder_ == null) {
          if (!other.accumulatorUpdates_.isEmpty()) {
            if (accumulatorUpdates_.isEmpty()) {
              accumulatorUpdates_ = other.accumulatorUpdates_;
              bitField0_ = (bitField0_ & ~0x00000200);
            } else {
              ensureAccumulatorUpdatesIsMutable();
              accumulatorUpdates_.addAll(other.accumulatorUpdates_);
            }
            onChanged();
          }
        } else {
          if (!other.accumulatorUpdates_.isEmpty()) {
            if (accumulatorUpdatesBuilder_.isEmpty()) {
              accumulatorUpdatesBuilder_.dispose();
              accumulatorUpdatesBuilder_ = null;
              accumulatorUpdates_ = other.accumulatorUpdates_;
              bitField0_ = (bitField0_ & ~0x00000200);
              accumulatorUpdatesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getAccumulatorUpdatesFieldBuilder() : null;
            } else {
              accumulatorUpdatesBuilder_.addAllMessages(other.accumulatorUpdates_);
            }
          }
        }
        internalGetMutableTasks().mergeFrom(
            other.internalGetTasks());
        internalGetMutableExecutorSummary().mergeFrom(
            other.internalGetExecutorSummary());
        if (other.hasSpeculationSummary()) {
          mergeSpeculationSummary(other.getSpeculationSummary());
        }
        internalGetMutableKilledTasksSummary().mergeFrom(
            other.internalGetKilledTasksSummary());
        if (other.getResourceProfileId() != 0) {
          setResourceProfileId(other.getResourceProfileId());
        }
        if (other.hasPeakExecutorMetrics()) {
          mergePeakExecutorMetrics(other.getPeakExecutorMetrics());
        }
        if (other.hasTaskMetricsDistributions()) {
          mergeTaskMetricsDistributions(other.getTaskMetricsDistributions());
        }
        if (other.hasExecutorMetricsDistributions()) {
          mergeExecutorMetricsDistributions(other.getExecutorMetricsDistributions());
        }
        if (other.getShuffleCorruptMergedBlockChunks() != 0L) {
          setShuffleCorruptMergedBlockChunks(other.getShuffleCorruptMergedBlockChunks());
        }
        if (other.getShuffleMergedFetchFallbackCount() != 0L) {
          setShuffleMergedFetchFallbackCount(other.getShuffleMergedFetchFallbackCount());
        }
        if (other.getShuffleMergedRemoteBlocksFetched() != 0L) {
          setShuffleMergedRemoteBlocksFetched(other.getShuffleMergedRemoteBlocksFetched());
        }
        if (other.getShuffleMergedLocalBlocksFetched() != 0L) {
          setShuffleMergedLocalBlocksFetched(other.getShuffleMergedLocalBlocksFetched());
        }
        if (other.getShuffleMergedRemoteChunksFetched() != 0L) {
          setShuffleMergedRemoteChunksFetched(other.getShuffleMergedRemoteChunksFetched());
        }
        if (other.getShuffleMergedLocalChunksFetched() != 0L) {
          setShuffleMergedLocalChunksFetched(other.getShuffleMergedLocalChunksFetched());
        }
        if (other.getShuffleMergedRemoteBytesRead() != 0L) {
          setShuffleMergedRemoteBytesRead(other.getShuffleMergedRemoteBytesRead());
        }
        if (other.getShuffleMergedLocalBytesRead() != 0L) {
          setShuffleMergedLocalBytesRead(other.getShuffleMergedLocalBytesRead());
        }
        if (other.getShuffleRemoteReqsDuration() != 0L) {
          setShuffleRemoteReqsDuration(other.getShuffleRemoteReqsDuration());
        }
        if (other.getShuffleMergedRemoteReqsDuration() != 0L) {
          setShuffleMergedRemoteReqsDuration(other.getShuffleMergedRemoteReqsDuration());
        }
        if (other.getIsShufflePushEnabled() != false) {
          setIsShufflePushEnabled(other.getIsShufflePushEnabled());
        }
        if (other.getShuffleMergersCount() != 0) {
          setShuffleMergersCount(other.getShuffleMergersCount());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.StageData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.StageData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int status_ = 0;
      /**
       * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
       * @return The enum numeric value on the wire for status.
       */
      @java.lang.Override public int getStatusValue() {
        return status_;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
       * @param value The enum numeric value on the wire for status to set.
       * @return This builder for chaining.
       */
      public Builder setStatusValue(int value) {
        
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
       * @return The status.
       */
      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StageStatus getStatus() {
        @SuppressWarnings("deprecation")
        org.apache.spark.status.protobuf.StoreTypes.StageStatus result = org.apache.spark.status.protobuf.StoreTypes.StageStatus.valueOf(status_);
        return result == null ? org.apache.spark.status.protobuf.StoreTypes.StageStatus.UNRECOGNIZED : result;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
       * @param value The status to set.
       * @return This builder for chaining.
       */
      public Builder setStatus(org.apache.spark.status.protobuf.StoreTypes.StageStatus value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        status_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StageStatus status = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStatus() {
        
        status_ = 0;
        onChanged();
        return this;
      }

      private long stageId_ ;
      /**
       * <code>int64 stage_id = 2;</code>
       * @return The stageId.
       */
      @java.lang.Override
      public long getStageId() {
        return stageId_;
      }
      /**
       * <code>int64 stage_id = 2;</code>
       * @param value The stageId to set.
       * @return This builder for chaining.
       */
      public Builder setStageId(long value) {
        
        stageId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 stage_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageId() {
        
        stageId_ = 0L;
        onChanged();
        return this;
      }

      private int attemptId_ ;
      /**
       * <code>int32 attempt_id = 3;</code>
       * @return The attemptId.
       */
      @java.lang.Override
      public int getAttemptId() {
        return attemptId_;
      }
      /**
       * <code>int32 attempt_id = 3;</code>
       * @param value The attemptId to set.
       * @return This builder for chaining.
       */
      public Builder setAttemptId(int value) {
        
        attemptId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 attempt_id = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearAttemptId() {
        
        attemptId_ = 0;
        onChanged();
        return this;
      }

      private int numTasks_ ;
      /**
       * <code>int32 num_tasks = 4;</code>
       * @return The numTasks.
       */
      @java.lang.Override
      public int getNumTasks() {
        return numTasks_;
      }
      /**
       * <code>int32 num_tasks = 4;</code>
       * @param value The numTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumTasks(int value) {
        
        numTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_tasks = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumTasks() {
        
        numTasks_ = 0;
        onChanged();
        return this;
      }

      private int numActiveTasks_ ;
      /**
       * <code>int32 num_active_tasks = 5;</code>
       * @return The numActiveTasks.
       */
      @java.lang.Override
      public int getNumActiveTasks() {
        return numActiveTasks_;
      }
      /**
       * <code>int32 num_active_tasks = 5;</code>
       * @param value The numActiveTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumActiveTasks(int value) {
        
        numActiveTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_active_tasks = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumActiveTasks() {
        
        numActiveTasks_ = 0;
        onChanged();
        return this;
      }

      private int numCompleteTasks_ ;
      /**
       * <code>int32 num_complete_tasks = 6;</code>
       * @return The numCompleteTasks.
       */
      @java.lang.Override
      public int getNumCompleteTasks() {
        return numCompleteTasks_;
      }
      /**
       * <code>int32 num_complete_tasks = 6;</code>
       * @param value The numCompleteTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumCompleteTasks(int value) {
        
        numCompleteTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_complete_tasks = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCompleteTasks() {
        
        numCompleteTasks_ = 0;
        onChanged();
        return this;
      }

      private int numFailedTasks_ ;
      /**
       * <code>int32 num_failed_tasks = 7;</code>
       * @return The numFailedTasks.
       */
      @java.lang.Override
      public int getNumFailedTasks() {
        return numFailedTasks_;
      }
      /**
       * <code>int32 num_failed_tasks = 7;</code>
       * @param value The numFailedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumFailedTasks(int value) {
        
        numFailedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_failed_tasks = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumFailedTasks() {
        
        numFailedTasks_ = 0;
        onChanged();
        return this;
      }

      private int numKilledTasks_ ;
      /**
       * <code>int32 num_killed_tasks = 8;</code>
       * @return The numKilledTasks.
       */
      @java.lang.Override
      public int getNumKilledTasks() {
        return numKilledTasks_;
      }
      /**
       * <code>int32 num_killed_tasks = 8;</code>
       * @param value The numKilledTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumKilledTasks(int value) {
        
        numKilledTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_killed_tasks = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumKilledTasks() {
        
        numKilledTasks_ = 0;
        onChanged();
        return this;
      }

      private int numCompletedIndices_ ;
      /**
       * <code>int32 num_completed_indices = 9;</code>
       * @return The numCompletedIndices.
       */
      @java.lang.Override
      public int getNumCompletedIndices() {
        return numCompletedIndices_;
      }
      /**
       * <code>int32 num_completed_indices = 9;</code>
       * @param value The numCompletedIndices to set.
       * @return This builder for chaining.
       */
      public Builder setNumCompletedIndices(int value) {
        
        numCompletedIndices_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_completed_indices = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCompletedIndices() {
        
        numCompletedIndices_ = 0;
        onChanged();
        return this;
      }

      private long submissionTime_ ;
      /**
       * <code>int64 submission_time = 10;</code>
       * @return Whether the submissionTime field is set.
       */
      @java.lang.Override
      public boolean hasSubmissionTime() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>int64 submission_time = 10;</code>
       * @return The submissionTime.
       */
      @java.lang.Override
      public long getSubmissionTime() {
        return submissionTime_;
      }
      /**
       * <code>int64 submission_time = 10;</code>
       * @param value The submissionTime to set.
       * @return This builder for chaining.
       */
      public Builder setSubmissionTime(long value) {
        bitField0_ |= 0x00000001;
        submissionTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 submission_time = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearSubmissionTime() {
        bitField0_ = (bitField0_ & ~0x00000001);
        submissionTime_ = 0L;
        onChanged();
        return this;
      }

      private long firstTaskLaunchedTime_ ;
      /**
       * <code>int64 first_task_launched_time = 11;</code>
       * @return Whether the firstTaskLaunchedTime field is set.
       */
      @java.lang.Override
      public boolean hasFirstTaskLaunchedTime() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>int64 first_task_launched_time = 11;</code>
       * @return The firstTaskLaunchedTime.
       */
      @java.lang.Override
      public long getFirstTaskLaunchedTime() {
        return firstTaskLaunchedTime_;
      }
      /**
       * <code>int64 first_task_launched_time = 11;</code>
       * @param value The firstTaskLaunchedTime to set.
       * @return This builder for chaining.
       */
      public Builder setFirstTaskLaunchedTime(long value) {
        bitField0_ |= 0x00000002;
        firstTaskLaunchedTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 first_task_launched_time = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearFirstTaskLaunchedTime() {
        bitField0_ = (bitField0_ & ~0x00000002);
        firstTaskLaunchedTime_ = 0L;
        onChanged();
        return this;
      }

      private long completionTime_ ;
      /**
       * <code>int64 completion_time = 12;</code>
       * @return Whether the completionTime field is set.
       */
      @java.lang.Override
      public boolean hasCompletionTime() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>int64 completion_time = 12;</code>
       * @return The completionTime.
       */
      @java.lang.Override
      public long getCompletionTime() {
        return completionTime_;
      }
      /**
       * <code>int64 completion_time = 12;</code>
       * @param value The completionTime to set.
       * @return This builder for chaining.
       */
      public Builder setCompletionTime(long value) {
        bitField0_ |= 0x00000004;
        completionTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 completion_time = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearCompletionTime() {
        bitField0_ = (bitField0_ & ~0x00000004);
        completionTime_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object failureReason_ = "";
      /**
       * <code>string failure_reason = 13;</code>
       * @return Whether the failureReason field is set.
       */
      public boolean hasFailureReason() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>string failure_reason = 13;</code>
       * @return The failureReason.
       */
      public java.lang.String getFailureReason() {
        java.lang.Object ref = failureReason_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          failureReason_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string failure_reason = 13;</code>
       * @return The bytes for failureReason.
       */
      public com.google.protobuf.ByteString
          getFailureReasonBytes() {
        java.lang.Object ref = failureReason_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          failureReason_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string failure_reason = 13;</code>
       * @param value The failureReason to set.
       * @return This builder for chaining.
       */
      public Builder setFailureReason(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        failureReason_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string failure_reason = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearFailureReason() {
        bitField0_ = (bitField0_ & ~0x00000008);
        failureReason_ = getDefaultInstance().getFailureReason();
        onChanged();
        return this;
      }
      /**
       * <code>string failure_reason = 13;</code>
       * @param value The bytes for failureReason to set.
       * @return This builder for chaining.
       */
      public Builder setFailureReasonBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000008;
        failureReason_ = value;
        onChanged();
        return this;
      }

      private long executorDeserializeTime_ ;
      /**
       * <code>int64 executor_deserialize_time = 14;</code>
       * @return The executorDeserializeTime.
       */
      @java.lang.Override
      public long getExecutorDeserializeTime() {
        return executorDeserializeTime_;
      }
      /**
       * <code>int64 executor_deserialize_time = 14;</code>
       * @param value The executorDeserializeTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeTime(long value) {
        
        executorDeserializeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_deserialize_time = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeTime() {
        
        executorDeserializeTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorDeserializeCpuTime_ ;
      /**
       * <code>int64 executor_deserialize_cpu_time = 15;</code>
       * @return The executorDeserializeCpuTime.
       */
      @java.lang.Override
      public long getExecutorDeserializeCpuTime() {
        return executorDeserializeCpuTime_;
      }
      /**
       * <code>int64 executor_deserialize_cpu_time = 15;</code>
       * @param value The executorDeserializeCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeCpuTime(long value) {
        
        executorDeserializeCpuTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_deserialize_cpu_time = 15;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeCpuTime() {
        
        executorDeserializeCpuTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorRunTime_ ;
      /**
       * <code>int64 executor_run_time = 16;</code>
       * @return The executorRunTime.
       */
      @java.lang.Override
      public long getExecutorRunTime() {
        return executorRunTime_;
      }
      /**
       * <code>int64 executor_run_time = 16;</code>
       * @param value The executorRunTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorRunTime(long value) {
        
        executorRunTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_run_time = 16;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorRunTime() {
        
        executorRunTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorCpuTime_ ;
      /**
       * <code>int64 executor_cpu_time = 17;</code>
       * @return The executorCpuTime.
       */
      @java.lang.Override
      public long getExecutorCpuTime() {
        return executorCpuTime_;
      }
      /**
       * <code>int64 executor_cpu_time = 17;</code>
       * @param value The executorCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorCpuTime(long value) {
        
        executorCpuTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_cpu_time = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorCpuTime() {
        
        executorCpuTime_ = 0L;
        onChanged();
        return this;
      }

      private long resultSize_ ;
      /**
       * <code>int64 result_size = 18;</code>
       * @return The resultSize.
       */
      @java.lang.Override
      public long getResultSize() {
        return resultSize_;
      }
      /**
       * <code>int64 result_size = 18;</code>
       * @param value The resultSize to set.
       * @return This builder for chaining.
       */
      public Builder setResultSize(long value) {
        
        resultSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 result_size = 18;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSize() {
        
        resultSize_ = 0L;
        onChanged();
        return this;
      }

      private long jvmGcTime_ ;
      /**
       * <code>int64 jvm_gc_time = 19;</code>
       * @return The jvmGcTime.
       */
      @java.lang.Override
      public long getJvmGcTime() {
        return jvmGcTime_;
      }
      /**
       * <code>int64 jvm_gc_time = 19;</code>
       * @param value The jvmGcTime to set.
       * @return This builder for chaining.
       */
      public Builder setJvmGcTime(long value) {
        
        jvmGcTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 jvm_gc_time = 19;</code>
       * @return This builder for chaining.
       */
      public Builder clearJvmGcTime() {
        
        jvmGcTime_ = 0L;
        onChanged();
        return this;
      }

      private long resultSerializationTime_ ;
      /**
       * <code>int64 result_serialization_time = 20;</code>
       * @return The resultSerializationTime.
       */
      @java.lang.Override
      public long getResultSerializationTime() {
        return resultSerializationTime_;
      }
      /**
       * <code>int64 result_serialization_time = 20;</code>
       * @param value The resultSerializationTime to set.
       * @return This builder for chaining.
       */
      public Builder setResultSerializationTime(long value) {
        
        resultSerializationTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 result_serialization_time = 20;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSerializationTime() {
        
        resultSerializationTime_ = 0L;
        onChanged();
        return this;
      }

      private long memoryBytesSpilled_ ;
      /**
       * <code>int64 memory_bytes_spilled = 21;</code>
       * @return The memoryBytesSpilled.
       */
      @java.lang.Override
      public long getMemoryBytesSpilled() {
        return memoryBytesSpilled_;
      }
      /**
       * <code>int64 memory_bytes_spilled = 21;</code>
       * @param value The memoryBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryBytesSpilled(long value) {
        
        memoryBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_bytes_spilled = 21;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryBytesSpilled() {
        
        memoryBytesSpilled_ = 0L;
        onChanged();
        return this;
      }

      private long diskBytesSpilled_ ;
      /**
       * <code>int64 disk_bytes_spilled = 22;</code>
       * @return The diskBytesSpilled.
       */
      @java.lang.Override
      public long getDiskBytesSpilled() {
        return diskBytesSpilled_;
      }
      /**
       * <code>int64 disk_bytes_spilled = 22;</code>
       * @param value The diskBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setDiskBytesSpilled(long value) {
        
        diskBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_bytes_spilled = 22;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskBytesSpilled() {
        
        diskBytesSpilled_ = 0L;
        onChanged();
        return this;
      }

      private long peakExecutionMemory_ ;
      /**
       * <code>int64 peak_execution_memory = 23;</code>
       * @return The peakExecutionMemory.
       */
      @java.lang.Override
      public long getPeakExecutionMemory() {
        return peakExecutionMemory_;
      }
      /**
       * <code>int64 peak_execution_memory = 23;</code>
       * @param value The peakExecutionMemory to set.
       * @return This builder for chaining.
       */
      public Builder setPeakExecutionMemory(long value) {
        
        peakExecutionMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 peak_execution_memory = 23;</code>
       * @return This builder for chaining.
       */
      public Builder clearPeakExecutionMemory() {
        
        peakExecutionMemory_ = 0L;
        onChanged();
        return this;
      }

      private long inputBytes_ ;
      /**
       * <code>int64 input_bytes = 24;</code>
       * @return The inputBytes.
       */
      @java.lang.Override
      public long getInputBytes() {
        return inputBytes_;
      }
      /**
       * <code>int64 input_bytes = 24;</code>
       * @param value The inputBytes to set.
       * @return This builder for chaining.
       */
      public Builder setInputBytes(long value) {
        
        inputBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 input_bytes = 24;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputBytes() {
        
        inputBytes_ = 0L;
        onChanged();
        return this;
      }

      private long inputRecords_ ;
      /**
       * <code>int64 input_records = 25;</code>
       * @return The inputRecords.
       */
      @java.lang.Override
      public long getInputRecords() {
        return inputRecords_;
      }
      /**
       * <code>int64 input_records = 25;</code>
       * @param value The inputRecords to set.
       * @return This builder for chaining.
       */
      public Builder setInputRecords(long value) {
        
        inputRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 input_records = 25;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputRecords() {
        
        inputRecords_ = 0L;
        onChanged();
        return this;
      }

      private long outputBytes_ ;
      /**
       * <code>int64 output_bytes = 26;</code>
       * @return The outputBytes.
       */
      @java.lang.Override
      public long getOutputBytes() {
        return outputBytes_;
      }
      /**
       * <code>int64 output_bytes = 26;</code>
       * @param value The outputBytes to set.
       * @return This builder for chaining.
       */
      public Builder setOutputBytes(long value) {
        
        outputBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 output_bytes = 26;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputBytes() {
        
        outputBytes_ = 0L;
        onChanged();
        return this;
      }

      private long outputRecords_ ;
      /**
       * <code>int64 output_records = 27;</code>
       * @return The outputRecords.
       */
      @java.lang.Override
      public long getOutputRecords() {
        return outputRecords_;
      }
      /**
       * <code>int64 output_records = 27;</code>
       * @param value The outputRecords to set.
       * @return This builder for chaining.
       */
      public Builder setOutputRecords(long value) {
        
        outputRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 output_records = 27;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputRecords() {
        
        outputRecords_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRemoteBlocksFetched_ ;
      /**
       * <code>int64 shuffle_remote_blocks_fetched = 28;</code>
       * @return The shuffleRemoteBlocksFetched.
       */
      @java.lang.Override
      public long getShuffleRemoteBlocksFetched() {
        return shuffleRemoteBlocksFetched_;
      }
      /**
       * <code>int64 shuffle_remote_blocks_fetched = 28;</code>
       * @param value The shuffleRemoteBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBlocksFetched(long value) {
        
        shuffleRemoteBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_remote_blocks_fetched = 28;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBlocksFetched() {
        
        shuffleRemoteBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleLocalBlocksFetched_ ;
      /**
       * <code>int64 shuffle_local_blocks_fetched = 29;</code>
       * @return The shuffleLocalBlocksFetched.
       */
      @java.lang.Override
      public long getShuffleLocalBlocksFetched() {
        return shuffleLocalBlocksFetched_;
      }
      /**
       * <code>int64 shuffle_local_blocks_fetched = 29;</code>
       * @param value The shuffleLocalBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleLocalBlocksFetched(long value) {
        
        shuffleLocalBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_local_blocks_fetched = 29;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleLocalBlocksFetched() {
        
        shuffleLocalBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleFetchWaitTime_ ;
      /**
       * <code>int64 shuffle_fetch_wait_time = 30;</code>
       * @return The shuffleFetchWaitTime.
       */
      @java.lang.Override
      public long getShuffleFetchWaitTime() {
        return shuffleFetchWaitTime_;
      }
      /**
       * <code>int64 shuffle_fetch_wait_time = 30;</code>
       * @param value The shuffleFetchWaitTime to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleFetchWaitTime(long value) {
        
        shuffleFetchWaitTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_fetch_wait_time = 30;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleFetchWaitTime() {
        
        shuffleFetchWaitTime_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRemoteBytesRead_ ;
      /**
       * <code>int64 shuffle_remote_bytes_read = 31;</code>
       * @return The shuffleRemoteBytesRead.
       */
      @java.lang.Override
      public long getShuffleRemoteBytesRead() {
        return shuffleRemoteBytesRead_;
      }
      /**
       * <code>int64 shuffle_remote_bytes_read = 31;</code>
       * @param value The shuffleRemoteBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBytesRead(long value) {
        
        shuffleRemoteBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_remote_bytes_read = 31;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBytesRead() {
        
        shuffleRemoteBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRemoteBytesReadToDisk_ ;
      /**
       * <code>int64 shuffle_remote_bytes_read_to_disk = 32;</code>
       * @return The shuffleRemoteBytesReadToDisk.
       */
      @java.lang.Override
      public long getShuffleRemoteBytesReadToDisk() {
        return shuffleRemoteBytesReadToDisk_;
      }
      /**
       * <code>int64 shuffle_remote_bytes_read_to_disk = 32;</code>
       * @param value The shuffleRemoteBytesReadToDisk to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteBytesReadToDisk(long value) {
        
        shuffleRemoteBytesReadToDisk_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_remote_bytes_read_to_disk = 32;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteBytesReadToDisk() {
        
        shuffleRemoteBytesReadToDisk_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleLocalBytesRead_ ;
      /**
       * <code>int64 shuffle_local_bytes_read = 33;</code>
       * @return The shuffleLocalBytesRead.
       */
      @java.lang.Override
      public long getShuffleLocalBytesRead() {
        return shuffleLocalBytesRead_;
      }
      /**
       * <code>int64 shuffle_local_bytes_read = 33;</code>
       * @param value The shuffleLocalBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleLocalBytesRead(long value) {
        
        shuffleLocalBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_local_bytes_read = 33;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleLocalBytesRead() {
        
        shuffleLocalBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleReadBytes_ ;
      /**
       * <code>int64 shuffle_read_bytes = 34;</code>
       * @return The shuffleReadBytes.
       */
      @java.lang.Override
      public long getShuffleReadBytes() {
        return shuffleReadBytes_;
      }
      /**
       * <code>int64 shuffle_read_bytes = 34;</code>
       * @param value The shuffleReadBytes to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleReadBytes(long value) {
        
        shuffleReadBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_read_bytes = 34;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleReadBytes() {
        
        shuffleReadBytes_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleReadRecords_ ;
      /**
       * <code>int64 shuffle_read_records = 35;</code>
       * @return The shuffleReadRecords.
       */
      @java.lang.Override
      public long getShuffleReadRecords() {
        return shuffleReadRecords_;
      }
      /**
       * <code>int64 shuffle_read_records = 35;</code>
       * @param value The shuffleReadRecords to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleReadRecords(long value) {
        
        shuffleReadRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_read_records = 35;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleReadRecords() {
        
        shuffleReadRecords_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleWriteBytes_ ;
      /**
       * <code>int64 shuffle_write_bytes = 36;</code>
       * @return The shuffleWriteBytes.
       */
      @java.lang.Override
      public long getShuffleWriteBytes() {
        return shuffleWriteBytes_;
      }
      /**
       * <code>int64 shuffle_write_bytes = 36;</code>
       * @param value The shuffleWriteBytes to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteBytes(long value) {
        
        shuffleWriteBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_write_bytes = 36;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteBytes() {
        
        shuffleWriteBytes_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleWriteTime_ ;
      /**
       * <code>int64 shuffle_write_time = 37;</code>
       * @return The shuffleWriteTime.
       */
      @java.lang.Override
      public long getShuffleWriteTime() {
        return shuffleWriteTime_;
      }
      /**
       * <code>int64 shuffle_write_time = 37;</code>
       * @param value The shuffleWriteTime to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteTime(long value) {
        
        shuffleWriteTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_write_time = 37;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteTime() {
        
        shuffleWriteTime_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleWriteRecords_ ;
      /**
       * <code>int64 shuffle_write_records = 38;</code>
       * @return The shuffleWriteRecords.
       */
      @java.lang.Override
      public long getShuffleWriteRecords() {
        return shuffleWriteRecords_;
      }
      /**
       * <code>int64 shuffle_write_records = 38;</code>
       * @param value The shuffleWriteRecords to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteRecords(long value) {
        
        shuffleWriteRecords_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_write_records = 38;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteRecords() {
        
        shuffleWriteRecords_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 39;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>string name = 39;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 39;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 39;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 39;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000010);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 39;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000010;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object description_ = "";
      /**
       * <code>string description = 40;</code>
       * @return Whether the description field is set.
       */
      public boolean hasDescription() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>string description = 40;</code>
       * @return The description.
       */
      public java.lang.String getDescription() {
        java.lang.Object ref = description_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          description_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string description = 40;</code>
       * @return The bytes for description.
       */
      public com.google.protobuf.ByteString
          getDescriptionBytes() {
        java.lang.Object ref = description_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          description_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string description = 40;</code>
       * @param value The description to set.
       * @return This builder for chaining.
       */
      public Builder setDescription(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        description_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string description = 40;</code>
       * @return This builder for chaining.
       */
      public Builder clearDescription() {
        bitField0_ = (bitField0_ & ~0x00000020);
        description_ = getDefaultInstance().getDescription();
        onChanged();
        return this;
      }
      /**
       * <code>string description = 40;</code>
       * @param value The bytes for description to set.
       * @return This builder for chaining.
       */
      public Builder setDescriptionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000020;
        description_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object details_ = "";
      /**
       * <code>string details = 41;</code>
       * @return Whether the details field is set.
       */
      public boolean hasDetails() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>string details = 41;</code>
       * @return The details.
       */
      public java.lang.String getDetails() {
        java.lang.Object ref = details_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          details_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string details = 41;</code>
       * @return The bytes for details.
       */
      public com.google.protobuf.ByteString
          getDetailsBytes() {
        java.lang.Object ref = details_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          details_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string details = 41;</code>
       * @param value The details to set.
       * @return This builder for chaining.
       */
      public Builder setDetails(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        details_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string details = 41;</code>
       * @return This builder for chaining.
       */
      public Builder clearDetails() {
        bitField0_ = (bitField0_ & ~0x00000040);
        details_ = getDefaultInstance().getDetails();
        onChanged();
        return this;
      }
      /**
       * <code>string details = 41;</code>
       * @param value The bytes for details to set.
       * @return This builder for chaining.
       */
      public Builder setDetailsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000040;
        details_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object schedulingPool_ = "";
      /**
       * <code>string scheduling_pool = 42;</code>
       * @return Whether the schedulingPool field is set.
       */
      public boolean hasSchedulingPool() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>string scheduling_pool = 42;</code>
       * @return The schedulingPool.
       */
      public java.lang.String getSchedulingPool() {
        java.lang.Object ref = schedulingPool_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          schedulingPool_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string scheduling_pool = 42;</code>
       * @return The bytes for schedulingPool.
       */
      public com.google.protobuf.ByteString
          getSchedulingPoolBytes() {
        java.lang.Object ref = schedulingPool_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          schedulingPool_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string scheduling_pool = 42;</code>
       * @param value The schedulingPool to set.
       * @return This builder for chaining.
       */
      public Builder setSchedulingPool(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        schedulingPool_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string scheduling_pool = 42;</code>
       * @return This builder for chaining.
       */
      public Builder clearSchedulingPool() {
        bitField0_ = (bitField0_ & ~0x00000080);
        schedulingPool_ = getDefaultInstance().getSchedulingPool();
        onChanged();
        return this;
      }
      /**
       * <code>string scheduling_pool = 42;</code>
       * @param value The bytes for schedulingPool to set.
       * @return This builder for chaining.
       */
      public Builder setSchedulingPoolBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000080;
        schedulingPool_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList rddIds_ = emptyLongList();
      private void ensureRddIdsIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          rddIds_ = mutableCopy(rddIds_);
          bitField0_ |= 0x00000100;
         }
      }
      /**
       * <code>repeated int64 rdd_ids = 43;</code>
       * @return A list containing the rddIds.
       */
      public java.util.List<java.lang.Long>
          getRddIdsList() {
        return ((bitField0_ & 0x00000100) != 0) ?
                 java.util.Collections.unmodifiableList(rddIds_) : rddIds_;
      }
      /**
       * <code>repeated int64 rdd_ids = 43;</code>
       * @return The count of rddIds.
       */
      public int getRddIdsCount() {
        return rddIds_.size();
      }
      /**
       * <code>repeated int64 rdd_ids = 43;</code>
       * @param index The index of the element to return.
       * @return The rddIds at the given index.
       */
      public long getRddIds(int index) {
        return rddIds_.getLong(index);
      }
      /**
       * <code>repeated int64 rdd_ids = 43;</code>
       * @param index The index to set the value at.
       * @param value The rddIds to set.
       * @return This builder for chaining.
       */
      public Builder setRddIds(
          int index, long value) {
        ensureRddIdsIsMutable();
        rddIds_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 rdd_ids = 43;</code>
       * @param value The rddIds to add.
       * @return This builder for chaining.
       */
      public Builder addRddIds(long value) {
        ensureRddIdsIsMutable();
        rddIds_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 rdd_ids = 43;</code>
       * @param values The rddIds to add.
       * @return This builder for chaining.
       */
      public Builder addAllRddIds(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureRddIdsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, rddIds_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 rdd_ids = 43;</code>
       * @return This builder for chaining.
       */
      public Builder clearRddIds() {
        rddIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000100);
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> accumulatorUpdates_ =
        java.util.Collections.emptyList();
      private void ensureAccumulatorUpdatesIsMutable() {
        if (!((bitField0_ & 0x00000200) != 0)) {
          accumulatorUpdates_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo>(accumulatorUpdates_);
          bitField0_ |= 0x00000200;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> accumulatorUpdatesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> getAccumulatorUpdatesList() {
        if (accumulatorUpdatesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(accumulatorUpdates_);
        } else {
          return accumulatorUpdatesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public int getAccumulatorUpdatesCount() {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.size();
        } else {
          return accumulatorUpdatesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo getAccumulatorUpdates(int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.get(index);
        } else {
          return accumulatorUpdatesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder setAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.set(index, value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder setAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.set(index, builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder addAccumulatorUpdates(org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder addAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo value) {
        if (accumulatorUpdatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(index, value);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder addAccumulatorUpdates(
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder addAccumulatorUpdates(
          int index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder builderForValue) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.add(index, builderForValue.build());
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder addAllAccumulatorUpdates(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo> values) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, accumulatorUpdates_);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder clearAccumulatorUpdates() {
        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdates_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public Builder removeAccumulatorUpdates(int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          ensureAccumulatorUpdatesIsMutable();
          accumulatorUpdates_.remove(index);
          onChanged();
        } else {
          accumulatorUpdatesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder getAccumulatorUpdatesBuilder(
          int index) {
        return getAccumulatorUpdatesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder getAccumulatorUpdatesOrBuilder(
          int index) {
        if (accumulatorUpdatesBuilder_ == null) {
          return accumulatorUpdates_.get(index);  } else {
          return accumulatorUpdatesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
           getAccumulatorUpdatesOrBuilderList() {
        if (accumulatorUpdatesBuilder_ != null) {
          return accumulatorUpdatesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(accumulatorUpdates_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder addAccumulatorUpdatesBuilder() {
        return getAccumulatorUpdatesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder addAccumulatorUpdatesBuilder(
          int index) {
        return getAccumulatorUpdatesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.AccumulableInfo accumulator_updates = 44;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder> 
           getAccumulatorUpdatesBuilderList() {
        return getAccumulatorUpdatesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder> 
          getAccumulatorUpdatesFieldBuilder() {
        if (accumulatorUpdatesBuilder_ == null) {
          accumulatorUpdatesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfo.Builder, org.apache.spark.status.protobuf.StoreTypes.AccumulableInfoOrBuilder>(
                  accumulatorUpdates_,
                  ((bitField0_ & 0x00000200) != 0),
                  getParentForChildren(),
                  isClean());
          accumulatorUpdates_ = null;
        }
        return accumulatorUpdatesBuilder_;
      }

      private com.google.protobuf.MapField<
          java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> tasks_;
      private com.google.protobuf.MapField<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>
      internalGetTasks() {
        if (tasks_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              TasksDefaultEntryHolder.defaultEntry);
        }
        return tasks_;
      }
      private com.google.protobuf.MapField<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>
      internalGetMutableTasks() {
        onChanged();;
        if (tasks_ == null) {
          tasks_ = com.google.protobuf.MapField.newMapField(
              TasksDefaultEntryHolder.defaultEntry);
        }
        if (!tasks_.isMutable()) {
          tasks_ = tasks_.copy();
        }
        return tasks_;
      }

      public int getTasksCount() {
        return internalGetTasks().getMap().size();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
       */

      @java.lang.Override
      public boolean containsTasks(
          long key) {
        
        return internalGetTasks().getMap().containsKey(key);
      }
      /**
       * Use {@link #getTasksMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> getTasks() {
        return getTasksMap();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> getTasksMap() {
        return internalGetTasks().getMap();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.TaskData getTasksOrDefault(
          long key,
          org.apache.spark.status.protobuf.StoreTypes.TaskData defaultValue) {
        
        java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> map =
            internalGetTasks().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.TaskData getTasksOrThrow(
          long key) {
        
        java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> map =
            internalGetTasks().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearTasks() {
        internalGetMutableTasks().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
       */

      public Builder removeTasks(
          long key) {
        
        internalGetMutableTasks().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData>
      getMutableTasks() {
        return internalGetMutableTasks().getMutableMap();
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
       */
      public Builder putTasks(
          long key,
          org.apache.spark.status.protobuf.StoreTypes.TaskData value) {
        
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableTasks().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;int64, .org.apache.spark.status.protobuf.TaskData&gt; tasks = 45;</code>
       */

      public Builder putAllTasks(
          java.util.Map<java.lang.Long, org.apache.spark.status.protobuf.StoreTypes.TaskData> values) {
        internalGetMutableTasks().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> executorSummary_;
      private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>
      internalGetExecutorSummary() {
        if (executorSummary_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ExecutorSummaryDefaultEntryHolder.defaultEntry);
        }
        return executorSummary_;
      }
      private com.google.protobuf.MapField<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>
      internalGetMutableExecutorSummary() {
        onChanged();;
        if (executorSummary_ == null) {
          executorSummary_ = com.google.protobuf.MapField.newMapField(
              ExecutorSummaryDefaultEntryHolder.defaultEntry);
        }
        if (!executorSummary_.isMutable()) {
          executorSummary_ = executorSummary_.copy();
        }
        return executorSummary_;
      }

      public int getExecutorSummaryCount() {
        return internalGetExecutorSummary().getMap().size();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
       */

      @java.lang.Override
      public boolean containsExecutorSummary(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetExecutorSummary().getMap().containsKey(key);
      }
      /**
       * Use {@link #getExecutorSummaryMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> getExecutorSummary() {
        return getExecutorSummaryMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> getExecutorSummaryMap() {
        return internalGetExecutorSummary().getMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getExecutorSummaryOrDefault(
          java.lang.String key,
          org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> map =
            internalGetExecutorSummary().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
       */
      @java.lang.Override

      public org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary getExecutorSummaryOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> map =
            internalGetExecutorSummary().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearExecutorSummary() {
        internalGetMutableExecutorSummary().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
       */

      public Builder removeExecutorSummary(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableExecutorSummary().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary>
      getMutableExecutorSummary() {
        return internalGetMutableExecutorSummary().getMutableMap();
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
       */
      public Builder putExecutorSummary(
          java.lang.String key,
          org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableExecutorSummary().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, .org.apache.spark.status.protobuf.ExecutorStageSummary&gt; executor_summary = 46;</code>
       */

      public Builder putAllExecutorSummary(
          java.util.Map<java.lang.String, org.apache.spark.status.protobuf.StoreTypes.ExecutorStageSummary> values) {
        internalGetMutableExecutorSummary().getMutableMap()
            .putAll(values);
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary speculationSummary_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder> speculationSummaryBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       * @return Whether the speculationSummary field is set.
       */
      public boolean hasSpeculationSummary() {
        return ((bitField0_ & 0x00001000) != 0);
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       * @return The speculationSummary.
       */
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary getSpeculationSummary() {
        if (speculationSummaryBuilder_ == null) {
          return speculationSummary_ == null ? org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance() : speculationSummary_;
        } else {
          return speculationSummaryBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       */
      public Builder setSpeculationSummary(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary value) {
        if (speculationSummaryBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          speculationSummary_ = value;
          onChanged();
        } else {
          speculationSummaryBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       */
      public Builder setSpeculationSummary(
          org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder builderForValue) {
        if (speculationSummaryBuilder_ == null) {
          speculationSummary_ = builderForValue.build();
          onChanged();
        } else {
          speculationSummaryBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       */
      public Builder mergeSpeculationSummary(org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary value) {
        if (speculationSummaryBuilder_ == null) {
          if (((bitField0_ & 0x00001000) != 0) &&
              speculationSummary_ != null &&
              speculationSummary_ != org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance()) {
            speculationSummary_ =
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.newBuilder(speculationSummary_).mergeFrom(value).buildPartial();
          } else {
            speculationSummary_ = value;
          }
          onChanged();
        } else {
          speculationSummaryBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       */
      public Builder clearSpeculationSummary() {
        if (speculationSummaryBuilder_ == null) {
          speculationSummary_ = null;
          onChanged();
        } else {
          speculationSummaryBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00001000);
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder getSpeculationSummaryBuilder() {
        bitField0_ |= 0x00001000;
        onChanged();
        return getSpeculationSummaryFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder getSpeculationSummaryOrBuilder() {
        if (speculationSummaryBuilder_ != null) {
          return speculationSummaryBuilder_.getMessageOrBuilder();
        } else {
          return speculationSummary_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.getDefaultInstance() : speculationSummary_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SpeculationStageSummary speculation_summary = 47;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder> 
          getSpeculationSummaryFieldBuilder() {
        if (speculationSummaryBuilder_ == null) {
          speculationSummaryBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummary.Builder, org.apache.spark.status.protobuf.StoreTypes.SpeculationStageSummaryOrBuilder>(
                  getSpeculationSummary(),
                  getParentForChildren(),
                  isClean());
          speculationSummary_ = null;
        }
        return speculationSummaryBuilder_;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.Integer> killedTasksSummary_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
      internalGetKilledTasksSummary() {
        if (killedTasksSummary_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              KilledTasksSummaryDefaultEntryHolder.defaultEntry);
        }
        return killedTasksSummary_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
      internalGetMutableKilledTasksSummary() {
        onChanged();;
        if (killedTasksSummary_ == null) {
          killedTasksSummary_ = com.google.protobuf.MapField.newMapField(
              KilledTasksSummaryDefaultEntryHolder.defaultEntry);
        }
        if (!killedTasksSummary_.isMutable()) {
          killedTasksSummary_ = killedTasksSummary_.copy();
        }
        return killedTasksSummary_;
      }

      public int getKilledTasksSummaryCount() {
        return internalGetKilledTasksSummary().getMap().size();
      }
      /**
       * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
       */

      @java.lang.Override
      public boolean containsKilledTasksSummary(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetKilledTasksSummary().getMap().containsKey(key);
      }
      /**
       * Use {@link #getKilledTasksSummaryMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Integer> getKilledTasksSummary() {
        return getKilledTasksSummaryMap();
      }
      /**
       * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.Integer> getKilledTasksSummaryMap() {
        return internalGetKilledTasksSummary().getMap();
      }
      /**
       * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
       */
      @java.lang.Override

      public int getKilledTasksSummaryOrDefault(
          java.lang.String key,
          int defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Integer> map =
            internalGetKilledTasksSummary().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
       */
      @java.lang.Override

      public int getKilledTasksSummaryOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Integer> map =
            internalGetKilledTasksSummary().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearKilledTasksSummary() {
        internalGetMutableKilledTasksSummary().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
       */

      public Builder removeKilledTasksSummary(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableKilledTasksSummary().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Integer>
      getMutableKilledTasksSummary() {
        return internalGetMutableKilledTasksSummary().getMutableMap();
      }
      /**
       * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
       */
      public Builder putKilledTasksSummary(
          java.lang.String key,
          int value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        
        internalGetMutableKilledTasksSummary().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, int32&gt; killed_tasks_summary = 48;</code>
       */

      public Builder putAllKilledTasksSummary(
          java.util.Map<java.lang.String, java.lang.Integer> values) {
        internalGetMutableKilledTasksSummary().getMutableMap()
            .putAll(values);
        return this;
      }

      private int resourceProfileId_ ;
      /**
       * <code>int32 resource_profile_id = 49;</code>
       * @return The resourceProfileId.
       */
      @java.lang.Override
      public int getResourceProfileId() {
        return resourceProfileId_;
      }
      /**
       * <code>int32 resource_profile_id = 49;</code>
       * @param value The resourceProfileId to set.
       * @return This builder for chaining.
       */
      public Builder setResourceProfileId(int value) {
        
        resourceProfileId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 resource_profile_id = 49;</code>
       * @return This builder for chaining.
       */
      public Builder clearResourceProfileId() {
        
        resourceProfileId_ = 0;
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics peakExecutorMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> peakExecutorMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       * @return Whether the peakExecutorMetrics field is set.
       */
      public boolean hasPeakExecutorMetrics() {
        return ((bitField0_ & 0x00004000) != 0);
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       * @return The peakExecutorMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getPeakExecutorMetrics() {
        if (peakExecutorMetricsBuilder_ == null) {
          return peakExecutorMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakExecutorMetrics_;
        } else {
          return peakExecutorMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       */
      public Builder setPeakExecutorMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (peakExecutorMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          peakExecutorMetrics_ = value;
          onChanged();
        } else {
          peakExecutorMetricsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       */
      public Builder setPeakExecutorMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder builderForValue) {
        if (peakExecutorMetricsBuilder_ == null) {
          peakExecutorMetrics_ = builderForValue.build();
          onChanged();
        } else {
          peakExecutorMetricsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       */
      public Builder mergePeakExecutorMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (peakExecutorMetricsBuilder_ == null) {
          if (((bitField0_ & 0x00004000) != 0) &&
              peakExecutorMetrics_ != null &&
              peakExecutorMetrics_ != org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance()) {
            peakExecutorMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.newBuilder(peakExecutorMetrics_).mergeFrom(value).buildPartial();
          } else {
            peakExecutorMetrics_ = value;
          }
          onChanged();
        } else {
          peakExecutorMetricsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       */
      public Builder clearPeakExecutorMetrics() {
        if (peakExecutorMetricsBuilder_ == null) {
          peakExecutorMetrics_ = null;
          onChanged();
        } else {
          peakExecutorMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00004000);
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder getPeakExecutorMetricsBuilder() {
        bitField0_ |= 0x00004000;
        onChanged();
        return getPeakExecutorMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getPeakExecutorMetricsOrBuilder() {
        if (peakExecutorMetricsBuilder_ != null) {
          return peakExecutorMetricsBuilder_.getMessageOrBuilder();
        } else {
          return peakExecutorMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance() : peakExecutorMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetrics peak_executor_metrics = 50;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> 
          getPeakExecutorMetricsFieldBuilder() {
        if (peakExecutorMetricsBuilder_ == null) {
          peakExecutorMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder>(
                  getPeakExecutorMetrics(),
                  getParentForChildren(),
                  isClean());
          peakExecutorMetrics_ = null;
        }
        return peakExecutorMetricsBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions taskMetricsDistributions_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributionsOrBuilder> taskMetricsDistributionsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       * @return Whether the taskMetricsDistributions field is set.
       */
      public boolean hasTaskMetricsDistributions() {
        return ((bitField0_ & 0x00008000) != 0);
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       * @return The taskMetricsDistributions.
       */
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions getTaskMetricsDistributions() {
        if (taskMetricsDistributionsBuilder_ == null) {
          return taskMetricsDistributions_ == null ? org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.getDefaultInstance() : taskMetricsDistributions_;
        } else {
          return taskMetricsDistributionsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       */
      public Builder setTaskMetricsDistributions(org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions value) {
        if (taskMetricsDistributionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          taskMetricsDistributions_ = value;
          onChanged();
        } else {
          taskMetricsDistributionsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00008000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       */
      public Builder setTaskMetricsDistributions(
          org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.Builder builderForValue) {
        if (taskMetricsDistributionsBuilder_ == null) {
          taskMetricsDistributions_ = builderForValue.build();
          onChanged();
        } else {
          taskMetricsDistributionsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00008000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       */
      public Builder mergeTaskMetricsDistributions(org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions value) {
        if (taskMetricsDistributionsBuilder_ == null) {
          if (((bitField0_ & 0x00008000) != 0) &&
              taskMetricsDistributions_ != null &&
              taskMetricsDistributions_ != org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.getDefaultInstance()) {
            taskMetricsDistributions_ =
              org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.newBuilder(taskMetricsDistributions_).mergeFrom(value).buildPartial();
          } else {
            taskMetricsDistributions_ = value;
          }
          onChanged();
        } else {
          taskMetricsDistributionsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00008000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       */
      public Builder clearTaskMetricsDistributions() {
        if (taskMetricsDistributionsBuilder_ == null) {
          taskMetricsDistributions_ = null;
          onChanged();
        } else {
          taskMetricsDistributionsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00008000);
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.Builder getTaskMetricsDistributionsBuilder() {
        bitField0_ |= 0x00008000;
        onChanged();
        return getTaskMetricsDistributionsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributionsOrBuilder getTaskMetricsDistributionsOrBuilder() {
        if (taskMetricsDistributionsBuilder_ != null) {
          return taskMetricsDistributionsBuilder_.getMessageOrBuilder();
        } else {
          return taskMetricsDistributions_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.getDefaultInstance() : taskMetricsDistributions_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.TaskMetricDistributions task_metrics_distributions = 51;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributionsOrBuilder> 
          getTaskMetricsDistributionsFieldBuilder() {
        if (taskMetricsDistributionsBuilder_ == null) {
          taskMetricsDistributionsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributionsOrBuilder>(
                  getTaskMetricsDistributions(),
                  getParentForChildren(),
                  isClean());
          taskMetricsDistributions_ = null;
        }
        return taskMetricsDistributionsBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions executorMetricsDistributions_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributionsOrBuilder> executorMetricsDistributionsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       * @return Whether the executorMetricsDistributions field is set.
       */
      public boolean hasExecutorMetricsDistributions() {
        return ((bitField0_ & 0x00010000) != 0);
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       * @return The executorMetricsDistributions.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions getExecutorMetricsDistributions() {
        if (executorMetricsDistributionsBuilder_ == null) {
          return executorMetricsDistributions_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.getDefaultInstance() : executorMetricsDistributions_;
        } else {
          return executorMetricsDistributionsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       */
      public Builder setExecutorMetricsDistributions(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions value) {
        if (executorMetricsDistributionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          executorMetricsDistributions_ = value;
          onChanged();
        } else {
          executorMetricsDistributionsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00010000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       */
      public Builder setExecutorMetricsDistributions(
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.Builder builderForValue) {
        if (executorMetricsDistributionsBuilder_ == null) {
          executorMetricsDistributions_ = builderForValue.build();
          onChanged();
        } else {
          executorMetricsDistributionsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00010000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       */
      public Builder mergeExecutorMetricsDistributions(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions value) {
        if (executorMetricsDistributionsBuilder_ == null) {
          if (((bitField0_ & 0x00010000) != 0) &&
              executorMetricsDistributions_ != null &&
              executorMetricsDistributions_ != org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.getDefaultInstance()) {
            executorMetricsDistributions_ =
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.newBuilder(executorMetricsDistributions_).mergeFrom(value).buildPartial();
          } else {
            executorMetricsDistributions_ = value;
          }
          onChanged();
        } else {
          executorMetricsDistributionsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00010000;
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       */
      public Builder clearExecutorMetricsDistributions() {
        if (executorMetricsDistributionsBuilder_ == null) {
          executorMetricsDistributions_ = null;
          onChanged();
        } else {
          executorMetricsDistributionsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00010000);
        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.Builder getExecutorMetricsDistributionsBuilder() {
        bitField0_ |= 0x00010000;
        onChanged();
        return getExecutorMetricsDistributionsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributionsOrBuilder getExecutorMetricsDistributionsOrBuilder() {
        if (executorMetricsDistributionsBuilder_ != null) {
          return executorMetricsDistributionsBuilder_.getMessageOrBuilder();
        } else {
          return executorMetricsDistributions_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.getDefaultInstance() : executorMetricsDistributions_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorMetricsDistributions executor_metrics_distributions = 52;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributionsOrBuilder> 
          getExecutorMetricsDistributionsFieldBuilder() {
        if (executorMetricsDistributionsBuilder_ == null) {
          executorMetricsDistributionsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributionsOrBuilder>(
                  getExecutorMetricsDistributions(),
                  getParentForChildren(),
                  isClean());
          executorMetricsDistributions_ = null;
        }
        return executorMetricsDistributionsBuilder_;
      }

      private long shuffleCorruptMergedBlockChunks_ ;
      /**
       * <code>int64 shuffle_corrupt_merged_block_chunks = 53;</code>
       * @return The shuffleCorruptMergedBlockChunks.
       */
      @java.lang.Override
      public long getShuffleCorruptMergedBlockChunks() {
        return shuffleCorruptMergedBlockChunks_;
      }
      /**
       * <code>int64 shuffle_corrupt_merged_block_chunks = 53;</code>
       * @param value The shuffleCorruptMergedBlockChunks to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleCorruptMergedBlockChunks(long value) {
        
        shuffleCorruptMergedBlockChunks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_corrupt_merged_block_chunks = 53;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleCorruptMergedBlockChunks() {
        
        shuffleCorruptMergedBlockChunks_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedFetchFallbackCount_ ;
      /**
       * <code>int64 shuffle_merged_fetch_fallback_count = 54;</code>
       * @return The shuffleMergedFetchFallbackCount.
       */
      @java.lang.Override
      public long getShuffleMergedFetchFallbackCount() {
        return shuffleMergedFetchFallbackCount_;
      }
      /**
       * <code>int64 shuffle_merged_fetch_fallback_count = 54;</code>
       * @param value The shuffleMergedFetchFallbackCount to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedFetchFallbackCount(long value) {
        
        shuffleMergedFetchFallbackCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_fetch_fallback_count = 54;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedFetchFallbackCount() {
        
        shuffleMergedFetchFallbackCount_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedRemoteBlocksFetched_ ;
      /**
       * <code>int64 shuffle_merged_remote_blocks_fetched = 55;</code>
       * @return The shuffleMergedRemoteBlocksFetched.
       */
      @java.lang.Override
      public long getShuffleMergedRemoteBlocksFetched() {
        return shuffleMergedRemoteBlocksFetched_;
      }
      /**
       * <code>int64 shuffle_merged_remote_blocks_fetched = 55;</code>
       * @param value The shuffleMergedRemoteBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteBlocksFetched(long value) {
        
        shuffleMergedRemoteBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_remote_blocks_fetched = 55;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteBlocksFetched() {
        
        shuffleMergedRemoteBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedLocalBlocksFetched_ ;
      /**
       * <code>int64 shuffle_merged_local_blocks_fetched = 56;</code>
       * @return The shuffleMergedLocalBlocksFetched.
       */
      @java.lang.Override
      public long getShuffleMergedLocalBlocksFetched() {
        return shuffleMergedLocalBlocksFetched_;
      }
      /**
       * <code>int64 shuffle_merged_local_blocks_fetched = 56;</code>
       * @param value The shuffleMergedLocalBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalBlocksFetched(long value) {
        
        shuffleMergedLocalBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_local_blocks_fetched = 56;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalBlocksFetched() {
        
        shuffleMergedLocalBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedRemoteChunksFetched_ ;
      /**
       * <code>int64 shuffle_merged_remote_chunks_fetched = 57;</code>
       * @return The shuffleMergedRemoteChunksFetched.
       */
      @java.lang.Override
      public long getShuffleMergedRemoteChunksFetched() {
        return shuffleMergedRemoteChunksFetched_;
      }
      /**
       * <code>int64 shuffle_merged_remote_chunks_fetched = 57;</code>
       * @param value The shuffleMergedRemoteChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteChunksFetched(long value) {
        
        shuffleMergedRemoteChunksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_remote_chunks_fetched = 57;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteChunksFetched() {
        
        shuffleMergedRemoteChunksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedLocalChunksFetched_ ;
      /**
       * <code>int64 shuffle_merged_local_chunks_fetched = 58;</code>
       * @return The shuffleMergedLocalChunksFetched.
       */
      @java.lang.Override
      public long getShuffleMergedLocalChunksFetched() {
        return shuffleMergedLocalChunksFetched_;
      }
      /**
       * <code>int64 shuffle_merged_local_chunks_fetched = 58;</code>
       * @param value The shuffleMergedLocalChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalChunksFetched(long value) {
        
        shuffleMergedLocalChunksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_local_chunks_fetched = 58;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalChunksFetched() {
        
        shuffleMergedLocalChunksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedRemoteBytesRead_ ;
      /**
       * <code>int64 shuffle_merged_remote_bytes_read = 59;</code>
       * @return The shuffleMergedRemoteBytesRead.
       */
      @java.lang.Override
      public long getShuffleMergedRemoteBytesRead() {
        return shuffleMergedRemoteBytesRead_;
      }
      /**
       * <code>int64 shuffle_merged_remote_bytes_read = 59;</code>
       * @param value The shuffleMergedRemoteBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteBytesRead(long value) {
        
        shuffleMergedRemoteBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_remote_bytes_read = 59;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteBytesRead() {
        
        shuffleMergedRemoteBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedLocalBytesRead_ ;
      /**
       * <code>int64 shuffle_merged_local_bytes_read = 60;</code>
       * @return The shuffleMergedLocalBytesRead.
       */
      @java.lang.Override
      public long getShuffleMergedLocalBytesRead() {
        return shuffleMergedLocalBytesRead_;
      }
      /**
       * <code>int64 shuffle_merged_local_bytes_read = 60;</code>
       * @param value The shuffleMergedLocalBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedLocalBytesRead(long value) {
        
        shuffleMergedLocalBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_local_bytes_read = 60;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedLocalBytesRead() {
        
        shuffleMergedLocalBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleRemoteReqsDuration_ ;
      /**
       * <code>int64 shuffle_remote_reqs_duration = 61;</code>
       * @return The shuffleRemoteReqsDuration.
       */
      @java.lang.Override
      public long getShuffleRemoteReqsDuration() {
        return shuffleRemoteReqsDuration_;
      }
      /**
       * <code>int64 shuffle_remote_reqs_duration = 61;</code>
       * @param value The shuffleRemoteReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRemoteReqsDuration(long value) {
        
        shuffleRemoteReqsDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_remote_reqs_duration = 61;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRemoteReqsDuration() {
        
        shuffleRemoteReqsDuration_ = 0L;
        onChanged();
        return this;
      }

      private long shuffleMergedRemoteReqsDuration_ ;
      /**
       * <code>int64 shuffle_merged_remote_reqs_duration = 62;</code>
       * @return The shuffleMergedRemoteReqsDuration.
       */
      @java.lang.Override
      public long getShuffleMergedRemoteReqsDuration() {
        return shuffleMergedRemoteReqsDuration_;
      }
      /**
       * <code>int64 shuffle_merged_remote_reqs_duration = 62;</code>
       * @param value The shuffleMergedRemoteReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergedRemoteReqsDuration(long value) {
        
        shuffleMergedRemoteReqsDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 shuffle_merged_remote_reqs_duration = 62;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergedRemoteReqsDuration() {
        
        shuffleMergedRemoteReqsDuration_ = 0L;
        onChanged();
        return this;
      }

      private boolean isShufflePushEnabled_ ;
      /**
       * <code>bool is_shuffle_push_enabled = 63;</code>
       * @return The isShufflePushEnabled.
       */
      @java.lang.Override
      public boolean getIsShufflePushEnabled() {
        return isShufflePushEnabled_;
      }
      /**
       * <code>bool is_shuffle_push_enabled = 63;</code>
       * @param value The isShufflePushEnabled to set.
       * @return This builder for chaining.
       */
      public Builder setIsShufflePushEnabled(boolean value) {
        
        isShufflePushEnabled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool is_shuffle_push_enabled = 63;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsShufflePushEnabled() {
        
        isShufflePushEnabled_ = false;
        onChanged();
        return this;
      }

      private int shuffleMergersCount_ ;
      /**
       * <code>int32 shuffle_mergers_count = 64;</code>
       * @return The shuffleMergersCount.
       */
      @java.lang.Override
      public int getShuffleMergersCount() {
        return shuffleMergersCount_;
      }
      /**
       * <code>int32 shuffle_mergers_count = 64;</code>
       * @param value The shuffleMergersCount to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleMergersCount(int value) {
        
        shuffleMergersCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 shuffle_mergers_count = 64;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleMergersCount() {
        
        shuffleMergersCount_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.StageData)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.StageData)
    private static final org.apache.spark.status.protobuf.StoreTypes.StageData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.StageData();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StageData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StageData>
        PARSER = new com.google.protobuf.AbstractParser<StageData>() {
      @java.lang.Override
      public StageData parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StageData(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StageData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StageData> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StageData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.TaskMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 executor_deserialize_time = 1;</code>
     * @return The executorDeserializeTime.
     */
    long getExecutorDeserializeTime();

    /**
     * <code>int64 executor_deserialize_cpu_time = 2;</code>
     * @return The executorDeserializeCpuTime.
     */
    long getExecutorDeserializeCpuTime();

    /**
     * <code>int64 executor_run_time = 3;</code>
     * @return The executorRunTime.
     */
    long getExecutorRunTime();

    /**
     * <code>int64 executor_cpu_time = 4;</code>
     * @return The executorCpuTime.
     */
    long getExecutorCpuTime();

    /**
     * <code>int64 result_size = 5;</code>
     * @return The resultSize.
     */
    long getResultSize();

    /**
     * <code>int64 jvm_gc_time = 6;</code>
     * @return The jvmGcTime.
     */
    long getJvmGcTime();

    /**
     * <code>int64 result_serialization_time = 7;</code>
     * @return The resultSerializationTime.
     */
    long getResultSerializationTime();

    /**
     * <code>int64 memory_bytes_spilled = 8;</code>
     * @return The memoryBytesSpilled.
     */
    long getMemoryBytesSpilled();

    /**
     * <code>int64 disk_bytes_spilled = 9;</code>
     * @return The diskBytesSpilled.
     */
    long getDiskBytesSpilled();

    /**
     * <code>int64 peak_execution_memory = 10;</code>
     * @return The peakExecutionMemory.
     */
    long getPeakExecutionMemory();

    /**
     * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
     * @return Whether the inputMetrics field is set.
     */
    boolean hasInputMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
     * @return The inputMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.InputMetrics getInputMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.InputMetricsOrBuilder getInputMetricsOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
     * @return Whether the outputMetrics field is set.
     */
    boolean hasOutputMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
     * @return The outputMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.OutputMetrics getOutputMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.OutputMetricsOrBuilder getOutputMetricsOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
     * @return Whether the shuffleReadMetrics field is set.
     */
    boolean hasShuffleReadMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
     * @return The shuffleReadMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics getShuffleReadMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricsOrBuilder getShuffleReadMetricsOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
     * @return Whether the shuffleWriteMetrics field is set.
     */
    boolean hasShuffleWriteMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
     * @return The shuffleWriteMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics getShuffleWriteMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricsOrBuilder getShuffleWriteMetricsOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.TaskMetrics}
   */
  public static final class TaskMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.TaskMetrics)
      TaskMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskMetrics.newBuilder() to construct.
    private TaskMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskMetrics() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new TaskMetrics();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskMetrics(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              executorDeserializeTime_ = input.readInt64();
              break;
            }
            case 16: {

              executorDeserializeCpuTime_ = input.readInt64();
              break;
            }
            case 24: {

              executorRunTime_ = input.readInt64();
              break;
            }
            case 32: {

              executorCpuTime_ = input.readInt64();
              break;
            }
            case 40: {

              resultSize_ = input.readInt64();
              break;
            }
            case 48: {

              jvmGcTime_ = input.readInt64();
              break;
            }
            case 56: {

              resultSerializationTime_ = input.readInt64();
              break;
            }
            case 64: {

              memoryBytesSpilled_ = input.readInt64();
              break;
            }
            case 72: {

              diskBytesSpilled_ = input.readInt64();
              break;
            }
            case 80: {

              peakExecutionMemory_ = input.readInt64();
              break;
            }
            case 90: {
              org.apache.spark.status.protobuf.StoreTypes.InputMetrics.Builder subBuilder = null;
              if (inputMetrics_ != null) {
                subBuilder = inputMetrics_.toBuilder();
              }
              inputMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.InputMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(inputMetrics_);
                inputMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            case 98: {
              org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.Builder subBuilder = null;
              if (outputMetrics_ != null) {
                subBuilder = outputMetrics_.toBuilder();
              }
              outputMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(outputMetrics_);
                outputMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            case 106: {
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.Builder subBuilder = null;
              if (shuffleReadMetrics_ != null) {
                subBuilder = shuffleReadMetrics_.toBuilder();
              }
              shuffleReadMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(shuffleReadMetrics_);
                shuffleReadMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            case 114: {
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.Builder subBuilder = null;
              if (shuffleWriteMetrics_ != null) {
                subBuilder = shuffleWriteMetrics_.toBuilder();
              }
              shuffleWriteMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(shuffleWriteMetrics_);
                shuffleWriteMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetrics_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.class, org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.Builder.class);
    }

    public static final int EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER = 1;
    private long executorDeserializeTime_;
    /**
     * <code>int64 executor_deserialize_time = 1;</code>
     * @return The executorDeserializeTime.
     */
    @java.lang.Override
    public long getExecutorDeserializeTime() {
      return executorDeserializeTime_;
    }

    public static final int EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER = 2;
    private long executorDeserializeCpuTime_;
    /**
     * <code>int64 executor_deserialize_cpu_time = 2;</code>
     * @return The executorDeserializeCpuTime.
     */
    @java.lang.Override
    public long getExecutorDeserializeCpuTime() {
      return executorDeserializeCpuTime_;
    }

    public static final int EXECUTOR_RUN_TIME_FIELD_NUMBER = 3;
    private long executorRunTime_;
    /**
     * <code>int64 executor_run_time = 3;</code>
     * @return The executorRunTime.
     */
    @java.lang.Override
    public long getExecutorRunTime() {
      return executorRunTime_;
    }

    public static final int EXECUTOR_CPU_TIME_FIELD_NUMBER = 4;
    private long executorCpuTime_;
    /**
     * <code>int64 executor_cpu_time = 4;</code>
     * @return The executorCpuTime.
     */
    @java.lang.Override
    public long getExecutorCpuTime() {
      return executorCpuTime_;
    }

    public static final int RESULT_SIZE_FIELD_NUMBER = 5;
    private long resultSize_;
    /**
     * <code>int64 result_size = 5;</code>
     * @return The resultSize.
     */
    @java.lang.Override
    public long getResultSize() {
      return resultSize_;
    }

    public static final int JVM_GC_TIME_FIELD_NUMBER = 6;
    private long jvmGcTime_;
    /**
     * <code>int64 jvm_gc_time = 6;</code>
     * @return The jvmGcTime.
     */
    @java.lang.Override
    public long getJvmGcTime() {
      return jvmGcTime_;
    }

    public static final int RESULT_SERIALIZATION_TIME_FIELD_NUMBER = 7;
    private long resultSerializationTime_;
    /**
     * <code>int64 result_serialization_time = 7;</code>
     * @return The resultSerializationTime.
     */
    @java.lang.Override
    public long getResultSerializationTime() {
      return resultSerializationTime_;
    }

    public static final int MEMORY_BYTES_SPILLED_FIELD_NUMBER = 8;
    private long memoryBytesSpilled_;
    /**
     * <code>int64 memory_bytes_spilled = 8;</code>
     * @return The memoryBytesSpilled.
     */
    @java.lang.Override
    public long getMemoryBytesSpilled() {
      return memoryBytesSpilled_;
    }

    public static final int DISK_BYTES_SPILLED_FIELD_NUMBER = 9;
    private long diskBytesSpilled_;
    /**
     * <code>int64 disk_bytes_spilled = 9;</code>
     * @return The diskBytesSpilled.
     */
    @java.lang.Override
    public long getDiskBytesSpilled() {
      return diskBytesSpilled_;
    }

    public static final int PEAK_EXECUTION_MEMORY_FIELD_NUMBER = 10;
    private long peakExecutionMemory_;
    /**
     * <code>int64 peak_execution_memory = 10;</code>
     * @return The peakExecutionMemory.
     */
    @java.lang.Override
    public long getPeakExecutionMemory() {
      return peakExecutionMemory_;
    }

    public static final int INPUT_METRICS_FIELD_NUMBER = 11;
    private org.apache.spark.status.protobuf.StoreTypes.InputMetrics inputMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
     * @return Whether the inputMetrics field is set.
     */
    @java.lang.Override
    public boolean hasInputMetrics() {
      return inputMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
     * @return The inputMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.InputMetrics getInputMetrics() {
      return inputMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.InputMetrics.getDefaultInstance() : inputMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.InputMetricsOrBuilder getInputMetricsOrBuilder() {
      return getInputMetrics();
    }

    public static final int OUTPUT_METRICS_FIELD_NUMBER = 12;
    private org.apache.spark.status.protobuf.StoreTypes.OutputMetrics outputMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
     * @return Whether the outputMetrics field is set.
     */
    @java.lang.Override
    public boolean hasOutputMetrics() {
      return outputMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
     * @return The outputMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.OutputMetrics getOutputMetrics() {
      return outputMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.getDefaultInstance() : outputMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.OutputMetricsOrBuilder getOutputMetricsOrBuilder() {
      return getOutputMetrics();
    }

    public static final int SHUFFLE_READ_METRICS_FIELD_NUMBER = 13;
    private org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics shuffleReadMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
     * @return Whether the shuffleReadMetrics field is set.
     */
    @java.lang.Override
    public boolean hasShuffleReadMetrics() {
      return shuffleReadMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
     * @return The shuffleReadMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics getShuffleReadMetrics() {
      return shuffleReadMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.getDefaultInstance() : shuffleReadMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricsOrBuilder getShuffleReadMetricsOrBuilder() {
      return getShuffleReadMetrics();
    }

    public static final int SHUFFLE_WRITE_METRICS_FIELD_NUMBER = 14;
    private org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics shuffleWriteMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
     * @return Whether the shuffleWriteMetrics field is set.
     */
    @java.lang.Override
    public boolean hasShuffleWriteMetrics() {
      return shuffleWriteMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
     * @return The shuffleWriteMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics getShuffleWriteMetrics() {
      return shuffleWriteMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.getDefaultInstance() : shuffleWriteMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricsOrBuilder getShuffleWriteMetricsOrBuilder() {
      return getShuffleWriteMetrics();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (executorDeserializeTime_ != 0L) {
        output.writeInt64(1, executorDeserializeTime_);
      }
      if (executorDeserializeCpuTime_ != 0L) {
        output.writeInt64(2, executorDeserializeCpuTime_);
      }
      if (executorRunTime_ != 0L) {
        output.writeInt64(3, executorRunTime_);
      }
      if (executorCpuTime_ != 0L) {
        output.writeInt64(4, executorCpuTime_);
      }
      if (resultSize_ != 0L) {
        output.writeInt64(5, resultSize_);
      }
      if (jvmGcTime_ != 0L) {
        output.writeInt64(6, jvmGcTime_);
      }
      if (resultSerializationTime_ != 0L) {
        output.writeInt64(7, resultSerializationTime_);
      }
      if (memoryBytesSpilled_ != 0L) {
        output.writeInt64(8, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0L) {
        output.writeInt64(9, diskBytesSpilled_);
      }
      if (peakExecutionMemory_ != 0L) {
        output.writeInt64(10, peakExecutionMemory_);
      }
      if (inputMetrics_ != null) {
        output.writeMessage(11, getInputMetrics());
      }
      if (outputMetrics_ != null) {
        output.writeMessage(12, getOutputMetrics());
      }
      if (shuffleReadMetrics_ != null) {
        output.writeMessage(13, getShuffleReadMetrics());
      }
      if (shuffleWriteMetrics_ != null) {
        output.writeMessage(14, getShuffleWriteMetrics());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (executorDeserializeTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, executorDeserializeTime_);
      }
      if (executorDeserializeCpuTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, executorDeserializeCpuTime_);
      }
      if (executorRunTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, executorRunTime_);
      }
      if (executorCpuTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, executorCpuTime_);
      }
      if (resultSize_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, resultSize_);
      }
      if (jvmGcTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, jvmGcTime_);
      }
      if (resultSerializationTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, resultSerializationTime_);
      }
      if (memoryBytesSpilled_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, memoryBytesSpilled_);
      }
      if (diskBytesSpilled_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(9, diskBytesSpilled_);
      }
      if (peakExecutionMemory_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(10, peakExecutionMemory_);
      }
      if (inputMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, getInputMetrics());
      }
      if (outputMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(12, getOutputMetrics());
      }
      if (shuffleReadMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(13, getShuffleReadMetrics());
      }
      if (shuffleWriteMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(14, getShuffleWriteMetrics());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.TaskMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.TaskMetrics other = (org.apache.spark.status.protobuf.StoreTypes.TaskMetrics) obj;

      if (getExecutorDeserializeTime()
          != other.getExecutorDeserializeTime()) return false;
      if (getExecutorDeserializeCpuTime()
          != other.getExecutorDeserializeCpuTime()) return false;
      if (getExecutorRunTime()
          != other.getExecutorRunTime()) return false;
      if (getExecutorCpuTime()
          != other.getExecutorCpuTime()) return false;
      if (getResultSize()
          != other.getResultSize()) return false;
      if (getJvmGcTime()
          != other.getJvmGcTime()) return false;
      if (getResultSerializationTime()
          != other.getResultSerializationTime()) return false;
      if (getMemoryBytesSpilled()
          != other.getMemoryBytesSpilled()) return false;
      if (getDiskBytesSpilled()
          != other.getDiskBytesSpilled()) return false;
      if (getPeakExecutionMemory()
          != other.getPeakExecutionMemory()) return false;
      if (hasInputMetrics() != other.hasInputMetrics()) return false;
      if (hasInputMetrics()) {
        if (!getInputMetrics()
            .equals(other.getInputMetrics())) return false;
      }
      if (hasOutputMetrics() != other.hasOutputMetrics()) return false;
      if (hasOutputMetrics()) {
        if (!getOutputMetrics()
            .equals(other.getOutputMetrics())) return false;
      }
      if (hasShuffleReadMetrics() != other.hasShuffleReadMetrics()) return false;
      if (hasShuffleReadMetrics()) {
        if (!getShuffleReadMetrics()
            .equals(other.getShuffleReadMetrics())) return false;
      }
      if (hasShuffleWriteMetrics() != other.hasShuffleWriteMetrics()) return false;
      if (hasShuffleWriteMetrics()) {
        if (!getShuffleWriteMetrics()
            .equals(other.getShuffleWriteMetrics())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorDeserializeTime());
      hash = (37 * hash) + EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorDeserializeCpuTime());
      hash = (37 * hash) + EXECUTOR_RUN_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorRunTime());
      hash = (37 * hash) + EXECUTOR_CPU_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getExecutorCpuTime());
      hash = (37 * hash) + RESULT_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getResultSize());
      hash = (37 * hash) + JVM_GC_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getJvmGcTime());
      hash = (37 * hash) + RESULT_SERIALIZATION_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getResultSerializationTime());
      hash = (37 * hash) + MEMORY_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryBytesSpilled());
      hash = (37 * hash) + DISK_BYTES_SPILLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDiskBytesSpilled());
      hash = (37 * hash) + PEAK_EXECUTION_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getPeakExecutionMemory());
      if (hasInputMetrics()) {
        hash = (37 * hash) + INPUT_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getInputMetrics().hashCode();
      }
      if (hasOutputMetrics()) {
        hash = (37 * hash) + OUTPUT_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getOutputMetrics().hashCode();
      }
      if (hasShuffleReadMetrics()) {
        hash = (37 * hash) + SHUFFLE_READ_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getShuffleReadMetrics().hashCode();
      }
      if (hasShuffleWriteMetrics()) {
        hash = (37 * hash) + SHUFFLE_WRITE_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getShuffleWriteMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.TaskMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.TaskMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.TaskMetrics)
        org.apache.spark.status.protobuf.StoreTypes.TaskMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetrics_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.class, org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        executorDeserializeTime_ = 0L;

        executorDeserializeCpuTime_ = 0L;

        executorRunTime_ = 0L;

        executorCpuTime_ = 0L;

        resultSize_ = 0L;

        jvmGcTime_ = 0L;

        resultSerializationTime_ = 0L;

        memoryBytesSpilled_ = 0L;

        diskBytesSpilled_ = 0L;

        peakExecutionMemory_ = 0L;

        if (inputMetricsBuilder_ == null) {
          inputMetrics_ = null;
        } else {
          inputMetrics_ = null;
          inputMetricsBuilder_ = null;
        }
        if (outputMetricsBuilder_ == null) {
          outputMetrics_ = null;
        } else {
          outputMetrics_ = null;
          outputMetricsBuilder_ = null;
        }
        if (shuffleReadMetricsBuilder_ == null) {
          shuffleReadMetrics_ = null;
        } else {
          shuffleReadMetrics_ = null;
          shuffleReadMetricsBuilder_ = null;
        }
        if (shuffleWriteMetricsBuilder_ == null) {
          shuffleWriteMetrics_ = null;
        } else {
          shuffleWriteMetrics_ = null;
          shuffleWriteMetricsBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetrics_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetrics getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetrics build() {
        org.apache.spark.status.protobuf.StoreTypes.TaskMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetrics buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.TaskMetrics result = new org.apache.spark.status.protobuf.StoreTypes.TaskMetrics(this);
        result.executorDeserializeTime_ = executorDeserializeTime_;
        result.executorDeserializeCpuTime_ = executorDeserializeCpuTime_;
        result.executorRunTime_ = executorRunTime_;
        result.executorCpuTime_ = executorCpuTime_;
        result.resultSize_ = resultSize_;
        result.jvmGcTime_ = jvmGcTime_;
        result.resultSerializationTime_ = resultSerializationTime_;
        result.memoryBytesSpilled_ = memoryBytesSpilled_;
        result.diskBytesSpilled_ = diskBytesSpilled_;
        result.peakExecutionMemory_ = peakExecutionMemory_;
        if (inputMetricsBuilder_ == null) {
          result.inputMetrics_ = inputMetrics_;
        } else {
          result.inputMetrics_ = inputMetricsBuilder_.build();
        }
        if (outputMetricsBuilder_ == null) {
          result.outputMetrics_ = outputMetrics_;
        } else {
          result.outputMetrics_ = outputMetricsBuilder_.build();
        }
        if (shuffleReadMetricsBuilder_ == null) {
          result.shuffleReadMetrics_ = shuffleReadMetrics_;
        } else {
          result.shuffleReadMetrics_ = shuffleReadMetricsBuilder_.build();
        }
        if (shuffleWriteMetricsBuilder_ == null) {
          result.shuffleWriteMetrics_ = shuffleWriteMetrics_;
        } else {
          result.shuffleWriteMetrics_ = shuffleWriteMetricsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.TaskMetrics) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.TaskMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.TaskMetrics other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.TaskMetrics.getDefaultInstance()) return this;
        if (other.getExecutorDeserializeTime() != 0L) {
          setExecutorDeserializeTime(other.getExecutorDeserializeTime());
        }
        if (other.getExecutorDeserializeCpuTime() != 0L) {
          setExecutorDeserializeCpuTime(other.getExecutorDeserializeCpuTime());
        }
        if (other.getExecutorRunTime() != 0L) {
          setExecutorRunTime(other.getExecutorRunTime());
        }
        if (other.getExecutorCpuTime() != 0L) {
          setExecutorCpuTime(other.getExecutorCpuTime());
        }
        if (other.getResultSize() != 0L) {
          setResultSize(other.getResultSize());
        }
        if (other.getJvmGcTime() != 0L) {
          setJvmGcTime(other.getJvmGcTime());
        }
        if (other.getResultSerializationTime() != 0L) {
          setResultSerializationTime(other.getResultSerializationTime());
        }
        if (other.getMemoryBytesSpilled() != 0L) {
          setMemoryBytesSpilled(other.getMemoryBytesSpilled());
        }
        if (other.getDiskBytesSpilled() != 0L) {
          setDiskBytesSpilled(other.getDiskBytesSpilled());
        }
        if (other.getPeakExecutionMemory() != 0L) {
          setPeakExecutionMemory(other.getPeakExecutionMemory());
        }
        if (other.hasInputMetrics()) {
          mergeInputMetrics(other.getInputMetrics());
        }
        if (other.hasOutputMetrics()) {
          mergeOutputMetrics(other.getOutputMetrics());
        }
        if (other.hasShuffleReadMetrics()) {
          mergeShuffleReadMetrics(other.getShuffleReadMetrics());
        }
        if (other.hasShuffleWriteMetrics()) {
          mergeShuffleWriteMetrics(other.getShuffleWriteMetrics());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.TaskMetrics parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.TaskMetrics) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long executorDeserializeTime_ ;
      /**
       * <code>int64 executor_deserialize_time = 1;</code>
       * @return The executorDeserializeTime.
       */
      @java.lang.Override
      public long getExecutorDeserializeTime() {
        return executorDeserializeTime_;
      }
      /**
       * <code>int64 executor_deserialize_time = 1;</code>
       * @param value The executorDeserializeTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeTime(long value) {
        
        executorDeserializeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_deserialize_time = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeTime() {
        
        executorDeserializeTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorDeserializeCpuTime_ ;
      /**
       * <code>int64 executor_deserialize_cpu_time = 2;</code>
       * @return The executorDeserializeCpuTime.
       */
      @java.lang.Override
      public long getExecutorDeserializeCpuTime() {
        return executorDeserializeCpuTime_;
      }
      /**
       * <code>int64 executor_deserialize_cpu_time = 2;</code>
       * @param value The executorDeserializeCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeCpuTime(long value) {
        
        executorDeserializeCpuTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_deserialize_cpu_time = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeCpuTime() {
        
        executorDeserializeCpuTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorRunTime_ ;
      /**
       * <code>int64 executor_run_time = 3;</code>
       * @return The executorRunTime.
       */
      @java.lang.Override
      public long getExecutorRunTime() {
        return executorRunTime_;
      }
      /**
       * <code>int64 executor_run_time = 3;</code>
       * @param value The executorRunTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorRunTime(long value) {
        
        executorRunTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_run_time = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorRunTime() {
        
        executorRunTime_ = 0L;
        onChanged();
        return this;
      }

      private long executorCpuTime_ ;
      /**
       * <code>int64 executor_cpu_time = 4;</code>
       * @return The executorCpuTime.
       */
      @java.lang.Override
      public long getExecutorCpuTime() {
        return executorCpuTime_;
      }
      /**
       * <code>int64 executor_cpu_time = 4;</code>
       * @param value The executorCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorCpuTime(long value) {
        
        executorCpuTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 executor_cpu_time = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorCpuTime() {
        
        executorCpuTime_ = 0L;
        onChanged();
        return this;
      }

      private long resultSize_ ;
      /**
       * <code>int64 result_size = 5;</code>
       * @return The resultSize.
       */
      @java.lang.Override
      public long getResultSize() {
        return resultSize_;
      }
      /**
       * <code>int64 result_size = 5;</code>
       * @param value The resultSize to set.
       * @return This builder for chaining.
       */
      public Builder setResultSize(long value) {
        
        resultSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 result_size = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSize() {
        
        resultSize_ = 0L;
        onChanged();
        return this;
      }

      private long jvmGcTime_ ;
      /**
       * <code>int64 jvm_gc_time = 6;</code>
       * @return The jvmGcTime.
       */
      @java.lang.Override
      public long getJvmGcTime() {
        return jvmGcTime_;
      }
      /**
       * <code>int64 jvm_gc_time = 6;</code>
       * @param value The jvmGcTime to set.
       * @return This builder for chaining.
       */
      public Builder setJvmGcTime(long value) {
        
        jvmGcTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 jvm_gc_time = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearJvmGcTime() {
        
        jvmGcTime_ = 0L;
        onChanged();
        return this;
      }

      private long resultSerializationTime_ ;
      /**
       * <code>int64 result_serialization_time = 7;</code>
       * @return The resultSerializationTime.
       */
      @java.lang.Override
      public long getResultSerializationTime() {
        return resultSerializationTime_;
      }
      /**
       * <code>int64 result_serialization_time = 7;</code>
       * @param value The resultSerializationTime to set.
       * @return This builder for chaining.
       */
      public Builder setResultSerializationTime(long value) {
        
        resultSerializationTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 result_serialization_time = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSerializationTime() {
        
        resultSerializationTime_ = 0L;
        onChanged();
        return this;
      }

      private long memoryBytesSpilled_ ;
      /**
       * <code>int64 memory_bytes_spilled = 8;</code>
       * @return The memoryBytesSpilled.
       */
      @java.lang.Override
      public long getMemoryBytesSpilled() {
        return memoryBytesSpilled_;
      }
      /**
       * <code>int64 memory_bytes_spilled = 8;</code>
       * @param value The memoryBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryBytesSpilled(long value) {
        
        memoryBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_bytes_spilled = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryBytesSpilled() {
        
        memoryBytesSpilled_ = 0L;
        onChanged();
        return this;
      }

      private long diskBytesSpilled_ ;
      /**
       * <code>int64 disk_bytes_spilled = 9;</code>
       * @return The diskBytesSpilled.
       */
      @java.lang.Override
      public long getDiskBytesSpilled() {
        return diskBytesSpilled_;
      }
      /**
       * <code>int64 disk_bytes_spilled = 9;</code>
       * @param value The diskBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setDiskBytesSpilled(long value) {
        
        diskBytesSpilled_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 disk_bytes_spilled = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskBytesSpilled() {
        
        diskBytesSpilled_ = 0L;
        onChanged();
        return this;
      }

      private long peakExecutionMemory_ ;
      /**
       * <code>int64 peak_execution_memory = 10;</code>
       * @return The peakExecutionMemory.
       */
      @java.lang.Override
      public long getPeakExecutionMemory() {
        return peakExecutionMemory_;
      }
      /**
       * <code>int64 peak_execution_memory = 10;</code>
       * @param value The peakExecutionMemory to set.
       * @return This builder for chaining.
       */
      public Builder setPeakExecutionMemory(long value) {
        
        peakExecutionMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 peak_execution_memory = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearPeakExecutionMemory() {
        
        peakExecutionMemory_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.InputMetrics inputMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.InputMetrics, org.apache.spark.status.protobuf.StoreTypes.InputMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.InputMetricsOrBuilder> inputMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       * @return Whether the inputMetrics field is set.
       */
      public boolean hasInputMetrics() {
        return inputMetricsBuilder_ != null || inputMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       * @return The inputMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.InputMetrics getInputMetrics() {
        if (inputMetricsBuilder_ == null) {
          return inputMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.InputMetrics.getDefaultInstance() : inputMetrics_;
        } else {
          return inputMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       */
      public Builder setInputMetrics(org.apache.spark.status.protobuf.StoreTypes.InputMetrics value) {
        if (inputMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          inputMetrics_ = value;
          onChanged();
        } else {
          inputMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       */
      public Builder setInputMetrics(
          org.apache.spark.status.protobuf.StoreTypes.InputMetrics.Builder builderForValue) {
        if (inputMetricsBuilder_ == null) {
          inputMetrics_ = builderForValue.build();
          onChanged();
        } else {
          inputMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       */
      public Builder mergeInputMetrics(org.apache.spark.status.protobuf.StoreTypes.InputMetrics value) {
        if (inputMetricsBuilder_ == null) {
          if (inputMetrics_ != null) {
            inputMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.InputMetrics.newBuilder(inputMetrics_).mergeFrom(value).buildPartial();
          } else {
            inputMetrics_ = value;
          }
          onChanged();
        } else {
          inputMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       */
      public Builder clearInputMetrics() {
        if (inputMetricsBuilder_ == null) {
          inputMetrics_ = null;
          onChanged();
        } else {
          inputMetrics_ = null;
          inputMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.InputMetrics.Builder getInputMetricsBuilder() {
        
        onChanged();
        return getInputMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.InputMetricsOrBuilder getInputMetricsOrBuilder() {
        if (inputMetricsBuilder_ != null) {
          return inputMetricsBuilder_.getMessageOrBuilder();
        } else {
          return inputMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.InputMetrics.getDefaultInstance() : inputMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetrics input_metrics = 11;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.InputMetrics, org.apache.spark.status.protobuf.StoreTypes.InputMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.InputMetricsOrBuilder> 
          getInputMetricsFieldBuilder() {
        if (inputMetricsBuilder_ == null) {
          inputMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.InputMetrics, org.apache.spark.status.protobuf.StoreTypes.InputMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.InputMetricsOrBuilder>(
                  getInputMetrics(),
                  getParentForChildren(),
                  isClean());
          inputMetrics_ = null;
        }
        return inputMetricsBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.OutputMetrics outputMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.OutputMetrics, org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.OutputMetricsOrBuilder> outputMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       * @return Whether the outputMetrics field is set.
       */
      public boolean hasOutputMetrics() {
        return outputMetricsBuilder_ != null || outputMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       * @return The outputMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetrics getOutputMetrics() {
        if (outputMetricsBuilder_ == null) {
          return outputMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.getDefaultInstance() : outputMetrics_;
        } else {
          return outputMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       */
      public Builder setOutputMetrics(org.apache.spark.status.protobuf.StoreTypes.OutputMetrics value) {
        if (outputMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          outputMetrics_ = value;
          onChanged();
        } else {
          outputMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       */
      public Builder setOutputMetrics(
          org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.Builder builderForValue) {
        if (outputMetricsBuilder_ == null) {
          outputMetrics_ = builderForValue.build();
          onChanged();
        } else {
          outputMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       */
      public Builder mergeOutputMetrics(org.apache.spark.status.protobuf.StoreTypes.OutputMetrics value) {
        if (outputMetricsBuilder_ == null) {
          if (outputMetrics_ != null) {
            outputMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.newBuilder(outputMetrics_).mergeFrom(value).buildPartial();
          } else {
            outputMetrics_ = value;
          }
          onChanged();
        } else {
          outputMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       */
      public Builder clearOutputMetrics() {
        if (outputMetricsBuilder_ == null) {
          outputMetrics_ = null;
          onChanged();
        } else {
          outputMetrics_ = null;
          outputMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.Builder getOutputMetricsBuilder() {
        
        onChanged();
        return getOutputMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetricsOrBuilder getOutputMetricsOrBuilder() {
        if (outputMetricsBuilder_ != null) {
          return outputMetricsBuilder_.getMessageOrBuilder();
        } else {
          return outputMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.getDefaultInstance() : outputMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetrics output_metrics = 12;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.OutputMetrics, org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.OutputMetricsOrBuilder> 
          getOutputMetricsFieldBuilder() {
        if (outputMetricsBuilder_ == null) {
          outputMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.OutputMetrics, org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.OutputMetricsOrBuilder>(
                  getOutputMetrics(),
                  getParentForChildren(),
                  isClean());
          outputMetrics_ = null;
        }
        return outputMetricsBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics shuffleReadMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricsOrBuilder> shuffleReadMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       * @return Whether the shuffleReadMetrics field is set.
       */
      public boolean hasShuffleReadMetrics() {
        return shuffleReadMetricsBuilder_ != null || shuffleReadMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       * @return The shuffleReadMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics getShuffleReadMetrics() {
        if (shuffleReadMetricsBuilder_ == null) {
          return shuffleReadMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.getDefaultInstance() : shuffleReadMetrics_;
        } else {
          return shuffleReadMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       */
      public Builder setShuffleReadMetrics(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics value) {
        if (shuffleReadMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          shuffleReadMetrics_ = value;
          onChanged();
        } else {
          shuffleReadMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       */
      public Builder setShuffleReadMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.Builder builderForValue) {
        if (shuffleReadMetricsBuilder_ == null) {
          shuffleReadMetrics_ = builderForValue.build();
          onChanged();
        } else {
          shuffleReadMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       */
      public Builder mergeShuffleReadMetrics(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics value) {
        if (shuffleReadMetricsBuilder_ == null) {
          if (shuffleReadMetrics_ != null) {
            shuffleReadMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.newBuilder(shuffleReadMetrics_).mergeFrom(value).buildPartial();
          } else {
            shuffleReadMetrics_ = value;
          }
          onChanged();
        } else {
          shuffleReadMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       */
      public Builder clearShuffleReadMetrics() {
        if (shuffleReadMetricsBuilder_ == null) {
          shuffleReadMetrics_ = null;
          onChanged();
        } else {
          shuffleReadMetrics_ = null;
          shuffleReadMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.Builder getShuffleReadMetricsBuilder() {
        
        onChanged();
        return getShuffleReadMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricsOrBuilder getShuffleReadMetricsOrBuilder() {
        if (shuffleReadMetricsBuilder_ != null) {
          return shuffleReadMetricsBuilder_.getMessageOrBuilder();
        } else {
          return shuffleReadMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.getDefaultInstance() : shuffleReadMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetrics shuffle_read_metrics = 13;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricsOrBuilder> 
          getShuffleReadMetricsFieldBuilder() {
        if (shuffleReadMetricsBuilder_ == null) {
          shuffleReadMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricsOrBuilder>(
                  getShuffleReadMetrics(),
                  getParentForChildren(),
                  isClean());
          shuffleReadMetrics_ = null;
        }
        return shuffleReadMetricsBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics shuffleWriteMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricsOrBuilder> shuffleWriteMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       * @return Whether the shuffleWriteMetrics field is set.
       */
      public boolean hasShuffleWriteMetrics() {
        return shuffleWriteMetricsBuilder_ != null || shuffleWriteMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       * @return The shuffleWriteMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics getShuffleWriteMetrics() {
        if (shuffleWriteMetricsBuilder_ == null) {
          return shuffleWriteMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.getDefaultInstance() : shuffleWriteMetrics_;
        } else {
          return shuffleWriteMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       */
      public Builder setShuffleWriteMetrics(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics value) {
        if (shuffleWriteMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          shuffleWriteMetrics_ = value;
          onChanged();
        } else {
          shuffleWriteMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       */
      public Builder setShuffleWriteMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.Builder builderForValue) {
        if (shuffleWriteMetricsBuilder_ == null) {
          shuffleWriteMetrics_ = builderForValue.build();
          onChanged();
        } else {
          shuffleWriteMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       */
      public Builder mergeShuffleWriteMetrics(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics value) {
        if (shuffleWriteMetricsBuilder_ == null) {
          if (shuffleWriteMetrics_ != null) {
            shuffleWriteMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.newBuilder(shuffleWriteMetrics_).mergeFrom(value).buildPartial();
          } else {
            shuffleWriteMetrics_ = value;
          }
          onChanged();
        } else {
          shuffleWriteMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       */
      public Builder clearShuffleWriteMetrics() {
        if (shuffleWriteMetricsBuilder_ == null) {
          shuffleWriteMetrics_ = null;
          onChanged();
        } else {
          shuffleWriteMetrics_ = null;
          shuffleWriteMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.Builder getShuffleWriteMetricsBuilder() {
        
        onChanged();
        return getShuffleWriteMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricsOrBuilder getShuffleWriteMetricsOrBuilder() {
        if (shuffleWriteMetricsBuilder_ != null) {
          return shuffleWriteMetricsBuilder_.getMessageOrBuilder();
        } else {
          return shuffleWriteMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.getDefaultInstance() : shuffleWriteMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetrics shuffle_write_metrics = 14;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricsOrBuilder> 
          getShuffleWriteMetricsFieldBuilder() {
        if (shuffleWriteMetricsBuilder_ == null) {
          shuffleWriteMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricsOrBuilder>(
                  getShuffleWriteMetrics(),
                  getParentForChildren(),
                  isClean());
          shuffleWriteMetrics_ = null;
        }
        return shuffleWriteMetricsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.TaskMetrics)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.TaskMetrics)
    private static final org.apache.spark.status.protobuf.StoreTypes.TaskMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.TaskMetrics();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<TaskMetrics>
        PARSER = new com.google.protobuf.AbstractParser<TaskMetrics>() {
      @java.lang.Override
      public TaskMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TaskMetrics(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<TaskMetrics> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<TaskMetrics> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface InputMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.InputMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 bytes_read = 1;</code>
     * @return The bytesRead.
     */
    long getBytesRead();

    /**
     * <code>int64 records_read = 2;</code>
     * @return The recordsRead.
     */
    long getRecordsRead();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.InputMetrics}
   */
  public static final class InputMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.InputMetrics)
      InputMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use InputMetrics.newBuilder() to construct.
    private InputMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private InputMetrics() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new InputMetrics();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private InputMetrics(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              bytesRead_ = input.readInt64();
              break;
            }
            case 16: {

              recordsRead_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetrics_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.InputMetrics.class, org.apache.spark.status.protobuf.StoreTypes.InputMetrics.Builder.class);
    }

    public static final int BYTES_READ_FIELD_NUMBER = 1;
    private long bytesRead_;
    /**
     * <code>int64 bytes_read = 1;</code>
     * @return The bytesRead.
     */
    @java.lang.Override
    public long getBytesRead() {
      return bytesRead_;
    }

    public static final int RECORDS_READ_FIELD_NUMBER = 2;
    private long recordsRead_;
    /**
     * <code>int64 records_read = 2;</code>
     * @return The recordsRead.
     */
    @java.lang.Override
    public long getRecordsRead() {
      return recordsRead_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (bytesRead_ != 0L) {
        output.writeInt64(1, bytesRead_);
      }
      if (recordsRead_ != 0L) {
        output.writeInt64(2, recordsRead_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (bytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, bytesRead_);
      }
      if (recordsRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, recordsRead_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.InputMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.InputMetrics other = (org.apache.spark.status.protobuf.StoreTypes.InputMetrics) obj;

      if (getBytesRead()
          != other.getBytesRead()) return false;
      if (getRecordsRead()
          != other.getRecordsRead()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getBytesRead());
      hash = (37 * hash) + RECORDS_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRecordsRead());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.InputMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.InputMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.InputMetrics)
        org.apache.spark.status.protobuf.StoreTypes.InputMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetrics_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.InputMetrics.class, org.apache.spark.status.protobuf.StoreTypes.InputMetrics.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.InputMetrics.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bytesRead_ = 0L;

        recordsRead_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetrics_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.InputMetrics getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.InputMetrics.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.InputMetrics build() {
        org.apache.spark.status.protobuf.StoreTypes.InputMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.InputMetrics buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.InputMetrics result = new org.apache.spark.status.protobuf.StoreTypes.InputMetrics(this);
        result.bytesRead_ = bytesRead_;
        result.recordsRead_ = recordsRead_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.InputMetrics) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.InputMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.InputMetrics other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.InputMetrics.getDefaultInstance()) return this;
        if (other.getBytesRead() != 0L) {
          setBytesRead(other.getBytesRead());
        }
        if (other.getRecordsRead() != 0L) {
          setRecordsRead(other.getRecordsRead());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.InputMetrics parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.InputMetrics) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long bytesRead_ ;
      /**
       * <code>int64 bytes_read = 1;</code>
       * @return The bytesRead.
       */
      @java.lang.Override
      public long getBytesRead() {
        return bytesRead_;
      }
      /**
       * <code>int64 bytes_read = 1;</code>
       * @param value The bytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setBytesRead(long value) {
        
        bytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 bytes_read = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBytesRead() {
        
        bytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long recordsRead_ ;
      /**
       * <code>int64 records_read = 2;</code>
       * @return The recordsRead.
       */
      @java.lang.Override
      public long getRecordsRead() {
        return recordsRead_;
      }
      /**
       * <code>int64 records_read = 2;</code>
       * @param value The recordsRead to set.
       * @return This builder for chaining.
       */
      public Builder setRecordsRead(long value) {
        
        recordsRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 records_read = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearRecordsRead() {
        
        recordsRead_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.InputMetrics)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.InputMetrics)
    private static final org.apache.spark.status.protobuf.StoreTypes.InputMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.InputMetrics();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.InputMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<InputMetrics>
        PARSER = new com.google.protobuf.AbstractParser<InputMetrics>() {
      @java.lang.Override
      public InputMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new InputMetrics(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<InputMetrics> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<InputMetrics> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.InputMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface OutputMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.OutputMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 bytes_written = 1;</code>
     * @return The bytesWritten.
     */
    long getBytesWritten();

    /**
     * <code>int64 records_written = 2;</code>
     * @return The recordsWritten.
     */
    long getRecordsWritten();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.OutputMetrics}
   */
  public static final class OutputMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.OutputMetrics)
      OutputMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use OutputMetrics.newBuilder() to construct.
    private OutputMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private OutputMetrics() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new OutputMetrics();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private OutputMetrics(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              bytesWritten_ = input.readInt64();
              break;
            }
            case 16: {

              recordsWritten_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetrics_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.class, org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.Builder.class);
    }

    public static final int BYTES_WRITTEN_FIELD_NUMBER = 1;
    private long bytesWritten_;
    /**
     * <code>int64 bytes_written = 1;</code>
     * @return The bytesWritten.
     */
    @java.lang.Override
    public long getBytesWritten() {
      return bytesWritten_;
    }

    public static final int RECORDS_WRITTEN_FIELD_NUMBER = 2;
    private long recordsWritten_;
    /**
     * <code>int64 records_written = 2;</code>
     * @return The recordsWritten.
     */
    @java.lang.Override
    public long getRecordsWritten() {
      return recordsWritten_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (bytesWritten_ != 0L) {
        output.writeInt64(1, bytesWritten_);
      }
      if (recordsWritten_ != 0L) {
        output.writeInt64(2, recordsWritten_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (bytesWritten_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, bytesWritten_);
      }
      if (recordsWritten_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, recordsWritten_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.OutputMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.OutputMetrics other = (org.apache.spark.status.protobuf.StoreTypes.OutputMetrics) obj;

      if (getBytesWritten()
          != other.getBytesWritten()) return false;
      if (getRecordsWritten()
          != other.getRecordsWritten()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + BYTES_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getBytesWritten());
      hash = (37 * hash) + RECORDS_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRecordsWritten());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.OutputMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.OutputMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.OutputMetrics)
        org.apache.spark.status.protobuf.StoreTypes.OutputMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetrics_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.class, org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bytesWritten_ = 0L;

        recordsWritten_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetrics_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetrics getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetrics build() {
        org.apache.spark.status.protobuf.StoreTypes.OutputMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetrics buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.OutputMetrics result = new org.apache.spark.status.protobuf.StoreTypes.OutputMetrics(this);
        result.bytesWritten_ = bytesWritten_;
        result.recordsWritten_ = recordsWritten_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.OutputMetrics) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.OutputMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.OutputMetrics other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.OutputMetrics.getDefaultInstance()) return this;
        if (other.getBytesWritten() != 0L) {
          setBytesWritten(other.getBytesWritten());
        }
        if (other.getRecordsWritten() != 0L) {
          setRecordsWritten(other.getRecordsWritten());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.OutputMetrics parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.OutputMetrics) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long bytesWritten_ ;
      /**
       * <code>int64 bytes_written = 1;</code>
       * @return The bytesWritten.
       */
      @java.lang.Override
      public long getBytesWritten() {
        return bytesWritten_;
      }
      /**
       * <code>int64 bytes_written = 1;</code>
       * @param value The bytesWritten to set.
       * @return This builder for chaining.
       */
      public Builder setBytesWritten(long value) {
        
        bytesWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 bytes_written = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBytesWritten() {
        
        bytesWritten_ = 0L;
        onChanged();
        return this;
      }

      private long recordsWritten_ ;
      /**
       * <code>int64 records_written = 2;</code>
       * @return The recordsWritten.
       */
      @java.lang.Override
      public long getRecordsWritten() {
        return recordsWritten_;
      }
      /**
       * <code>int64 records_written = 2;</code>
       * @param value The recordsWritten to set.
       * @return This builder for chaining.
       */
      public Builder setRecordsWritten(long value) {
        
        recordsWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 records_written = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearRecordsWritten() {
        
        recordsWritten_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.OutputMetrics)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.OutputMetrics)
    private static final org.apache.spark.status.protobuf.StoreTypes.OutputMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.OutputMetrics();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<OutputMetrics>
        PARSER = new com.google.protobuf.AbstractParser<OutputMetrics>() {
      @java.lang.Override
      public OutputMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new OutputMetrics(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<OutputMetrics> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<OutputMetrics> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.OutputMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ShuffleReadMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ShuffleReadMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 remote_blocks_fetched = 1;</code>
     * @return The remoteBlocksFetched.
     */
    long getRemoteBlocksFetched();

    /**
     * <code>int64 local_blocks_fetched = 2;</code>
     * @return The localBlocksFetched.
     */
    long getLocalBlocksFetched();

    /**
     * <code>int64 fetch_wait_time = 3;</code>
     * @return The fetchWaitTime.
     */
    long getFetchWaitTime();

    /**
     * <code>int64 remote_bytes_read = 4;</code>
     * @return The remoteBytesRead.
     */
    long getRemoteBytesRead();

    /**
     * <code>int64 remote_bytes_read_to_disk = 5;</code>
     * @return The remoteBytesReadToDisk.
     */
    long getRemoteBytesReadToDisk();

    /**
     * <code>int64 local_bytes_read = 6;</code>
     * @return The localBytesRead.
     */
    long getLocalBytesRead();

    /**
     * <code>int64 records_read = 7;</code>
     * @return The recordsRead.
     */
    long getRecordsRead();

    /**
     * <code>int64 remote_reqs_duration = 8;</code>
     * @return The remoteReqsDuration.
     */
    long getRemoteReqsDuration();

    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
     * @return Whether the shufflePushReadMetrics field is set.
     */
    boolean hasShufflePushReadMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
     * @return The shufflePushReadMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics getShufflePushReadMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricsOrBuilder getShufflePushReadMetricsOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ShuffleReadMetrics}
   */
  public static final class ShuffleReadMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ShuffleReadMetrics)
      ShuffleReadMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ShuffleReadMetrics.newBuilder() to construct.
    private ShuffleReadMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ShuffleReadMetrics() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ShuffleReadMetrics();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ShuffleReadMetrics(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              remoteBlocksFetched_ = input.readInt64();
              break;
            }
            case 16: {

              localBlocksFetched_ = input.readInt64();
              break;
            }
            case 24: {

              fetchWaitTime_ = input.readInt64();
              break;
            }
            case 32: {

              remoteBytesRead_ = input.readInt64();
              break;
            }
            case 40: {

              remoteBytesReadToDisk_ = input.readInt64();
              break;
            }
            case 48: {

              localBytesRead_ = input.readInt64();
              break;
            }
            case 56: {

              recordsRead_ = input.readInt64();
              break;
            }
            case 64: {

              remoteReqsDuration_ = input.readInt64();
              break;
            }
            case 74: {
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.Builder subBuilder = null;
              if (shufflePushReadMetrics_ != null) {
                subBuilder = shufflePushReadMetrics_.toBuilder();
              }
              shufflePushReadMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(shufflePushReadMetrics_);
                shufflePushReadMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.class, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.Builder.class);
    }

    public static final int REMOTE_BLOCKS_FETCHED_FIELD_NUMBER = 1;
    private long remoteBlocksFetched_;
    /**
     * <code>int64 remote_blocks_fetched = 1;</code>
     * @return The remoteBlocksFetched.
     */
    @java.lang.Override
    public long getRemoteBlocksFetched() {
      return remoteBlocksFetched_;
    }

    public static final int LOCAL_BLOCKS_FETCHED_FIELD_NUMBER = 2;
    private long localBlocksFetched_;
    /**
     * <code>int64 local_blocks_fetched = 2;</code>
     * @return The localBlocksFetched.
     */
    @java.lang.Override
    public long getLocalBlocksFetched() {
      return localBlocksFetched_;
    }

    public static final int FETCH_WAIT_TIME_FIELD_NUMBER = 3;
    private long fetchWaitTime_;
    /**
     * <code>int64 fetch_wait_time = 3;</code>
     * @return The fetchWaitTime.
     */
    @java.lang.Override
    public long getFetchWaitTime() {
      return fetchWaitTime_;
    }

    public static final int REMOTE_BYTES_READ_FIELD_NUMBER = 4;
    private long remoteBytesRead_;
    /**
     * <code>int64 remote_bytes_read = 4;</code>
     * @return The remoteBytesRead.
     */
    @java.lang.Override
    public long getRemoteBytesRead() {
      return remoteBytesRead_;
    }

    public static final int REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER = 5;
    private long remoteBytesReadToDisk_;
    /**
     * <code>int64 remote_bytes_read_to_disk = 5;</code>
     * @return The remoteBytesReadToDisk.
     */
    @java.lang.Override
    public long getRemoteBytesReadToDisk() {
      return remoteBytesReadToDisk_;
    }

    public static final int LOCAL_BYTES_READ_FIELD_NUMBER = 6;
    private long localBytesRead_;
    /**
     * <code>int64 local_bytes_read = 6;</code>
     * @return The localBytesRead.
     */
    @java.lang.Override
    public long getLocalBytesRead() {
      return localBytesRead_;
    }

    public static final int RECORDS_READ_FIELD_NUMBER = 7;
    private long recordsRead_;
    /**
     * <code>int64 records_read = 7;</code>
     * @return The recordsRead.
     */
    @java.lang.Override
    public long getRecordsRead() {
      return recordsRead_;
    }

    public static final int REMOTE_REQS_DURATION_FIELD_NUMBER = 8;
    private long remoteReqsDuration_;
    /**
     * <code>int64 remote_reqs_duration = 8;</code>
     * @return The remoteReqsDuration.
     */
    @java.lang.Override
    public long getRemoteReqsDuration() {
      return remoteReqsDuration_;
    }

    public static final int SHUFFLE_PUSH_READ_METRICS_FIELD_NUMBER = 9;
    private org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics shufflePushReadMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
     * @return Whether the shufflePushReadMetrics field is set.
     */
    @java.lang.Override
    public boolean hasShufflePushReadMetrics() {
      return shufflePushReadMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
     * @return The shufflePushReadMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics getShufflePushReadMetrics() {
      return shufflePushReadMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.getDefaultInstance() : shufflePushReadMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricsOrBuilder getShufflePushReadMetricsOrBuilder() {
      return getShufflePushReadMetrics();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (remoteBlocksFetched_ != 0L) {
        output.writeInt64(1, remoteBlocksFetched_);
      }
      if (localBlocksFetched_ != 0L) {
        output.writeInt64(2, localBlocksFetched_);
      }
      if (fetchWaitTime_ != 0L) {
        output.writeInt64(3, fetchWaitTime_);
      }
      if (remoteBytesRead_ != 0L) {
        output.writeInt64(4, remoteBytesRead_);
      }
      if (remoteBytesReadToDisk_ != 0L) {
        output.writeInt64(5, remoteBytesReadToDisk_);
      }
      if (localBytesRead_ != 0L) {
        output.writeInt64(6, localBytesRead_);
      }
      if (recordsRead_ != 0L) {
        output.writeInt64(7, recordsRead_);
      }
      if (remoteReqsDuration_ != 0L) {
        output.writeInt64(8, remoteReqsDuration_);
      }
      if (shufflePushReadMetrics_ != null) {
        output.writeMessage(9, getShufflePushReadMetrics());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (remoteBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, remoteBlocksFetched_);
      }
      if (localBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, localBlocksFetched_);
      }
      if (fetchWaitTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, fetchWaitTime_);
      }
      if (remoteBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, remoteBytesRead_);
      }
      if (remoteBytesReadToDisk_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, remoteBytesReadToDisk_);
      }
      if (localBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, localBytesRead_);
      }
      if (recordsRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, recordsRead_);
      }
      if (remoteReqsDuration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, remoteReqsDuration_);
      }
      if (shufflePushReadMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, getShufflePushReadMetrics());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics other = (org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics) obj;

      if (getRemoteBlocksFetched()
          != other.getRemoteBlocksFetched()) return false;
      if (getLocalBlocksFetched()
          != other.getLocalBlocksFetched()) return false;
      if (getFetchWaitTime()
          != other.getFetchWaitTime()) return false;
      if (getRemoteBytesRead()
          != other.getRemoteBytesRead()) return false;
      if (getRemoteBytesReadToDisk()
          != other.getRemoteBytesReadToDisk()) return false;
      if (getLocalBytesRead()
          != other.getLocalBytesRead()) return false;
      if (getRecordsRead()
          != other.getRecordsRead()) return false;
      if (getRemoteReqsDuration()
          != other.getRemoteReqsDuration()) return false;
      if (hasShufflePushReadMetrics() != other.hasShufflePushReadMetrics()) return false;
      if (hasShufflePushReadMetrics()) {
        if (!getShufflePushReadMetrics()
            .equals(other.getShufflePushReadMetrics())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + REMOTE_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRemoteBlocksFetched());
      hash = (37 * hash) + LOCAL_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getLocalBlocksFetched());
      hash = (37 * hash) + FETCH_WAIT_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getFetchWaitTime());
      hash = (37 * hash) + REMOTE_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRemoteBytesRead());
      hash = (37 * hash) + REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRemoteBytesReadToDisk());
      hash = (37 * hash) + LOCAL_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getLocalBytesRead());
      hash = (37 * hash) + RECORDS_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRecordsRead());
      hash = (37 * hash) + REMOTE_REQS_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRemoteReqsDuration());
      if (hasShufflePushReadMetrics()) {
        hash = (37 * hash) + SHUFFLE_PUSH_READ_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getShufflePushReadMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ShuffleReadMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ShuffleReadMetrics)
        org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.class, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        remoteBlocksFetched_ = 0L;

        localBlocksFetched_ = 0L;

        fetchWaitTime_ = 0L;

        remoteBytesRead_ = 0L;

        remoteBytesReadToDisk_ = 0L;

        localBytesRead_ = 0L;

        recordsRead_ = 0L;

        remoteReqsDuration_ = 0L;

        if (shufflePushReadMetricsBuilder_ == null) {
          shufflePushReadMetrics_ = null;
        } else {
          shufflePushReadMetrics_ = null;
          shufflePushReadMetricsBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics build() {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics result = new org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics(this);
        result.remoteBlocksFetched_ = remoteBlocksFetched_;
        result.localBlocksFetched_ = localBlocksFetched_;
        result.fetchWaitTime_ = fetchWaitTime_;
        result.remoteBytesRead_ = remoteBytesRead_;
        result.remoteBytesReadToDisk_ = remoteBytesReadToDisk_;
        result.localBytesRead_ = localBytesRead_;
        result.recordsRead_ = recordsRead_;
        result.remoteReqsDuration_ = remoteReqsDuration_;
        if (shufflePushReadMetricsBuilder_ == null) {
          result.shufflePushReadMetrics_ = shufflePushReadMetrics_;
        } else {
          result.shufflePushReadMetrics_ = shufflePushReadMetricsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics.getDefaultInstance()) return this;
        if (other.getRemoteBlocksFetched() != 0L) {
          setRemoteBlocksFetched(other.getRemoteBlocksFetched());
        }
        if (other.getLocalBlocksFetched() != 0L) {
          setLocalBlocksFetched(other.getLocalBlocksFetched());
        }
        if (other.getFetchWaitTime() != 0L) {
          setFetchWaitTime(other.getFetchWaitTime());
        }
        if (other.getRemoteBytesRead() != 0L) {
          setRemoteBytesRead(other.getRemoteBytesRead());
        }
        if (other.getRemoteBytesReadToDisk() != 0L) {
          setRemoteBytesReadToDisk(other.getRemoteBytesReadToDisk());
        }
        if (other.getLocalBytesRead() != 0L) {
          setLocalBytesRead(other.getLocalBytesRead());
        }
        if (other.getRecordsRead() != 0L) {
          setRecordsRead(other.getRecordsRead());
        }
        if (other.getRemoteReqsDuration() != 0L) {
          setRemoteReqsDuration(other.getRemoteReqsDuration());
        }
        if (other.hasShufflePushReadMetrics()) {
          mergeShufflePushReadMetrics(other.getShufflePushReadMetrics());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long remoteBlocksFetched_ ;
      /**
       * <code>int64 remote_blocks_fetched = 1;</code>
       * @return The remoteBlocksFetched.
       */
      @java.lang.Override
      public long getRemoteBlocksFetched() {
        return remoteBlocksFetched_;
      }
      /**
       * <code>int64 remote_blocks_fetched = 1;</code>
       * @param value The remoteBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteBlocksFetched(long value) {
        
        remoteBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remote_blocks_fetched = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteBlocksFetched() {
        
        remoteBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long localBlocksFetched_ ;
      /**
       * <code>int64 local_blocks_fetched = 2;</code>
       * @return The localBlocksFetched.
       */
      @java.lang.Override
      public long getLocalBlocksFetched() {
        return localBlocksFetched_;
      }
      /**
       * <code>int64 local_blocks_fetched = 2;</code>
       * @param value The localBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setLocalBlocksFetched(long value) {
        
        localBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 local_blocks_fetched = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalBlocksFetched() {
        
        localBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long fetchWaitTime_ ;
      /**
       * <code>int64 fetch_wait_time = 3;</code>
       * @return The fetchWaitTime.
       */
      @java.lang.Override
      public long getFetchWaitTime() {
        return fetchWaitTime_;
      }
      /**
       * <code>int64 fetch_wait_time = 3;</code>
       * @param value The fetchWaitTime to set.
       * @return This builder for chaining.
       */
      public Builder setFetchWaitTime(long value) {
        
        fetchWaitTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 fetch_wait_time = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearFetchWaitTime() {
        
        fetchWaitTime_ = 0L;
        onChanged();
        return this;
      }

      private long remoteBytesRead_ ;
      /**
       * <code>int64 remote_bytes_read = 4;</code>
       * @return The remoteBytesRead.
       */
      @java.lang.Override
      public long getRemoteBytesRead() {
        return remoteBytesRead_;
      }
      /**
       * <code>int64 remote_bytes_read = 4;</code>
       * @param value The remoteBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteBytesRead(long value) {
        
        remoteBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remote_bytes_read = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteBytesRead() {
        
        remoteBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long remoteBytesReadToDisk_ ;
      /**
       * <code>int64 remote_bytes_read_to_disk = 5;</code>
       * @return The remoteBytesReadToDisk.
       */
      @java.lang.Override
      public long getRemoteBytesReadToDisk() {
        return remoteBytesReadToDisk_;
      }
      /**
       * <code>int64 remote_bytes_read_to_disk = 5;</code>
       * @param value The remoteBytesReadToDisk to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteBytesReadToDisk(long value) {
        
        remoteBytesReadToDisk_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remote_bytes_read_to_disk = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteBytesReadToDisk() {
        
        remoteBytesReadToDisk_ = 0L;
        onChanged();
        return this;
      }

      private long localBytesRead_ ;
      /**
       * <code>int64 local_bytes_read = 6;</code>
       * @return The localBytesRead.
       */
      @java.lang.Override
      public long getLocalBytesRead() {
        return localBytesRead_;
      }
      /**
       * <code>int64 local_bytes_read = 6;</code>
       * @param value The localBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setLocalBytesRead(long value) {
        
        localBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 local_bytes_read = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalBytesRead() {
        
        localBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long recordsRead_ ;
      /**
       * <code>int64 records_read = 7;</code>
       * @return The recordsRead.
       */
      @java.lang.Override
      public long getRecordsRead() {
        return recordsRead_;
      }
      /**
       * <code>int64 records_read = 7;</code>
       * @param value The recordsRead to set.
       * @return This builder for chaining.
       */
      public Builder setRecordsRead(long value) {
        
        recordsRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 records_read = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearRecordsRead() {
        
        recordsRead_ = 0L;
        onChanged();
        return this;
      }

      private long remoteReqsDuration_ ;
      /**
       * <code>int64 remote_reqs_duration = 8;</code>
       * @return The remoteReqsDuration.
       */
      @java.lang.Override
      public long getRemoteReqsDuration() {
        return remoteReqsDuration_;
      }
      /**
       * <code>int64 remote_reqs_duration = 8;</code>
       * @param value The remoteReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteReqsDuration(long value) {
        
        remoteReqsDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remote_reqs_duration = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteReqsDuration() {
        
        remoteReqsDuration_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics shufflePushReadMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricsOrBuilder> shufflePushReadMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       * @return Whether the shufflePushReadMetrics field is set.
       */
      public boolean hasShufflePushReadMetrics() {
        return shufflePushReadMetricsBuilder_ != null || shufflePushReadMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       * @return The shufflePushReadMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics getShufflePushReadMetrics() {
        if (shufflePushReadMetricsBuilder_ == null) {
          return shufflePushReadMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.getDefaultInstance() : shufflePushReadMetrics_;
        } else {
          return shufflePushReadMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       */
      public Builder setShufflePushReadMetrics(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics value) {
        if (shufflePushReadMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          shufflePushReadMetrics_ = value;
          onChanged();
        } else {
          shufflePushReadMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       */
      public Builder setShufflePushReadMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.Builder builderForValue) {
        if (shufflePushReadMetricsBuilder_ == null) {
          shufflePushReadMetrics_ = builderForValue.build();
          onChanged();
        } else {
          shufflePushReadMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       */
      public Builder mergeShufflePushReadMetrics(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics value) {
        if (shufflePushReadMetricsBuilder_ == null) {
          if (shufflePushReadMetrics_ != null) {
            shufflePushReadMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.newBuilder(shufflePushReadMetrics_).mergeFrom(value).buildPartial();
          } else {
            shufflePushReadMetrics_ = value;
          }
          onChanged();
        } else {
          shufflePushReadMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       */
      public Builder clearShufflePushReadMetrics() {
        if (shufflePushReadMetricsBuilder_ == null) {
          shufflePushReadMetrics_ = null;
          onChanged();
        } else {
          shufflePushReadMetrics_ = null;
          shufflePushReadMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.Builder getShufflePushReadMetricsBuilder() {
        
        onChanged();
        return getShufflePushReadMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricsOrBuilder getShufflePushReadMetricsOrBuilder() {
        if (shufflePushReadMetricsBuilder_ != null) {
          return shufflePushReadMetricsBuilder_.getMessageOrBuilder();
        } else {
          return shufflePushReadMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.getDefaultInstance() : shufflePushReadMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetrics shuffle_push_read_metrics = 9;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricsOrBuilder> 
          getShufflePushReadMetricsFieldBuilder() {
        if (shufflePushReadMetricsBuilder_ == null) {
          shufflePushReadMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricsOrBuilder>(
                  getShufflePushReadMetrics(),
                  getParentForChildren(),
                  isClean());
          shufflePushReadMetrics_ = null;
        }
        return shufflePushReadMetricsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ShuffleReadMetrics)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ShuffleReadMetrics)
    private static final org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ShuffleReadMetrics>
        PARSER = new com.google.protobuf.AbstractParser<ShuffleReadMetrics>() {
      @java.lang.Override
      public ShuffleReadMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ShuffleReadMetrics(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ShuffleReadMetrics> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ShuffleReadMetrics> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ShufflePushReadMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ShufflePushReadMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 corrupt_merged_block_chunks = 1;</code>
     * @return The corruptMergedBlockChunks.
     */
    long getCorruptMergedBlockChunks();

    /**
     * <code>int64 merged_fetch_fallback_count = 2;</code>
     * @return The mergedFetchFallbackCount.
     */
    long getMergedFetchFallbackCount();

    /**
     * <code>int64 remote_merged_blocks_fetched = 3;</code>
     * @return The remoteMergedBlocksFetched.
     */
    long getRemoteMergedBlocksFetched();

    /**
     * <code>int64 local_merged_blocks_fetched = 4;</code>
     * @return The localMergedBlocksFetched.
     */
    long getLocalMergedBlocksFetched();

    /**
     * <code>int64 remote_merged_chunks_fetched = 5;</code>
     * @return The remoteMergedChunksFetched.
     */
    long getRemoteMergedChunksFetched();

    /**
     * <code>int64 local_merged_chunks_fetched = 6;</code>
     * @return The localMergedChunksFetched.
     */
    long getLocalMergedChunksFetched();

    /**
     * <code>int64 remote_merged_bytes_read = 7;</code>
     * @return The remoteMergedBytesRead.
     */
    long getRemoteMergedBytesRead();

    /**
     * <code>int64 local_merged_bytes_read = 8;</code>
     * @return The localMergedBytesRead.
     */
    long getLocalMergedBytesRead();

    /**
     * <code>int64 remote_merged_reqs_duration = 9;</code>
     * @return The remoteMergedReqsDuration.
     */
    long getRemoteMergedReqsDuration();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ShufflePushReadMetrics}
   */
  public static final class ShufflePushReadMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ShufflePushReadMetrics)
      ShufflePushReadMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ShufflePushReadMetrics.newBuilder() to construct.
    private ShufflePushReadMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ShufflePushReadMetrics() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ShufflePushReadMetrics();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ShufflePushReadMetrics(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              corruptMergedBlockChunks_ = input.readInt64();
              break;
            }
            case 16: {

              mergedFetchFallbackCount_ = input.readInt64();
              break;
            }
            case 24: {

              remoteMergedBlocksFetched_ = input.readInt64();
              break;
            }
            case 32: {

              localMergedBlocksFetched_ = input.readInt64();
              break;
            }
            case 40: {

              remoteMergedChunksFetched_ = input.readInt64();
              break;
            }
            case 48: {

              localMergedChunksFetched_ = input.readInt64();
              break;
            }
            case 56: {

              remoteMergedBytesRead_ = input.readInt64();
              break;
            }
            case 64: {

              localMergedBytesRead_ = input.readInt64();
              break;
            }
            case 72: {

              remoteMergedReqsDuration_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.class, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.Builder.class);
    }

    public static final int CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER = 1;
    private long corruptMergedBlockChunks_;
    /**
     * <code>int64 corrupt_merged_block_chunks = 1;</code>
     * @return The corruptMergedBlockChunks.
     */
    @java.lang.Override
    public long getCorruptMergedBlockChunks() {
      return corruptMergedBlockChunks_;
    }

    public static final int MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER = 2;
    private long mergedFetchFallbackCount_;
    /**
     * <code>int64 merged_fetch_fallback_count = 2;</code>
     * @return The mergedFetchFallbackCount.
     */
    @java.lang.Override
    public long getMergedFetchFallbackCount() {
      return mergedFetchFallbackCount_;
    }

    public static final int REMOTE_MERGED_BLOCKS_FETCHED_FIELD_NUMBER = 3;
    private long remoteMergedBlocksFetched_;
    /**
     * <code>int64 remote_merged_blocks_fetched = 3;</code>
     * @return The remoteMergedBlocksFetched.
     */
    @java.lang.Override
    public long getRemoteMergedBlocksFetched() {
      return remoteMergedBlocksFetched_;
    }

    public static final int LOCAL_MERGED_BLOCKS_FETCHED_FIELD_NUMBER = 4;
    private long localMergedBlocksFetched_;
    /**
     * <code>int64 local_merged_blocks_fetched = 4;</code>
     * @return The localMergedBlocksFetched.
     */
    @java.lang.Override
    public long getLocalMergedBlocksFetched() {
      return localMergedBlocksFetched_;
    }

    public static final int REMOTE_MERGED_CHUNKS_FETCHED_FIELD_NUMBER = 5;
    private long remoteMergedChunksFetched_;
    /**
     * <code>int64 remote_merged_chunks_fetched = 5;</code>
     * @return The remoteMergedChunksFetched.
     */
    @java.lang.Override
    public long getRemoteMergedChunksFetched() {
      return remoteMergedChunksFetched_;
    }

    public static final int LOCAL_MERGED_CHUNKS_FETCHED_FIELD_NUMBER = 6;
    private long localMergedChunksFetched_;
    /**
     * <code>int64 local_merged_chunks_fetched = 6;</code>
     * @return The localMergedChunksFetched.
     */
    @java.lang.Override
    public long getLocalMergedChunksFetched() {
      return localMergedChunksFetched_;
    }

    public static final int REMOTE_MERGED_BYTES_READ_FIELD_NUMBER = 7;
    private long remoteMergedBytesRead_;
    /**
     * <code>int64 remote_merged_bytes_read = 7;</code>
     * @return The remoteMergedBytesRead.
     */
    @java.lang.Override
    public long getRemoteMergedBytesRead() {
      return remoteMergedBytesRead_;
    }

    public static final int LOCAL_MERGED_BYTES_READ_FIELD_NUMBER = 8;
    private long localMergedBytesRead_;
    /**
     * <code>int64 local_merged_bytes_read = 8;</code>
     * @return The localMergedBytesRead.
     */
    @java.lang.Override
    public long getLocalMergedBytesRead() {
      return localMergedBytesRead_;
    }

    public static final int REMOTE_MERGED_REQS_DURATION_FIELD_NUMBER = 9;
    private long remoteMergedReqsDuration_;
    /**
     * <code>int64 remote_merged_reqs_duration = 9;</code>
     * @return The remoteMergedReqsDuration.
     */
    @java.lang.Override
    public long getRemoteMergedReqsDuration() {
      return remoteMergedReqsDuration_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (corruptMergedBlockChunks_ != 0L) {
        output.writeInt64(1, corruptMergedBlockChunks_);
      }
      if (mergedFetchFallbackCount_ != 0L) {
        output.writeInt64(2, mergedFetchFallbackCount_);
      }
      if (remoteMergedBlocksFetched_ != 0L) {
        output.writeInt64(3, remoteMergedBlocksFetched_);
      }
      if (localMergedBlocksFetched_ != 0L) {
        output.writeInt64(4, localMergedBlocksFetched_);
      }
      if (remoteMergedChunksFetched_ != 0L) {
        output.writeInt64(5, remoteMergedChunksFetched_);
      }
      if (localMergedChunksFetched_ != 0L) {
        output.writeInt64(6, localMergedChunksFetched_);
      }
      if (remoteMergedBytesRead_ != 0L) {
        output.writeInt64(7, remoteMergedBytesRead_);
      }
      if (localMergedBytesRead_ != 0L) {
        output.writeInt64(8, localMergedBytesRead_);
      }
      if (remoteMergedReqsDuration_ != 0L) {
        output.writeInt64(9, remoteMergedReqsDuration_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (corruptMergedBlockChunks_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, corruptMergedBlockChunks_);
      }
      if (mergedFetchFallbackCount_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, mergedFetchFallbackCount_);
      }
      if (remoteMergedBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, remoteMergedBlocksFetched_);
      }
      if (localMergedBlocksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, localMergedBlocksFetched_);
      }
      if (remoteMergedChunksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, remoteMergedChunksFetched_);
      }
      if (localMergedChunksFetched_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, localMergedChunksFetched_);
      }
      if (remoteMergedBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, remoteMergedBytesRead_);
      }
      if (localMergedBytesRead_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, localMergedBytesRead_);
      }
      if (remoteMergedReqsDuration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(9, remoteMergedReqsDuration_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics other = (org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics) obj;

      if (getCorruptMergedBlockChunks()
          != other.getCorruptMergedBlockChunks()) return false;
      if (getMergedFetchFallbackCount()
          != other.getMergedFetchFallbackCount()) return false;
      if (getRemoteMergedBlocksFetched()
          != other.getRemoteMergedBlocksFetched()) return false;
      if (getLocalMergedBlocksFetched()
          != other.getLocalMergedBlocksFetched()) return false;
      if (getRemoteMergedChunksFetched()
          != other.getRemoteMergedChunksFetched()) return false;
      if (getLocalMergedChunksFetched()
          != other.getLocalMergedChunksFetched()) return false;
      if (getRemoteMergedBytesRead()
          != other.getRemoteMergedBytesRead()) return false;
      if (getLocalMergedBytesRead()
          != other.getLocalMergedBytesRead()) return false;
      if (getRemoteMergedReqsDuration()
          != other.getRemoteMergedReqsDuration()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getCorruptMergedBlockChunks());
      hash = (37 * hash) + MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMergedFetchFallbackCount());
      hash = (37 * hash) + REMOTE_MERGED_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRemoteMergedBlocksFetched());
      hash = (37 * hash) + LOCAL_MERGED_BLOCKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getLocalMergedBlocksFetched());
      hash = (37 * hash) + REMOTE_MERGED_CHUNKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRemoteMergedChunksFetched());
      hash = (37 * hash) + LOCAL_MERGED_CHUNKS_FETCHED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getLocalMergedChunksFetched());
      hash = (37 * hash) + REMOTE_MERGED_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRemoteMergedBytesRead());
      hash = (37 * hash) + LOCAL_MERGED_BYTES_READ_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getLocalMergedBytesRead());
      hash = (37 * hash) + REMOTE_MERGED_REQS_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRemoteMergedReqsDuration());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ShufflePushReadMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ShufflePushReadMetrics)
        org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.class, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        corruptMergedBlockChunks_ = 0L;

        mergedFetchFallbackCount_ = 0L;

        remoteMergedBlocksFetched_ = 0L;

        localMergedBlocksFetched_ = 0L;

        remoteMergedChunksFetched_ = 0L;

        localMergedChunksFetched_ = 0L;

        remoteMergedBytesRead_ = 0L;

        localMergedBytesRead_ = 0L;

        remoteMergedReqsDuration_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics build() {
        org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics result = new org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics(this);
        result.corruptMergedBlockChunks_ = corruptMergedBlockChunks_;
        result.mergedFetchFallbackCount_ = mergedFetchFallbackCount_;
        result.remoteMergedBlocksFetched_ = remoteMergedBlocksFetched_;
        result.localMergedBlocksFetched_ = localMergedBlocksFetched_;
        result.remoteMergedChunksFetched_ = remoteMergedChunksFetched_;
        result.localMergedChunksFetched_ = localMergedChunksFetched_;
        result.remoteMergedBytesRead_ = remoteMergedBytesRead_;
        result.localMergedBytesRead_ = localMergedBytesRead_;
        result.remoteMergedReqsDuration_ = remoteMergedReqsDuration_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics.getDefaultInstance()) return this;
        if (other.getCorruptMergedBlockChunks() != 0L) {
          setCorruptMergedBlockChunks(other.getCorruptMergedBlockChunks());
        }
        if (other.getMergedFetchFallbackCount() != 0L) {
          setMergedFetchFallbackCount(other.getMergedFetchFallbackCount());
        }
        if (other.getRemoteMergedBlocksFetched() != 0L) {
          setRemoteMergedBlocksFetched(other.getRemoteMergedBlocksFetched());
        }
        if (other.getLocalMergedBlocksFetched() != 0L) {
          setLocalMergedBlocksFetched(other.getLocalMergedBlocksFetched());
        }
        if (other.getRemoteMergedChunksFetched() != 0L) {
          setRemoteMergedChunksFetched(other.getRemoteMergedChunksFetched());
        }
        if (other.getLocalMergedChunksFetched() != 0L) {
          setLocalMergedChunksFetched(other.getLocalMergedChunksFetched());
        }
        if (other.getRemoteMergedBytesRead() != 0L) {
          setRemoteMergedBytesRead(other.getRemoteMergedBytesRead());
        }
        if (other.getLocalMergedBytesRead() != 0L) {
          setLocalMergedBytesRead(other.getLocalMergedBytesRead());
        }
        if (other.getRemoteMergedReqsDuration() != 0L) {
          setRemoteMergedReqsDuration(other.getRemoteMergedReqsDuration());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long corruptMergedBlockChunks_ ;
      /**
       * <code>int64 corrupt_merged_block_chunks = 1;</code>
       * @return The corruptMergedBlockChunks.
       */
      @java.lang.Override
      public long getCorruptMergedBlockChunks() {
        return corruptMergedBlockChunks_;
      }
      /**
       * <code>int64 corrupt_merged_block_chunks = 1;</code>
       * @param value The corruptMergedBlockChunks to set.
       * @return This builder for chaining.
       */
      public Builder setCorruptMergedBlockChunks(long value) {
        
        corruptMergedBlockChunks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 corrupt_merged_block_chunks = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearCorruptMergedBlockChunks() {
        
        corruptMergedBlockChunks_ = 0L;
        onChanged();
        return this;
      }

      private long mergedFetchFallbackCount_ ;
      /**
       * <code>int64 merged_fetch_fallback_count = 2;</code>
       * @return The mergedFetchFallbackCount.
       */
      @java.lang.Override
      public long getMergedFetchFallbackCount() {
        return mergedFetchFallbackCount_;
      }
      /**
       * <code>int64 merged_fetch_fallback_count = 2;</code>
       * @param value The mergedFetchFallbackCount to set.
       * @return This builder for chaining.
       */
      public Builder setMergedFetchFallbackCount(long value) {
        
        mergedFetchFallbackCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 merged_fetch_fallback_count = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearMergedFetchFallbackCount() {
        
        mergedFetchFallbackCount_ = 0L;
        onChanged();
        return this;
      }

      private long remoteMergedBlocksFetched_ ;
      /**
       * <code>int64 remote_merged_blocks_fetched = 3;</code>
       * @return The remoteMergedBlocksFetched.
       */
      @java.lang.Override
      public long getRemoteMergedBlocksFetched() {
        return remoteMergedBlocksFetched_;
      }
      /**
       * <code>int64 remote_merged_blocks_fetched = 3;</code>
       * @param value The remoteMergedBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteMergedBlocksFetched(long value) {
        
        remoteMergedBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remote_merged_blocks_fetched = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteMergedBlocksFetched() {
        
        remoteMergedBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long localMergedBlocksFetched_ ;
      /**
       * <code>int64 local_merged_blocks_fetched = 4;</code>
       * @return The localMergedBlocksFetched.
       */
      @java.lang.Override
      public long getLocalMergedBlocksFetched() {
        return localMergedBlocksFetched_;
      }
      /**
       * <code>int64 local_merged_blocks_fetched = 4;</code>
       * @param value The localMergedBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setLocalMergedBlocksFetched(long value) {
        
        localMergedBlocksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 local_merged_blocks_fetched = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalMergedBlocksFetched() {
        
        localMergedBlocksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long remoteMergedChunksFetched_ ;
      /**
       * <code>int64 remote_merged_chunks_fetched = 5;</code>
       * @return The remoteMergedChunksFetched.
       */
      @java.lang.Override
      public long getRemoteMergedChunksFetched() {
        return remoteMergedChunksFetched_;
      }
      /**
       * <code>int64 remote_merged_chunks_fetched = 5;</code>
       * @param value The remoteMergedChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteMergedChunksFetched(long value) {
        
        remoteMergedChunksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remote_merged_chunks_fetched = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteMergedChunksFetched() {
        
        remoteMergedChunksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long localMergedChunksFetched_ ;
      /**
       * <code>int64 local_merged_chunks_fetched = 6;</code>
       * @return The localMergedChunksFetched.
       */
      @java.lang.Override
      public long getLocalMergedChunksFetched() {
        return localMergedChunksFetched_;
      }
      /**
       * <code>int64 local_merged_chunks_fetched = 6;</code>
       * @param value The localMergedChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setLocalMergedChunksFetched(long value) {
        
        localMergedChunksFetched_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 local_merged_chunks_fetched = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalMergedChunksFetched() {
        
        localMergedChunksFetched_ = 0L;
        onChanged();
        return this;
      }

      private long remoteMergedBytesRead_ ;
      /**
       * <code>int64 remote_merged_bytes_read = 7;</code>
       * @return The remoteMergedBytesRead.
       */
      @java.lang.Override
      public long getRemoteMergedBytesRead() {
        return remoteMergedBytesRead_;
      }
      /**
       * <code>int64 remote_merged_bytes_read = 7;</code>
       * @param value The remoteMergedBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteMergedBytesRead(long value) {
        
        remoteMergedBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remote_merged_bytes_read = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteMergedBytesRead() {
        
        remoteMergedBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long localMergedBytesRead_ ;
      /**
       * <code>int64 local_merged_bytes_read = 8;</code>
       * @return The localMergedBytesRead.
       */
      @java.lang.Override
      public long getLocalMergedBytesRead() {
        return localMergedBytesRead_;
      }
      /**
       * <code>int64 local_merged_bytes_read = 8;</code>
       * @param value The localMergedBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setLocalMergedBytesRead(long value) {
        
        localMergedBytesRead_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 local_merged_bytes_read = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalMergedBytesRead() {
        
        localMergedBytesRead_ = 0L;
        onChanged();
        return this;
      }

      private long remoteMergedReqsDuration_ ;
      /**
       * <code>int64 remote_merged_reqs_duration = 9;</code>
       * @return The remoteMergedReqsDuration.
       */
      @java.lang.Override
      public long getRemoteMergedReqsDuration() {
        return remoteMergedReqsDuration_;
      }
      /**
       * <code>int64 remote_merged_reqs_duration = 9;</code>
       * @param value The remoteMergedReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteMergedReqsDuration(long value) {
        
        remoteMergedReqsDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 remote_merged_reqs_duration = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteMergedReqsDuration() {
        
        remoteMergedReqsDuration_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ShufflePushReadMetrics)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ShufflePushReadMetrics)
    private static final org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ShufflePushReadMetrics>
        PARSER = new com.google.protobuf.AbstractParser<ShufflePushReadMetrics>() {
      @java.lang.Override
      public ShufflePushReadMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ShufflePushReadMetrics(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ShufflePushReadMetrics> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ShufflePushReadMetrics> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ShuffleWriteMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ShuffleWriteMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int64 bytes_written = 1;</code>
     * @return The bytesWritten.
     */
    long getBytesWritten();

    /**
     * <code>int64 write_time = 2;</code>
     * @return The writeTime.
     */
    long getWriteTime();

    /**
     * <code>int64 records_written = 3;</code>
     * @return The recordsWritten.
     */
    long getRecordsWritten();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ShuffleWriteMetrics}
   */
  public static final class ShuffleWriteMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ShuffleWriteMetrics)
      ShuffleWriteMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ShuffleWriteMetrics.newBuilder() to construct.
    private ShuffleWriteMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ShuffleWriteMetrics() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ShuffleWriteMetrics();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ShuffleWriteMetrics(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              bytesWritten_ = input.readInt64();
              break;
            }
            case 16: {

              writeTime_ = input.readInt64();
              break;
            }
            case 24: {

              recordsWritten_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.class, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.Builder.class);
    }

    public static final int BYTES_WRITTEN_FIELD_NUMBER = 1;
    private long bytesWritten_;
    /**
     * <code>int64 bytes_written = 1;</code>
     * @return The bytesWritten.
     */
    @java.lang.Override
    public long getBytesWritten() {
      return bytesWritten_;
    }

    public static final int WRITE_TIME_FIELD_NUMBER = 2;
    private long writeTime_;
    /**
     * <code>int64 write_time = 2;</code>
     * @return The writeTime.
     */
    @java.lang.Override
    public long getWriteTime() {
      return writeTime_;
    }

    public static final int RECORDS_WRITTEN_FIELD_NUMBER = 3;
    private long recordsWritten_;
    /**
     * <code>int64 records_written = 3;</code>
     * @return The recordsWritten.
     */
    @java.lang.Override
    public long getRecordsWritten() {
      return recordsWritten_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (bytesWritten_ != 0L) {
        output.writeInt64(1, bytesWritten_);
      }
      if (writeTime_ != 0L) {
        output.writeInt64(2, writeTime_);
      }
      if (recordsWritten_ != 0L) {
        output.writeInt64(3, recordsWritten_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (bytesWritten_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, bytesWritten_);
      }
      if (writeTime_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, writeTime_);
      }
      if (recordsWritten_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, recordsWritten_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics other = (org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics) obj;

      if (getBytesWritten()
          != other.getBytesWritten()) return false;
      if (getWriteTime()
          != other.getWriteTime()) return false;
      if (getRecordsWritten()
          != other.getRecordsWritten()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + BYTES_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getBytesWritten());
      hash = (37 * hash) + WRITE_TIME_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getWriteTime());
      hash = (37 * hash) + RECORDS_WRITTEN_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRecordsWritten());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ShuffleWriteMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ShuffleWriteMetrics)
        org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.class, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bytesWritten_ = 0L;

        writeTime_ = 0L;

        recordsWritten_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics build() {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics result = new org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics(this);
        result.bytesWritten_ = bytesWritten_;
        result.writeTime_ = writeTime_;
        result.recordsWritten_ = recordsWritten_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics.getDefaultInstance()) return this;
        if (other.getBytesWritten() != 0L) {
          setBytesWritten(other.getBytesWritten());
        }
        if (other.getWriteTime() != 0L) {
          setWriteTime(other.getWriteTime());
        }
        if (other.getRecordsWritten() != 0L) {
          setRecordsWritten(other.getRecordsWritten());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long bytesWritten_ ;
      /**
       * <code>int64 bytes_written = 1;</code>
       * @return The bytesWritten.
       */
      @java.lang.Override
      public long getBytesWritten() {
        return bytesWritten_;
      }
      /**
       * <code>int64 bytes_written = 1;</code>
       * @param value The bytesWritten to set.
       * @return This builder for chaining.
       */
      public Builder setBytesWritten(long value) {
        
        bytesWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 bytes_written = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBytesWritten() {
        
        bytesWritten_ = 0L;
        onChanged();
        return this;
      }

      private long writeTime_ ;
      /**
       * <code>int64 write_time = 2;</code>
       * @return The writeTime.
       */
      @java.lang.Override
      public long getWriteTime() {
        return writeTime_;
      }
      /**
       * <code>int64 write_time = 2;</code>
       * @param value The writeTime to set.
       * @return This builder for chaining.
       */
      public Builder setWriteTime(long value) {
        
        writeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 write_time = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearWriteTime() {
        
        writeTime_ = 0L;
        onChanged();
        return this;
      }

      private long recordsWritten_ ;
      /**
       * <code>int64 records_written = 3;</code>
       * @return The recordsWritten.
       */
      @java.lang.Override
      public long getRecordsWritten() {
        return recordsWritten_;
      }
      /**
       * <code>int64 records_written = 3;</code>
       * @param value The recordsWritten to set.
       * @return This builder for chaining.
       */
      public Builder setRecordsWritten(long value) {
        
        recordsWritten_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 records_written = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearRecordsWritten() {
        
        recordsWritten_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ShuffleWriteMetrics)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ShuffleWriteMetrics)
    private static final org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ShuffleWriteMetrics>
        PARSER = new com.google.protobuf.AbstractParser<ShuffleWriteMetrics>() {
      @java.lang.Override
      public ShuffleWriteMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ShuffleWriteMetrics(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ShuffleWriteMetrics> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ShuffleWriteMetrics> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskMetricDistributionsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.TaskMetricDistributions)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated double quantiles = 1;</code>
     * @return A list containing the quantiles.
     */
    java.util.List<java.lang.Double> getQuantilesList();
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return The count of quantiles.
     */
    int getQuantilesCount();
    /**
     * <code>repeated double quantiles = 1;</code>
     * @param index The index of the element to return.
     * @return The quantiles at the given index.
     */
    double getQuantiles(int index);

    /**
     * <code>repeated double duration = 2;</code>
     * @return A list containing the duration.
     */
    java.util.List<java.lang.Double> getDurationList();
    /**
     * <code>repeated double duration = 2;</code>
     * @return The count of duration.
     */
    int getDurationCount();
    /**
     * <code>repeated double duration = 2;</code>
     * @param index The index of the element to return.
     * @return The duration at the given index.
     */
    double getDuration(int index);

    /**
     * <code>repeated double executor_deserialize_time = 3;</code>
     * @return A list containing the executorDeserializeTime.
     */
    java.util.List<java.lang.Double> getExecutorDeserializeTimeList();
    /**
     * <code>repeated double executor_deserialize_time = 3;</code>
     * @return The count of executorDeserializeTime.
     */
    int getExecutorDeserializeTimeCount();
    /**
     * <code>repeated double executor_deserialize_time = 3;</code>
     * @param index The index of the element to return.
     * @return The executorDeserializeTime at the given index.
     */
    double getExecutorDeserializeTime(int index);

    /**
     * <code>repeated double executor_deserialize_cpu_time = 4;</code>
     * @return A list containing the executorDeserializeCpuTime.
     */
    java.util.List<java.lang.Double> getExecutorDeserializeCpuTimeList();
    /**
     * <code>repeated double executor_deserialize_cpu_time = 4;</code>
     * @return The count of executorDeserializeCpuTime.
     */
    int getExecutorDeserializeCpuTimeCount();
    /**
     * <code>repeated double executor_deserialize_cpu_time = 4;</code>
     * @param index The index of the element to return.
     * @return The executorDeserializeCpuTime at the given index.
     */
    double getExecutorDeserializeCpuTime(int index);

    /**
     * <code>repeated double executor_run_time = 5;</code>
     * @return A list containing the executorRunTime.
     */
    java.util.List<java.lang.Double> getExecutorRunTimeList();
    /**
     * <code>repeated double executor_run_time = 5;</code>
     * @return The count of executorRunTime.
     */
    int getExecutorRunTimeCount();
    /**
     * <code>repeated double executor_run_time = 5;</code>
     * @param index The index of the element to return.
     * @return The executorRunTime at the given index.
     */
    double getExecutorRunTime(int index);

    /**
     * <code>repeated double executor_cpu_time = 6;</code>
     * @return A list containing the executorCpuTime.
     */
    java.util.List<java.lang.Double> getExecutorCpuTimeList();
    /**
     * <code>repeated double executor_cpu_time = 6;</code>
     * @return The count of executorCpuTime.
     */
    int getExecutorCpuTimeCount();
    /**
     * <code>repeated double executor_cpu_time = 6;</code>
     * @param index The index of the element to return.
     * @return The executorCpuTime at the given index.
     */
    double getExecutorCpuTime(int index);

    /**
     * <code>repeated double result_size = 7;</code>
     * @return A list containing the resultSize.
     */
    java.util.List<java.lang.Double> getResultSizeList();
    /**
     * <code>repeated double result_size = 7;</code>
     * @return The count of resultSize.
     */
    int getResultSizeCount();
    /**
     * <code>repeated double result_size = 7;</code>
     * @param index The index of the element to return.
     * @return The resultSize at the given index.
     */
    double getResultSize(int index);

    /**
     * <code>repeated double jvm_gc_time = 8;</code>
     * @return A list containing the jvmGcTime.
     */
    java.util.List<java.lang.Double> getJvmGcTimeList();
    /**
     * <code>repeated double jvm_gc_time = 8;</code>
     * @return The count of jvmGcTime.
     */
    int getJvmGcTimeCount();
    /**
     * <code>repeated double jvm_gc_time = 8;</code>
     * @param index The index of the element to return.
     * @return The jvmGcTime at the given index.
     */
    double getJvmGcTime(int index);

    /**
     * <code>repeated double result_serialization_time = 9;</code>
     * @return A list containing the resultSerializationTime.
     */
    java.util.List<java.lang.Double> getResultSerializationTimeList();
    /**
     * <code>repeated double result_serialization_time = 9;</code>
     * @return The count of resultSerializationTime.
     */
    int getResultSerializationTimeCount();
    /**
     * <code>repeated double result_serialization_time = 9;</code>
     * @param index The index of the element to return.
     * @return The resultSerializationTime at the given index.
     */
    double getResultSerializationTime(int index);

    /**
     * <code>repeated double getting_result_time = 10;</code>
     * @return A list containing the gettingResultTime.
     */
    java.util.List<java.lang.Double> getGettingResultTimeList();
    /**
     * <code>repeated double getting_result_time = 10;</code>
     * @return The count of gettingResultTime.
     */
    int getGettingResultTimeCount();
    /**
     * <code>repeated double getting_result_time = 10;</code>
     * @param index The index of the element to return.
     * @return The gettingResultTime at the given index.
     */
    double getGettingResultTime(int index);

    /**
     * <code>repeated double scheduler_delay = 11;</code>
     * @return A list containing the schedulerDelay.
     */
    java.util.List<java.lang.Double> getSchedulerDelayList();
    /**
     * <code>repeated double scheduler_delay = 11;</code>
     * @return The count of schedulerDelay.
     */
    int getSchedulerDelayCount();
    /**
     * <code>repeated double scheduler_delay = 11;</code>
     * @param index The index of the element to return.
     * @return The schedulerDelay at the given index.
     */
    double getSchedulerDelay(int index);

    /**
     * <code>repeated double peak_execution_memory = 12;</code>
     * @return A list containing the peakExecutionMemory.
     */
    java.util.List<java.lang.Double> getPeakExecutionMemoryList();
    /**
     * <code>repeated double peak_execution_memory = 12;</code>
     * @return The count of peakExecutionMemory.
     */
    int getPeakExecutionMemoryCount();
    /**
     * <code>repeated double peak_execution_memory = 12;</code>
     * @param index The index of the element to return.
     * @return The peakExecutionMemory at the given index.
     */
    double getPeakExecutionMemory(int index);

    /**
     * <code>repeated double memory_bytes_spilled = 13;</code>
     * @return A list containing the memoryBytesSpilled.
     */
    java.util.List<java.lang.Double> getMemoryBytesSpilledList();
    /**
     * <code>repeated double memory_bytes_spilled = 13;</code>
     * @return The count of memoryBytesSpilled.
     */
    int getMemoryBytesSpilledCount();
    /**
     * <code>repeated double memory_bytes_spilled = 13;</code>
     * @param index The index of the element to return.
     * @return The memoryBytesSpilled at the given index.
     */
    double getMemoryBytesSpilled(int index);

    /**
     * <code>repeated double disk_bytes_spilled = 14;</code>
     * @return A list containing the diskBytesSpilled.
     */
    java.util.List<java.lang.Double> getDiskBytesSpilledList();
    /**
     * <code>repeated double disk_bytes_spilled = 14;</code>
     * @return The count of diskBytesSpilled.
     */
    int getDiskBytesSpilledCount();
    /**
     * <code>repeated double disk_bytes_spilled = 14;</code>
     * @param index The index of the element to return.
     * @return The diskBytesSpilled at the given index.
     */
    double getDiskBytesSpilled(int index);

    /**
     * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
     * @return Whether the inputMetrics field is set.
     */
    boolean hasInputMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
     * @return The inputMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions getInputMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributionsOrBuilder getInputMetricsOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
     * @return Whether the outputMetrics field is set.
     */
    boolean hasOutputMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
     * @return The outputMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions getOutputMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributionsOrBuilder getOutputMetricsOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
     * @return Whether the shuffleReadMetrics field is set.
     */
    boolean hasShuffleReadMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
     * @return The shuffleReadMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions getShuffleReadMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributionsOrBuilder getShuffleReadMetricsOrBuilder();

    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
     * @return Whether the shuffleWriteMetrics field is set.
     */
    boolean hasShuffleWriteMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
     * @return The shuffleWriteMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions getShuffleWriteMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributionsOrBuilder getShuffleWriteMetricsOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.TaskMetricDistributions}
   */
  public static final class TaskMetricDistributions extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.TaskMetricDistributions)
      TaskMetricDistributionsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskMetricDistributions.newBuilder() to construct.
    private TaskMetricDistributions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskMetricDistributions() {
      quantiles_ = emptyDoubleList();
      duration_ = emptyDoubleList();
      executorDeserializeTime_ = emptyDoubleList();
      executorDeserializeCpuTime_ = emptyDoubleList();
      executorRunTime_ = emptyDoubleList();
      executorCpuTime_ = emptyDoubleList();
      resultSize_ = emptyDoubleList();
      jvmGcTime_ = emptyDoubleList();
      resultSerializationTime_ = emptyDoubleList();
      gettingResultTime_ = emptyDoubleList();
      schedulerDelay_ = emptyDoubleList();
      peakExecutionMemory_ = emptyDoubleList();
      memoryBytesSpilled_ = emptyDoubleList();
      diskBytesSpilled_ = emptyDoubleList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new TaskMetricDistributions();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskMetricDistributions(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                quantiles_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              quantiles_.addDouble(input.readDouble());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                quantiles_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                quantiles_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 17: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                duration_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              duration_.addDouble(input.readDouble());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                duration_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                duration_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 25: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                executorDeserializeTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              executorDeserializeTime_.addDouble(input.readDouble());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000004) != 0) && input.getBytesUntilLimit() > 0) {
                executorDeserializeTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              while (input.getBytesUntilLimit() > 0) {
                executorDeserializeTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 33: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                executorDeserializeCpuTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000008;
              }
              executorDeserializeCpuTime_.addDouble(input.readDouble());
              break;
            }
            case 34: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000008) != 0) && input.getBytesUntilLimit() > 0) {
                executorDeserializeCpuTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000008;
              }
              while (input.getBytesUntilLimit() > 0) {
                executorDeserializeCpuTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 41: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                executorRunTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000010;
              }
              executorRunTime_.addDouble(input.readDouble());
              break;
            }
            case 42: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000010) != 0) && input.getBytesUntilLimit() > 0) {
                executorRunTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000010;
              }
              while (input.getBytesUntilLimit() > 0) {
                executorRunTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 49: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                executorCpuTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000020;
              }
              executorCpuTime_.addDouble(input.readDouble());
              break;
            }
            case 50: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000020) != 0) && input.getBytesUntilLimit() > 0) {
                executorCpuTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000020;
              }
              while (input.getBytesUntilLimit() > 0) {
                executorCpuTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 57: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                resultSize_ = newDoubleList();
                mutable_bitField0_ |= 0x00000040;
              }
              resultSize_.addDouble(input.readDouble());
              break;
            }
            case 58: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000040) != 0) && input.getBytesUntilLimit() > 0) {
                resultSize_ = newDoubleList();
                mutable_bitField0_ |= 0x00000040;
              }
              while (input.getBytesUntilLimit() > 0) {
                resultSize_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 65: {
              if (!((mutable_bitField0_ & 0x00000080) != 0)) {
                jvmGcTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000080;
              }
              jvmGcTime_.addDouble(input.readDouble());
              break;
            }
            case 66: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000080) != 0) && input.getBytesUntilLimit() > 0) {
                jvmGcTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000080;
              }
              while (input.getBytesUntilLimit() > 0) {
                jvmGcTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 73: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                resultSerializationTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000100;
              }
              resultSerializationTime_.addDouble(input.readDouble());
              break;
            }
            case 74: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000100) != 0) && input.getBytesUntilLimit() > 0) {
                resultSerializationTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000100;
              }
              while (input.getBytesUntilLimit() > 0) {
                resultSerializationTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 81: {
              if (!((mutable_bitField0_ & 0x00000200) != 0)) {
                gettingResultTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000200;
              }
              gettingResultTime_.addDouble(input.readDouble());
              break;
            }
            case 82: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000200) != 0) && input.getBytesUntilLimit() > 0) {
                gettingResultTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000200;
              }
              while (input.getBytesUntilLimit() > 0) {
                gettingResultTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 89: {
              if (!((mutable_bitField0_ & 0x00000400) != 0)) {
                schedulerDelay_ = newDoubleList();
                mutable_bitField0_ |= 0x00000400;
              }
              schedulerDelay_.addDouble(input.readDouble());
              break;
            }
            case 90: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000400) != 0) && input.getBytesUntilLimit() > 0) {
                schedulerDelay_ = newDoubleList();
                mutable_bitField0_ |= 0x00000400;
              }
              while (input.getBytesUntilLimit() > 0) {
                schedulerDelay_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 97: {
              if (!((mutable_bitField0_ & 0x00000800) != 0)) {
                peakExecutionMemory_ = newDoubleList();
                mutable_bitField0_ |= 0x00000800;
              }
              peakExecutionMemory_.addDouble(input.readDouble());
              break;
            }
            case 98: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000800) != 0) && input.getBytesUntilLimit() > 0) {
                peakExecutionMemory_ = newDoubleList();
                mutable_bitField0_ |= 0x00000800;
              }
              while (input.getBytesUntilLimit() > 0) {
                peakExecutionMemory_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 105: {
              if (!((mutable_bitField0_ & 0x00001000) != 0)) {
                memoryBytesSpilled_ = newDoubleList();
                mutable_bitField0_ |= 0x00001000;
              }
              memoryBytesSpilled_.addDouble(input.readDouble());
              break;
            }
            case 106: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00001000) != 0) && input.getBytesUntilLimit() > 0) {
                memoryBytesSpilled_ = newDoubleList();
                mutable_bitField0_ |= 0x00001000;
              }
              while (input.getBytesUntilLimit() > 0) {
                memoryBytesSpilled_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 113: {
              if (!((mutable_bitField0_ & 0x00002000) != 0)) {
                diskBytesSpilled_ = newDoubleList();
                mutable_bitField0_ |= 0x00002000;
              }
              diskBytesSpilled_.addDouble(input.readDouble());
              break;
            }
            case 114: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00002000) != 0) && input.getBytesUntilLimit() > 0) {
                diskBytesSpilled_ = newDoubleList();
                mutable_bitField0_ |= 0x00002000;
              }
              while (input.getBytesUntilLimit() > 0) {
                diskBytesSpilled_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 122: {
              org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.Builder subBuilder = null;
              if (inputMetrics_ != null) {
                subBuilder = inputMetrics_.toBuilder();
              }
              inputMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(inputMetrics_);
                inputMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            case 130: {
              org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.Builder subBuilder = null;
              if (outputMetrics_ != null) {
                subBuilder = outputMetrics_.toBuilder();
              }
              outputMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(outputMetrics_);
                outputMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            case 138: {
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.Builder subBuilder = null;
              if (shuffleReadMetrics_ != null) {
                subBuilder = shuffleReadMetrics_.toBuilder();
              }
              shuffleReadMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(shuffleReadMetrics_);
                shuffleReadMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            case 146: {
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.Builder subBuilder = null;
              if (shuffleWriteMetrics_ != null) {
                subBuilder = shuffleWriteMetrics_.toBuilder();
              }
              shuffleWriteMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(shuffleWriteMetrics_);
                shuffleWriteMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          quantiles_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          duration_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          executorDeserializeTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          executorDeserializeCpuTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          executorRunTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          executorCpuTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          resultSize_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000080) != 0)) {
          jvmGcTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          resultSerializationTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000200) != 0)) {
          gettingResultTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000400) != 0)) {
          schedulerDelay_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000800) != 0)) {
          peakExecutionMemory_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00001000) != 0)) {
          memoryBytesSpilled_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00002000) != 0)) {
          diskBytesSpilled_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.Builder.class);
    }

    public static final int QUANTILES_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.DoubleList quantiles_;
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return A list containing the quantiles.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getQuantilesList() {
      return quantiles_;
    }
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return The count of quantiles.
     */
    public int getQuantilesCount() {
      return quantiles_.size();
    }
    /**
     * <code>repeated double quantiles = 1;</code>
     * @param index The index of the element to return.
     * @return The quantiles at the given index.
     */
    public double getQuantiles(int index) {
      return quantiles_.getDouble(index);
    }
    private int quantilesMemoizedSerializedSize = -1;

    public static final int DURATION_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.DoubleList duration_;
    /**
     * <code>repeated double duration = 2;</code>
     * @return A list containing the duration.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getDurationList() {
      return duration_;
    }
    /**
     * <code>repeated double duration = 2;</code>
     * @return The count of duration.
     */
    public int getDurationCount() {
      return duration_.size();
    }
    /**
     * <code>repeated double duration = 2;</code>
     * @param index The index of the element to return.
     * @return The duration at the given index.
     */
    public double getDuration(int index) {
      return duration_.getDouble(index);
    }
    private int durationMemoizedSerializedSize = -1;

    public static final int EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.DoubleList executorDeserializeTime_;
    /**
     * <code>repeated double executor_deserialize_time = 3;</code>
     * @return A list containing the executorDeserializeTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getExecutorDeserializeTimeList() {
      return executorDeserializeTime_;
    }
    /**
     * <code>repeated double executor_deserialize_time = 3;</code>
     * @return The count of executorDeserializeTime.
     */
    public int getExecutorDeserializeTimeCount() {
      return executorDeserializeTime_.size();
    }
    /**
     * <code>repeated double executor_deserialize_time = 3;</code>
     * @param index The index of the element to return.
     * @return The executorDeserializeTime at the given index.
     */
    public double getExecutorDeserializeTime(int index) {
      return executorDeserializeTime_.getDouble(index);
    }
    private int executorDeserializeTimeMemoizedSerializedSize = -1;

    public static final int EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER = 4;
    private com.google.protobuf.Internal.DoubleList executorDeserializeCpuTime_;
    /**
     * <code>repeated double executor_deserialize_cpu_time = 4;</code>
     * @return A list containing the executorDeserializeCpuTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getExecutorDeserializeCpuTimeList() {
      return executorDeserializeCpuTime_;
    }
    /**
     * <code>repeated double executor_deserialize_cpu_time = 4;</code>
     * @return The count of executorDeserializeCpuTime.
     */
    public int getExecutorDeserializeCpuTimeCount() {
      return executorDeserializeCpuTime_.size();
    }
    /**
     * <code>repeated double executor_deserialize_cpu_time = 4;</code>
     * @param index The index of the element to return.
     * @return The executorDeserializeCpuTime at the given index.
     */
    public double getExecutorDeserializeCpuTime(int index) {
      return executorDeserializeCpuTime_.getDouble(index);
    }
    private int executorDeserializeCpuTimeMemoizedSerializedSize = -1;

    public static final int EXECUTOR_RUN_TIME_FIELD_NUMBER = 5;
    private com.google.protobuf.Internal.DoubleList executorRunTime_;
    /**
     * <code>repeated double executor_run_time = 5;</code>
     * @return A list containing the executorRunTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getExecutorRunTimeList() {
      return executorRunTime_;
    }
    /**
     * <code>repeated double executor_run_time = 5;</code>
     * @return The count of executorRunTime.
     */
    public int getExecutorRunTimeCount() {
      return executorRunTime_.size();
    }
    /**
     * <code>repeated double executor_run_time = 5;</code>
     * @param index The index of the element to return.
     * @return The executorRunTime at the given index.
     */
    public double getExecutorRunTime(int index) {
      return executorRunTime_.getDouble(index);
    }
    private int executorRunTimeMemoizedSerializedSize = -1;

    public static final int EXECUTOR_CPU_TIME_FIELD_NUMBER = 6;
    private com.google.protobuf.Internal.DoubleList executorCpuTime_;
    /**
     * <code>repeated double executor_cpu_time = 6;</code>
     * @return A list containing the executorCpuTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getExecutorCpuTimeList() {
      return executorCpuTime_;
    }
    /**
     * <code>repeated double executor_cpu_time = 6;</code>
     * @return The count of executorCpuTime.
     */
    public int getExecutorCpuTimeCount() {
      return executorCpuTime_.size();
    }
    /**
     * <code>repeated double executor_cpu_time = 6;</code>
     * @param index The index of the element to return.
     * @return The executorCpuTime at the given index.
     */
    public double getExecutorCpuTime(int index) {
      return executorCpuTime_.getDouble(index);
    }
    private int executorCpuTimeMemoizedSerializedSize = -1;

    public static final int RESULT_SIZE_FIELD_NUMBER = 7;
    private com.google.protobuf.Internal.DoubleList resultSize_;
    /**
     * <code>repeated double result_size = 7;</code>
     * @return A list containing the resultSize.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getResultSizeList() {
      return resultSize_;
    }
    /**
     * <code>repeated double result_size = 7;</code>
     * @return The count of resultSize.
     */
    public int getResultSizeCount() {
      return resultSize_.size();
    }
    /**
     * <code>repeated double result_size = 7;</code>
     * @param index The index of the element to return.
     * @return The resultSize at the given index.
     */
    public double getResultSize(int index) {
      return resultSize_.getDouble(index);
    }
    private int resultSizeMemoizedSerializedSize = -1;

    public static final int JVM_GC_TIME_FIELD_NUMBER = 8;
    private com.google.protobuf.Internal.DoubleList jvmGcTime_;
    /**
     * <code>repeated double jvm_gc_time = 8;</code>
     * @return A list containing the jvmGcTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getJvmGcTimeList() {
      return jvmGcTime_;
    }
    /**
     * <code>repeated double jvm_gc_time = 8;</code>
     * @return The count of jvmGcTime.
     */
    public int getJvmGcTimeCount() {
      return jvmGcTime_.size();
    }
    /**
     * <code>repeated double jvm_gc_time = 8;</code>
     * @param index The index of the element to return.
     * @return The jvmGcTime at the given index.
     */
    public double getJvmGcTime(int index) {
      return jvmGcTime_.getDouble(index);
    }
    private int jvmGcTimeMemoizedSerializedSize = -1;

    public static final int RESULT_SERIALIZATION_TIME_FIELD_NUMBER = 9;
    private com.google.protobuf.Internal.DoubleList resultSerializationTime_;
    /**
     * <code>repeated double result_serialization_time = 9;</code>
     * @return A list containing the resultSerializationTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getResultSerializationTimeList() {
      return resultSerializationTime_;
    }
    /**
     * <code>repeated double result_serialization_time = 9;</code>
     * @return The count of resultSerializationTime.
     */
    public int getResultSerializationTimeCount() {
      return resultSerializationTime_.size();
    }
    /**
     * <code>repeated double result_serialization_time = 9;</code>
     * @param index The index of the element to return.
     * @return The resultSerializationTime at the given index.
     */
    public double getResultSerializationTime(int index) {
      return resultSerializationTime_.getDouble(index);
    }
    private int resultSerializationTimeMemoizedSerializedSize = -1;

    public static final int GETTING_RESULT_TIME_FIELD_NUMBER = 10;
    private com.google.protobuf.Internal.DoubleList gettingResultTime_;
    /**
     * <code>repeated double getting_result_time = 10;</code>
     * @return A list containing the gettingResultTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getGettingResultTimeList() {
      return gettingResultTime_;
    }
    /**
     * <code>repeated double getting_result_time = 10;</code>
     * @return The count of gettingResultTime.
     */
    public int getGettingResultTimeCount() {
      return gettingResultTime_.size();
    }
    /**
     * <code>repeated double getting_result_time = 10;</code>
     * @param index The index of the element to return.
     * @return The gettingResultTime at the given index.
     */
    public double getGettingResultTime(int index) {
      return gettingResultTime_.getDouble(index);
    }
    private int gettingResultTimeMemoizedSerializedSize = -1;

    public static final int SCHEDULER_DELAY_FIELD_NUMBER = 11;
    private com.google.protobuf.Internal.DoubleList schedulerDelay_;
    /**
     * <code>repeated double scheduler_delay = 11;</code>
     * @return A list containing the schedulerDelay.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getSchedulerDelayList() {
      return schedulerDelay_;
    }
    /**
     * <code>repeated double scheduler_delay = 11;</code>
     * @return The count of schedulerDelay.
     */
    public int getSchedulerDelayCount() {
      return schedulerDelay_.size();
    }
    /**
     * <code>repeated double scheduler_delay = 11;</code>
     * @param index The index of the element to return.
     * @return The schedulerDelay at the given index.
     */
    public double getSchedulerDelay(int index) {
      return schedulerDelay_.getDouble(index);
    }
    private int schedulerDelayMemoizedSerializedSize = -1;

    public static final int PEAK_EXECUTION_MEMORY_FIELD_NUMBER = 12;
    private com.google.protobuf.Internal.DoubleList peakExecutionMemory_;
    /**
     * <code>repeated double peak_execution_memory = 12;</code>
     * @return A list containing the peakExecutionMemory.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getPeakExecutionMemoryList() {
      return peakExecutionMemory_;
    }
    /**
     * <code>repeated double peak_execution_memory = 12;</code>
     * @return The count of peakExecutionMemory.
     */
    public int getPeakExecutionMemoryCount() {
      return peakExecutionMemory_.size();
    }
    /**
     * <code>repeated double peak_execution_memory = 12;</code>
     * @param index The index of the element to return.
     * @return The peakExecutionMemory at the given index.
     */
    public double getPeakExecutionMemory(int index) {
      return peakExecutionMemory_.getDouble(index);
    }
    private int peakExecutionMemoryMemoizedSerializedSize = -1;

    public static final int MEMORY_BYTES_SPILLED_FIELD_NUMBER = 13;
    private com.google.protobuf.Internal.DoubleList memoryBytesSpilled_;
    /**
     * <code>repeated double memory_bytes_spilled = 13;</code>
     * @return A list containing the memoryBytesSpilled.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getMemoryBytesSpilledList() {
      return memoryBytesSpilled_;
    }
    /**
     * <code>repeated double memory_bytes_spilled = 13;</code>
     * @return The count of memoryBytesSpilled.
     */
    public int getMemoryBytesSpilledCount() {
      return memoryBytesSpilled_.size();
    }
    /**
     * <code>repeated double memory_bytes_spilled = 13;</code>
     * @param index The index of the element to return.
     * @return The memoryBytesSpilled at the given index.
     */
    public double getMemoryBytesSpilled(int index) {
      return memoryBytesSpilled_.getDouble(index);
    }
    private int memoryBytesSpilledMemoizedSerializedSize = -1;

    public static final int DISK_BYTES_SPILLED_FIELD_NUMBER = 14;
    private com.google.protobuf.Internal.DoubleList diskBytesSpilled_;
    /**
     * <code>repeated double disk_bytes_spilled = 14;</code>
     * @return A list containing the diskBytesSpilled.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getDiskBytesSpilledList() {
      return diskBytesSpilled_;
    }
    /**
     * <code>repeated double disk_bytes_spilled = 14;</code>
     * @return The count of diskBytesSpilled.
     */
    public int getDiskBytesSpilledCount() {
      return diskBytesSpilled_.size();
    }
    /**
     * <code>repeated double disk_bytes_spilled = 14;</code>
     * @param index The index of the element to return.
     * @return The diskBytesSpilled at the given index.
     */
    public double getDiskBytesSpilled(int index) {
      return diskBytesSpilled_.getDouble(index);
    }
    private int diskBytesSpilledMemoizedSerializedSize = -1;

    public static final int INPUT_METRICS_FIELD_NUMBER = 15;
    private org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions inputMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
     * @return Whether the inputMetrics field is set.
     */
    @java.lang.Override
    public boolean hasInputMetrics() {
      return inputMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
     * @return The inputMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions getInputMetrics() {
      return inputMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.getDefaultInstance() : inputMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributionsOrBuilder getInputMetricsOrBuilder() {
      return getInputMetrics();
    }

    public static final int OUTPUT_METRICS_FIELD_NUMBER = 16;
    private org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions outputMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
     * @return Whether the outputMetrics field is set.
     */
    @java.lang.Override
    public boolean hasOutputMetrics() {
      return outputMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
     * @return The outputMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions getOutputMetrics() {
      return outputMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.getDefaultInstance() : outputMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributionsOrBuilder getOutputMetricsOrBuilder() {
      return getOutputMetrics();
    }

    public static final int SHUFFLE_READ_METRICS_FIELD_NUMBER = 17;
    private org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions shuffleReadMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
     * @return Whether the shuffleReadMetrics field is set.
     */
    @java.lang.Override
    public boolean hasShuffleReadMetrics() {
      return shuffleReadMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
     * @return The shuffleReadMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions getShuffleReadMetrics() {
      return shuffleReadMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.getDefaultInstance() : shuffleReadMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributionsOrBuilder getShuffleReadMetricsOrBuilder() {
      return getShuffleReadMetrics();
    }

    public static final int SHUFFLE_WRITE_METRICS_FIELD_NUMBER = 18;
    private org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions shuffleWriteMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
     * @return Whether the shuffleWriteMetrics field is set.
     */
    @java.lang.Override
    public boolean hasShuffleWriteMetrics() {
      return shuffleWriteMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
     * @return The shuffleWriteMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions getShuffleWriteMetrics() {
      return shuffleWriteMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.getDefaultInstance() : shuffleWriteMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributionsOrBuilder getShuffleWriteMetricsOrBuilder() {
      return getShuffleWriteMetrics();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getQuantilesList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(quantilesMemoizedSerializedSize);
      }
      for (int i = 0; i < quantiles_.size(); i++) {
        output.writeDoubleNoTag(quantiles_.getDouble(i));
      }
      if (getDurationList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(durationMemoizedSerializedSize);
      }
      for (int i = 0; i < duration_.size(); i++) {
        output.writeDoubleNoTag(duration_.getDouble(i));
      }
      if (getExecutorDeserializeTimeList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(executorDeserializeTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < executorDeserializeTime_.size(); i++) {
        output.writeDoubleNoTag(executorDeserializeTime_.getDouble(i));
      }
      if (getExecutorDeserializeCpuTimeList().size() > 0) {
        output.writeUInt32NoTag(34);
        output.writeUInt32NoTag(executorDeserializeCpuTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < executorDeserializeCpuTime_.size(); i++) {
        output.writeDoubleNoTag(executorDeserializeCpuTime_.getDouble(i));
      }
      if (getExecutorRunTimeList().size() > 0) {
        output.writeUInt32NoTag(42);
        output.writeUInt32NoTag(executorRunTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < executorRunTime_.size(); i++) {
        output.writeDoubleNoTag(executorRunTime_.getDouble(i));
      }
      if (getExecutorCpuTimeList().size() > 0) {
        output.writeUInt32NoTag(50);
        output.writeUInt32NoTag(executorCpuTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < executorCpuTime_.size(); i++) {
        output.writeDoubleNoTag(executorCpuTime_.getDouble(i));
      }
      if (getResultSizeList().size() > 0) {
        output.writeUInt32NoTag(58);
        output.writeUInt32NoTag(resultSizeMemoizedSerializedSize);
      }
      for (int i = 0; i < resultSize_.size(); i++) {
        output.writeDoubleNoTag(resultSize_.getDouble(i));
      }
      if (getJvmGcTimeList().size() > 0) {
        output.writeUInt32NoTag(66);
        output.writeUInt32NoTag(jvmGcTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < jvmGcTime_.size(); i++) {
        output.writeDoubleNoTag(jvmGcTime_.getDouble(i));
      }
      if (getResultSerializationTimeList().size() > 0) {
        output.writeUInt32NoTag(74);
        output.writeUInt32NoTag(resultSerializationTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < resultSerializationTime_.size(); i++) {
        output.writeDoubleNoTag(resultSerializationTime_.getDouble(i));
      }
      if (getGettingResultTimeList().size() > 0) {
        output.writeUInt32NoTag(82);
        output.writeUInt32NoTag(gettingResultTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < gettingResultTime_.size(); i++) {
        output.writeDoubleNoTag(gettingResultTime_.getDouble(i));
      }
      if (getSchedulerDelayList().size() > 0) {
        output.writeUInt32NoTag(90);
        output.writeUInt32NoTag(schedulerDelayMemoizedSerializedSize);
      }
      for (int i = 0; i < schedulerDelay_.size(); i++) {
        output.writeDoubleNoTag(schedulerDelay_.getDouble(i));
      }
      if (getPeakExecutionMemoryList().size() > 0) {
        output.writeUInt32NoTag(98);
        output.writeUInt32NoTag(peakExecutionMemoryMemoizedSerializedSize);
      }
      for (int i = 0; i < peakExecutionMemory_.size(); i++) {
        output.writeDoubleNoTag(peakExecutionMemory_.getDouble(i));
      }
      if (getMemoryBytesSpilledList().size() > 0) {
        output.writeUInt32NoTag(106);
        output.writeUInt32NoTag(memoryBytesSpilledMemoizedSerializedSize);
      }
      for (int i = 0; i < memoryBytesSpilled_.size(); i++) {
        output.writeDoubleNoTag(memoryBytesSpilled_.getDouble(i));
      }
      if (getDiskBytesSpilledList().size() > 0) {
        output.writeUInt32NoTag(114);
        output.writeUInt32NoTag(diskBytesSpilledMemoizedSerializedSize);
      }
      for (int i = 0; i < diskBytesSpilled_.size(); i++) {
        output.writeDoubleNoTag(diskBytesSpilled_.getDouble(i));
      }
      if (inputMetrics_ != null) {
        output.writeMessage(15, getInputMetrics());
      }
      if (outputMetrics_ != null) {
        output.writeMessage(16, getOutputMetrics());
      }
      if (shuffleReadMetrics_ != null) {
        output.writeMessage(17, getShuffleReadMetrics());
      }
      if (shuffleWriteMetrics_ != null) {
        output.writeMessage(18, getShuffleWriteMetrics());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        dataSize = 8 * getQuantilesList().size();
        size += dataSize;
        if (!getQuantilesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        quantilesMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getDurationList().size();
        size += dataSize;
        if (!getDurationList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        durationMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getExecutorDeserializeTimeList().size();
        size += dataSize;
        if (!getExecutorDeserializeTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        executorDeserializeTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getExecutorDeserializeCpuTimeList().size();
        size += dataSize;
        if (!getExecutorDeserializeCpuTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        executorDeserializeCpuTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getExecutorRunTimeList().size();
        size += dataSize;
        if (!getExecutorRunTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        executorRunTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getExecutorCpuTimeList().size();
        size += dataSize;
        if (!getExecutorCpuTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        executorCpuTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getResultSizeList().size();
        size += dataSize;
        if (!getResultSizeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        resultSizeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getJvmGcTimeList().size();
        size += dataSize;
        if (!getJvmGcTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        jvmGcTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getResultSerializationTimeList().size();
        size += dataSize;
        if (!getResultSerializationTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        resultSerializationTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getGettingResultTimeList().size();
        size += dataSize;
        if (!getGettingResultTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        gettingResultTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getSchedulerDelayList().size();
        size += dataSize;
        if (!getSchedulerDelayList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        schedulerDelayMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getPeakExecutionMemoryList().size();
        size += dataSize;
        if (!getPeakExecutionMemoryList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        peakExecutionMemoryMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getMemoryBytesSpilledList().size();
        size += dataSize;
        if (!getMemoryBytesSpilledList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        memoryBytesSpilledMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getDiskBytesSpilledList().size();
        size += dataSize;
        if (!getDiskBytesSpilledList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        diskBytesSpilledMemoizedSerializedSize = dataSize;
      }
      if (inputMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(15, getInputMetrics());
      }
      if (outputMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(16, getOutputMetrics());
      }
      if (shuffleReadMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(17, getShuffleReadMetrics());
      }
      if (shuffleWriteMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(18, getShuffleWriteMetrics());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions other = (org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions) obj;

      if (!getQuantilesList()
          .equals(other.getQuantilesList())) return false;
      if (!getDurationList()
          .equals(other.getDurationList())) return false;
      if (!getExecutorDeserializeTimeList()
          .equals(other.getExecutorDeserializeTimeList())) return false;
      if (!getExecutorDeserializeCpuTimeList()
          .equals(other.getExecutorDeserializeCpuTimeList())) return false;
      if (!getExecutorRunTimeList()
          .equals(other.getExecutorRunTimeList())) return false;
      if (!getExecutorCpuTimeList()
          .equals(other.getExecutorCpuTimeList())) return false;
      if (!getResultSizeList()
          .equals(other.getResultSizeList())) return false;
      if (!getJvmGcTimeList()
          .equals(other.getJvmGcTimeList())) return false;
      if (!getResultSerializationTimeList()
          .equals(other.getResultSerializationTimeList())) return false;
      if (!getGettingResultTimeList()
          .equals(other.getGettingResultTimeList())) return false;
      if (!getSchedulerDelayList()
          .equals(other.getSchedulerDelayList())) return false;
      if (!getPeakExecutionMemoryList()
          .equals(other.getPeakExecutionMemoryList())) return false;
      if (!getMemoryBytesSpilledList()
          .equals(other.getMemoryBytesSpilledList())) return false;
      if (!getDiskBytesSpilledList()
          .equals(other.getDiskBytesSpilledList())) return false;
      if (hasInputMetrics() != other.hasInputMetrics()) return false;
      if (hasInputMetrics()) {
        if (!getInputMetrics()
            .equals(other.getInputMetrics())) return false;
      }
      if (hasOutputMetrics() != other.hasOutputMetrics()) return false;
      if (hasOutputMetrics()) {
        if (!getOutputMetrics()
            .equals(other.getOutputMetrics())) return false;
      }
      if (hasShuffleReadMetrics() != other.hasShuffleReadMetrics()) return false;
      if (hasShuffleReadMetrics()) {
        if (!getShuffleReadMetrics()
            .equals(other.getShuffleReadMetrics())) return false;
      }
      if (hasShuffleWriteMetrics() != other.hasShuffleWriteMetrics()) return false;
      if (hasShuffleWriteMetrics()) {
        if (!getShuffleWriteMetrics()
            .equals(other.getShuffleWriteMetrics())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getQuantilesCount() > 0) {
        hash = (37 * hash) + QUANTILES_FIELD_NUMBER;
        hash = (53 * hash) + getQuantilesList().hashCode();
      }
      if (getDurationCount() > 0) {
        hash = (37 * hash) + DURATION_FIELD_NUMBER;
        hash = (53 * hash) + getDurationList().hashCode();
      }
      if (getExecutorDeserializeTimeCount() > 0) {
        hash = (37 * hash) + EXECUTOR_DESERIALIZE_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorDeserializeTimeList().hashCode();
      }
      if (getExecutorDeserializeCpuTimeCount() > 0) {
        hash = (37 * hash) + EXECUTOR_DESERIALIZE_CPU_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorDeserializeCpuTimeList().hashCode();
      }
      if (getExecutorRunTimeCount() > 0) {
        hash = (37 * hash) + EXECUTOR_RUN_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorRunTimeList().hashCode();
      }
      if (getExecutorCpuTimeCount() > 0) {
        hash = (37 * hash) + EXECUTOR_CPU_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorCpuTimeList().hashCode();
      }
      if (getResultSizeCount() > 0) {
        hash = (37 * hash) + RESULT_SIZE_FIELD_NUMBER;
        hash = (53 * hash) + getResultSizeList().hashCode();
      }
      if (getJvmGcTimeCount() > 0) {
        hash = (37 * hash) + JVM_GC_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getJvmGcTimeList().hashCode();
      }
      if (getResultSerializationTimeCount() > 0) {
        hash = (37 * hash) + RESULT_SERIALIZATION_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getResultSerializationTimeList().hashCode();
      }
      if (getGettingResultTimeCount() > 0) {
        hash = (37 * hash) + GETTING_RESULT_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getGettingResultTimeList().hashCode();
      }
      if (getSchedulerDelayCount() > 0) {
        hash = (37 * hash) + SCHEDULER_DELAY_FIELD_NUMBER;
        hash = (53 * hash) + getSchedulerDelayList().hashCode();
      }
      if (getPeakExecutionMemoryCount() > 0) {
        hash = (37 * hash) + PEAK_EXECUTION_MEMORY_FIELD_NUMBER;
        hash = (53 * hash) + getPeakExecutionMemoryList().hashCode();
      }
      if (getMemoryBytesSpilledCount() > 0) {
        hash = (37 * hash) + MEMORY_BYTES_SPILLED_FIELD_NUMBER;
        hash = (53 * hash) + getMemoryBytesSpilledList().hashCode();
      }
      if (getDiskBytesSpilledCount() > 0) {
        hash = (37 * hash) + DISK_BYTES_SPILLED_FIELD_NUMBER;
        hash = (53 * hash) + getDiskBytesSpilledList().hashCode();
      }
      if (hasInputMetrics()) {
        hash = (37 * hash) + INPUT_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getInputMetrics().hashCode();
      }
      if (hasOutputMetrics()) {
        hash = (37 * hash) + OUTPUT_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getOutputMetrics().hashCode();
      }
      if (hasShuffleReadMetrics()) {
        hash = (37 * hash) + SHUFFLE_READ_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getShuffleReadMetrics().hashCode();
      }
      if (hasShuffleWriteMetrics()) {
        hash = (37 * hash) + SHUFFLE_WRITE_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getShuffleWriteMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.TaskMetricDistributions}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.TaskMetricDistributions)
        org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        quantiles_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        duration_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        executorDeserializeTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        executorDeserializeCpuTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000008);
        executorRunTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000010);
        executorCpuTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000020);
        resultSize_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000040);
        jvmGcTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000080);
        resultSerializationTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000100);
        gettingResultTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000200);
        schedulerDelay_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000400);
        peakExecutionMemory_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000800);
        memoryBytesSpilled_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00001000);
        diskBytesSpilled_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00002000);
        if (inputMetricsBuilder_ == null) {
          inputMetrics_ = null;
        } else {
          inputMetrics_ = null;
          inputMetricsBuilder_ = null;
        }
        if (outputMetricsBuilder_ == null) {
          outputMetrics_ = null;
        } else {
          outputMetrics_ = null;
          outputMetricsBuilder_ = null;
        }
        if (shuffleReadMetricsBuilder_ == null) {
          shuffleReadMetrics_ = null;
        } else {
          shuffleReadMetrics_ = null;
          shuffleReadMetricsBuilder_ = null;
        }
        if (shuffleWriteMetricsBuilder_ == null) {
          shuffleWriteMetrics_ = null;
        } else {
          shuffleWriteMetrics_ = null;
          shuffleWriteMetricsBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions build() {
        org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions result = new org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          quantiles_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.quantiles_ = quantiles_;
        if (((bitField0_ & 0x00000002) != 0)) {
          duration_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.duration_ = duration_;
        if (((bitField0_ & 0x00000004) != 0)) {
          executorDeserializeTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.executorDeserializeTime_ = executorDeserializeTime_;
        if (((bitField0_ & 0x00000008) != 0)) {
          executorDeserializeCpuTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.executorDeserializeCpuTime_ = executorDeserializeCpuTime_;
        if (((bitField0_ & 0x00000010) != 0)) {
          executorRunTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.executorRunTime_ = executorRunTime_;
        if (((bitField0_ & 0x00000020) != 0)) {
          executorCpuTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000020);
        }
        result.executorCpuTime_ = executorCpuTime_;
        if (((bitField0_ & 0x00000040) != 0)) {
          resultSize_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000040);
        }
        result.resultSize_ = resultSize_;
        if (((bitField0_ & 0x00000080) != 0)) {
          jvmGcTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000080);
        }
        result.jvmGcTime_ = jvmGcTime_;
        if (((bitField0_ & 0x00000100) != 0)) {
          resultSerializationTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000100);
        }
        result.resultSerializationTime_ = resultSerializationTime_;
        if (((bitField0_ & 0x00000200) != 0)) {
          gettingResultTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000200);
        }
        result.gettingResultTime_ = gettingResultTime_;
        if (((bitField0_ & 0x00000400) != 0)) {
          schedulerDelay_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000400);
        }
        result.schedulerDelay_ = schedulerDelay_;
        if (((bitField0_ & 0x00000800) != 0)) {
          peakExecutionMemory_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000800);
        }
        result.peakExecutionMemory_ = peakExecutionMemory_;
        if (((bitField0_ & 0x00001000) != 0)) {
          memoryBytesSpilled_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00001000);
        }
        result.memoryBytesSpilled_ = memoryBytesSpilled_;
        if (((bitField0_ & 0x00002000) != 0)) {
          diskBytesSpilled_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00002000);
        }
        result.diskBytesSpilled_ = diskBytesSpilled_;
        if (inputMetricsBuilder_ == null) {
          result.inputMetrics_ = inputMetrics_;
        } else {
          result.inputMetrics_ = inputMetricsBuilder_.build();
        }
        if (outputMetricsBuilder_ == null) {
          result.outputMetrics_ = outputMetrics_;
        } else {
          result.outputMetrics_ = outputMetricsBuilder_.build();
        }
        if (shuffleReadMetricsBuilder_ == null) {
          result.shuffleReadMetrics_ = shuffleReadMetrics_;
        } else {
          result.shuffleReadMetrics_ = shuffleReadMetricsBuilder_.build();
        }
        if (shuffleWriteMetricsBuilder_ == null) {
          result.shuffleWriteMetrics_ = shuffleWriteMetrics_;
        } else {
          result.shuffleWriteMetrics_ = shuffleWriteMetricsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions.getDefaultInstance()) return this;
        if (!other.quantiles_.isEmpty()) {
          if (quantiles_.isEmpty()) {
            quantiles_ = other.quantiles_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureQuantilesIsMutable();
            quantiles_.addAll(other.quantiles_);
          }
          onChanged();
        }
        if (!other.duration_.isEmpty()) {
          if (duration_.isEmpty()) {
            duration_ = other.duration_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureDurationIsMutable();
            duration_.addAll(other.duration_);
          }
          onChanged();
        }
        if (!other.executorDeserializeTime_.isEmpty()) {
          if (executorDeserializeTime_.isEmpty()) {
            executorDeserializeTime_ = other.executorDeserializeTime_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureExecutorDeserializeTimeIsMutable();
            executorDeserializeTime_.addAll(other.executorDeserializeTime_);
          }
          onChanged();
        }
        if (!other.executorDeserializeCpuTime_.isEmpty()) {
          if (executorDeserializeCpuTime_.isEmpty()) {
            executorDeserializeCpuTime_ = other.executorDeserializeCpuTime_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureExecutorDeserializeCpuTimeIsMutable();
            executorDeserializeCpuTime_.addAll(other.executorDeserializeCpuTime_);
          }
          onChanged();
        }
        if (!other.executorRunTime_.isEmpty()) {
          if (executorRunTime_.isEmpty()) {
            executorRunTime_ = other.executorRunTime_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureExecutorRunTimeIsMutable();
            executorRunTime_.addAll(other.executorRunTime_);
          }
          onChanged();
        }
        if (!other.executorCpuTime_.isEmpty()) {
          if (executorCpuTime_.isEmpty()) {
            executorCpuTime_ = other.executorCpuTime_;
            bitField0_ = (bitField0_ & ~0x00000020);
          } else {
            ensureExecutorCpuTimeIsMutable();
            executorCpuTime_.addAll(other.executorCpuTime_);
          }
          onChanged();
        }
        if (!other.resultSize_.isEmpty()) {
          if (resultSize_.isEmpty()) {
            resultSize_ = other.resultSize_;
            bitField0_ = (bitField0_ & ~0x00000040);
          } else {
            ensureResultSizeIsMutable();
            resultSize_.addAll(other.resultSize_);
          }
          onChanged();
        }
        if (!other.jvmGcTime_.isEmpty()) {
          if (jvmGcTime_.isEmpty()) {
            jvmGcTime_ = other.jvmGcTime_;
            bitField0_ = (bitField0_ & ~0x00000080);
          } else {
            ensureJvmGcTimeIsMutable();
            jvmGcTime_.addAll(other.jvmGcTime_);
          }
          onChanged();
        }
        if (!other.resultSerializationTime_.isEmpty()) {
          if (resultSerializationTime_.isEmpty()) {
            resultSerializationTime_ = other.resultSerializationTime_;
            bitField0_ = (bitField0_ & ~0x00000100);
          } else {
            ensureResultSerializationTimeIsMutable();
            resultSerializationTime_.addAll(other.resultSerializationTime_);
          }
          onChanged();
        }
        if (!other.gettingResultTime_.isEmpty()) {
          if (gettingResultTime_.isEmpty()) {
            gettingResultTime_ = other.gettingResultTime_;
            bitField0_ = (bitField0_ & ~0x00000200);
          } else {
            ensureGettingResultTimeIsMutable();
            gettingResultTime_.addAll(other.gettingResultTime_);
          }
          onChanged();
        }
        if (!other.schedulerDelay_.isEmpty()) {
          if (schedulerDelay_.isEmpty()) {
            schedulerDelay_ = other.schedulerDelay_;
            bitField0_ = (bitField0_ & ~0x00000400);
          } else {
            ensureSchedulerDelayIsMutable();
            schedulerDelay_.addAll(other.schedulerDelay_);
          }
          onChanged();
        }
        if (!other.peakExecutionMemory_.isEmpty()) {
          if (peakExecutionMemory_.isEmpty()) {
            peakExecutionMemory_ = other.peakExecutionMemory_;
            bitField0_ = (bitField0_ & ~0x00000800);
          } else {
            ensurePeakExecutionMemoryIsMutable();
            peakExecutionMemory_.addAll(other.peakExecutionMemory_);
          }
          onChanged();
        }
        if (!other.memoryBytesSpilled_.isEmpty()) {
          if (memoryBytesSpilled_.isEmpty()) {
            memoryBytesSpilled_ = other.memoryBytesSpilled_;
            bitField0_ = (bitField0_ & ~0x00001000);
          } else {
            ensureMemoryBytesSpilledIsMutable();
            memoryBytesSpilled_.addAll(other.memoryBytesSpilled_);
          }
          onChanged();
        }
        if (!other.diskBytesSpilled_.isEmpty()) {
          if (diskBytesSpilled_.isEmpty()) {
            diskBytesSpilled_ = other.diskBytesSpilled_;
            bitField0_ = (bitField0_ & ~0x00002000);
          } else {
            ensureDiskBytesSpilledIsMutable();
            diskBytesSpilled_.addAll(other.diskBytesSpilled_);
          }
          onChanged();
        }
        if (other.hasInputMetrics()) {
          mergeInputMetrics(other.getInputMetrics());
        }
        if (other.hasOutputMetrics()) {
          mergeOutputMetrics(other.getOutputMetrics());
        }
        if (other.hasShuffleReadMetrics()) {
          mergeShuffleReadMetrics(other.getShuffleReadMetrics());
        }
        if (other.hasShuffleWriteMetrics()) {
          mergeShuffleWriteMetrics(other.getShuffleWriteMetrics());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.DoubleList quantiles_ = emptyDoubleList();
      private void ensureQuantilesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          quantiles_ = mutableCopy(quantiles_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return A list containing the quantiles.
       */
      public java.util.List<java.lang.Double>
          getQuantilesList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(quantiles_) : quantiles_;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return The count of quantiles.
       */
      public int getQuantilesCount() {
        return quantiles_.size();
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param index The index of the element to return.
       * @return The quantiles at the given index.
       */
      public double getQuantiles(int index) {
        return quantiles_.getDouble(index);
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param index The index to set the value at.
       * @param value The quantiles to set.
       * @return This builder for chaining.
       */
      public Builder setQuantiles(
          int index, double value) {
        ensureQuantilesIsMutable();
        quantiles_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param value The quantiles to add.
       * @return This builder for chaining.
       */
      public Builder addQuantiles(double value) {
        ensureQuantilesIsMutable();
        quantiles_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param values The quantiles to add.
       * @return This builder for chaining.
       */
      public Builder addAllQuantiles(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureQuantilesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, quantiles_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearQuantiles() {
        quantiles_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList duration_ = emptyDoubleList();
      private void ensureDurationIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          duration_ = mutableCopy(duration_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated double duration = 2;</code>
       * @return A list containing the duration.
       */
      public java.util.List<java.lang.Double>
          getDurationList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(duration_) : duration_;
      }
      /**
       * <code>repeated double duration = 2;</code>
       * @return The count of duration.
       */
      public int getDurationCount() {
        return duration_.size();
      }
      /**
       * <code>repeated double duration = 2;</code>
       * @param index The index of the element to return.
       * @return The duration at the given index.
       */
      public double getDuration(int index) {
        return duration_.getDouble(index);
      }
      /**
       * <code>repeated double duration = 2;</code>
       * @param index The index to set the value at.
       * @param value The duration to set.
       * @return This builder for chaining.
       */
      public Builder setDuration(
          int index, double value) {
        ensureDurationIsMutable();
        duration_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double duration = 2;</code>
       * @param value The duration to add.
       * @return This builder for chaining.
       */
      public Builder addDuration(double value) {
        ensureDurationIsMutable();
        duration_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double duration = 2;</code>
       * @param values The duration to add.
       * @return This builder for chaining.
       */
      public Builder addAllDuration(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureDurationIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, duration_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double duration = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDuration() {
        duration_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList executorDeserializeTime_ = emptyDoubleList();
      private void ensureExecutorDeserializeTimeIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          executorDeserializeTime_ = mutableCopy(executorDeserializeTime_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated double executor_deserialize_time = 3;</code>
       * @return A list containing the executorDeserializeTime.
       */
      public java.util.List<java.lang.Double>
          getExecutorDeserializeTimeList() {
        return ((bitField0_ & 0x00000004) != 0) ?
                 java.util.Collections.unmodifiableList(executorDeserializeTime_) : executorDeserializeTime_;
      }
      /**
       * <code>repeated double executor_deserialize_time = 3;</code>
       * @return The count of executorDeserializeTime.
       */
      public int getExecutorDeserializeTimeCount() {
        return executorDeserializeTime_.size();
      }
      /**
       * <code>repeated double executor_deserialize_time = 3;</code>
       * @param index The index of the element to return.
       * @return The executorDeserializeTime at the given index.
       */
      public double getExecutorDeserializeTime(int index) {
        return executorDeserializeTime_.getDouble(index);
      }
      /**
       * <code>repeated double executor_deserialize_time = 3;</code>
       * @param index The index to set the value at.
       * @param value The executorDeserializeTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeTime(
          int index, double value) {
        ensureExecutorDeserializeTimeIsMutable();
        executorDeserializeTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_deserialize_time = 3;</code>
       * @param value The executorDeserializeTime to add.
       * @return This builder for chaining.
       */
      public Builder addExecutorDeserializeTime(double value) {
        ensureExecutorDeserializeTimeIsMutable();
        executorDeserializeTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_deserialize_time = 3;</code>
       * @param values The executorDeserializeTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllExecutorDeserializeTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureExecutorDeserializeTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, executorDeserializeTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_deserialize_time = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeTime() {
        executorDeserializeTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList executorDeserializeCpuTime_ = emptyDoubleList();
      private void ensureExecutorDeserializeCpuTimeIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          executorDeserializeCpuTime_ = mutableCopy(executorDeserializeCpuTime_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated double executor_deserialize_cpu_time = 4;</code>
       * @return A list containing the executorDeserializeCpuTime.
       */
      public java.util.List<java.lang.Double>
          getExecutorDeserializeCpuTimeList() {
        return ((bitField0_ & 0x00000008) != 0) ?
                 java.util.Collections.unmodifiableList(executorDeserializeCpuTime_) : executorDeserializeCpuTime_;
      }
      /**
       * <code>repeated double executor_deserialize_cpu_time = 4;</code>
       * @return The count of executorDeserializeCpuTime.
       */
      public int getExecutorDeserializeCpuTimeCount() {
        return executorDeserializeCpuTime_.size();
      }
      /**
       * <code>repeated double executor_deserialize_cpu_time = 4;</code>
       * @param index The index of the element to return.
       * @return The executorDeserializeCpuTime at the given index.
       */
      public double getExecutorDeserializeCpuTime(int index) {
        return executorDeserializeCpuTime_.getDouble(index);
      }
      /**
       * <code>repeated double executor_deserialize_cpu_time = 4;</code>
       * @param index The index to set the value at.
       * @param value The executorDeserializeCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorDeserializeCpuTime(
          int index, double value) {
        ensureExecutorDeserializeCpuTimeIsMutable();
        executorDeserializeCpuTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_deserialize_cpu_time = 4;</code>
       * @param value The executorDeserializeCpuTime to add.
       * @return This builder for chaining.
       */
      public Builder addExecutorDeserializeCpuTime(double value) {
        ensureExecutorDeserializeCpuTimeIsMutable();
        executorDeserializeCpuTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_deserialize_cpu_time = 4;</code>
       * @param values The executorDeserializeCpuTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllExecutorDeserializeCpuTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureExecutorDeserializeCpuTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, executorDeserializeCpuTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_deserialize_cpu_time = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorDeserializeCpuTime() {
        executorDeserializeCpuTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList executorRunTime_ = emptyDoubleList();
      private void ensureExecutorRunTimeIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          executorRunTime_ = mutableCopy(executorRunTime_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <code>repeated double executor_run_time = 5;</code>
       * @return A list containing the executorRunTime.
       */
      public java.util.List<java.lang.Double>
          getExecutorRunTimeList() {
        return ((bitField0_ & 0x00000010) != 0) ?
                 java.util.Collections.unmodifiableList(executorRunTime_) : executorRunTime_;
      }
      /**
       * <code>repeated double executor_run_time = 5;</code>
       * @return The count of executorRunTime.
       */
      public int getExecutorRunTimeCount() {
        return executorRunTime_.size();
      }
      /**
       * <code>repeated double executor_run_time = 5;</code>
       * @param index The index of the element to return.
       * @return The executorRunTime at the given index.
       */
      public double getExecutorRunTime(int index) {
        return executorRunTime_.getDouble(index);
      }
      /**
       * <code>repeated double executor_run_time = 5;</code>
       * @param index The index to set the value at.
       * @param value The executorRunTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorRunTime(
          int index, double value) {
        ensureExecutorRunTimeIsMutable();
        executorRunTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_run_time = 5;</code>
       * @param value The executorRunTime to add.
       * @return This builder for chaining.
       */
      public Builder addExecutorRunTime(double value) {
        ensureExecutorRunTimeIsMutable();
        executorRunTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_run_time = 5;</code>
       * @param values The executorRunTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllExecutorRunTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureExecutorRunTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, executorRunTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_run_time = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorRunTime() {
        executorRunTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList executorCpuTime_ = emptyDoubleList();
      private void ensureExecutorCpuTimeIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          executorCpuTime_ = mutableCopy(executorCpuTime_);
          bitField0_ |= 0x00000020;
         }
      }
      /**
       * <code>repeated double executor_cpu_time = 6;</code>
       * @return A list containing the executorCpuTime.
       */
      public java.util.List<java.lang.Double>
          getExecutorCpuTimeList() {
        return ((bitField0_ & 0x00000020) != 0) ?
                 java.util.Collections.unmodifiableList(executorCpuTime_) : executorCpuTime_;
      }
      /**
       * <code>repeated double executor_cpu_time = 6;</code>
       * @return The count of executorCpuTime.
       */
      public int getExecutorCpuTimeCount() {
        return executorCpuTime_.size();
      }
      /**
       * <code>repeated double executor_cpu_time = 6;</code>
       * @param index The index of the element to return.
       * @return The executorCpuTime at the given index.
       */
      public double getExecutorCpuTime(int index) {
        return executorCpuTime_.getDouble(index);
      }
      /**
       * <code>repeated double executor_cpu_time = 6;</code>
       * @param index The index to set the value at.
       * @param value The executorCpuTime to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorCpuTime(
          int index, double value) {
        ensureExecutorCpuTimeIsMutable();
        executorCpuTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_cpu_time = 6;</code>
       * @param value The executorCpuTime to add.
       * @return This builder for chaining.
       */
      public Builder addExecutorCpuTime(double value) {
        ensureExecutorCpuTimeIsMutable();
        executorCpuTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_cpu_time = 6;</code>
       * @param values The executorCpuTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllExecutorCpuTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureExecutorCpuTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, executorCpuTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double executor_cpu_time = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorCpuTime() {
        executorCpuTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000020);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList resultSize_ = emptyDoubleList();
      private void ensureResultSizeIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          resultSize_ = mutableCopy(resultSize_);
          bitField0_ |= 0x00000040;
         }
      }
      /**
       * <code>repeated double result_size = 7;</code>
       * @return A list containing the resultSize.
       */
      public java.util.List<java.lang.Double>
          getResultSizeList() {
        return ((bitField0_ & 0x00000040) != 0) ?
                 java.util.Collections.unmodifiableList(resultSize_) : resultSize_;
      }
      /**
       * <code>repeated double result_size = 7;</code>
       * @return The count of resultSize.
       */
      public int getResultSizeCount() {
        return resultSize_.size();
      }
      /**
       * <code>repeated double result_size = 7;</code>
       * @param index The index of the element to return.
       * @return The resultSize at the given index.
       */
      public double getResultSize(int index) {
        return resultSize_.getDouble(index);
      }
      /**
       * <code>repeated double result_size = 7;</code>
       * @param index The index to set the value at.
       * @param value The resultSize to set.
       * @return This builder for chaining.
       */
      public Builder setResultSize(
          int index, double value) {
        ensureResultSizeIsMutable();
        resultSize_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double result_size = 7;</code>
       * @param value The resultSize to add.
       * @return This builder for chaining.
       */
      public Builder addResultSize(double value) {
        ensureResultSizeIsMutable();
        resultSize_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double result_size = 7;</code>
       * @param values The resultSize to add.
       * @return This builder for chaining.
       */
      public Builder addAllResultSize(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureResultSizeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, resultSize_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double result_size = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSize() {
        resultSize_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000040);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList jvmGcTime_ = emptyDoubleList();
      private void ensureJvmGcTimeIsMutable() {
        if (!((bitField0_ & 0x00000080) != 0)) {
          jvmGcTime_ = mutableCopy(jvmGcTime_);
          bitField0_ |= 0x00000080;
         }
      }
      /**
       * <code>repeated double jvm_gc_time = 8;</code>
       * @return A list containing the jvmGcTime.
       */
      public java.util.List<java.lang.Double>
          getJvmGcTimeList() {
        return ((bitField0_ & 0x00000080) != 0) ?
                 java.util.Collections.unmodifiableList(jvmGcTime_) : jvmGcTime_;
      }
      /**
       * <code>repeated double jvm_gc_time = 8;</code>
       * @return The count of jvmGcTime.
       */
      public int getJvmGcTimeCount() {
        return jvmGcTime_.size();
      }
      /**
       * <code>repeated double jvm_gc_time = 8;</code>
       * @param index The index of the element to return.
       * @return The jvmGcTime at the given index.
       */
      public double getJvmGcTime(int index) {
        return jvmGcTime_.getDouble(index);
      }
      /**
       * <code>repeated double jvm_gc_time = 8;</code>
       * @param index The index to set the value at.
       * @param value The jvmGcTime to set.
       * @return This builder for chaining.
       */
      public Builder setJvmGcTime(
          int index, double value) {
        ensureJvmGcTimeIsMutable();
        jvmGcTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double jvm_gc_time = 8;</code>
       * @param value The jvmGcTime to add.
       * @return This builder for chaining.
       */
      public Builder addJvmGcTime(double value) {
        ensureJvmGcTimeIsMutable();
        jvmGcTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double jvm_gc_time = 8;</code>
       * @param values The jvmGcTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllJvmGcTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureJvmGcTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, jvmGcTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double jvm_gc_time = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearJvmGcTime() {
        jvmGcTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000080);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList resultSerializationTime_ = emptyDoubleList();
      private void ensureResultSerializationTimeIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          resultSerializationTime_ = mutableCopy(resultSerializationTime_);
          bitField0_ |= 0x00000100;
         }
      }
      /**
       * <code>repeated double result_serialization_time = 9;</code>
       * @return A list containing the resultSerializationTime.
       */
      public java.util.List<java.lang.Double>
          getResultSerializationTimeList() {
        return ((bitField0_ & 0x00000100) != 0) ?
                 java.util.Collections.unmodifiableList(resultSerializationTime_) : resultSerializationTime_;
      }
      /**
       * <code>repeated double result_serialization_time = 9;</code>
       * @return The count of resultSerializationTime.
       */
      public int getResultSerializationTimeCount() {
        return resultSerializationTime_.size();
      }
      /**
       * <code>repeated double result_serialization_time = 9;</code>
       * @param index The index of the element to return.
       * @return The resultSerializationTime at the given index.
       */
      public double getResultSerializationTime(int index) {
        return resultSerializationTime_.getDouble(index);
      }
      /**
       * <code>repeated double result_serialization_time = 9;</code>
       * @param index The index to set the value at.
       * @param value The resultSerializationTime to set.
       * @return This builder for chaining.
       */
      public Builder setResultSerializationTime(
          int index, double value) {
        ensureResultSerializationTimeIsMutable();
        resultSerializationTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double result_serialization_time = 9;</code>
       * @param value The resultSerializationTime to add.
       * @return This builder for chaining.
       */
      public Builder addResultSerializationTime(double value) {
        ensureResultSerializationTimeIsMutable();
        resultSerializationTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double result_serialization_time = 9;</code>
       * @param values The resultSerializationTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllResultSerializationTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureResultSerializationTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, resultSerializationTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double result_serialization_time = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearResultSerializationTime() {
        resultSerializationTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000100);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList gettingResultTime_ = emptyDoubleList();
      private void ensureGettingResultTimeIsMutable() {
        if (!((bitField0_ & 0x00000200) != 0)) {
          gettingResultTime_ = mutableCopy(gettingResultTime_);
          bitField0_ |= 0x00000200;
         }
      }
      /**
       * <code>repeated double getting_result_time = 10;</code>
       * @return A list containing the gettingResultTime.
       */
      public java.util.List<java.lang.Double>
          getGettingResultTimeList() {
        return ((bitField0_ & 0x00000200) != 0) ?
                 java.util.Collections.unmodifiableList(gettingResultTime_) : gettingResultTime_;
      }
      /**
       * <code>repeated double getting_result_time = 10;</code>
       * @return The count of gettingResultTime.
       */
      public int getGettingResultTimeCount() {
        return gettingResultTime_.size();
      }
      /**
       * <code>repeated double getting_result_time = 10;</code>
       * @param index The index of the element to return.
       * @return The gettingResultTime at the given index.
       */
      public double getGettingResultTime(int index) {
        return gettingResultTime_.getDouble(index);
      }
      /**
       * <code>repeated double getting_result_time = 10;</code>
       * @param index The index to set the value at.
       * @param value The gettingResultTime to set.
       * @return This builder for chaining.
       */
      public Builder setGettingResultTime(
          int index, double value) {
        ensureGettingResultTimeIsMutable();
        gettingResultTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double getting_result_time = 10;</code>
       * @param value The gettingResultTime to add.
       * @return This builder for chaining.
       */
      public Builder addGettingResultTime(double value) {
        ensureGettingResultTimeIsMutable();
        gettingResultTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double getting_result_time = 10;</code>
       * @param values The gettingResultTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllGettingResultTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureGettingResultTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, gettingResultTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double getting_result_time = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearGettingResultTime() {
        gettingResultTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000200);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList schedulerDelay_ = emptyDoubleList();
      private void ensureSchedulerDelayIsMutable() {
        if (!((bitField0_ & 0x00000400) != 0)) {
          schedulerDelay_ = mutableCopy(schedulerDelay_);
          bitField0_ |= 0x00000400;
         }
      }
      /**
       * <code>repeated double scheduler_delay = 11;</code>
       * @return A list containing the schedulerDelay.
       */
      public java.util.List<java.lang.Double>
          getSchedulerDelayList() {
        return ((bitField0_ & 0x00000400) != 0) ?
                 java.util.Collections.unmodifiableList(schedulerDelay_) : schedulerDelay_;
      }
      /**
       * <code>repeated double scheduler_delay = 11;</code>
       * @return The count of schedulerDelay.
       */
      public int getSchedulerDelayCount() {
        return schedulerDelay_.size();
      }
      /**
       * <code>repeated double scheduler_delay = 11;</code>
       * @param index The index of the element to return.
       * @return The schedulerDelay at the given index.
       */
      public double getSchedulerDelay(int index) {
        return schedulerDelay_.getDouble(index);
      }
      /**
       * <code>repeated double scheduler_delay = 11;</code>
       * @param index The index to set the value at.
       * @param value The schedulerDelay to set.
       * @return This builder for chaining.
       */
      public Builder setSchedulerDelay(
          int index, double value) {
        ensureSchedulerDelayIsMutable();
        schedulerDelay_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double scheduler_delay = 11;</code>
       * @param value The schedulerDelay to add.
       * @return This builder for chaining.
       */
      public Builder addSchedulerDelay(double value) {
        ensureSchedulerDelayIsMutable();
        schedulerDelay_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double scheduler_delay = 11;</code>
       * @param values The schedulerDelay to add.
       * @return This builder for chaining.
       */
      public Builder addAllSchedulerDelay(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureSchedulerDelayIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, schedulerDelay_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double scheduler_delay = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearSchedulerDelay() {
        schedulerDelay_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000400);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList peakExecutionMemory_ = emptyDoubleList();
      private void ensurePeakExecutionMemoryIsMutable() {
        if (!((bitField0_ & 0x00000800) != 0)) {
          peakExecutionMemory_ = mutableCopy(peakExecutionMemory_);
          bitField0_ |= 0x00000800;
         }
      }
      /**
       * <code>repeated double peak_execution_memory = 12;</code>
       * @return A list containing the peakExecutionMemory.
       */
      public java.util.List<java.lang.Double>
          getPeakExecutionMemoryList() {
        return ((bitField0_ & 0x00000800) != 0) ?
                 java.util.Collections.unmodifiableList(peakExecutionMemory_) : peakExecutionMemory_;
      }
      /**
       * <code>repeated double peak_execution_memory = 12;</code>
       * @return The count of peakExecutionMemory.
       */
      public int getPeakExecutionMemoryCount() {
        return peakExecutionMemory_.size();
      }
      /**
       * <code>repeated double peak_execution_memory = 12;</code>
       * @param index The index of the element to return.
       * @return The peakExecutionMemory at the given index.
       */
      public double getPeakExecutionMemory(int index) {
        return peakExecutionMemory_.getDouble(index);
      }
      /**
       * <code>repeated double peak_execution_memory = 12;</code>
       * @param index The index to set the value at.
       * @param value The peakExecutionMemory to set.
       * @return This builder for chaining.
       */
      public Builder setPeakExecutionMemory(
          int index, double value) {
        ensurePeakExecutionMemoryIsMutable();
        peakExecutionMemory_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double peak_execution_memory = 12;</code>
       * @param value The peakExecutionMemory to add.
       * @return This builder for chaining.
       */
      public Builder addPeakExecutionMemory(double value) {
        ensurePeakExecutionMemoryIsMutable();
        peakExecutionMemory_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double peak_execution_memory = 12;</code>
       * @param values The peakExecutionMemory to add.
       * @return This builder for chaining.
       */
      public Builder addAllPeakExecutionMemory(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensurePeakExecutionMemoryIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, peakExecutionMemory_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double peak_execution_memory = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearPeakExecutionMemory() {
        peakExecutionMemory_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000800);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList memoryBytesSpilled_ = emptyDoubleList();
      private void ensureMemoryBytesSpilledIsMutable() {
        if (!((bitField0_ & 0x00001000) != 0)) {
          memoryBytesSpilled_ = mutableCopy(memoryBytesSpilled_);
          bitField0_ |= 0x00001000;
         }
      }
      /**
       * <code>repeated double memory_bytes_spilled = 13;</code>
       * @return A list containing the memoryBytesSpilled.
       */
      public java.util.List<java.lang.Double>
          getMemoryBytesSpilledList() {
        return ((bitField0_ & 0x00001000) != 0) ?
                 java.util.Collections.unmodifiableList(memoryBytesSpilled_) : memoryBytesSpilled_;
      }
      /**
       * <code>repeated double memory_bytes_spilled = 13;</code>
       * @return The count of memoryBytesSpilled.
       */
      public int getMemoryBytesSpilledCount() {
        return memoryBytesSpilled_.size();
      }
      /**
       * <code>repeated double memory_bytes_spilled = 13;</code>
       * @param index The index of the element to return.
       * @return The memoryBytesSpilled at the given index.
       */
      public double getMemoryBytesSpilled(int index) {
        return memoryBytesSpilled_.getDouble(index);
      }
      /**
       * <code>repeated double memory_bytes_spilled = 13;</code>
       * @param index The index to set the value at.
       * @param value The memoryBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryBytesSpilled(
          int index, double value) {
        ensureMemoryBytesSpilledIsMutable();
        memoryBytesSpilled_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double memory_bytes_spilled = 13;</code>
       * @param value The memoryBytesSpilled to add.
       * @return This builder for chaining.
       */
      public Builder addMemoryBytesSpilled(double value) {
        ensureMemoryBytesSpilledIsMutable();
        memoryBytesSpilled_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double memory_bytes_spilled = 13;</code>
       * @param values The memoryBytesSpilled to add.
       * @return This builder for chaining.
       */
      public Builder addAllMemoryBytesSpilled(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureMemoryBytesSpilledIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, memoryBytesSpilled_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double memory_bytes_spilled = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryBytesSpilled() {
        memoryBytesSpilled_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00001000);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList diskBytesSpilled_ = emptyDoubleList();
      private void ensureDiskBytesSpilledIsMutable() {
        if (!((bitField0_ & 0x00002000) != 0)) {
          diskBytesSpilled_ = mutableCopy(diskBytesSpilled_);
          bitField0_ |= 0x00002000;
         }
      }
      /**
       * <code>repeated double disk_bytes_spilled = 14;</code>
       * @return A list containing the diskBytesSpilled.
       */
      public java.util.List<java.lang.Double>
          getDiskBytesSpilledList() {
        return ((bitField0_ & 0x00002000) != 0) ?
                 java.util.Collections.unmodifiableList(diskBytesSpilled_) : diskBytesSpilled_;
      }
      /**
       * <code>repeated double disk_bytes_spilled = 14;</code>
       * @return The count of diskBytesSpilled.
       */
      public int getDiskBytesSpilledCount() {
        return diskBytesSpilled_.size();
      }
      /**
       * <code>repeated double disk_bytes_spilled = 14;</code>
       * @param index The index of the element to return.
       * @return The diskBytesSpilled at the given index.
       */
      public double getDiskBytesSpilled(int index) {
        return diskBytesSpilled_.getDouble(index);
      }
      /**
       * <code>repeated double disk_bytes_spilled = 14;</code>
       * @param index The index to set the value at.
       * @param value The diskBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setDiskBytesSpilled(
          int index, double value) {
        ensureDiskBytesSpilledIsMutable();
        diskBytesSpilled_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double disk_bytes_spilled = 14;</code>
       * @param value The diskBytesSpilled to add.
       * @return This builder for chaining.
       */
      public Builder addDiskBytesSpilled(double value) {
        ensureDiskBytesSpilledIsMutable();
        diskBytesSpilled_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double disk_bytes_spilled = 14;</code>
       * @param values The diskBytesSpilled to add.
       * @return This builder for chaining.
       */
      public Builder addAllDiskBytesSpilled(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureDiskBytesSpilledIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, diskBytesSpilled_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double disk_bytes_spilled = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskBytesSpilled() {
        diskBytesSpilled_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00002000);
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions inputMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributionsOrBuilder> inputMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       * @return Whether the inputMetrics field is set.
       */
      public boolean hasInputMetrics() {
        return inputMetricsBuilder_ != null || inputMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       * @return The inputMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions getInputMetrics() {
        if (inputMetricsBuilder_ == null) {
          return inputMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.getDefaultInstance() : inputMetrics_;
        } else {
          return inputMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       */
      public Builder setInputMetrics(org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions value) {
        if (inputMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          inputMetrics_ = value;
          onChanged();
        } else {
          inputMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       */
      public Builder setInputMetrics(
          org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.Builder builderForValue) {
        if (inputMetricsBuilder_ == null) {
          inputMetrics_ = builderForValue.build();
          onChanged();
        } else {
          inputMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       */
      public Builder mergeInputMetrics(org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions value) {
        if (inputMetricsBuilder_ == null) {
          if (inputMetrics_ != null) {
            inputMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.newBuilder(inputMetrics_).mergeFrom(value).buildPartial();
          } else {
            inputMetrics_ = value;
          }
          onChanged();
        } else {
          inputMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       */
      public Builder clearInputMetrics() {
        if (inputMetricsBuilder_ == null) {
          inputMetrics_ = null;
          onChanged();
        } else {
          inputMetrics_ = null;
          inputMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.Builder getInputMetricsBuilder() {
        
        onChanged();
        return getInputMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributionsOrBuilder getInputMetricsOrBuilder() {
        if (inputMetricsBuilder_ != null) {
          return inputMetricsBuilder_.getMessageOrBuilder();
        } else {
          return inputMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.getDefaultInstance() : inputMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.InputMetricDistributions input_metrics = 15;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributionsOrBuilder> 
          getInputMetricsFieldBuilder() {
        if (inputMetricsBuilder_ == null) {
          inputMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributionsOrBuilder>(
                  getInputMetrics(),
                  getParentForChildren(),
                  isClean());
          inputMetrics_ = null;
        }
        return inputMetricsBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions outputMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributionsOrBuilder> outputMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       * @return Whether the outputMetrics field is set.
       */
      public boolean hasOutputMetrics() {
        return outputMetricsBuilder_ != null || outputMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       * @return The outputMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions getOutputMetrics() {
        if (outputMetricsBuilder_ == null) {
          return outputMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.getDefaultInstance() : outputMetrics_;
        } else {
          return outputMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       */
      public Builder setOutputMetrics(org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions value) {
        if (outputMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          outputMetrics_ = value;
          onChanged();
        } else {
          outputMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       */
      public Builder setOutputMetrics(
          org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.Builder builderForValue) {
        if (outputMetricsBuilder_ == null) {
          outputMetrics_ = builderForValue.build();
          onChanged();
        } else {
          outputMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       */
      public Builder mergeOutputMetrics(org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions value) {
        if (outputMetricsBuilder_ == null) {
          if (outputMetrics_ != null) {
            outputMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.newBuilder(outputMetrics_).mergeFrom(value).buildPartial();
          } else {
            outputMetrics_ = value;
          }
          onChanged();
        } else {
          outputMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       */
      public Builder clearOutputMetrics() {
        if (outputMetricsBuilder_ == null) {
          outputMetrics_ = null;
          onChanged();
        } else {
          outputMetrics_ = null;
          outputMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.Builder getOutputMetricsBuilder() {
        
        onChanged();
        return getOutputMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributionsOrBuilder getOutputMetricsOrBuilder() {
        if (outputMetricsBuilder_ != null) {
          return outputMetricsBuilder_.getMessageOrBuilder();
        } else {
          return outputMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.getDefaultInstance() : outputMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.OutputMetricDistributions output_metrics = 16;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributionsOrBuilder> 
          getOutputMetricsFieldBuilder() {
        if (outputMetricsBuilder_ == null) {
          outputMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributionsOrBuilder>(
                  getOutputMetrics(),
                  getParentForChildren(),
                  isClean());
          outputMetrics_ = null;
        }
        return outputMetricsBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions shuffleReadMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributionsOrBuilder> shuffleReadMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       * @return Whether the shuffleReadMetrics field is set.
       */
      public boolean hasShuffleReadMetrics() {
        return shuffleReadMetricsBuilder_ != null || shuffleReadMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       * @return The shuffleReadMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions getShuffleReadMetrics() {
        if (shuffleReadMetricsBuilder_ == null) {
          return shuffleReadMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.getDefaultInstance() : shuffleReadMetrics_;
        } else {
          return shuffleReadMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       */
      public Builder setShuffleReadMetrics(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions value) {
        if (shuffleReadMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          shuffleReadMetrics_ = value;
          onChanged();
        } else {
          shuffleReadMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       */
      public Builder setShuffleReadMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.Builder builderForValue) {
        if (shuffleReadMetricsBuilder_ == null) {
          shuffleReadMetrics_ = builderForValue.build();
          onChanged();
        } else {
          shuffleReadMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       */
      public Builder mergeShuffleReadMetrics(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions value) {
        if (shuffleReadMetricsBuilder_ == null) {
          if (shuffleReadMetrics_ != null) {
            shuffleReadMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.newBuilder(shuffleReadMetrics_).mergeFrom(value).buildPartial();
          } else {
            shuffleReadMetrics_ = value;
          }
          onChanged();
        } else {
          shuffleReadMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       */
      public Builder clearShuffleReadMetrics() {
        if (shuffleReadMetricsBuilder_ == null) {
          shuffleReadMetrics_ = null;
          onChanged();
        } else {
          shuffleReadMetrics_ = null;
          shuffleReadMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.Builder getShuffleReadMetricsBuilder() {
        
        onChanged();
        return getShuffleReadMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributionsOrBuilder getShuffleReadMetricsOrBuilder() {
        if (shuffleReadMetricsBuilder_ != null) {
          return shuffleReadMetricsBuilder_.getMessageOrBuilder();
        } else {
          return shuffleReadMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.getDefaultInstance() : shuffleReadMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleReadMetricDistributions shuffle_read_metrics = 17;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributionsOrBuilder> 
          getShuffleReadMetricsFieldBuilder() {
        if (shuffleReadMetricsBuilder_ == null) {
          shuffleReadMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributionsOrBuilder>(
                  getShuffleReadMetrics(),
                  getParentForChildren(),
                  isClean());
          shuffleReadMetrics_ = null;
        }
        return shuffleReadMetricsBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions shuffleWriteMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributionsOrBuilder> shuffleWriteMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       * @return Whether the shuffleWriteMetrics field is set.
       */
      public boolean hasShuffleWriteMetrics() {
        return shuffleWriteMetricsBuilder_ != null || shuffleWriteMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       * @return The shuffleWriteMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions getShuffleWriteMetrics() {
        if (shuffleWriteMetricsBuilder_ == null) {
          return shuffleWriteMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.getDefaultInstance() : shuffleWriteMetrics_;
        } else {
          return shuffleWriteMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       */
      public Builder setShuffleWriteMetrics(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions value) {
        if (shuffleWriteMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          shuffleWriteMetrics_ = value;
          onChanged();
        } else {
          shuffleWriteMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       */
      public Builder setShuffleWriteMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.Builder builderForValue) {
        if (shuffleWriteMetricsBuilder_ == null) {
          shuffleWriteMetrics_ = builderForValue.build();
          onChanged();
        } else {
          shuffleWriteMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       */
      public Builder mergeShuffleWriteMetrics(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions value) {
        if (shuffleWriteMetricsBuilder_ == null) {
          if (shuffleWriteMetrics_ != null) {
            shuffleWriteMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.newBuilder(shuffleWriteMetrics_).mergeFrom(value).buildPartial();
          } else {
            shuffleWriteMetrics_ = value;
          }
          onChanged();
        } else {
          shuffleWriteMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       */
      public Builder clearShuffleWriteMetrics() {
        if (shuffleWriteMetricsBuilder_ == null) {
          shuffleWriteMetrics_ = null;
          onChanged();
        } else {
          shuffleWriteMetrics_ = null;
          shuffleWriteMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.Builder getShuffleWriteMetricsBuilder() {
        
        onChanged();
        return getShuffleWriteMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributionsOrBuilder getShuffleWriteMetricsOrBuilder() {
        if (shuffleWriteMetricsBuilder_ != null) {
          return shuffleWriteMetricsBuilder_.getMessageOrBuilder();
        } else {
          return shuffleWriteMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.getDefaultInstance() : shuffleWriteMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions shuffle_write_metrics = 18;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributionsOrBuilder> 
          getShuffleWriteMetricsFieldBuilder() {
        if (shuffleWriteMetricsBuilder_ == null) {
          shuffleWriteMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributionsOrBuilder>(
                  getShuffleWriteMetrics(),
                  getParentForChildren(),
                  isClean());
          shuffleWriteMetrics_ = null;
        }
        return shuffleWriteMetricsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.TaskMetricDistributions)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.TaskMetricDistributions)
    private static final org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<TaskMetricDistributions>
        PARSER = new com.google.protobuf.AbstractParser<TaskMetricDistributions>() {
      @java.lang.Override
      public TaskMetricDistributions parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TaskMetricDistributions(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<TaskMetricDistributions> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<TaskMetricDistributions> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.TaskMetricDistributions getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface InputMetricDistributionsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.InputMetricDistributions)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated double bytes_read = 1;</code>
     * @return A list containing the bytesRead.
     */
    java.util.List<java.lang.Double> getBytesReadList();
    /**
     * <code>repeated double bytes_read = 1;</code>
     * @return The count of bytesRead.
     */
    int getBytesReadCount();
    /**
     * <code>repeated double bytes_read = 1;</code>
     * @param index The index of the element to return.
     * @return The bytesRead at the given index.
     */
    double getBytesRead(int index);

    /**
     * <code>repeated double records_read = 2;</code>
     * @return A list containing the recordsRead.
     */
    java.util.List<java.lang.Double> getRecordsReadList();
    /**
     * <code>repeated double records_read = 2;</code>
     * @return The count of recordsRead.
     */
    int getRecordsReadCount();
    /**
     * <code>repeated double records_read = 2;</code>
     * @param index The index of the element to return.
     * @return The recordsRead at the given index.
     */
    double getRecordsRead(int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.InputMetricDistributions}
   */
  public static final class InputMetricDistributions extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.InputMetricDistributions)
      InputMetricDistributionsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use InputMetricDistributions.newBuilder() to construct.
    private InputMetricDistributions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private InputMetricDistributions() {
      bytesRead_ = emptyDoubleList();
      recordsRead_ = emptyDoubleList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new InputMetricDistributions();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private InputMetricDistributions(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                bytesRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              bytesRead_.addDouble(input.readDouble());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                bytesRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                bytesRead_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 17: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                recordsRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              recordsRead_.addDouble(input.readDouble());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                recordsRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                recordsRead_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          bytesRead_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          recordsRead_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.Builder.class);
    }

    public static final int BYTES_READ_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.DoubleList bytesRead_;
    /**
     * <code>repeated double bytes_read = 1;</code>
     * @return A list containing the bytesRead.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getBytesReadList() {
      return bytesRead_;
    }
    /**
     * <code>repeated double bytes_read = 1;</code>
     * @return The count of bytesRead.
     */
    public int getBytesReadCount() {
      return bytesRead_.size();
    }
    /**
     * <code>repeated double bytes_read = 1;</code>
     * @param index The index of the element to return.
     * @return The bytesRead at the given index.
     */
    public double getBytesRead(int index) {
      return bytesRead_.getDouble(index);
    }
    private int bytesReadMemoizedSerializedSize = -1;

    public static final int RECORDS_READ_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.DoubleList recordsRead_;
    /**
     * <code>repeated double records_read = 2;</code>
     * @return A list containing the recordsRead.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRecordsReadList() {
      return recordsRead_;
    }
    /**
     * <code>repeated double records_read = 2;</code>
     * @return The count of recordsRead.
     */
    public int getRecordsReadCount() {
      return recordsRead_.size();
    }
    /**
     * <code>repeated double records_read = 2;</code>
     * @param index The index of the element to return.
     * @return The recordsRead at the given index.
     */
    public double getRecordsRead(int index) {
      return recordsRead_.getDouble(index);
    }
    private int recordsReadMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getBytesReadList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(bytesReadMemoizedSerializedSize);
      }
      for (int i = 0; i < bytesRead_.size(); i++) {
        output.writeDoubleNoTag(bytesRead_.getDouble(i));
      }
      if (getRecordsReadList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(recordsReadMemoizedSerializedSize);
      }
      for (int i = 0; i < recordsRead_.size(); i++) {
        output.writeDoubleNoTag(recordsRead_.getDouble(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        dataSize = 8 * getBytesReadList().size();
        size += dataSize;
        if (!getBytesReadList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        bytesReadMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRecordsReadList().size();
        size += dataSize;
        if (!getRecordsReadList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        recordsReadMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions other = (org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions) obj;

      if (!getBytesReadList()
          .equals(other.getBytesReadList())) return false;
      if (!getRecordsReadList()
          .equals(other.getRecordsReadList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getBytesReadCount() > 0) {
        hash = (37 * hash) + BYTES_READ_FIELD_NUMBER;
        hash = (53 * hash) + getBytesReadList().hashCode();
      }
      if (getRecordsReadCount() > 0) {
        hash = (37 * hash) + RECORDS_READ_FIELD_NUMBER;
        hash = (53 * hash) + getRecordsReadList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.InputMetricDistributions}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.InputMetricDistributions)
        org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bytesRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        recordsRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions build() {
        org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions result = new org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          bytesRead_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.bytesRead_ = bytesRead_;
        if (((bitField0_ & 0x00000002) != 0)) {
          recordsRead_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.recordsRead_ = recordsRead_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions.getDefaultInstance()) return this;
        if (!other.bytesRead_.isEmpty()) {
          if (bytesRead_.isEmpty()) {
            bytesRead_ = other.bytesRead_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureBytesReadIsMutable();
            bytesRead_.addAll(other.bytesRead_);
          }
          onChanged();
        }
        if (!other.recordsRead_.isEmpty()) {
          if (recordsRead_.isEmpty()) {
            recordsRead_ = other.recordsRead_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureRecordsReadIsMutable();
            recordsRead_.addAll(other.recordsRead_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.DoubleList bytesRead_ = emptyDoubleList();
      private void ensureBytesReadIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          bytesRead_ = mutableCopy(bytesRead_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated double bytes_read = 1;</code>
       * @return A list containing the bytesRead.
       */
      public java.util.List<java.lang.Double>
          getBytesReadList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(bytesRead_) : bytesRead_;
      }
      /**
       * <code>repeated double bytes_read = 1;</code>
       * @return The count of bytesRead.
       */
      public int getBytesReadCount() {
        return bytesRead_.size();
      }
      /**
       * <code>repeated double bytes_read = 1;</code>
       * @param index The index of the element to return.
       * @return The bytesRead at the given index.
       */
      public double getBytesRead(int index) {
        return bytesRead_.getDouble(index);
      }
      /**
       * <code>repeated double bytes_read = 1;</code>
       * @param index The index to set the value at.
       * @param value The bytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setBytesRead(
          int index, double value) {
        ensureBytesReadIsMutable();
        bytesRead_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double bytes_read = 1;</code>
       * @param value The bytesRead to add.
       * @return This builder for chaining.
       */
      public Builder addBytesRead(double value) {
        ensureBytesReadIsMutable();
        bytesRead_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double bytes_read = 1;</code>
       * @param values The bytesRead to add.
       * @return This builder for chaining.
       */
      public Builder addAllBytesRead(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureBytesReadIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, bytesRead_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double bytes_read = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBytesRead() {
        bytesRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList recordsRead_ = emptyDoubleList();
      private void ensureRecordsReadIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          recordsRead_ = mutableCopy(recordsRead_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated double records_read = 2;</code>
       * @return A list containing the recordsRead.
       */
      public java.util.List<java.lang.Double>
          getRecordsReadList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(recordsRead_) : recordsRead_;
      }
      /**
       * <code>repeated double records_read = 2;</code>
       * @return The count of recordsRead.
       */
      public int getRecordsReadCount() {
        return recordsRead_.size();
      }
      /**
       * <code>repeated double records_read = 2;</code>
       * @param index The index of the element to return.
       * @return The recordsRead at the given index.
       */
      public double getRecordsRead(int index) {
        return recordsRead_.getDouble(index);
      }
      /**
       * <code>repeated double records_read = 2;</code>
       * @param index The index to set the value at.
       * @param value The recordsRead to set.
       * @return This builder for chaining.
       */
      public Builder setRecordsRead(
          int index, double value) {
        ensureRecordsReadIsMutable();
        recordsRead_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double records_read = 2;</code>
       * @param value The recordsRead to add.
       * @return This builder for chaining.
       */
      public Builder addRecordsRead(double value) {
        ensureRecordsReadIsMutable();
        recordsRead_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double records_read = 2;</code>
       * @param values The recordsRead to add.
       * @return This builder for chaining.
       */
      public Builder addAllRecordsRead(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRecordsReadIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, recordsRead_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double records_read = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearRecordsRead() {
        recordsRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.InputMetricDistributions)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.InputMetricDistributions)
    private static final org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<InputMetricDistributions>
        PARSER = new com.google.protobuf.AbstractParser<InputMetricDistributions>() {
      @java.lang.Override
      public InputMetricDistributions parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new InputMetricDistributions(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<InputMetricDistributions> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<InputMetricDistributions> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.InputMetricDistributions getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface OutputMetricDistributionsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.OutputMetricDistributions)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated double bytes_written = 1;</code>
     * @return A list containing the bytesWritten.
     */
    java.util.List<java.lang.Double> getBytesWrittenList();
    /**
     * <code>repeated double bytes_written = 1;</code>
     * @return The count of bytesWritten.
     */
    int getBytesWrittenCount();
    /**
     * <code>repeated double bytes_written = 1;</code>
     * @param index The index of the element to return.
     * @return The bytesWritten at the given index.
     */
    double getBytesWritten(int index);

    /**
     * <code>repeated double records_written = 2;</code>
     * @return A list containing the recordsWritten.
     */
    java.util.List<java.lang.Double> getRecordsWrittenList();
    /**
     * <code>repeated double records_written = 2;</code>
     * @return The count of recordsWritten.
     */
    int getRecordsWrittenCount();
    /**
     * <code>repeated double records_written = 2;</code>
     * @param index The index of the element to return.
     * @return The recordsWritten at the given index.
     */
    double getRecordsWritten(int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.OutputMetricDistributions}
   */
  public static final class OutputMetricDistributions extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.OutputMetricDistributions)
      OutputMetricDistributionsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use OutputMetricDistributions.newBuilder() to construct.
    private OutputMetricDistributions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private OutputMetricDistributions() {
      bytesWritten_ = emptyDoubleList();
      recordsWritten_ = emptyDoubleList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new OutputMetricDistributions();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private OutputMetricDistributions(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                bytesWritten_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              bytesWritten_.addDouble(input.readDouble());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                bytesWritten_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                bytesWritten_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 17: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                recordsWritten_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              recordsWritten_.addDouble(input.readDouble());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                recordsWritten_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                recordsWritten_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          bytesWritten_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          recordsWritten_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.Builder.class);
    }

    public static final int BYTES_WRITTEN_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.DoubleList bytesWritten_;
    /**
     * <code>repeated double bytes_written = 1;</code>
     * @return A list containing the bytesWritten.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getBytesWrittenList() {
      return bytesWritten_;
    }
    /**
     * <code>repeated double bytes_written = 1;</code>
     * @return The count of bytesWritten.
     */
    public int getBytesWrittenCount() {
      return bytesWritten_.size();
    }
    /**
     * <code>repeated double bytes_written = 1;</code>
     * @param index The index of the element to return.
     * @return The bytesWritten at the given index.
     */
    public double getBytesWritten(int index) {
      return bytesWritten_.getDouble(index);
    }
    private int bytesWrittenMemoizedSerializedSize = -1;

    public static final int RECORDS_WRITTEN_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.DoubleList recordsWritten_;
    /**
     * <code>repeated double records_written = 2;</code>
     * @return A list containing the recordsWritten.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRecordsWrittenList() {
      return recordsWritten_;
    }
    /**
     * <code>repeated double records_written = 2;</code>
     * @return The count of recordsWritten.
     */
    public int getRecordsWrittenCount() {
      return recordsWritten_.size();
    }
    /**
     * <code>repeated double records_written = 2;</code>
     * @param index The index of the element to return.
     * @return The recordsWritten at the given index.
     */
    public double getRecordsWritten(int index) {
      return recordsWritten_.getDouble(index);
    }
    private int recordsWrittenMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getBytesWrittenList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(bytesWrittenMemoizedSerializedSize);
      }
      for (int i = 0; i < bytesWritten_.size(); i++) {
        output.writeDoubleNoTag(bytesWritten_.getDouble(i));
      }
      if (getRecordsWrittenList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(recordsWrittenMemoizedSerializedSize);
      }
      for (int i = 0; i < recordsWritten_.size(); i++) {
        output.writeDoubleNoTag(recordsWritten_.getDouble(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        dataSize = 8 * getBytesWrittenList().size();
        size += dataSize;
        if (!getBytesWrittenList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        bytesWrittenMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRecordsWrittenList().size();
        size += dataSize;
        if (!getRecordsWrittenList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        recordsWrittenMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions other = (org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions) obj;

      if (!getBytesWrittenList()
          .equals(other.getBytesWrittenList())) return false;
      if (!getRecordsWrittenList()
          .equals(other.getRecordsWrittenList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getBytesWrittenCount() > 0) {
        hash = (37 * hash) + BYTES_WRITTEN_FIELD_NUMBER;
        hash = (53 * hash) + getBytesWrittenList().hashCode();
      }
      if (getRecordsWrittenCount() > 0) {
        hash = (37 * hash) + RECORDS_WRITTEN_FIELD_NUMBER;
        hash = (53 * hash) + getRecordsWrittenList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.OutputMetricDistributions}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.OutputMetricDistributions)
        org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bytesWritten_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        recordsWritten_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions build() {
        org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions result = new org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          bytesWritten_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.bytesWritten_ = bytesWritten_;
        if (((bitField0_ & 0x00000002) != 0)) {
          recordsWritten_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.recordsWritten_ = recordsWritten_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions.getDefaultInstance()) return this;
        if (!other.bytesWritten_.isEmpty()) {
          if (bytesWritten_.isEmpty()) {
            bytesWritten_ = other.bytesWritten_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureBytesWrittenIsMutable();
            bytesWritten_.addAll(other.bytesWritten_);
          }
          onChanged();
        }
        if (!other.recordsWritten_.isEmpty()) {
          if (recordsWritten_.isEmpty()) {
            recordsWritten_ = other.recordsWritten_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureRecordsWrittenIsMutable();
            recordsWritten_.addAll(other.recordsWritten_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.DoubleList bytesWritten_ = emptyDoubleList();
      private void ensureBytesWrittenIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          bytesWritten_ = mutableCopy(bytesWritten_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated double bytes_written = 1;</code>
       * @return A list containing the bytesWritten.
       */
      public java.util.List<java.lang.Double>
          getBytesWrittenList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(bytesWritten_) : bytesWritten_;
      }
      /**
       * <code>repeated double bytes_written = 1;</code>
       * @return The count of bytesWritten.
       */
      public int getBytesWrittenCount() {
        return bytesWritten_.size();
      }
      /**
       * <code>repeated double bytes_written = 1;</code>
       * @param index The index of the element to return.
       * @return The bytesWritten at the given index.
       */
      public double getBytesWritten(int index) {
        return bytesWritten_.getDouble(index);
      }
      /**
       * <code>repeated double bytes_written = 1;</code>
       * @param index The index to set the value at.
       * @param value The bytesWritten to set.
       * @return This builder for chaining.
       */
      public Builder setBytesWritten(
          int index, double value) {
        ensureBytesWrittenIsMutable();
        bytesWritten_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double bytes_written = 1;</code>
       * @param value The bytesWritten to add.
       * @return This builder for chaining.
       */
      public Builder addBytesWritten(double value) {
        ensureBytesWrittenIsMutable();
        bytesWritten_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double bytes_written = 1;</code>
       * @param values The bytesWritten to add.
       * @return This builder for chaining.
       */
      public Builder addAllBytesWritten(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureBytesWrittenIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, bytesWritten_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double bytes_written = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBytesWritten() {
        bytesWritten_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList recordsWritten_ = emptyDoubleList();
      private void ensureRecordsWrittenIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          recordsWritten_ = mutableCopy(recordsWritten_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated double records_written = 2;</code>
       * @return A list containing the recordsWritten.
       */
      public java.util.List<java.lang.Double>
          getRecordsWrittenList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(recordsWritten_) : recordsWritten_;
      }
      /**
       * <code>repeated double records_written = 2;</code>
       * @return The count of recordsWritten.
       */
      public int getRecordsWrittenCount() {
        return recordsWritten_.size();
      }
      /**
       * <code>repeated double records_written = 2;</code>
       * @param index The index of the element to return.
       * @return The recordsWritten at the given index.
       */
      public double getRecordsWritten(int index) {
        return recordsWritten_.getDouble(index);
      }
      /**
       * <code>repeated double records_written = 2;</code>
       * @param index The index to set the value at.
       * @param value The recordsWritten to set.
       * @return This builder for chaining.
       */
      public Builder setRecordsWritten(
          int index, double value) {
        ensureRecordsWrittenIsMutable();
        recordsWritten_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double records_written = 2;</code>
       * @param value The recordsWritten to add.
       * @return This builder for chaining.
       */
      public Builder addRecordsWritten(double value) {
        ensureRecordsWrittenIsMutable();
        recordsWritten_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double records_written = 2;</code>
       * @param values The recordsWritten to add.
       * @return This builder for chaining.
       */
      public Builder addAllRecordsWritten(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRecordsWrittenIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, recordsWritten_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double records_written = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearRecordsWritten() {
        recordsWritten_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.OutputMetricDistributions)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.OutputMetricDistributions)
    private static final org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<OutputMetricDistributions>
        PARSER = new com.google.protobuf.AbstractParser<OutputMetricDistributions>() {
      @java.lang.Override
      public OutputMetricDistributions parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new OutputMetricDistributions(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<OutputMetricDistributions> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<OutputMetricDistributions> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.OutputMetricDistributions getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ShuffleReadMetricDistributionsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ShuffleReadMetricDistributions)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated double read_bytes = 1;</code>
     * @return A list containing the readBytes.
     */
    java.util.List<java.lang.Double> getReadBytesList();
    /**
     * <code>repeated double read_bytes = 1;</code>
     * @return The count of readBytes.
     */
    int getReadBytesCount();
    /**
     * <code>repeated double read_bytes = 1;</code>
     * @param index The index of the element to return.
     * @return The readBytes at the given index.
     */
    double getReadBytes(int index);

    /**
     * <code>repeated double read_records = 2;</code>
     * @return A list containing the readRecords.
     */
    java.util.List<java.lang.Double> getReadRecordsList();
    /**
     * <code>repeated double read_records = 2;</code>
     * @return The count of readRecords.
     */
    int getReadRecordsCount();
    /**
     * <code>repeated double read_records = 2;</code>
     * @param index The index of the element to return.
     * @return The readRecords at the given index.
     */
    double getReadRecords(int index);

    /**
     * <code>repeated double remote_blocks_fetched = 3;</code>
     * @return A list containing the remoteBlocksFetched.
     */
    java.util.List<java.lang.Double> getRemoteBlocksFetchedList();
    /**
     * <code>repeated double remote_blocks_fetched = 3;</code>
     * @return The count of remoteBlocksFetched.
     */
    int getRemoteBlocksFetchedCount();
    /**
     * <code>repeated double remote_blocks_fetched = 3;</code>
     * @param index The index of the element to return.
     * @return The remoteBlocksFetched at the given index.
     */
    double getRemoteBlocksFetched(int index);

    /**
     * <code>repeated double local_blocks_fetched = 4;</code>
     * @return A list containing the localBlocksFetched.
     */
    java.util.List<java.lang.Double> getLocalBlocksFetchedList();
    /**
     * <code>repeated double local_blocks_fetched = 4;</code>
     * @return The count of localBlocksFetched.
     */
    int getLocalBlocksFetchedCount();
    /**
     * <code>repeated double local_blocks_fetched = 4;</code>
     * @param index The index of the element to return.
     * @return The localBlocksFetched at the given index.
     */
    double getLocalBlocksFetched(int index);

    /**
     * <code>repeated double fetch_wait_time = 5;</code>
     * @return A list containing the fetchWaitTime.
     */
    java.util.List<java.lang.Double> getFetchWaitTimeList();
    /**
     * <code>repeated double fetch_wait_time = 5;</code>
     * @return The count of fetchWaitTime.
     */
    int getFetchWaitTimeCount();
    /**
     * <code>repeated double fetch_wait_time = 5;</code>
     * @param index The index of the element to return.
     * @return The fetchWaitTime at the given index.
     */
    double getFetchWaitTime(int index);

    /**
     * <code>repeated double remote_bytes_read = 6;</code>
     * @return A list containing the remoteBytesRead.
     */
    java.util.List<java.lang.Double> getRemoteBytesReadList();
    /**
     * <code>repeated double remote_bytes_read = 6;</code>
     * @return The count of remoteBytesRead.
     */
    int getRemoteBytesReadCount();
    /**
     * <code>repeated double remote_bytes_read = 6;</code>
     * @param index The index of the element to return.
     * @return The remoteBytesRead at the given index.
     */
    double getRemoteBytesRead(int index);

    /**
     * <code>repeated double remote_bytes_read_to_disk = 7;</code>
     * @return A list containing the remoteBytesReadToDisk.
     */
    java.util.List<java.lang.Double> getRemoteBytesReadToDiskList();
    /**
     * <code>repeated double remote_bytes_read_to_disk = 7;</code>
     * @return The count of remoteBytesReadToDisk.
     */
    int getRemoteBytesReadToDiskCount();
    /**
     * <code>repeated double remote_bytes_read_to_disk = 7;</code>
     * @param index The index of the element to return.
     * @return The remoteBytesReadToDisk at the given index.
     */
    double getRemoteBytesReadToDisk(int index);

    /**
     * <code>repeated double total_blocks_fetched = 8;</code>
     * @return A list containing the totalBlocksFetched.
     */
    java.util.List<java.lang.Double> getTotalBlocksFetchedList();
    /**
     * <code>repeated double total_blocks_fetched = 8;</code>
     * @return The count of totalBlocksFetched.
     */
    int getTotalBlocksFetchedCount();
    /**
     * <code>repeated double total_blocks_fetched = 8;</code>
     * @param index The index of the element to return.
     * @return The totalBlocksFetched at the given index.
     */
    double getTotalBlocksFetched(int index);

    /**
     * <code>repeated double remote_reqs_duration = 9;</code>
     * @return A list containing the remoteReqsDuration.
     */
    java.util.List<java.lang.Double> getRemoteReqsDurationList();
    /**
     * <code>repeated double remote_reqs_duration = 9;</code>
     * @return The count of remoteReqsDuration.
     */
    int getRemoteReqsDurationCount();
    /**
     * <code>repeated double remote_reqs_duration = 9;</code>
     * @param index The index of the element to return.
     * @return The remoteReqsDuration at the given index.
     */
    double getRemoteReqsDuration(int index);

    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
     * @return Whether the shufflePushReadMetricsDist field is set.
     */
    boolean hasShufflePushReadMetricsDist();
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
     * @return The shufflePushReadMetricsDist.
     */
    org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions getShufflePushReadMetricsDist();
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributionsOrBuilder getShufflePushReadMetricsDistOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ShuffleReadMetricDistributions}
   */
  public static final class ShuffleReadMetricDistributions extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ShuffleReadMetricDistributions)
      ShuffleReadMetricDistributionsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ShuffleReadMetricDistributions.newBuilder() to construct.
    private ShuffleReadMetricDistributions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ShuffleReadMetricDistributions() {
      readBytes_ = emptyDoubleList();
      readRecords_ = emptyDoubleList();
      remoteBlocksFetched_ = emptyDoubleList();
      localBlocksFetched_ = emptyDoubleList();
      fetchWaitTime_ = emptyDoubleList();
      remoteBytesRead_ = emptyDoubleList();
      remoteBytesReadToDisk_ = emptyDoubleList();
      totalBlocksFetched_ = emptyDoubleList();
      remoteReqsDuration_ = emptyDoubleList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ShuffleReadMetricDistributions();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ShuffleReadMetricDistributions(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                readBytes_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              readBytes_.addDouble(input.readDouble());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                readBytes_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                readBytes_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 17: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                readRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              readRecords_.addDouble(input.readDouble());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                readRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                readRecords_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 25: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                remoteBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              remoteBlocksFetched_.addDouble(input.readDouble());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000004) != 0) && input.getBytesUntilLimit() > 0) {
                remoteBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              while (input.getBytesUntilLimit() > 0) {
                remoteBlocksFetched_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 33: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                localBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000008;
              }
              localBlocksFetched_.addDouble(input.readDouble());
              break;
            }
            case 34: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000008) != 0) && input.getBytesUntilLimit() > 0) {
                localBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000008;
              }
              while (input.getBytesUntilLimit() > 0) {
                localBlocksFetched_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 41: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                fetchWaitTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000010;
              }
              fetchWaitTime_.addDouble(input.readDouble());
              break;
            }
            case 42: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000010) != 0) && input.getBytesUntilLimit() > 0) {
                fetchWaitTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000010;
              }
              while (input.getBytesUntilLimit() > 0) {
                fetchWaitTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 49: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                remoteBytesRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000020;
              }
              remoteBytesRead_.addDouble(input.readDouble());
              break;
            }
            case 50: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000020) != 0) && input.getBytesUntilLimit() > 0) {
                remoteBytesRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000020;
              }
              while (input.getBytesUntilLimit() > 0) {
                remoteBytesRead_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 57: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                remoteBytesReadToDisk_ = newDoubleList();
                mutable_bitField0_ |= 0x00000040;
              }
              remoteBytesReadToDisk_.addDouble(input.readDouble());
              break;
            }
            case 58: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000040) != 0) && input.getBytesUntilLimit() > 0) {
                remoteBytesReadToDisk_ = newDoubleList();
                mutable_bitField0_ |= 0x00000040;
              }
              while (input.getBytesUntilLimit() > 0) {
                remoteBytesReadToDisk_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 65: {
              if (!((mutable_bitField0_ & 0x00000080) != 0)) {
                totalBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000080;
              }
              totalBlocksFetched_.addDouble(input.readDouble());
              break;
            }
            case 66: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000080) != 0) && input.getBytesUntilLimit() > 0) {
                totalBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000080;
              }
              while (input.getBytesUntilLimit() > 0) {
                totalBlocksFetched_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 73: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                remoteReqsDuration_ = newDoubleList();
                mutable_bitField0_ |= 0x00000100;
              }
              remoteReqsDuration_.addDouble(input.readDouble());
              break;
            }
            case 74: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000100) != 0) && input.getBytesUntilLimit() > 0) {
                remoteReqsDuration_ = newDoubleList();
                mutable_bitField0_ |= 0x00000100;
              }
              while (input.getBytesUntilLimit() > 0) {
                remoteReqsDuration_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 82: {
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.Builder subBuilder = null;
              if (shufflePushReadMetricsDist_ != null) {
                subBuilder = shufflePushReadMetricsDist_.toBuilder();
              }
              shufflePushReadMetricsDist_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(shufflePushReadMetricsDist_);
                shufflePushReadMetricsDist_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          readBytes_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          readRecords_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          remoteBlocksFetched_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          localBlocksFetched_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          fetchWaitTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          remoteBytesRead_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          remoteBytesReadToDisk_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000080) != 0)) {
          totalBlocksFetched_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          remoteReqsDuration_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.Builder.class);
    }

    public static final int READ_BYTES_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.DoubleList readBytes_;
    /**
     * <code>repeated double read_bytes = 1;</code>
     * @return A list containing the readBytes.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getReadBytesList() {
      return readBytes_;
    }
    /**
     * <code>repeated double read_bytes = 1;</code>
     * @return The count of readBytes.
     */
    public int getReadBytesCount() {
      return readBytes_.size();
    }
    /**
     * <code>repeated double read_bytes = 1;</code>
     * @param index The index of the element to return.
     * @return The readBytes at the given index.
     */
    public double getReadBytes(int index) {
      return readBytes_.getDouble(index);
    }
    private int readBytesMemoizedSerializedSize = -1;

    public static final int READ_RECORDS_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.DoubleList readRecords_;
    /**
     * <code>repeated double read_records = 2;</code>
     * @return A list containing the readRecords.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getReadRecordsList() {
      return readRecords_;
    }
    /**
     * <code>repeated double read_records = 2;</code>
     * @return The count of readRecords.
     */
    public int getReadRecordsCount() {
      return readRecords_.size();
    }
    /**
     * <code>repeated double read_records = 2;</code>
     * @param index The index of the element to return.
     * @return The readRecords at the given index.
     */
    public double getReadRecords(int index) {
      return readRecords_.getDouble(index);
    }
    private int readRecordsMemoizedSerializedSize = -1;

    public static final int REMOTE_BLOCKS_FETCHED_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.DoubleList remoteBlocksFetched_;
    /**
     * <code>repeated double remote_blocks_fetched = 3;</code>
     * @return A list containing the remoteBlocksFetched.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRemoteBlocksFetchedList() {
      return remoteBlocksFetched_;
    }
    /**
     * <code>repeated double remote_blocks_fetched = 3;</code>
     * @return The count of remoteBlocksFetched.
     */
    public int getRemoteBlocksFetchedCount() {
      return remoteBlocksFetched_.size();
    }
    /**
     * <code>repeated double remote_blocks_fetched = 3;</code>
     * @param index The index of the element to return.
     * @return The remoteBlocksFetched at the given index.
     */
    public double getRemoteBlocksFetched(int index) {
      return remoteBlocksFetched_.getDouble(index);
    }
    private int remoteBlocksFetchedMemoizedSerializedSize = -1;

    public static final int LOCAL_BLOCKS_FETCHED_FIELD_NUMBER = 4;
    private com.google.protobuf.Internal.DoubleList localBlocksFetched_;
    /**
     * <code>repeated double local_blocks_fetched = 4;</code>
     * @return A list containing the localBlocksFetched.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getLocalBlocksFetchedList() {
      return localBlocksFetched_;
    }
    /**
     * <code>repeated double local_blocks_fetched = 4;</code>
     * @return The count of localBlocksFetched.
     */
    public int getLocalBlocksFetchedCount() {
      return localBlocksFetched_.size();
    }
    /**
     * <code>repeated double local_blocks_fetched = 4;</code>
     * @param index The index of the element to return.
     * @return The localBlocksFetched at the given index.
     */
    public double getLocalBlocksFetched(int index) {
      return localBlocksFetched_.getDouble(index);
    }
    private int localBlocksFetchedMemoizedSerializedSize = -1;

    public static final int FETCH_WAIT_TIME_FIELD_NUMBER = 5;
    private com.google.protobuf.Internal.DoubleList fetchWaitTime_;
    /**
     * <code>repeated double fetch_wait_time = 5;</code>
     * @return A list containing the fetchWaitTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getFetchWaitTimeList() {
      return fetchWaitTime_;
    }
    /**
     * <code>repeated double fetch_wait_time = 5;</code>
     * @return The count of fetchWaitTime.
     */
    public int getFetchWaitTimeCount() {
      return fetchWaitTime_.size();
    }
    /**
     * <code>repeated double fetch_wait_time = 5;</code>
     * @param index The index of the element to return.
     * @return The fetchWaitTime at the given index.
     */
    public double getFetchWaitTime(int index) {
      return fetchWaitTime_.getDouble(index);
    }
    private int fetchWaitTimeMemoizedSerializedSize = -1;

    public static final int REMOTE_BYTES_READ_FIELD_NUMBER = 6;
    private com.google.protobuf.Internal.DoubleList remoteBytesRead_;
    /**
     * <code>repeated double remote_bytes_read = 6;</code>
     * @return A list containing the remoteBytesRead.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRemoteBytesReadList() {
      return remoteBytesRead_;
    }
    /**
     * <code>repeated double remote_bytes_read = 6;</code>
     * @return The count of remoteBytesRead.
     */
    public int getRemoteBytesReadCount() {
      return remoteBytesRead_.size();
    }
    /**
     * <code>repeated double remote_bytes_read = 6;</code>
     * @param index The index of the element to return.
     * @return The remoteBytesRead at the given index.
     */
    public double getRemoteBytesRead(int index) {
      return remoteBytesRead_.getDouble(index);
    }
    private int remoteBytesReadMemoizedSerializedSize = -1;

    public static final int REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER = 7;
    private com.google.protobuf.Internal.DoubleList remoteBytesReadToDisk_;
    /**
     * <code>repeated double remote_bytes_read_to_disk = 7;</code>
     * @return A list containing the remoteBytesReadToDisk.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRemoteBytesReadToDiskList() {
      return remoteBytesReadToDisk_;
    }
    /**
     * <code>repeated double remote_bytes_read_to_disk = 7;</code>
     * @return The count of remoteBytesReadToDisk.
     */
    public int getRemoteBytesReadToDiskCount() {
      return remoteBytesReadToDisk_.size();
    }
    /**
     * <code>repeated double remote_bytes_read_to_disk = 7;</code>
     * @param index The index of the element to return.
     * @return The remoteBytesReadToDisk at the given index.
     */
    public double getRemoteBytesReadToDisk(int index) {
      return remoteBytesReadToDisk_.getDouble(index);
    }
    private int remoteBytesReadToDiskMemoizedSerializedSize = -1;

    public static final int TOTAL_BLOCKS_FETCHED_FIELD_NUMBER = 8;
    private com.google.protobuf.Internal.DoubleList totalBlocksFetched_;
    /**
     * <code>repeated double total_blocks_fetched = 8;</code>
     * @return A list containing the totalBlocksFetched.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getTotalBlocksFetchedList() {
      return totalBlocksFetched_;
    }
    /**
     * <code>repeated double total_blocks_fetched = 8;</code>
     * @return The count of totalBlocksFetched.
     */
    public int getTotalBlocksFetchedCount() {
      return totalBlocksFetched_.size();
    }
    /**
     * <code>repeated double total_blocks_fetched = 8;</code>
     * @param index The index of the element to return.
     * @return The totalBlocksFetched at the given index.
     */
    public double getTotalBlocksFetched(int index) {
      return totalBlocksFetched_.getDouble(index);
    }
    private int totalBlocksFetchedMemoizedSerializedSize = -1;

    public static final int REMOTE_REQS_DURATION_FIELD_NUMBER = 9;
    private com.google.protobuf.Internal.DoubleList remoteReqsDuration_;
    /**
     * <code>repeated double remote_reqs_duration = 9;</code>
     * @return A list containing the remoteReqsDuration.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRemoteReqsDurationList() {
      return remoteReqsDuration_;
    }
    /**
     * <code>repeated double remote_reqs_duration = 9;</code>
     * @return The count of remoteReqsDuration.
     */
    public int getRemoteReqsDurationCount() {
      return remoteReqsDuration_.size();
    }
    /**
     * <code>repeated double remote_reqs_duration = 9;</code>
     * @param index The index of the element to return.
     * @return The remoteReqsDuration at the given index.
     */
    public double getRemoteReqsDuration(int index) {
      return remoteReqsDuration_.getDouble(index);
    }
    private int remoteReqsDurationMemoizedSerializedSize = -1;

    public static final int SHUFFLE_PUSH_READ_METRICS_DIST_FIELD_NUMBER = 10;
    private org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions shufflePushReadMetricsDist_;
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
     * @return Whether the shufflePushReadMetricsDist field is set.
     */
    @java.lang.Override
    public boolean hasShufflePushReadMetricsDist() {
      return shufflePushReadMetricsDist_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
     * @return The shufflePushReadMetricsDist.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions getShufflePushReadMetricsDist() {
      return shufflePushReadMetricsDist_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.getDefaultInstance() : shufflePushReadMetricsDist_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributionsOrBuilder getShufflePushReadMetricsDistOrBuilder() {
      return getShufflePushReadMetricsDist();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getReadBytesList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(readBytesMemoizedSerializedSize);
      }
      for (int i = 0; i < readBytes_.size(); i++) {
        output.writeDoubleNoTag(readBytes_.getDouble(i));
      }
      if (getReadRecordsList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(readRecordsMemoizedSerializedSize);
      }
      for (int i = 0; i < readRecords_.size(); i++) {
        output.writeDoubleNoTag(readRecords_.getDouble(i));
      }
      if (getRemoteBlocksFetchedList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(remoteBlocksFetchedMemoizedSerializedSize);
      }
      for (int i = 0; i < remoteBlocksFetched_.size(); i++) {
        output.writeDoubleNoTag(remoteBlocksFetched_.getDouble(i));
      }
      if (getLocalBlocksFetchedList().size() > 0) {
        output.writeUInt32NoTag(34);
        output.writeUInt32NoTag(localBlocksFetchedMemoizedSerializedSize);
      }
      for (int i = 0; i < localBlocksFetched_.size(); i++) {
        output.writeDoubleNoTag(localBlocksFetched_.getDouble(i));
      }
      if (getFetchWaitTimeList().size() > 0) {
        output.writeUInt32NoTag(42);
        output.writeUInt32NoTag(fetchWaitTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < fetchWaitTime_.size(); i++) {
        output.writeDoubleNoTag(fetchWaitTime_.getDouble(i));
      }
      if (getRemoteBytesReadList().size() > 0) {
        output.writeUInt32NoTag(50);
        output.writeUInt32NoTag(remoteBytesReadMemoizedSerializedSize);
      }
      for (int i = 0; i < remoteBytesRead_.size(); i++) {
        output.writeDoubleNoTag(remoteBytesRead_.getDouble(i));
      }
      if (getRemoteBytesReadToDiskList().size() > 0) {
        output.writeUInt32NoTag(58);
        output.writeUInt32NoTag(remoteBytesReadToDiskMemoizedSerializedSize);
      }
      for (int i = 0; i < remoteBytesReadToDisk_.size(); i++) {
        output.writeDoubleNoTag(remoteBytesReadToDisk_.getDouble(i));
      }
      if (getTotalBlocksFetchedList().size() > 0) {
        output.writeUInt32NoTag(66);
        output.writeUInt32NoTag(totalBlocksFetchedMemoizedSerializedSize);
      }
      for (int i = 0; i < totalBlocksFetched_.size(); i++) {
        output.writeDoubleNoTag(totalBlocksFetched_.getDouble(i));
      }
      if (getRemoteReqsDurationList().size() > 0) {
        output.writeUInt32NoTag(74);
        output.writeUInt32NoTag(remoteReqsDurationMemoizedSerializedSize);
      }
      for (int i = 0; i < remoteReqsDuration_.size(); i++) {
        output.writeDoubleNoTag(remoteReqsDuration_.getDouble(i));
      }
      if (shufflePushReadMetricsDist_ != null) {
        output.writeMessage(10, getShufflePushReadMetricsDist());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        dataSize = 8 * getReadBytesList().size();
        size += dataSize;
        if (!getReadBytesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        readBytesMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getReadRecordsList().size();
        size += dataSize;
        if (!getReadRecordsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        readRecordsMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRemoteBlocksFetchedList().size();
        size += dataSize;
        if (!getRemoteBlocksFetchedList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        remoteBlocksFetchedMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getLocalBlocksFetchedList().size();
        size += dataSize;
        if (!getLocalBlocksFetchedList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        localBlocksFetchedMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getFetchWaitTimeList().size();
        size += dataSize;
        if (!getFetchWaitTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        fetchWaitTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRemoteBytesReadList().size();
        size += dataSize;
        if (!getRemoteBytesReadList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        remoteBytesReadMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRemoteBytesReadToDiskList().size();
        size += dataSize;
        if (!getRemoteBytesReadToDiskList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        remoteBytesReadToDiskMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getTotalBlocksFetchedList().size();
        size += dataSize;
        if (!getTotalBlocksFetchedList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        totalBlocksFetchedMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRemoteReqsDurationList().size();
        size += dataSize;
        if (!getRemoteReqsDurationList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        remoteReqsDurationMemoizedSerializedSize = dataSize;
      }
      if (shufflePushReadMetricsDist_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, getShufflePushReadMetricsDist());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions other = (org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions) obj;

      if (!getReadBytesList()
          .equals(other.getReadBytesList())) return false;
      if (!getReadRecordsList()
          .equals(other.getReadRecordsList())) return false;
      if (!getRemoteBlocksFetchedList()
          .equals(other.getRemoteBlocksFetchedList())) return false;
      if (!getLocalBlocksFetchedList()
          .equals(other.getLocalBlocksFetchedList())) return false;
      if (!getFetchWaitTimeList()
          .equals(other.getFetchWaitTimeList())) return false;
      if (!getRemoteBytesReadList()
          .equals(other.getRemoteBytesReadList())) return false;
      if (!getRemoteBytesReadToDiskList()
          .equals(other.getRemoteBytesReadToDiskList())) return false;
      if (!getTotalBlocksFetchedList()
          .equals(other.getTotalBlocksFetchedList())) return false;
      if (!getRemoteReqsDurationList()
          .equals(other.getRemoteReqsDurationList())) return false;
      if (hasShufflePushReadMetricsDist() != other.hasShufflePushReadMetricsDist()) return false;
      if (hasShufflePushReadMetricsDist()) {
        if (!getShufflePushReadMetricsDist()
            .equals(other.getShufflePushReadMetricsDist())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getReadBytesCount() > 0) {
        hash = (37 * hash) + READ_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + getReadBytesList().hashCode();
      }
      if (getReadRecordsCount() > 0) {
        hash = (37 * hash) + READ_RECORDS_FIELD_NUMBER;
        hash = (53 * hash) + getReadRecordsList().hashCode();
      }
      if (getRemoteBlocksFetchedCount() > 0) {
        hash = (37 * hash) + REMOTE_BLOCKS_FETCHED_FIELD_NUMBER;
        hash = (53 * hash) + getRemoteBlocksFetchedList().hashCode();
      }
      if (getLocalBlocksFetchedCount() > 0) {
        hash = (37 * hash) + LOCAL_BLOCKS_FETCHED_FIELD_NUMBER;
        hash = (53 * hash) + getLocalBlocksFetchedList().hashCode();
      }
      if (getFetchWaitTimeCount() > 0) {
        hash = (37 * hash) + FETCH_WAIT_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getFetchWaitTimeList().hashCode();
      }
      if (getRemoteBytesReadCount() > 0) {
        hash = (37 * hash) + REMOTE_BYTES_READ_FIELD_NUMBER;
        hash = (53 * hash) + getRemoteBytesReadList().hashCode();
      }
      if (getRemoteBytesReadToDiskCount() > 0) {
        hash = (37 * hash) + REMOTE_BYTES_READ_TO_DISK_FIELD_NUMBER;
        hash = (53 * hash) + getRemoteBytesReadToDiskList().hashCode();
      }
      if (getTotalBlocksFetchedCount() > 0) {
        hash = (37 * hash) + TOTAL_BLOCKS_FETCHED_FIELD_NUMBER;
        hash = (53 * hash) + getTotalBlocksFetchedList().hashCode();
      }
      if (getRemoteReqsDurationCount() > 0) {
        hash = (37 * hash) + REMOTE_REQS_DURATION_FIELD_NUMBER;
        hash = (53 * hash) + getRemoteReqsDurationList().hashCode();
      }
      if (hasShufflePushReadMetricsDist()) {
        hash = (37 * hash) + SHUFFLE_PUSH_READ_METRICS_DIST_FIELD_NUMBER;
        hash = (53 * hash) + getShufflePushReadMetricsDist().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ShuffleReadMetricDistributions}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ShuffleReadMetricDistributions)
        org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        readBytes_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        readRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        remoteBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        localBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000008);
        fetchWaitTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000010);
        remoteBytesRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000020);
        remoteBytesReadToDisk_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000040);
        totalBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000080);
        remoteReqsDuration_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000100);
        if (shufflePushReadMetricsDistBuilder_ == null) {
          shufflePushReadMetricsDist_ = null;
        } else {
          shufflePushReadMetricsDist_ = null;
          shufflePushReadMetricsDistBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions build() {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions result = new org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          readBytes_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.readBytes_ = readBytes_;
        if (((bitField0_ & 0x00000002) != 0)) {
          readRecords_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.readRecords_ = readRecords_;
        if (((bitField0_ & 0x00000004) != 0)) {
          remoteBlocksFetched_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.remoteBlocksFetched_ = remoteBlocksFetched_;
        if (((bitField0_ & 0x00000008) != 0)) {
          localBlocksFetched_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.localBlocksFetched_ = localBlocksFetched_;
        if (((bitField0_ & 0x00000010) != 0)) {
          fetchWaitTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.fetchWaitTime_ = fetchWaitTime_;
        if (((bitField0_ & 0x00000020) != 0)) {
          remoteBytesRead_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000020);
        }
        result.remoteBytesRead_ = remoteBytesRead_;
        if (((bitField0_ & 0x00000040) != 0)) {
          remoteBytesReadToDisk_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000040);
        }
        result.remoteBytesReadToDisk_ = remoteBytesReadToDisk_;
        if (((bitField0_ & 0x00000080) != 0)) {
          totalBlocksFetched_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000080);
        }
        result.totalBlocksFetched_ = totalBlocksFetched_;
        if (((bitField0_ & 0x00000100) != 0)) {
          remoteReqsDuration_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000100);
        }
        result.remoteReqsDuration_ = remoteReqsDuration_;
        if (shufflePushReadMetricsDistBuilder_ == null) {
          result.shufflePushReadMetricsDist_ = shufflePushReadMetricsDist_;
        } else {
          result.shufflePushReadMetricsDist_ = shufflePushReadMetricsDistBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions.getDefaultInstance()) return this;
        if (!other.readBytes_.isEmpty()) {
          if (readBytes_.isEmpty()) {
            readBytes_ = other.readBytes_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureReadBytesIsMutable();
            readBytes_.addAll(other.readBytes_);
          }
          onChanged();
        }
        if (!other.readRecords_.isEmpty()) {
          if (readRecords_.isEmpty()) {
            readRecords_ = other.readRecords_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureReadRecordsIsMutable();
            readRecords_.addAll(other.readRecords_);
          }
          onChanged();
        }
        if (!other.remoteBlocksFetched_.isEmpty()) {
          if (remoteBlocksFetched_.isEmpty()) {
            remoteBlocksFetched_ = other.remoteBlocksFetched_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureRemoteBlocksFetchedIsMutable();
            remoteBlocksFetched_.addAll(other.remoteBlocksFetched_);
          }
          onChanged();
        }
        if (!other.localBlocksFetched_.isEmpty()) {
          if (localBlocksFetched_.isEmpty()) {
            localBlocksFetched_ = other.localBlocksFetched_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureLocalBlocksFetchedIsMutable();
            localBlocksFetched_.addAll(other.localBlocksFetched_);
          }
          onChanged();
        }
        if (!other.fetchWaitTime_.isEmpty()) {
          if (fetchWaitTime_.isEmpty()) {
            fetchWaitTime_ = other.fetchWaitTime_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureFetchWaitTimeIsMutable();
            fetchWaitTime_.addAll(other.fetchWaitTime_);
          }
          onChanged();
        }
        if (!other.remoteBytesRead_.isEmpty()) {
          if (remoteBytesRead_.isEmpty()) {
            remoteBytesRead_ = other.remoteBytesRead_;
            bitField0_ = (bitField0_ & ~0x00000020);
          } else {
            ensureRemoteBytesReadIsMutable();
            remoteBytesRead_.addAll(other.remoteBytesRead_);
          }
          onChanged();
        }
        if (!other.remoteBytesReadToDisk_.isEmpty()) {
          if (remoteBytesReadToDisk_.isEmpty()) {
            remoteBytesReadToDisk_ = other.remoteBytesReadToDisk_;
            bitField0_ = (bitField0_ & ~0x00000040);
          } else {
            ensureRemoteBytesReadToDiskIsMutable();
            remoteBytesReadToDisk_.addAll(other.remoteBytesReadToDisk_);
          }
          onChanged();
        }
        if (!other.totalBlocksFetched_.isEmpty()) {
          if (totalBlocksFetched_.isEmpty()) {
            totalBlocksFetched_ = other.totalBlocksFetched_;
            bitField0_ = (bitField0_ & ~0x00000080);
          } else {
            ensureTotalBlocksFetchedIsMutable();
            totalBlocksFetched_.addAll(other.totalBlocksFetched_);
          }
          onChanged();
        }
        if (!other.remoteReqsDuration_.isEmpty()) {
          if (remoteReqsDuration_.isEmpty()) {
            remoteReqsDuration_ = other.remoteReqsDuration_;
            bitField0_ = (bitField0_ & ~0x00000100);
          } else {
            ensureRemoteReqsDurationIsMutable();
            remoteReqsDuration_.addAll(other.remoteReqsDuration_);
          }
          onChanged();
        }
        if (other.hasShufflePushReadMetricsDist()) {
          mergeShufflePushReadMetricsDist(other.getShufflePushReadMetricsDist());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.DoubleList readBytes_ = emptyDoubleList();
      private void ensureReadBytesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          readBytes_ = mutableCopy(readBytes_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated double read_bytes = 1;</code>
       * @return A list containing the readBytes.
       */
      public java.util.List<java.lang.Double>
          getReadBytesList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(readBytes_) : readBytes_;
      }
      /**
       * <code>repeated double read_bytes = 1;</code>
       * @return The count of readBytes.
       */
      public int getReadBytesCount() {
        return readBytes_.size();
      }
      /**
       * <code>repeated double read_bytes = 1;</code>
       * @param index The index of the element to return.
       * @return The readBytes at the given index.
       */
      public double getReadBytes(int index) {
        return readBytes_.getDouble(index);
      }
      /**
       * <code>repeated double read_bytes = 1;</code>
       * @param index The index to set the value at.
       * @param value The readBytes to set.
       * @return This builder for chaining.
       */
      public Builder setReadBytes(
          int index, double value) {
        ensureReadBytesIsMutable();
        readBytes_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double read_bytes = 1;</code>
       * @param value The readBytes to add.
       * @return This builder for chaining.
       */
      public Builder addReadBytes(double value) {
        ensureReadBytesIsMutable();
        readBytes_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double read_bytes = 1;</code>
       * @param values The readBytes to add.
       * @return This builder for chaining.
       */
      public Builder addAllReadBytes(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureReadBytesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, readBytes_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double read_bytes = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearReadBytes() {
        readBytes_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList readRecords_ = emptyDoubleList();
      private void ensureReadRecordsIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          readRecords_ = mutableCopy(readRecords_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated double read_records = 2;</code>
       * @return A list containing the readRecords.
       */
      public java.util.List<java.lang.Double>
          getReadRecordsList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(readRecords_) : readRecords_;
      }
      /**
       * <code>repeated double read_records = 2;</code>
       * @return The count of readRecords.
       */
      public int getReadRecordsCount() {
        return readRecords_.size();
      }
      /**
       * <code>repeated double read_records = 2;</code>
       * @param index The index of the element to return.
       * @return The readRecords at the given index.
       */
      public double getReadRecords(int index) {
        return readRecords_.getDouble(index);
      }
      /**
       * <code>repeated double read_records = 2;</code>
       * @param index The index to set the value at.
       * @param value The readRecords to set.
       * @return This builder for chaining.
       */
      public Builder setReadRecords(
          int index, double value) {
        ensureReadRecordsIsMutable();
        readRecords_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double read_records = 2;</code>
       * @param value The readRecords to add.
       * @return This builder for chaining.
       */
      public Builder addReadRecords(double value) {
        ensureReadRecordsIsMutable();
        readRecords_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double read_records = 2;</code>
       * @param values The readRecords to add.
       * @return This builder for chaining.
       */
      public Builder addAllReadRecords(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureReadRecordsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, readRecords_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double read_records = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearReadRecords() {
        readRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList remoteBlocksFetched_ = emptyDoubleList();
      private void ensureRemoteBlocksFetchedIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          remoteBlocksFetched_ = mutableCopy(remoteBlocksFetched_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated double remote_blocks_fetched = 3;</code>
       * @return A list containing the remoteBlocksFetched.
       */
      public java.util.List<java.lang.Double>
          getRemoteBlocksFetchedList() {
        return ((bitField0_ & 0x00000004) != 0) ?
                 java.util.Collections.unmodifiableList(remoteBlocksFetched_) : remoteBlocksFetched_;
      }
      /**
       * <code>repeated double remote_blocks_fetched = 3;</code>
       * @return The count of remoteBlocksFetched.
       */
      public int getRemoteBlocksFetchedCount() {
        return remoteBlocksFetched_.size();
      }
      /**
       * <code>repeated double remote_blocks_fetched = 3;</code>
       * @param index The index of the element to return.
       * @return The remoteBlocksFetched at the given index.
       */
      public double getRemoteBlocksFetched(int index) {
        return remoteBlocksFetched_.getDouble(index);
      }
      /**
       * <code>repeated double remote_blocks_fetched = 3;</code>
       * @param index The index to set the value at.
       * @param value The remoteBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteBlocksFetched(
          int index, double value) {
        ensureRemoteBlocksFetchedIsMutable();
        remoteBlocksFetched_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_blocks_fetched = 3;</code>
       * @param value The remoteBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addRemoteBlocksFetched(double value) {
        ensureRemoteBlocksFetchedIsMutable();
        remoteBlocksFetched_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_blocks_fetched = 3;</code>
       * @param values The remoteBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addAllRemoteBlocksFetched(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRemoteBlocksFetchedIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, remoteBlocksFetched_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_blocks_fetched = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteBlocksFetched() {
        remoteBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList localBlocksFetched_ = emptyDoubleList();
      private void ensureLocalBlocksFetchedIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          localBlocksFetched_ = mutableCopy(localBlocksFetched_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated double local_blocks_fetched = 4;</code>
       * @return A list containing the localBlocksFetched.
       */
      public java.util.List<java.lang.Double>
          getLocalBlocksFetchedList() {
        return ((bitField0_ & 0x00000008) != 0) ?
                 java.util.Collections.unmodifiableList(localBlocksFetched_) : localBlocksFetched_;
      }
      /**
       * <code>repeated double local_blocks_fetched = 4;</code>
       * @return The count of localBlocksFetched.
       */
      public int getLocalBlocksFetchedCount() {
        return localBlocksFetched_.size();
      }
      /**
       * <code>repeated double local_blocks_fetched = 4;</code>
       * @param index The index of the element to return.
       * @return The localBlocksFetched at the given index.
       */
      public double getLocalBlocksFetched(int index) {
        return localBlocksFetched_.getDouble(index);
      }
      /**
       * <code>repeated double local_blocks_fetched = 4;</code>
       * @param index The index to set the value at.
       * @param value The localBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setLocalBlocksFetched(
          int index, double value) {
        ensureLocalBlocksFetchedIsMutable();
        localBlocksFetched_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_blocks_fetched = 4;</code>
       * @param value The localBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addLocalBlocksFetched(double value) {
        ensureLocalBlocksFetchedIsMutable();
        localBlocksFetched_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_blocks_fetched = 4;</code>
       * @param values The localBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addAllLocalBlocksFetched(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureLocalBlocksFetchedIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, localBlocksFetched_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_blocks_fetched = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalBlocksFetched() {
        localBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList fetchWaitTime_ = emptyDoubleList();
      private void ensureFetchWaitTimeIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          fetchWaitTime_ = mutableCopy(fetchWaitTime_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <code>repeated double fetch_wait_time = 5;</code>
       * @return A list containing the fetchWaitTime.
       */
      public java.util.List<java.lang.Double>
          getFetchWaitTimeList() {
        return ((bitField0_ & 0x00000010) != 0) ?
                 java.util.Collections.unmodifiableList(fetchWaitTime_) : fetchWaitTime_;
      }
      /**
       * <code>repeated double fetch_wait_time = 5;</code>
       * @return The count of fetchWaitTime.
       */
      public int getFetchWaitTimeCount() {
        return fetchWaitTime_.size();
      }
      /**
       * <code>repeated double fetch_wait_time = 5;</code>
       * @param index The index of the element to return.
       * @return The fetchWaitTime at the given index.
       */
      public double getFetchWaitTime(int index) {
        return fetchWaitTime_.getDouble(index);
      }
      /**
       * <code>repeated double fetch_wait_time = 5;</code>
       * @param index The index to set the value at.
       * @param value The fetchWaitTime to set.
       * @return This builder for chaining.
       */
      public Builder setFetchWaitTime(
          int index, double value) {
        ensureFetchWaitTimeIsMutable();
        fetchWaitTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double fetch_wait_time = 5;</code>
       * @param value The fetchWaitTime to add.
       * @return This builder for chaining.
       */
      public Builder addFetchWaitTime(double value) {
        ensureFetchWaitTimeIsMutable();
        fetchWaitTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double fetch_wait_time = 5;</code>
       * @param values The fetchWaitTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllFetchWaitTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureFetchWaitTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, fetchWaitTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double fetch_wait_time = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearFetchWaitTime() {
        fetchWaitTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList remoteBytesRead_ = emptyDoubleList();
      private void ensureRemoteBytesReadIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          remoteBytesRead_ = mutableCopy(remoteBytesRead_);
          bitField0_ |= 0x00000020;
         }
      }
      /**
       * <code>repeated double remote_bytes_read = 6;</code>
       * @return A list containing the remoteBytesRead.
       */
      public java.util.List<java.lang.Double>
          getRemoteBytesReadList() {
        return ((bitField0_ & 0x00000020) != 0) ?
                 java.util.Collections.unmodifiableList(remoteBytesRead_) : remoteBytesRead_;
      }
      /**
       * <code>repeated double remote_bytes_read = 6;</code>
       * @return The count of remoteBytesRead.
       */
      public int getRemoteBytesReadCount() {
        return remoteBytesRead_.size();
      }
      /**
       * <code>repeated double remote_bytes_read = 6;</code>
       * @param index The index of the element to return.
       * @return The remoteBytesRead at the given index.
       */
      public double getRemoteBytesRead(int index) {
        return remoteBytesRead_.getDouble(index);
      }
      /**
       * <code>repeated double remote_bytes_read = 6;</code>
       * @param index The index to set the value at.
       * @param value The remoteBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteBytesRead(
          int index, double value) {
        ensureRemoteBytesReadIsMutable();
        remoteBytesRead_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_bytes_read = 6;</code>
       * @param value The remoteBytesRead to add.
       * @return This builder for chaining.
       */
      public Builder addRemoteBytesRead(double value) {
        ensureRemoteBytesReadIsMutable();
        remoteBytesRead_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_bytes_read = 6;</code>
       * @param values The remoteBytesRead to add.
       * @return This builder for chaining.
       */
      public Builder addAllRemoteBytesRead(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRemoteBytesReadIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, remoteBytesRead_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_bytes_read = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteBytesRead() {
        remoteBytesRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000020);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList remoteBytesReadToDisk_ = emptyDoubleList();
      private void ensureRemoteBytesReadToDiskIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          remoteBytesReadToDisk_ = mutableCopy(remoteBytesReadToDisk_);
          bitField0_ |= 0x00000040;
         }
      }
      /**
       * <code>repeated double remote_bytes_read_to_disk = 7;</code>
       * @return A list containing the remoteBytesReadToDisk.
       */
      public java.util.List<java.lang.Double>
          getRemoteBytesReadToDiskList() {
        return ((bitField0_ & 0x00000040) != 0) ?
                 java.util.Collections.unmodifiableList(remoteBytesReadToDisk_) : remoteBytesReadToDisk_;
      }
      /**
       * <code>repeated double remote_bytes_read_to_disk = 7;</code>
       * @return The count of remoteBytesReadToDisk.
       */
      public int getRemoteBytesReadToDiskCount() {
        return remoteBytesReadToDisk_.size();
      }
      /**
       * <code>repeated double remote_bytes_read_to_disk = 7;</code>
       * @param index The index of the element to return.
       * @return The remoteBytesReadToDisk at the given index.
       */
      public double getRemoteBytesReadToDisk(int index) {
        return remoteBytesReadToDisk_.getDouble(index);
      }
      /**
       * <code>repeated double remote_bytes_read_to_disk = 7;</code>
       * @param index The index to set the value at.
       * @param value The remoteBytesReadToDisk to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteBytesReadToDisk(
          int index, double value) {
        ensureRemoteBytesReadToDiskIsMutable();
        remoteBytesReadToDisk_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_bytes_read_to_disk = 7;</code>
       * @param value The remoteBytesReadToDisk to add.
       * @return This builder for chaining.
       */
      public Builder addRemoteBytesReadToDisk(double value) {
        ensureRemoteBytesReadToDiskIsMutable();
        remoteBytesReadToDisk_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_bytes_read_to_disk = 7;</code>
       * @param values The remoteBytesReadToDisk to add.
       * @return This builder for chaining.
       */
      public Builder addAllRemoteBytesReadToDisk(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRemoteBytesReadToDiskIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, remoteBytesReadToDisk_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_bytes_read_to_disk = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteBytesReadToDisk() {
        remoteBytesReadToDisk_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000040);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList totalBlocksFetched_ = emptyDoubleList();
      private void ensureTotalBlocksFetchedIsMutable() {
        if (!((bitField0_ & 0x00000080) != 0)) {
          totalBlocksFetched_ = mutableCopy(totalBlocksFetched_);
          bitField0_ |= 0x00000080;
         }
      }
      /**
       * <code>repeated double total_blocks_fetched = 8;</code>
       * @return A list containing the totalBlocksFetched.
       */
      public java.util.List<java.lang.Double>
          getTotalBlocksFetchedList() {
        return ((bitField0_ & 0x00000080) != 0) ?
                 java.util.Collections.unmodifiableList(totalBlocksFetched_) : totalBlocksFetched_;
      }
      /**
       * <code>repeated double total_blocks_fetched = 8;</code>
       * @return The count of totalBlocksFetched.
       */
      public int getTotalBlocksFetchedCount() {
        return totalBlocksFetched_.size();
      }
      /**
       * <code>repeated double total_blocks_fetched = 8;</code>
       * @param index The index of the element to return.
       * @return The totalBlocksFetched at the given index.
       */
      public double getTotalBlocksFetched(int index) {
        return totalBlocksFetched_.getDouble(index);
      }
      /**
       * <code>repeated double total_blocks_fetched = 8;</code>
       * @param index The index to set the value at.
       * @param value The totalBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setTotalBlocksFetched(
          int index, double value) {
        ensureTotalBlocksFetchedIsMutable();
        totalBlocksFetched_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double total_blocks_fetched = 8;</code>
       * @param value The totalBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addTotalBlocksFetched(double value) {
        ensureTotalBlocksFetchedIsMutable();
        totalBlocksFetched_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double total_blocks_fetched = 8;</code>
       * @param values The totalBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addAllTotalBlocksFetched(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureTotalBlocksFetchedIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, totalBlocksFetched_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double total_blocks_fetched = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalBlocksFetched() {
        totalBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000080);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList remoteReqsDuration_ = emptyDoubleList();
      private void ensureRemoteReqsDurationIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          remoteReqsDuration_ = mutableCopy(remoteReqsDuration_);
          bitField0_ |= 0x00000100;
         }
      }
      /**
       * <code>repeated double remote_reqs_duration = 9;</code>
       * @return A list containing the remoteReqsDuration.
       */
      public java.util.List<java.lang.Double>
          getRemoteReqsDurationList() {
        return ((bitField0_ & 0x00000100) != 0) ?
                 java.util.Collections.unmodifiableList(remoteReqsDuration_) : remoteReqsDuration_;
      }
      /**
       * <code>repeated double remote_reqs_duration = 9;</code>
       * @return The count of remoteReqsDuration.
       */
      public int getRemoteReqsDurationCount() {
        return remoteReqsDuration_.size();
      }
      /**
       * <code>repeated double remote_reqs_duration = 9;</code>
       * @param index The index of the element to return.
       * @return The remoteReqsDuration at the given index.
       */
      public double getRemoteReqsDuration(int index) {
        return remoteReqsDuration_.getDouble(index);
      }
      /**
       * <code>repeated double remote_reqs_duration = 9;</code>
       * @param index The index to set the value at.
       * @param value The remoteReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteReqsDuration(
          int index, double value) {
        ensureRemoteReqsDurationIsMutable();
        remoteReqsDuration_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_reqs_duration = 9;</code>
       * @param value The remoteReqsDuration to add.
       * @return This builder for chaining.
       */
      public Builder addRemoteReqsDuration(double value) {
        ensureRemoteReqsDurationIsMutable();
        remoteReqsDuration_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_reqs_duration = 9;</code>
       * @param values The remoteReqsDuration to add.
       * @return This builder for chaining.
       */
      public Builder addAllRemoteReqsDuration(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRemoteReqsDurationIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, remoteReqsDuration_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_reqs_duration = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteReqsDuration() {
        remoteReqsDuration_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000100);
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions shufflePushReadMetricsDist_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributionsOrBuilder> shufflePushReadMetricsDistBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       * @return Whether the shufflePushReadMetricsDist field is set.
       */
      public boolean hasShufflePushReadMetricsDist() {
        return shufflePushReadMetricsDistBuilder_ != null || shufflePushReadMetricsDist_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       * @return The shufflePushReadMetricsDist.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions getShufflePushReadMetricsDist() {
        if (shufflePushReadMetricsDistBuilder_ == null) {
          return shufflePushReadMetricsDist_ == null ? org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.getDefaultInstance() : shufflePushReadMetricsDist_;
        } else {
          return shufflePushReadMetricsDistBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       */
      public Builder setShufflePushReadMetricsDist(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions value) {
        if (shufflePushReadMetricsDistBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          shufflePushReadMetricsDist_ = value;
          onChanged();
        } else {
          shufflePushReadMetricsDistBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       */
      public Builder setShufflePushReadMetricsDist(
          org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.Builder builderForValue) {
        if (shufflePushReadMetricsDistBuilder_ == null) {
          shufflePushReadMetricsDist_ = builderForValue.build();
          onChanged();
        } else {
          shufflePushReadMetricsDistBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       */
      public Builder mergeShufflePushReadMetricsDist(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions value) {
        if (shufflePushReadMetricsDistBuilder_ == null) {
          if (shufflePushReadMetricsDist_ != null) {
            shufflePushReadMetricsDist_ =
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.newBuilder(shufflePushReadMetricsDist_).mergeFrom(value).buildPartial();
          } else {
            shufflePushReadMetricsDist_ = value;
          }
          onChanged();
        } else {
          shufflePushReadMetricsDistBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       */
      public Builder clearShufflePushReadMetricsDist() {
        if (shufflePushReadMetricsDistBuilder_ == null) {
          shufflePushReadMetricsDist_ = null;
          onChanged();
        } else {
          shufflePushReadMetricsDist_ = null;
          shufflePushReadMetricsDistBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.Builder getShufflePushReadMetricsDistBuilder() {
        
        onChanged();
        return getShufflePushReadMetricsDistFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributionsOrBuilder getShufflePushReadMetricsDistOrBuilder() {
        if (shufflePushReadMetricsDistBuilder_ != null) {
          return shufflePushReadMetricsDistBuilder_.getMessageOrBuilder();
        } else {
          return shufflePushReadMetricsDist_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.getDefaultInstance() : shufflePushReadMetricsDist_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions shuffle_push_read_metrics_dist = 10;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributionsOrBuilder> 
          getShufflePushReadMetricsDistFieldBuilder() {
        if (shufflePushReadMetricsDistBuilder_ == null) {
          shufflePushReadMetricsDistBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributionsOrBuilder>(
                  getShufflePushReadMetricsDist(),
                  getParentForChildren(),
                  isClean());
          shufflePushReadMetricsDist_ = null;
        }
        return shufflePushReadMetricsDistBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ShuffleReadMetricDistributions)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ShuffleReadMetricDistributions)
    private static final org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ShuffleReadMetricDistributions>
        PARSER = new com.google.protobuf.AbstractParser<ShuffleReadMetricDistributions>() {
      @java.lang.Override
      public ShuffleReadMetricDistributions parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ShuffleReadMetricDistributions(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ShuffleReadMetricDistributions> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ShuffleReadMetricDistributions> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleReadMetricDistributions getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ShufflePushReadMetricDistributionsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated double corrupt_merged_block_chunks = 1;</code>
     * @return A list containing the corruptMergedBlockChunks.
     */
    java.util.List<java.lang.Double> getCorruptMergedBlockChunksList();
    /**
     * <code>repeated double corrupt_merged_block_chunks = 1;</code>
     * @return The count of corruptMergedBlockChunks.
     */
    int getCorruptMergedBlockChunksCount();
    /**
     * <code>repeated double corrupt_merged_block_chunks = 1;</code>
     * @param index The index of the element to return.
     * @return The corruptMergedBlockChunks at the given index.
     */
    double getCorruptMergedBlockChunks(int index);

    /**
     * <code>repeated double merged_fetch_fallback_count = 2;</code>
     * @return A list containing the mergedFetchFallbackCount.
     */
    java.util.List<java.lang.Double> getMergedFetchFallbackCountList();
    /**
     * <code>repeated double merged_fetch_fallback_count = 2;</code>
     * @return The count of mergedFetchFallbackCount.
     */
    int getMergedFetchFallbackCountCount();
    /**
     * <code>repeated double merged_fetch_fallback_count = 2;</code>
     * @param index The index of the element to return.
     * @return The mergedFetchFallbackCount at the given index.
     */
    double getMergedFetchFallbackCount(int index);

    /**
     * <code>repeated double remote_merged_blocks_fetched = 3;</code>
     * @return A list containing the remoteMergedBlocksFetched.
     */
    java.util.List<java.lang.Double> getRemoteMergedBlocksFetchedList();
    /**
     * <code>repeated double remote_merged_blocks_fetched = 3;</code>
     * @return The count of remoteMergedBlocksFetched.
     */
    int getRemoteMergedBlocksFetchedCount();
    /**
     * <code>repeated double remote_merged_blocks_fetched = 3;</code>
     * @param index The index of the element to return.
     * @return The remoteMergedBlocksFetched at the given index.
     */
    double getRemoteMergedBlocksFetched(int index);

    /**
     * <code>repeated double local_merged_blocks_fetched = 4;</code>
     * @return A list containing the localMergedBlocksFetched.
     */
    java.util.List<java.lang.Double> getLocalMergedBlocksFetchedList();
    /**
     * <code>repeated double local_merged_blocks_fetched = 4;</code>
     * @return The count of localMergedBlocksFetched.
     */
    int getLocalMergedBlocksFetchedCount();
    /**
     * <code>repeated double local_merged_blocks_fetched = 4;</code>
     * @param index The index of the element to return.
     * @return The localMergedBlocksFetched at the given index.
     */
    double getLocalMergedBlocksFetched(int index);

    /**
     * <code>repeated double remote_merged_chunks_fetched = 5;</code>
     * @return A list containing the remoteMergedChunksFetched.
     */
    java.util.List<java.lang.Double> getRemoteMergedChunksFetchedList();
    /**
     * <code>repeated double remote_merged_chunks_fetched = 5;</code>
     * @return The count of remoteMergedChunksFetched.
     */
    int getRemoteMergedChunksFetchedCount();
    /**
     * <code>repeated double remote_merged_chunks_fetched = 5;</code>
     * @param index The index of the element to return.
     * @return The remoteMergedChunksFetched at the given index.
     */
    double getRemoteMergedChunksFetched(int index);

    /**
     * <code>repeated double local_merged_chunks_fetched = 6;</code>
     * @return A list containing the localMergedChunksFetched.
     */
    java.util.List<java.lang.Double> getLocalMergedChunksFetchedList();
    /**
     * <code>repeated double local_merged_chunks_fetched = 6;</code>
     * @return The count of localMergedChunksFetched.
     */
    int getLocalMergedChunksFetchedCount();
    /**
     * <code>repeated double local_merged_chunks_fetched = 6;</code>
     * @param index The index of the element to return.
     * @return The localMergedChunksFetched at the given index.
     */
    double getLocalMergedChunksFetched(int index);

    /**
     * <code>repeated double remote_merged_bytes_read = 7;</code>
     * @return A list containing the remoteMergedBytesRead.
     */
    java.util.List<java.lang.Double> getRemoteMergedBytesReadList();
    /**
     * <code>repeated double remote_merged_bytes_read = 7;</code>
     * @return The count of remoteMergedBytesRead.
     */
    int getRemoteMergedBytesReadCount();
    /**
     * <code>repeated double remote_merged_bytes_read = 7;</code>
     * @param index The index of the element to return.
     * @return The remoteMergedBytesRead at the given index.
     */
    double getRemoteMergedBytesRead(int index);

    /**
     * <code>repeated double local_merged_bytes_read = 8;</code>
     * @return A list containing the localMergedBytesRead.
     */
    java.util.List<java.lang.Double> getLocalMergedBytesReadList();
    /**
     * <code>repeated double local_merged_bytes_read = 8;</code>
     * @return The count of localMergedBytesRead.
     */
    int getLocalMergedBytesReadCount();
    /**
     * <code>repeated double local_merged_bytes_read = 8;</code>
     * @param index The index of the element to return.
     * @return The localMergedBytesRead at the given index.
     */
    double getLocalMergedBytesRead(int index);

    /**
     * <code>repeated double remote_merged_reqs_duration = 9;</code>
     * @return A list containing the remoteMergedReqsDuration.
     */
    java.util.List<java.lang.Double> getRemoteMergedReqsDurationList();
    /**
     * <code>repeated double remote_merged_reqs_duration = 9;</code>
     * @return The count of remoteMergedReqsDuration.
     */
    int getRemoteMergedReqsDurationCount();
    /**
     * <code>repeated double remote_merged_reqs_duration = 9;</code>
     * @param index The index of the element to return.
     * @return The remoteMergedReqsDuration at the given index.
     */
    double getRemoteMergedReqsDuration(int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions}
   */
  public static final class ShufflePushReadMetricDistributions extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions)
      ShufflePushReadMetricDistributionsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ShufflePushReadMetricDistributions.newBuilder() to construct.
    private ShufflePushReadMetricDistributions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ShufflePushReadMetricDistributions() {
      corruptMergedBlockChunks_ = emptyDoubleList();
      mergedFetchFallbackCount_ = emptyDoubleList();
      remoteMergedBlocksFetched_ = emptyDoubleList();
      localMergedBlocksFetched_ = emptyDoubleList();
      remoteMergedChunksFetched_ = emptyDoubleList();
      localMergedChunksFetched_ = emptyDoubleList();
      remoteMergedBytesRead_ = emptyDoubleList();
      localMergedBytesRead_ = emptyDoubleList();
      remoteMergedReqsDuration_ = emptyDoubleList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ShufflePushReadMetricDistributions();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ShufflePushReadMetricDistributions(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                corruptMergedBlockChunks_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              corruptMergedBlockChunks_.addDouble(input.readDouble());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                corruptMergedBlockChunks_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                corruptMergedBlockChunks_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 17: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                mergedFetchFallbackCount_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              mergedFetchFallbackCount_.addDouble(input.readDouble());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                mergedFetchFallbackCount_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                mergedFetchFallbackCount_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 25: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                remoteMergedBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              remoteMergedBlocksFetched_.addDouble(input.readDouble());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000004) != 0) && input.getBytesUntilLimit() > 0) {
                remoteMergedBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              while (input.getBytesUntilLimit() > 0) {
                remoteMergedBlocksFetched_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 33: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                localMergedBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000008;
              }
              localMergedBlocksFetched_.addDouble(input.readDouble());
              break;
            }
            case 34: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000008) != 0) && input.getBytesUntilLimit() > 0) {
                localMergedBlocksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000008;
              }
              while (input.getBytesUntilLimit() > 0) {
                localMergedBlocksFetched_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 41: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                remoteMergedChunksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000010;
              }
              remoteMergedChunksFetched_.addDouble(input.readDouble());
              break;
            }
            case 42: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000010) != 0) && input.getBytesUntilLimit() > 0) {
                remoteMergedChunksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000010;
              }
              while (input.getBytesUntilLimit() > 0) {
                remoteMergedChunksFetched_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 49: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                localMergedChunksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000020;
              }
              localMergedChunksFetched_.addDouble(input.readDouble());
              break;
            }
            case 50: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000020) != 0) && input.getBytesUntilLimit() > 0) {
                localMergedChunksFetched_ = newDoubleList();
                mutable_bitField0_ |= 0x00000020;
              }
              while (input.getBytesUntilLimit() > 0) {
                localMergedChunksFetched_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 57: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                remoteMergedBytesRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000040;
              }
              remoteMergedBytesRead_.addDouble(input.readDouble());
              break;
            }
            case 58: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000040) != 0) && input.getBytesUntilLimit() > 0) {
                remoteMergedBytesRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000040;
              }
              while (input.getBytesUntilLimit() > 0) {
                remoteMergedBytesRead_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 65: {
              if (!((mutable_bitField0_ & 0x00000080) != 0)) {
                localMergedBytesRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000080;
              }
              localMergedBytesRead_.addDouble(input.readDouble());
              break;
            }
            case 66: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000080) != 0) && input.getBytesUntilLimit() > 0) {
                localMergedBytesRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000080;
              }
              while (input.getBytesUntilLimit() > 0) {
                localMergedBytesRead_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 73: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                remoteMergedReqsDuration_ = newDoubleList();
                mutable_bitField0_ |= 0x00000100;
              }
              remoteMergedReqsDuration_.addDouble(input.readDouble());
              break;
            }
            case 74: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000100) != 0) && input.getBytesUntilLimit() > 0) {
                remoteMergedReqsDuration_ = newDoubleList();
                mutable_bitField0_ |= 0x00000100;
              }
              while (input.getBytesUntilLimit() > 0) {
                remoteMergedReqsDuration_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          corruptMergedBlockChunks_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          mergedFetchFallbackCount_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          remoteMergedBlocksFetched_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          localMergedBlocksFetched_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          remoteMergedChunksFetched_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          localMergedChunksFetched_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          remoteMergedBytesRead_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000080) != 0)) {
          localMergedBytesRead_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          remoteMergedReqsDuration_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.Builder.class);
    }

    public static final int CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.DoubleList corruptMergedBlockChunks_;
    /**
     * <code>repeated double corrupt_merged_block_chunks = 1;</code>
     * @return A list containing the corruptMergedBlockChunks.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getCorruptMergedBlockChunksList() {
      return corruptMergedBlockChunks_;
    }
    /**
     * <code>repeated double corrupt_merged_block_chunks = 1;</code>
     * @return The count of corruptMergedBlockChunks.
     */
    public int getCorruptMergedBlockChunksCount() {
      return corruptMergedBlockChunks_.size();
    }
    /**
     * <code>repeated double corrupt_merged_block_chunks = 1;</code>
     * @param index The index of the element to return.
     * @return The corruptMergedBlockChunks at the given index.
     */
    public double getCorruptMergedBlockChunks(int index) {
      return corruptMergedBlockChunks_.getDouble(index);
    }
    private int corruptMergedBlockChunksMemoizedSerializedSize = -1;

    public static final int MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.DoubleList mergedFetchFallbackCount_;
    /**
     * <code>repeated double merged_fetch_fallback_count = 2;</code>
     * @return A list containing the mergedFetchFallbackCount.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getMergedFetchFallbackCountList() {
      return mergedFetchFallbackCount_;
    }
    /**
     * <code>repeated double merged_fetch_fallback_count = 2;</code>
     * @return The count of mergedFetchFallbackCount.
     */
    public int getMergedFetchFallbackCountCount() {
      return mergedFetchFallbackCount_.size();
    }
    /**
     * <code>repeated double merged_fetch_fallback_count = 2;</code>
     * @param index The index of the element to return.
     * @return The mergedFetchFallbackCount at the given index.
     */
    public double getMergedFetchFallbackCount(int index) {
      return mergedFetchFallbackCount_.getDouble(index);
    }
    private int mergedFetchFallbackCountMemoizedSerializedSize = -1;

    public static final int REMOTE_MERGED_BLOCKS_FETCHED_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.DoubleList remoteMergedBlocksFetched_;
    /**
     * <code>repeated double remote_merged_blocks_fetched = 3;</code>
     * @return A list containing the remoteMergedBlocksFetched.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRemoteMergedBlocksFetchedList() {
      return remoteMergedBlocksFetched_;
    }
    /**
     * <code>repeated double remote_merged_blocks_fetched = 3;</code>
     * @return The count of remoteMergedBlocksFetched.
     */
    public int getRemoteMergedBlocksFetchedCount() {
      return remoteMergedBlocksFetched_.size();
    }
    /**
     * <code>repeated double remote_merged_blocks_fetched = 3;</code>
     * @param index The index of the element to return.
     * @return The remoteMergedBlocksFetched at the given index.
     */
    public double getRemoteMergedBlocksFetched(int index) {
      return remoteMergedBlocksFetched_.getDouble(index);
    }
    private int remoteMergedBlocksFetchedMemoizedSerializedSize = -1;

    public static final int LOCAL_MERGED_BLOCKS_FETCHED_FIELD_NUMBER = 4;
    private com.google.protobuf.Internal.DoubleList localMergedBlocksFetched_;
    /**
     * <code>repeated double local_merged_blocks_fetched = 4;</code>
     * @return A list containing the localMergedBlocksFetched.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getLocalMergedBlocksFetchedList() {
      return localMergedBlocksFetched_;
    }
    /**
     * <code>repeated double local_merged_blocks_fetched = 4;</code>
     * @return The count of localMergedBlocksFetched.
     */
    public int getLocalMergedBlocksFetchedCount() {
      return localMergedBlocksFetched_.size();
    }
    /**
     * <code>repeated double local_merged_blocks_fetched = 4;</code>
     * @param index The index of the element to return.
     * @return The localMergedBlocksFetched at the given index.
     */
    public double getLocalMergedBlocksFetched(int index) {
      return localMergedBlocksFetched_.getDouble(index);
    }
    private int localMergedBlocksFetchedMemoizedSerializedSize = -1;

    public static final int REMOTE_MERGED_CHUNKS_FETCHED_FIELD_NUMBER = 5;
    private com.google.protobuf.Internal.DoubleList remoteMergedChunksFetched_;
    /**
     * <code>repeated double remote_merged_chunks_fetched = 5;</code>
     * @return A list containing the remoteMergedChunksFetched.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRemoteMergedChunksFetchedList() {
      return remoteMergedChunksFetched_;
    }
    /**
     * <code>repeated double remote_merged_chunks_fetched = 5;</code>
     * @return The count of remoteMergedChunksFetched.
     */
    public int getRemoteMergedChunksFetchedCount() {
      return remoteMergedChunksFetched_.size();
    }
    /**
     * <code>repeated double remote_merged_chunks_fetched = 5;</code>
     * @param index The index of the element to return.
     * @return The remoteMergedChunksFetched at the given index.
     */
    public double getRemoteMergedChunksFetched(int index) {
      return remoteMergedChunksFetched_.getDouble(index);
    }
    private int remoteMergedChunksFetchedMemoizedSerializedSize = -1;

    public static final int LOCAL_MERGED_CHUNKS_FETCHED_FIELD_NUMBER = 6;
    private com.google.protobuf.Internal.DoubleList localMergedChunksFetched_;
    /**
     * <code>repeated double local_merged_chunks_fetched = 6;</code>
     * @return A list containing the localMergedChunksFetched.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getLocalMergedChunksFetchedList() {
      return localMergedChunksFetched_;
    }
    /**
     * <code>repeated double local_merged_chunks_fetched = 6;</code>
     * @return The count of localMergedChunksFetched.
     */
    public int getLocalMergedChunksFetchedCount() {
      return localMergedChunksFetched_.size();
    }
    /**
     * <code>repeated double local_merged_chunks_fetched = 6;</code>
     * @param index The index of the element to return.
     * @return The localMergedChunksFetched at the given index.
     */
    public double getLocalMergedChunksFetched(int index) {
      return localMergedChunksFetched_.getDouble(index);
    }
    private int localMergedChunksFetchedMemoizedSerializedSize = -1;

    public static final int REMOTE_MERGED_BYTES_READ_FIELD_NUMBER = 7;
    private com.google.protobuf.Internal.DoubleList remoteMergedBytesRead_;
    /**
     * <code>repeated double remote_merged_bytes_read = 7;</code>
     * @return A list containing the remoteMergedBytesRead.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRemoteMergedBytesReadList() {
      return remoteMergedBytesRead_;
    }
    /**
     * <code>repeated double remote_merged_bytes_read = 7;</code>
     * @return The count of remoteMergedBytesRead.
     */
    public int getRemoteMergedBytesReadCount() {
      return remoteMergedBytesRead_.size();
    }
    /**
     * <code>repeated double remote_merged_bytes_read = 7;</code>
     * @param index The index of the element to return.
     * @return The remoteMergedBytesRead at the given index.
     */
    public double getRemoteMergedBytesRead(int index) {
      return remoteMergedBytesRead_.getDouble(index);
    }
    private int remoteMergedBytesReadMemoizedSerializedSize = -1;

    public static final int LOCAL_MERGED_BYTES_READ_FIELD_NUMBER = 8;
    private com.google.protobuf.Internal.DoubleList localMergedBytesRead_;
    /**
     * <code>repeated double local_merged_bytes_read = 8;</code>
     * @return A list containing the localMergedBytesRead.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getLocalMergedBytesReadList() {
      return localMergedBytesRead_;
    }
    /**
     * <code>repeated double local_merged_bytes_read = 8;</code>
     * @return The count of localMergedBytesRead.
     */
    public int getLocalMergedBytesReadCount() {
      return localMergedBytesRead_.size();
    }
    /**
     * <code>repeated double local_merged_bytes_read = 8;</code>
     * @param index The index of the element to return.
     * @return The localMergedBytesRead at the given index.
     */
    public double getLocalMergedBytesRead(int index) {
      return localMergedBytesRead_.getDouble(index);
    }
    private int localMergedBytesReadMemoizedSerializedSize = -1;

    public static final int REMOTE_MERGED_REQS_DURATION_FIELD_NUMBER = 9;
    private com.google.protobuf.Internal.DoubleList remoteMergedReqsDuration_;
    /**
     * <code>repeated double remote_merged_reqs_duration = 9;</code>
     * @return A list containing the remoteMergedReqsDuration.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getRemoteMergedReqsDurationList() {
      return remoteMergedReqsDuration_;
    }
    /**
     * <code>repeated double remote_merged_reqs_duration = 9;</code>
     * @return The count of remoteMergedReqsDuration.
     */
    public int getRemoteMergedReqsDurationCount() {
      return remoteMergedReqsDuration_.size();
    }
    /**
     * <code>repeated double remote_merged_reqs_duration = 9;</code>
     * @param index The index of the element to return.
     * @return The remoteMergedReqsDuration at the given index.
     */
    public double getRemoteMergedReqsDuration(int index) {
      return remoteMergedReqsDuration_.getDouble(index);
    }
    private int remoteMergedReqsDurationMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getCorruptMergedBlockChunksList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(corruptMergedBlockChunksMemoizedSerializedSize);
      }
      for (int i = 0; i < corruptMergedBlockChunks_.size(); i++) {
        output.writeDoubleNoTag(corruptMergedBlockChunks_.getDouble(i));
      }
      if (getMergedFetchFallbackCountList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(mergedFetchFallbackCountMemoizedSerializedSize);
      }
      for (int i = 0; i < mergedFetchFallbackCount_.size(); i++) {
        output.writeDoubleNoTag(mergedFetchFallbackCount_.getDouble(i));
      }
      if (getRemoteMergedBlocksFetchedList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(remoteMergedBlocksFetchedMemoizedSerializedSize);
      }
      for (int i = 0; i < remoteMergedBlocksFetched_.size(); i++) {
        output.writeDoubleNoTag(remoteMergedBlocksFetched_.getDouble(i));
      }
      if (getLocalMergedBlocksFetchedList().size() > 0) {
        output.writeUInt32NoTag(34);
        output.writeUInt32NoTag(localMergedBlocksFetchedMemoizedSerializedSize);
      }
      for (int i = 0; i < localMergedBlocksFetched_.size(); i++) {
        output.writeDoubleNoTag(localMergedBlocksFetched_.getDouble(i));
      }
      if (getRemoteMergedChunksFetchedList().size() > 0) {
        output.writeUInt32NoTag(42);
        output.writeUInt32NoTag(remoteMergedChunksFetchedMemoizedSerializedSize);
      }
      for (int i = 0; i < remoteMergedChunksFetched_.size(); i++) {
        output.writeDoubleNoTag(remoteMergedChunksFetched_.getDouble(i));
      }
      if (getLocalMergedChunksFetchedList().size() > 0) {
        output.writeUInt32NoTag(50);
        output.writeUInt32NoTag(localMergedChunksFetchedMemoizedSerializedSize);
      }
      for (int i = 0; i < localMergedChunksFetched_.size(); i++) {
        output.writeDoubleNoTag(localMergedChunksFetched_.getDouble(i));
      }
      if (getRemoteMergedBytesReadList().size() > 0) {
        output.writeUInt32NoTag(58);
        output.writeUInt32NoTag(remoteMergedBytesReadMemoizedSerializedSize);
      }
      for (int i = 0; i < remoteMergedBytesRead_.size(); i++) {
        output.writeDoubleNoTag(remoteMergedBytesRead_.getDouble(i));
      }
      if (getLocalMergedBytesReadList().size() > 0) {
        output.writeUInt32NoTag(66);
        output.writeUInt32NoTag(localMergedBytesReadMemoizedSerializedSize);
      }
      for (int i = 0; i < localMergedBytesRead_.size(); i++) {
        output.writeDoubleNoTag(localMergedBytesRead_.getDouble(i));
      }
      if (getRemoteMergedReqsDurationList().size() > 0) {
        output.writeUInt32NoTag(74);
        output.writeUInt32NoTag(remoteMergedReqsDurationMemoizedSerializedSize);
      }
      for (int i = 0; i < remoteMergedReqsDuration_.size(); i++) {
        output.writeDoubleNoTag(remoteMergedReqsDuration_.getDouble(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        dataSize = 8 * getCorruptMergedBlockChunksList().size();
        size += dataSize;
        if (!getCorruptMergedBlockChunksList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        corruptMergedBlockChunksMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getMergedFetchFallbackCountList().size();
        size += dataSize;
        if (!getMergedFetchFallbackCountList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        mergedFetchFallbackCountMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRemoteMergedBlocksFetchedList().size();
        size += dataSize;
        if (!getRemoteMergedBlocksFetchedList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        remoteMergedBlocksFetchedMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getLocalMergedBlocksFetchedList().size();
        size += dataSize;
        if (!getLocalMergedBlocksFetchedList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        localMergedBlocksFetchedMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRemoteMergedChunksFetchedList().size();
        size += dataSize;
        if (!getRemoteMergedChunksFetchedList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        remoteMergedChunksFetchedMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getLocalMergedChunksFetchedList().size();
        size += dataSize;
        if (!getLocalMergedChunksFetchedList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        localMergedChunksFetchedMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRemoteMergedBytesReadList().size();
        size += dataSize;
        if (!getRemoteMergedBytesReadList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        remoteMergedBytesReadMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getLocalMergedBytesReadList().size();
        size += dataSize;
        if (!getLocalMergedBytesReadList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        localMergedBytesReadMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getRemoteMergedReqsDurationList().size();
        size += dataSize;
        if (!getRemoteMergedReqsDurationList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        remoteMergedReqsDurationMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions other = (org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions) obj;

      if (!getCorruptMergedBlockChunksList()
          .equals(other.getCorruptMergedBlockChunksList())) return false;
      if (!getMergedFetchFallbackCountList()
          .equals(other.getMergedFetchFallbackCountList())) return false;
      if (!getRemoteMergedBlocksFetchedList()
          .equals(other.getRemoteMergedBlocksFetchedList())) return false;
      if (!getLocalMergedBlocksFetchedList()
          .equals(other.getLocalMergedBlocksFetchedList())) return false;
      if (!getRemoteMergedChunksFetchedList()
          .equals(other.getRemoteMergedChunksFetchedList())) return false;
      if (!getLocalMergedChunksFetchedList()
          .equals(other.getLocalMergedChunksFetchedList())) return false;
      if (!getRemoteMergedBytesReadList()
          .equals(other.getRemoteMergedBytesReadList())) return false;
      if (!getLocalMergedBytesReadList()
          .equals(other.getLocalMergedBytesReadList())) return false;
      if (!getRemoteMergedReqsDurationList()
          .equals(other.getRemoteMergedReqsDurationList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getCorruptMergedBlockChunksCount() > 0) {
        hash = (37 * hash) + CORRUPT_MERGED_BLOCK_CHUNKS_FIELD_NUMBER;
        hash = (53 * hash) + getCorruptMergedBlockChunksList().hashCode();
      }
      if (getMergedFetchFallbackCountCount() > 0) {
        hash = (37 * hash) + MERGED_FETCH_FALLBACK_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + getMergedFetchFallbackCountList().hashCode();
      }
      if (getRemoteMergedBlocksFetchedCount() > 0) {
        hash = (37 * hash) + REMOTE_MERGED_BLOCKS_FETCHED_FIELD_NUMBER;
        hash = (53 * hash) + getRemoteMergedBlocksFetchedList().hashCode();
      }
      if (getLocalMergedBlocksFetchedCount() > 0) {
        hash = (37 * hash) + LOCAL_MERGED_BLOCKS_FETCHED_FIELD_NUMBER;
        hash = (53 * hash) + getLocalMergedBlocksFetchedList().hashCode();
      }
      if (getRemoteMergedChunksFetchedCount() > 0) {
        hash = (37 * hash) + REMOTE_MERGED_CHUNKS_FETCHED_FIELD_NUMBER;
        hash = (53 * hash) + getRemoteMergedChunksFetchedList().hashCode();
      }
      if (getLocalMergedChunksFetchedCount() > 0) {
        hash = (37 * hash) + LOCAL_MERGED_CHUNKS_FETCHED_FIELD_NUMBER;
        hash = (53 * hash) + getLocalMergedChunksFetchedList().hashCode();
      }
      if (getRemoteMergedBytesReadCount() > 0) {
        hash = (37 * hash) + REMOTE_MERGED_BYTES_READ_FIELD_NUMBER;
        hash = (53 * hash) + getRemoteMergedBytesReadList().hashCode();
      }
      if (getLocalMergedBytesReadCount() > 0) {
        hash = (37 * hash) + LOCAL_MERGED_BYTES_READ_FIELD_NUMBER;
        hash = (53 * hash) + getLocalMergedBytesReadList().hashCode();
      }
      if (getRemoteMergedReqsDurationCount() > 0) {
        hash = (37 * hash) + REMOTE_MERGED_REQS_DURATION_FIELD_NUMBER;
        hash = (53 * hash) + getRemoteMergedReqsDurationList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions)
        org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        corruptMergedBlockChunks_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        mergedFetchFallbackCount_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        remoteMergedBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        localMergedBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000008);
        remoteMergedChunksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000010);
        localMergedChunksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000020);
        remoteMergedBytesRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000040);
        localMergedBytesRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000080);
        remoteMergedReqsDuration_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions build() {
        org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions result = new org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          corruptMergedBlockChunks_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.corruptMergedBlockChunks_ = corruptMergedBlockChunks_;
        if (((bitField0_ & 0x00000002) != 0)) {
          mergedFetchFallbackCount_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.mergedFetchFallbackCount_ = mergedFetchFallbackCount_;
        if (((bitField0_ & 0x00000004) != 0)) {
          remoteMergedBlocksFetched_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.remoteMergedBlocksFetched_ = remoteMergedBlocksFetched_;
        if (((bitField0_ & 0x00000008) != 0)) {
          localMergedBlocksFetched_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.localMergedBlocksFetched_ = localMergedBlocksFetched_;
        if (((bitField0_ & 0x00000010) != 0)) {
          remoteMergedChunksFetched_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.remoteMergedChunksFetched_ = remoteMergedChunksFetched_;
        if (((bitField0_ & 0x00000020) != 0)) {
          localMergedChunksFetched_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000020);
        }
        result.localMergedChunksFetched_ = localMergedChunksFetched_;
        if (((bitField0_ & 0x00000040) != 0)) {
          remoteMergedBytesRead_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000040);
        }
        result.remoteMergedBytesRead_ = remoteMergedBytesRead_;
        if (((bitField0_ & 0x00000080) != 0)) {
          localMergedBytesRead_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000080);
        }
        result.localMergedBytesRead_ = localMergedBytesRead_;
        if (((bitField0_ & 0x00000100) != 0)) {
          remoteMergedReqsDuration_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000100);
        }
        result.remoteMergedReqsDuration_ = remoteMergedReqsDuration_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions.getDefaultInstance()) return this;
        if (!other.corruptMergedBlockChunks_.isEmpty()) {
          if (corruptMergedBlockChunks_.isEmpty()) {
            corruptMergedBlockChunks_ = other.corruptMergedBlockChunks_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureCorruptMergedBlockChunksIsMutable();
            corruptMergedBlockChunks_.addAll(other.corruptMergedBlockChunks_);
          }
          onChanged();
        }
        if (!other.mergedFetchFallbackCount_.isEmpty()) {
          if (mergedFetchFallbackCount_.isEmpty()) {
            mergedFetchFallbackCount_ = other.mergedFetchFallbackCount_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureMergedFetchFallbackCountIsMutable();
            mergedFetchFallbackCount_.addAll(other.mergedFetchFallbackCount_);
          }
          onChanged();
        }
        if (!other.remoteMergedBlocksFetched_.isEmpty()) {
          if (remoteMergedBlocksFetched_.isEmpty()) {
            remoteMergedBlocksFetched_ = other.remoteMergedBlocksFetched_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureRemoteMergedBlocksFetchedIsMutable();
            remoteMergedBlocksFetched_.addAll(other.remoteMergedBlocksFetched_);
          }
          onChanged();
        }
        if (!other.localMergedBlocksFetched_.isEmpty()) {
          if (localMergedBlocksFetched_.isEmpty()) {
            localMergedBlocksFetched_ = other.localMergedBlocksFetched_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureLocalMergedBlocksFetchedIsMutable();
            localMergedBlocksFetched_.addAll(other.localMergedBlocksFetched_);
          }
          onChanged();
        }
        if (!other.remoteMergedChunksFetched_.isEmpty()) {
          if (remoteMergedChunksFetched_.isEmpty()) {
            remoteMergedChunksFetched_ = other.remoteMergedChunksFetched_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureRemoteMergedChunksFetchedIsMutable();
            remoteMergedChunksFetched_.addAll(other.remoteMergedChunksFetched_);
          }
          onChanged();
        }
        if (!other.localMergedChunksFetched_.isEmpty()) {
          if (localMergedChunksFetched_.isEmpty()) {
            localMergedChunksFetched_ = other.localMergedChunksFetched_;
            bitField0_ = (bitField0_ & ~0x00000020);
          } else {
            ensureLocalMergedChunksFetchedIsMutable();
            localMergedChunksFetched_.addAll(other.localMergedChunksFetched_);
          }
          onChanged();
        }
        if (!other.remoteMergedBytesRead_.isEmpty()) {
          if (remoteMergedBytesRead_.isEmpty()) {
            remoteMergedBytesRead_ = other.remoteMergedBytesRead_;
            bitField0_ = (bitField0_ & ~0x00000040);
          } else {
            ensureRemoteMergedBytesReadIsMutable();
            remoteMergedBytesRead_.addAll(other.remoteMergedBytesRead_);
          }
          onChanged();
        }
        if (!other.localMergedBytesRead_.isEmpty()) {
          if (localMergedBytesRead_.isEmpty()) {
            localMergedBytesRead_ = other.localMergedBytesRead_;
            bitField0_ = (bitField0_ & ~0x00000080);
          } else {
            ensureLocalMergedBytesReadIsMutable();
            localMergedBytesRead_.addAll(other.localMergedBytesRead_);
          }
          onChanged();
        }
        if (!other.remoteMergedReqsDuration_.isEmpty()) {
          if (remoteMergedReqsDuration_.isEmpty()) {
            remoteMergedReqsDuration_ = other.remoteMergedReqsDuration_;
            bitField0_ = (bitField0_ & ~0x00000100);
          } else {
            ensureRemoteMergedReqsDurationIsMutable();
            remoteMergedReqsDuration_.addAll(other.remoteMergedReqsDuration_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.DoubleList corruptMergedBlockChunks_ = emptyDoubleList();
      private void ensureCorruptMergedBlockChunksIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          corruptMergedBlockChunks_ = mutableCopy(corruptMergedBlockChunks_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated double corrupt_merged_block_chunks = 1;</code>
       * @return A list containing the corruptMergedBlockChunks.
       */
      public java.util.List<java.lang.Double>
          getCorruptMergedBlockChunksList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(corruptMergedBlockChunks_) : corruptMergedBlockChunks_;
      }
      /**
       * <code>repeated double corrupt_merged_block_chunks = 1;</code>
       * @return The count of corruptMergedBlockChunks.
       */
      public int getCorruptMergedBlockChunksCount() {
        return corruptMergedBlockChunks_.size();
      }
      /**
       * <code>repeated double corrupt_merged_block_chunks = 1;</code>
       * @param index The index of the element to return.
       * @return The corruptMergedBlockChunks at the given index.
       */
      public double getCorruptMergedBlockChunks(int index) {
        return corruptMergedBlockChunks_.getDouble(index);
      }
      /**
       * <code>repeated double corrupt_merged_block_chunks = 1;</code>
       * @param index The index to set the value at.
       * @param value The corruptMergedBlockChunks to set.
       * @return This builder for chaining.
       */
      public Builder setCorruptMergedBlockChunks(
          int index, double value) {
        ensureCorruptMergedBlockChunksIsMutable();
        corruptMergedBlockChunks_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double corrupt_merged_block_chunks = 1;</code>
       * @param value The corruptMergedBlockChunks to add.
       * @return This builder for chaining.
       */
      public Builder addCorruptMergedBlockChunks(double value) {
        ensureCorruptMergedBlockChunksIsMutable();
        corruptMergedBlockChunks_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double corrupt_merged_block_chunks = 1;</code>
       * @param values The corruptMergedBlockChunks to add.
       * @return This builder for chaining.
       */
      public Builder addAllCorruptMergedBlockChunks(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureCorruptMergedBlockChunksIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, corruptMergedBlockChunks_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double corrupt_merged_block_chunks = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearCorruptMergedBlockChunks() {
        corruptMergedBlockChunks_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList mergedFetchFallbackCount_ = emptyDoubleList();
      private void ensureMergedFetchFallbackCountIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          mergedFetchFallbackCount_ = mutableCopy(mergedFetchFallbackCount_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated double merged_fetch_fallback_count = 2;</code>
       * @return A list containing the mergedFetchFallbackCount.
       */
      public java.util.List<java.lang.Double>
          getMergedFetchFallbackCountList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(mergedFetchFallbackCount_) : mergedFetchFallbackCount_;
      }
      /**
       * <code>repeated double merged_fetch_fallback_count = 2;</code>
       * @return The count of mergedFetchFallbackCount.
       */
      public int getMergedFetchFallbackCountCount() {
        return mergedFetchFallbackCount_.size();
      }
      /**
       * <code>repeated double merged_fetch_fallback_count = 2;</code>
       * @param index The index of the element to return.
       * @return The mergedFetchFallbackCount at the given index.
       */
      public double getMergedFetchFallbackCount(int index) {
        return mergedFetchFallbackCount_.getDouble(index);
      }
      /**
       * <code>repeated double merged_fetch_fallback_count = 2;</code>
       * @param index The index to set the value at.
       * @param value The mergedFetchFallbackCount to set.
       * @return This builder for chaining.
       */
      public Builder setMergedFetchFallbackCount(
          int index, double value) {
        ensureMergedFetchFallbackCountIsMutable();
        mergedFetchFallbackCount_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double merged_fetch_fallback_count = 2;</code>
       * @param value The mergedFetchFallbackCount to add.
       * @return This builder for chaining.
       */
      public Builder addMergedFetchFallbackCount(double value) {
        ensureMergedFetchFallbackCountIsMutable();
        mergedFetchFallbackCount_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double merged_fetch_fallback_count = 2;</code>
       * @param values The mergedFetchFallbackCount to add.
       * @return This builder for chaining.
       */
      public Builder addAllMergedFetchFallbackCount(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureMergedFetchFallbackCountIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, mergedFetchFallbackCount_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double merged_fetch_fallback_count = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearMergedFetchFallbackCount() {
        mergedFetchFallbackCount_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList remoteMergedBlocksFetched_ = emptyDoubleList();
      private void ensureRemoteMergedBlocksFetchedIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          remoteMergedBlocksFetched_ = mutableCopy(remoteMergedBlocksFetched_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated double remote_merged_blocks_fetched = 3;</code>
       * @return A list containing the remoteMergedBlocksFetched.
       */
      public java.util.List<java.lang.Double>
          getRemoteMergedBlocksFetchedList() {
        return ((bitField0_ & 0x00000004) != 0) ?
                 java.util.Collections.unmodifiableList(remoteMergedBlocksFetched_) : remoteMergedBlocksFetched_;
      }
      /**
       * <code>repeated double remote_merged_blocks_fetched = 3;</code>
       * @return The count of remoteMergedBlocksFetched.
       */
      public int getRemoteMergedBlocksFetchedCount() {
        return remoteMergedBlocksFetched_.size();
      }
      /**
       * <code>repeated double remote_merged_blocks_fetched = 3;</code>
       * @param index The index of the element to return.
       * @return The remoteMergedBlocksFetched at the given index.
       */
      public double getRemoteMergedBlocksFetched(int index) {
        return remoteMergedBlocksFetched_.getDouble(index);
      }
      /**
       * <code>repeated double remote_merged_blocks_fetched = 3;</code>
       * @param index The index to set the value at.
       * @param value The remoteMergedBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteMergedBlocksFetched(
          int index, double value) {
        ensureRemoteMergedBlocksFetchedIsMutable();
        remoteMergedBlocksFetched_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_blocks_fetched = 3;</code>
       * @param value The remoteMergedBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addRemoteMergedBlocksFetched(double value) {
        ensureRemoteMergedBlocksFetchedIsMutable();
        remoteMergedBlocksFetched_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_blocks_fetched = 3;</code>
       * @param values The remoteMergedBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addAllRemoteMergedBlocksFetched(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRemoteMergedBlocksFetchedIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, remoteMergedBlocksFetched_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_blocks_fetched = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteMergedBlocksFetched() {
        remoteMergedBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList localMergedBlocksFetched_ = emptyDoubleList();
      private void ensureLocalMergedBlocksFetchedIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          localMergedBlocksFetched_ = mutableCopy(localMergedBlocksFetched_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated double local_merged_blocks_fetched = 4;</code>
       * @return A list containing the localMergedBlocksFetched.
       */
      public java.util.List<java.lang.Double>
          getLocalMergedBlocksFetchedList() {
        return ((bitField0_ & 0x00000008) != 0) ?
                 java.util.Collections.unmodifiableList(localMergedBlocksFetched_) : localMergedBlocksFetched_;
      }
      /**
       * <code>repeated double local_merged_blocks_fetched = 4;</code>
       * @return The count of localMergedBlocksFetched.
       */
      public int getLocalMergedBlocksFetchedCount() {
        return localMergedBlocksFetched_.size();
      }
      /**
       * <code>repeated double local_merged_blocks_fetched = 4;</code>
       * @param index The index of the element to return.
       * @return The localMergedBlocksFetched at the given index.
       */
      public double getLocalMergedBlocksFetched(int index) {
        return localMergedBlocksFetched_.getDouble(index);
      }
      /**
       * <code>repeated double local_merged_blocks_fetched = 4;</code>
       * @param index The index to set the value at.
       * @param value The localMergedBlocksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setLocalMergedBlocksFetched(
          int index, double value) {
        ensureLocalMergedBlocksFetchedIsMutable();
        localMergedBlocksFetched_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_blocks_fetched = 4;</code>
       * @param value The localMergedBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addLocalMergedBlocksFetched(double value) {
        ensureLocalMergedBlocksFetchedIsMutable();
        localMergedBlocksFetched_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_blocks_fetched = 4;</code>
       * @param values The localMergedBlocksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addAllLocalMergedBlocksFetched(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureLocalMergedBlocksFetchedIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, localMergedBlocksFetched_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_blocks_fetched = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalMergedBlocksFetched() {
        localMergedBlocksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList remoteMergedChunksFetched_ = emptyDoubleList();
      private void ensureRemoteMergedChunksFetchedIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          remoteMergedChunksFetched_ = mutableCopy(remoteMergedChunksFetched_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <code>repeated double remote_merged_chunks_fetched = 5;</code>
       * @return A list containing the remoteMergedChunksFetched.
       */
      public java.util.List<java.lang.Double>
          getRemoteMergedChunksFetchedList() {
        return ((bitField0_ & 0x00000010) != 0) ?
                 java.util.Collections.unmodifiableList(remoteMergedChunksFetched_) : remoteMergedChunksFetched_;
      }
      /**
       * <code>repeated double remote_merged_chunks_fetched = 5;</code>
       * @return The count of remoteMergedChunksFetched.
       */
      public int getRemoteMergedChunksFetchedCount() {
        return remoteMergedChunksFetched_.size();
      }
      /**
       * <code>repeated double remote_merged_chunks_fetched = 5;</code>
       * @param index The index of the element to return.
       * @return The remoteMergedChunksFetched at the given index.
       */
      public double getRemoteMergedChunksFetched(int index) {
        return remoteMergedChunksFetched_.getDouble(index);
      }
      /**
       * <code>repeated double remote_merged_chunks_fetched = 5;</code>
       * @param index The index to set the value at.
       * @param value The remoteMergedChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteMergedChunksFetched(
          int index, double value) {
        ensureRemoteMergedChunksFetchedIsMutable();
        remoteMergedChunksFetched_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_chunks_fetched = 5;</code>
       * @param value The remoteMergedChunksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addRemoteMergedChunksFetched(double value) {
        ensureRemoteMergedChunksFetchedIsMutable();
        remoteMergedChunksFetched_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_chunks_fetched = 5;</code>
       * @param values The remoteMergedChunksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addAllRemoteMergedChunksFetched(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRemoteMergedChunksFetchedIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, remoteMergedChunksFetched_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_chunks_fetched = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteMergedChunksFetched() {
        remoteMergedChunksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList localMergedChunksFetched_ = emptyDoubleList();
      private void ensureLocalMergedChunksFetchedIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          localMergedChunksFetched_ = mutableCopy(localMergedChunksFetched_);
          bitField0_ |= 0x00000020;
         }
      }
      /**
       * <code>repeated double local_merged_chunks_fetched = 6;</code>
       * @return A list containing the localMergedChunksFetched.
       */
      public java.util.List<java.lang.Double>
          getLocalMergedChunksFetchedList() {
        return ((bitField0_ & 0x00000020) != 0) ?
                 java.util.Collections.unmodifiableList(localMergedChunksFetched_) : localMergedChunksFetched_;
      }
      /**
       * <code>repeated double local_merged_chunks_fetched = 6;</code>
       * @return The count of localMergedChunksFetched.
       */
      public int getLocalMergedChunksFetchedCount() {
        return localMergedChunksFetched_.size();
      }
      /**
       * <code>repeated double local_merged_chunks_fetched = 6;</code>
       * @param index The index of the element to return.
       * @return The localMergedChunksFetched at the given index.
       */
      public double getLocalMergedChunksFetched(int index) {
        return localMergedChunksFetched_.getDouble(index);
      }
      /**
       * <code>repeated double local_merged_chunks_fetched = 6;</code>
       * @param index The index to set the value at.
       * @param value The localMergedChunksFetched to set.
       * @return This builder for chaining.
       */
      public Builder setLocalMergedChunksFetched(
          int index, double value) {
        ensureLocalMergedChunksFetchedIsMutable();
        localMergedChunksFetched_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_chunks_fetched = 6;</code>
       * @param value The localMergedChunksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addLocalMergedChunksFetched(double value) {
        ensureLocalMergedChunksFetchedIsMutable();
        localMergedChunksFetched_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_chunks_fetched = 6;</code>
       * @param values The localMergedChunksFetched to add.
       * @return This builder for chaining.
       */
      public Builder addAllLocalMergedChunksFetched(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureLocalMergedChunksFetchedIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, localMergedChunksFetched_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_chunks_fetched = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalMergedChunksFetched() {
        localMergedChunksFetched_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000020);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList remoteMergedBytesRead_ = emptyDoubleList();
      private void ensureRemoteMergedBytesReadIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          remoteMergedBytesRead_ = mutableCopy(remoteMergedBytesRead_);
          bitField0_ |= 0x00000040;
         }
      }
      /**
       * <code>repeated double remote_merged_bytes_read = 7;</code>
       * @return A list containing the remoteMergedBytesRead.
       */
      public java.util.List<java.lang.Double>
          getRemoteMergedBytesReadList() {
        return ((bitField0_ & 0x00000040) != 0) ?
                 java.util.Collections.unmodifiableList(remoteMergedBytesRead_) : remoteMergedBytesRead_;
      }
      /**
       * <code>repeated double remote_merged_bytes_read = 7;</code>
       * @return The count of remoteMergedBytesRead.
       */
      public int getRemoteMergedBytesReadCount() {
        return remoteMergedBytesRead_.size();
      }
      /**
       * <code>repeated double remote_merged_bytes_read = 7;</code>
       * @param index The index of the element to return.
       * @return The remoteMergedBytesRead at the given index.
       */
      public double getRemoteMergedBytesRead(int index) {
        return remoteMergedBytesRead_.getDouble(index);
      }
      /**
       * <code>repeated double remote_merged_bytes_read = 7;</code>
       * @param index The index to set the value at.
       * @param value The remoteMergedBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteMergedBytesRead(
          int index, double value) {
        ensureRemoteMergedBytesReadIsMutable();
        remoteMergedBytesRead_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_bytes_read = 7;</code>
       * @param value The remoteMergedBytesRead to add.
       * @return This builder for chaining.
       */
      public Builder addRemoteMergedBytesRead(double value) {
        ensureRemoteMergedBytesReadIsMutable();
        remoteMergedBytesRead_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_bytes_read = 7;</code>
       * @param values The remoteMergedBytesRead to add.
       * @return This builder for chaining.
       */
      public Builder addAllRemoteMergedBytesRead(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRemoteMergedBytesReadIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, remoteMergedBytesRead_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_bytes_read = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteMergedBytesRead() {
        remoteMergedBytesRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000040);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList localMergedBytesRead_ = emptyDoubleList();
      private void ensureLocalMergedBytesReadIsMutable() {
        if (!((bitField0_ & 0x00000080) != 0)) {
          localMergedBytesRead_ = mutableCopy(localMergedBytesRead_);
          bitField0_ |= 0x00000080;
         }
      }
      /**
       * <code>repeated double local_merged_bytes_read = 8;</code>
       * @return A list containing the localMergedBytesRead.
       */
      public java.util.List<java.lang.Double>
          getLocalMergedBytesReadList() {
        return ((bitField0_ & 0x00000080) != 0) ?
                 java.util.Collections.unmodifiableList(localMergedBytesRead_) : localMergedBytesRead_;
      }
      /**
       * <code>repeated double local_merged_bytes_read = 8;</code>
       * @return The count of localMergedBytesRead.
       */
      public int getLocalMergedBytesReadCount() {
        return localMergedBytesRead_.size();
      }
      /**
       * <code>repeated double local_merged_bytes_read = 8;</code>
       * @param index The index of the element to return.
       * @return The localMergedBytesRead at the given index.
       */
      public double getLocalMergedBytesRead(int index) {
        return localMergedBytesRead_.getDouble(index);
      }
      /**
       * <code>repeated double local_merged_bytes_read = 8;</code>
       * @param index The index to set the value at.
       * @param value The localMergedBytesRead to set.
       * @return This builder for chaining.
       */
      public Builder setLocalMergedBytesRead(
          int index, double value) {
        ensureLocalMergedBytesReadIsMutable();
        localMergedBytesRead_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_bytes_read = 8;</code>
       * @param value The localMergedBytesRead to add.
       * @return This builder for chaining.
       */
      public Builder addLocalMergedBytesRead(double value) {
        ensureLocalMergedBytesReadIsMutable();
        localMergedBytesRead_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_bytes_read = 8;</code>
       * @param values The localMergedBytesRead to add.
       * @return This builder for chaining.
       */
      public Builder addAllLocalMergedBytesRead(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureLocalMergedBytesReadIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, localMergedBytesRead_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double local_merged_bytes_read = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearLocalMergedBytesRead() {
        localMergedBytesRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000080);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList remoteMergedReqsDuration_ = emptyDoubleList();
      private void ensureRemoteMergedReqsDurationIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          remoteMergedReqsDuration_ = mutableCopy(remoteMergedReqsDuration_);
          bitField0_ |= 0x00000100;
         }
      }
      /**
       * <code>repeated double remote_merged_reqs_duration = 9;</code>
       * @return A list containing the remoteMergedReqsDuration.
       */
      public java.util.List<java.lang.Double>
          getRemoteMergedReqsDurationList() {
        return ((bitField0_ & 0x00000100) != 0) ?
                 java.util.Collections.unmodifiableList(remoteMergedReqsDuration_) : remoteMergedReqsDuration_;
      }
      /**
       * <code>repeated double remote_merged_reqs_duration = 9;</code>
       * @return The count of remoteMergedReqsDuration.
       */
      public int getRemoteMergedReqsDurationCount() {
        return remoteMergedReqsDuration_.size();
      }
      /**
       * <code>repeated double remote_merged_reqs_duration = 9;</code>
       * @param index The index of the element to return.
       * @return The remoteMergedReqsDuration at the given index.
       */
      public double getRemoteMergedReqsDuration(int index) {
        return remoteMergedReqsDuration_.getDouble(index);
      }
      /**
       * <code>repeated double remote_merged_reqs_duration = 9;</code>
       * @param index The index to set the value at.
       * @param value The remoteMergedReqsDuration to set.
       * @return This builder for chaining.
       */
      public Builder setRemoteMergedReqsDuration(
          int index, double value) {
        ensureRemoteMergedReqsDurationIsMutable();
        remoteMergedReqsDuration_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_reqs_duration = 9;</code>
       * @param value The remoteMergedReqsDuration to add.
       * @return This builder for chaining.
       */
      public Builder addRemoteMergedReqsDuration(double value) {
        ensureRemoteMergedReqsDurationIsMutable();
        remoteMergedReqsDuration_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_reqs_duration = 9;</code>
       * @param values The remoteMergedReqsDuration to add.
       * @return This builder for chaining.
       */
      public Builder addAllRemoteMergedReqsDuration(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureRemoteMergedReqsDurationIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, remoteMergedReqsDuration_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double remote_merged_reqs_duration = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearRemoteMergedReqsDuration() {
        remoteMergedReqsDuration_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000100);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ShufflePushReadMetricDistributions)
    private static final org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ShufflePushReadMetricDistributions>
        PARSER = new com.google.protobuf.AbstractParser<ShufflePushReadMetricDistributions>() {
      @java.lang.Override
      public ShufflePushReadMetricDistributions parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ShufflePushReadMetricDistributions(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ShufflePushReadMetricDistributions> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ShufflePushReadMetricDistributions> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShufflePushReadMetricDistributions getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ShuffleWriteMetricDistributionsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated double write_bytes = 1;</code>
     * @return A list containing the writeBytes.
     */
    java.util.List<java.lang.Double> getWriteBytesList();
    /**
     * <code>repeated double write_bytes = 1;</code>
     * @return The count of writeBytes.
     */
    int getWriteBytesCount();
    /**
     * <code>repeated double write_bytes = 1;</code>
     * @param index The index of the element to return.
     * @return The writeBytes at the given index.
     */
    double getWriteBytes(int index);

    /**
     * <code>repeated double write_records = 2;</code>
     * @return A list containing the writeRecords.
     */
    java.util.List<java.lang.Double> getWriteRecordsList();
    /**
     * <code>repeated double write_records = 2;</code>
     * @return The count of writeRecords.
     */
    int getWriteRecordsCount();
    /**
     * <code>repeated double write_records = 2;</code>
     * @param index The index of the element to return.
     * @return The writeRecords at the given index.
     */
    double getWriteRecords(int index);

    /**
     * <code>repeated double write_time = 3;</code>
     * @return A list containing the writeTime.
     */
    java.util.List<java.lang.Double> getWriteTimeList();
    /**
     * <code>repeated double write_time = 3;</code>
     * @return The count of writeTime.
     */
    int getWriteTimeCount();
    /**
     * <code>repeated double write_time = 3;</code>
     * @param index The index of the element to return.
     * @return The writeTime at the given index.
     */
    double getWriteTime(int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions}
   */
  public static final class ShuffleWriteMetricDistributions extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions)
      ShuffleWriteMetricDistributionsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ShuffleWriteMetricDistributions.newBuilder() to construct.
    private ShuffleWriteMetricDistributions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ShuffleWriteMetricDistributions() {
      writeBytes_ = emptyDoubleList();
      writeRecords_ = emptyDoubleList();
      writeTime_ = emptyDoubleList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ShuffleWriteMetricDistributions();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ShuffleWriteMetricDistributions(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                writeBytes_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              writeBytes_.addDouble(input.readDouble());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                writeBytes_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                writeBytes_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 17: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                writeRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              writeRecords_.addDouble(input.readDouble());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                writeRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                writeRecords_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 25: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                writeTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              writeTime_.addDouble(input.readDouble());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000004) != 0) && input.getBytesUntilLimit() > 0) {
                writeTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              while (input.getBytesUntilLimit() > 0) {
                writeTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          writeBytes_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          writeRecords_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          writeTime_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.Builder.class);
    }

    public static final int WRITE_BYTES_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.DoubleList writeBytes_;
    /**
     * <code>repeated double write_bytes = 1;</code>
     * @return A list containing the writeBytes.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getWriteBytesList() {
      return writeBytes_;
    }
    /**
     * <code>repeated double write_bytes = 1;</code>
     * @return The count of writeBytes.
     */
    public int getWriteBytesCount() {
      return writeBytes_.size();
    }
    /**
     * <code>repeated double write_bytes = 1;</code>
     * @param index The index of the element to return.
     * @return The writeBytes at the given index.
     */
    public double getWriteBytes(int index) {
      return writeBytes_.getDouble(index);
    }
    private int writeBytesMemoizedSerializedSize = -1;

    public static final int WRITE_RECORDS_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.DoubleList writeRecords_;
    /**
     * <code>repeated double write_records = 2;</code>
     * @return A list containing the writeRecords.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getWriteRecordsList() {
      return writeRecords_;
    }
    /**
     * <code>repeated double write_records = 2;</code>
     * @return The count of writeRecords.
     */
    public int getWriteRecordsCount() {
      return writeRecords_.size();
    }
    /**
     * <code>repeated double write_records = 2;</code>
     * @param index The index of the element to return.
     * @return The writeRecords at the given index.
     */
    public double getWriteRecords(int index) {
      return writeRecords_.getDouble(index);
    }
    private int writeRecordsMemoizedSerializedSize = -1;

    public static final int WRITE_TIME_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.DoubleList writeTime_;
    /**
     * <code>repeated double write_time = 3;</code>
     * @return A list containing the writeTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getWriteTimeList() {
      return writeTime_;
    }
    /**
     * <code>repeated double write_time = 3;</code>
     * @return The count of writeTime.
     */
    public int getWriteTimeCount() {
      return writeTime_.size();
    }
    /**
     * <code>repeated double write_time = 3;</code>
     * @param index The index of the element to return.
     * @return The writeTime at the given index.
     */
    public double getWriteTime(int index) {
      return writeTime_.getDouble(index);
    }
    private int writeTimeMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getWriteBytesList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(writeBytesMemoizedSerializedSize);
      }
      for (int i = 0; i < writeBytes_.size(); i++) {
        output.writeDoubleNoTag(writeBytes_.getDouble(i));
      }
      if (getWriteRecordsList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(writeRecordsMemoizedSerializedSize);
      }
      for (int i = 0; i < writeRecords_.size(); i++) {
        output.writeDoubleNoTag(writeRecords_.getDouble(i));
      }
      if (getWriteTimeList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(writeTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < writeTime_.size(); i++) {
        output.writeDoubleNoTag(writeTime_.getDouble(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        dataSize = 8 * getWriteBytesList().size();
        size += dataSize;
        if (!getWriteBytesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        writeBytesMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getWriteRecordsList().size();
        size += dataSize;
        if (!getWriteRecordsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        writeRecordsMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getWriteTimeList().size();
        size += dataSize;
        if (!getWriteTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        writeTimeMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions other = (org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions) obj;

      if (!getWriteBytesList()
          .equals(other.getWriteBytesList())) return false;
      if (!getWriteRecordsList()
          .equals(other.getWriteRecordsList())) return false;
      if (!getWriteTimeList()
          .equals(other.getWriteTimeList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getWriteBytesCount() > 0) {
        hash = (37 * hash) + WRITE_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + getWriteBytesList().hashCode();
      }
      if (getWriteRecordsCount() > 0) {
        hash = (37 * hash) + WRITE_RECORDS_FIELD_NUMBER;
        hash = (53 * hash) + getWriteRecordsList().hashCode();
      }
      if (getWriteTimeCount() > 0) {
        hash = (37 * hash) + WRITE_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getWriteTimeList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions)
        org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        writeBytes_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        writeRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        writeTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions build() {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions result = new org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          writeBytes_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.writeBytes_ = writeBytes_;
        if (((bitField0_ & 0x00000002) != 0)) {
          writeRecords_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.writeRecords_ = writeRecords_;
        if (((bitField0_ & 0x00000004) != 0)) {
          writeTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.writeTime_ = writeTime_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions.getDefaultInstance()) return this;
        if (!other.writeBytes_.isEmpty()) {
          if (writeBytes_.isEmpty()) {
            writeBytes_ = other.writeBytes_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureWriteBytesIsMutable();
            writeBytes_.addAll(other.writeBytes_);
          }
          onChanged();
        }
        if (!other.writeRecords_.isEmpty()) {
          if (writeRecords_.isEmpty()) {
            writeRecords_ = other.writeRecords_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureWriteRecordsIsMutable();
            writeRecords_.addAll(other.writeRecords_);
          }
          onChanged();
        }
        if (!other.writeTime_.isEmpty()) {
          if (writeTime_.isEmpty()) {
            writeTime_ = other.writeTime_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureWriteTimeIsMutable();
            writeTime_.addAll(other.writeTime_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.DoubleList writeBytes_ = emptyDoubleList();
      private void ensureWriteBytesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          writeBytes_ = mutableCopy(writeBytes_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated double write_bytes = 1;</code>
       * @return A list containing the writeBytes.
       */
      public java.util.List<java.lang.Double>
          getWriteBytesList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(writeBytes_) : writeBytes_;
      }
      /**
       * <code>repeated double write_bytes = 1;</code>
       * @return The count of writeBytes.
       */
      public int getWriteBytesCount() {
        return writeBytes_.size();
      }
      /**
       * <code>repeated double write_bytes = 1;</code>
       * @param index The index of the element to return.
       * @return The writeBytes at the given index.
       */
      public double getWriteBytes(int index) {
        return writeBytes_.getDouble(index);
      }
      /**
       * <code>repeated double write_bytes = 1;</code>
       * @param index The index to set the value at.
       * @param value The writeBytes to set.
       * @return This builder for chaining.
       */
      public Builder setWriteBytes(
          int index, double value) {
        ensureWriteBytesIsMutable();
        writeBytes_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_bytes = 1;</code>
       * @param value The writeBytes to add.
       * @return This builder for chaining.
       */
      public Builder addWriteBytes(double value) {
        ensureWriteBytesIsMutable();
        writeBytes_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_bytes = 1;</code>
       * @param values The writeBytes to add.
       * @return This builder for chaining.
       */
      public Builder addAllWriteBytes(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureWriteBytesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, writeBytes_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_bytes = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearWriteBytes() {
        writeBytes_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList writeRecords_ = emptyDoubleList();
      private void ensureWriteRecordsIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          writeRecords_ = mutableCopy(writeRecords_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated double write_records = 2;</code>
       * @return A list containing the writeRecords.
       */
      public java.util.List<java.lang.Double>
          getWriteRecordsList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(writeRecords_) : writeRecords_;
      }
      /**
       * <code>repeated double write_records = 2;</code>
       * @return The count of writeRecords.
       */
      public int getWriteRecordsCount() {
        return writeRecords_.size();
      }
      /**
       * <code>repeated double write_records = 2;</code>
       * @param index The index of the element to return.
       * @return The writeRecords at the given index.
       */
      public double getWriteRecords(int index) {
        return writeRecords_.getDouble(index);
      }
      /**
       * <code>repeated double write_records = 2;</code>
       * @param index The index to set the value at.
       * @param value The writeRecords to set.
       * @return This builder for chaining.
       */
      public Builder setWriteRecords(
          int index, double value) {
        ensureWriteRecordsIsMutable();
        writeRecords_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_records = 2;</code>
       * @param value The writeRecords to add.
       * @return This builder for chaining.
       */
      public Builder addWriteRecords(double value) {
        ensureWriteRecordsIsMutable();
        writeRecords_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_records = 2;</code>
       * @param values The writeRecords to add.
       * @return This builder for chaining.
       */
      public Builder addAllWriteRecords(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureWriteRecordsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, writeRecords_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_records = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearWriteRecords() {
        writeRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList writeTime_ = emptyDoubleList();
      private void ensureWriteTimeIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          writeTime_ = mutableCopy(writeTime_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated double write_time = 3;</code>
       * @return A list containing the writeTime.
       */
      public java.util.List<java.lang.Double>
          getWriteTimeList() {
        return ((bitField0_ & 0x00000004) != 0) ?
                 java.util.Collections.unmodifiableList(writeTime_) : writeTime_;
      }
      /**
       * <code>repeated double write_time = 3;</code>
       * @return The count of writeTime.
       */
      public int getWriteTimeCount() {
        return writeTime_.size();
      }
      /**
       * <code>repeated double write_time = 3;</code>
       * @param index The index of the element to return.
       * @return The writeTime at the given index.
       */
      public double getWriteTime(int index) {
        return writeTime_.getDouble(index);
      }
      /**
       * <code>repeated double write_time = 3;</code>
       * @param index The index to set the value at.
       * @param value The writeTime to set.
       * @return This builder for chaining.
       */
      public Builder setWriteTime(
          int index, double value) {
        ensureWriteTimeIsMutable();
        writeTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_time = 3;</code>
       * @param value The writeTime to add.
       * @return This builder for chaining.
       */
      public Builder addWriteTime(double value) {
        ensureWriteTimeIsMutable();
        writeTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_time = 3;</code>
       * @param values The writeTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllWriteTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureWriteTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, writeTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double write_time = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearWriteTime() {
        writeTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ShuffleWriteMetricDistributions)
    private static final org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ShuffleWriteMetricDistributions>
        PARSER = new com.google.protobuf.AbstractParser<ShuffleWriteMetricDistributions>() {
      @java.lang.Override
      public ShuffleWriteMetricDistributions parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ShuffleWriteMetricDistributions(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ShuffleWriteMetricDistributions> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ShuffleWriteMetricDistributions> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ShuffleWriteMetricDistributions getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExecutorMetricsDistributionsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ExecutorMetricsDistributions)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated double quantiles = 1;</code>
     * @return A list containing the quantiles.
     */
    java.util.List<java.lang.Double> getQuantilesList();
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return The count of quantiles.
     */
    int getQuantilesCount();
    /**
     * <code>repeated double quantiles = 1;</code>
     * @param index The index of the element to return.
     * @return The quantiles at the given index.
     */
    double getQuantiles(int index);

    /**
     * <code>repeated double task_time = 2;</code>
     * @return A list containing the taskTime.
     */
    java.util.List<java.lang.Double> getTaskTimeList();
    /**
     * <code>repeated double task_time = 2;</code>
     * @return The count of taskTime.
     */
    int getTaskTimeCount();
    /**
     * <code>repeated double task_time = 2;</code>
     * @param index The index of the element to return.
     * @return The taskTime at the given index.
     */
    double getTaskTime(int index);

    /**
     * <code>repeated double failed_tasks = 3;</code>
     * @return A list containing the failedTasks.
     */
    java.util.List<java.lang.Double> getFailedTasksList();
    /**
     * <code>repeated double failed_tasks = 3;</code>
     * @return The count of failedTasks.
     */
    int getFailedTasksCount();
    /**
     * <code>repeated double failed_tasks = 3;</code>
     * @param index The index of the element to return.
     * @return The failedTasks at the given index.
     */
    double getFailedTasks(int index);

    /**
     * <code>repeated double succeeded_tasks = 4;</code>
     * @return A list containing the succeededTasks.
     */
    java.util.List<java.lang.Double> getSucceededTasksList();
    /**
     * <code>repeated double succeeded_tasks = 4;</code>
     * @return The count of succeededTasks.
     */
    int getSucceededTasksCount();
    /**
     * <code>repeated double succeeded_tasks = 4;</code>
     * @param index The index of the element to return.
     * @return The succeededTasks at the given index.
     */
    double getSucceededTasks(int index);

    /**
     * <code>repeated double killed_tasks = 5;</code>
     * @return A list containing the killedTasks.
     */
    java.util.List<java.lang.Double> getKilledTasksList();
    /**
     * <code>repeated double killed_tasks = 5;</code>
     * @return The count of killedTasks.
     */
    int getKilledTasksCount();
    /**
     * <code>repeated double killed_tasks = 5;</code>
     * @param index The index of the element to return.
     * @return The killedTasks at the given index.
     */
    double getKilledTasks(int index);

    /**
     * <code>repeated double input_bytes = 6;</code>
     * @return A list containing the inputBytes.
     */
    java.util.List<java.lang.Double> getInputBytesList();
    /**
     * <code>repeated double input_bytes = 6;</code>
     * @return The count of inputBytes.
     */
    int getInputBytesCount();
    /**
     * <code>repeated double input_bytes = 6;</code>
     * @param index The index of the element to return.
     * @return The inputBytes at the given index.
     */
    double getInputBytes(int index);

    /**
     * <code>repeated double input_records = 7;</code>
     * @return A list containing the inputRecords.
     */
    java.util.List<java.lang.Double> getInputRecordsList();
    /**
     * <code>repeated double input_records = 7;</code>
     * @return The count of inputRecords.
     */
    int getInputRecordsCount();
    /**
     * <code>repeated double input_records = 7;</code>
     * @param index The index of the element to return.
     * @return The inputRecords at the given index.
     */
    double getInputRecords(int index);

    /**
     * <code>repeated double output_bytes = 8;</code>
     * @return A list containing the outputBytes.
     */
    java.util.List<java.lang.Double> getOutputBytesList();
    /**
     * <code>repeated double output_bytes = 8;</code>
     * @return The count of outputBytes.
     */
    int getOutputBytesCount();
    /**
     * <code>repeated double output_bytes = 8;</code>
     * @param index The index of the element to return.
     * @return The outputBytes at the given index.
     */
    double getOutputBytes(int index);

    /**
     * <code>repeated double output_records = 9;</code>
     * @return A list containing the outputRecords.
     */
    java.util.List<java.lang.Double> getOutputRecordsList();
    /**
     * <code>repeated double output_records = 9;</code>
     * @return The count of outputRecords.
     */
    int getOutputRecordsCount();
    /**
     * <code>repeated double output_records = 9;</code>
     * @param index The index of the element to return.
     * @return The outputRecords at the given index.
     */
    double getOutputRecords(int index);

    /**
     * <code>repeated double shuffle_read = 10;</code>
     * @return A list containing the shuffleRead.
     */
    java.util.List<java.lang.Double> getShuffleReadList();
    /**
     * <code>repeated double shuffle_read = 10;</code>
     * @return The count of shuffleRead.
     */
    int getShuffleReadCount();
    /**
     * <code>repeated double shuffle_read = 10;</code>
     * @param index The index of the element to return.
     * @return The shuffleRead at the given index.
     */
    double getShuffleRead(int index);

    /**
     * <code>repeated double shuffle_read_records = 11;</code>
     * @return A list containing the shuffleReadRecords.
     */
    java.util.List<java.lang.Double> getShuffleReadRecordsList();
    /**
     * <code>repeated double shuffle_read_records = 11;</code>
     * @return The count of shuffleReadRecords.
     */
    int getShuffleReadRecordsCount();
    /**
     * <code>repeated double shuffle_read_records = 11;</code>
     * @param index The index of the element to return.
     * @return The shuffleReadRecords at the given index.
     */
    double getShuffleReadRecords(int index);

    /**
     * <code>repeated double shuffle_write = 12;</code>
     * @return A list containing the shuffleWrite.
     */
    java.util.List<java.lang.Double> getShuffleWriteList();
    /**
     * <code>repeated double shuffle_write = 12;</code>
     * @return The count of shuffleWrite.
     */
    int getShuffleWriteCount();
    /**
     * <code>repeated double shuffle_write = 12;</code>
     * @param index The index of the element to return.
     * @return The shuffleWrite at the given index.
     */
    double getShuffleWrite(int index);

    /**
     * <code>repeated double shuffle_write_records = 13;</code>
     * @return A list containing the shuffleWriteRecords.
     */
    java.util.List<java.lang.Double> getShuffleWriteRecordsList();
    /**
     * <code>repeated double shuffle_write_records = 13;</code>
     * @return The count of shuffleWriteRecords.
     */
    int getShuffleWriteRecordsCount();
    /**
     * <code>repeated double shuffle_write_records = 13;</code>
     * @param index The index of the element to return.
     * @return The shuffleWriteRecords at the given index.
     */
    double getShuffleWriteRecords(int index);

    /**
     * <code>repeated double memory_bytes_spilled = 14;</code>
     * @return A list containing the memoryBytesSpilled.
     */
    java.util.List<java.lang.Double> getMemoryBytesSpilledList();
    /**
     * <code>repeated double memory_bytes_spilled = 14;</code>
     * @return The count of memoryBytesSpilled.
     */
    int getMemoryBytesSpilledCount();
    /**
     * <code>repeated double memory_bytes_spilled = 14;</code>
     * @param index The index of the element to return.
     * @return The memoryBytesSpilled at the given index.
     */
    double getMemoryBytesSpilled(int index);

    /**
     * <code>repeated double disk_bytes_spilled = 15;</code>
     * @return A list containing the diskBytesSpilled.
     */
    java.util.List<java.lang.Double> getDiskBytesSpilledList();
    /**
     * <code>repeated double disk_bytes_spilled = 15;</code>
     * @return The count of diskBytesSpilled.
     */
    int getDiskBytesSpilledCount();
    /**
     * <code>repeated double disk_bytes_spilled = 15;</code>
     * @param index The index of the element to return.
     * @return The diskBytesSpilled at the given index.
     */
    double getDiskBytesSpilled(int index);

    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
     * @return Whether the peakMemoryMetrics field is set.
     */
    boolean hasPeakMemoryMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
     * @return The peakMemoryMetrics.
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions getPeakMemoryMetrics();
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributionsOrBuilder getPeakMemoryMetricsOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorMetricsDistributions}
   */
  public static final class ExecutorMetricsDistributions extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ExecutorMetricsDistributions)
      ExecutorMetricsDistributionsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ExecutorMetricsDistributions.newBuilder() to construct.
    private ExecutorMetricsDistributions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ExecutorMetricsDistributions() {
      quantiles_ = emptyDoubleList();
      taskTime_ = emptyDoubleList();
      failedTasks_ = emptyDoubleList();
      succeededTasks_ = emptyDoubleList();
      killedTasks_ = emptyDoubleList();
      inputBytes_ = emptyDoubleList();
      inputRecords_ = emptyDoubleList();
      outputBytes_ = emptyDoubleList();
      outputRecords_ = emptyDoubleList();
      shuffleRead_ = emptyDoubleList();
      shuffleReadRecords_ = emptyDoubleList();
      shuffleWrite_ = emptyDoubleList();
      shuffleWriteRecords_ = emptyDoubleList();
      memoryBytesSpilled_ = emptyDoubleList();
      diskBytesSpilled_ = emptyDoubleList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ExecutorMetricsDistributions();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ExecutorMetricsDistributions(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                quantiles_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              quantiles_.addDouble(input.readDouble());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                quantiles_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                quantiles_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 17: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                taskTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              taskTime_.addDouble(input.readDouble());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                taskTime_ = newDoubleList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                taskTime_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 25: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                failedTasks_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              failedTasks_.addDouble(input.readDouble());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000004) != 0) && input.getBytesUntilLimit() > 0) {
                failedTasks_ = newDoubleList();
                mutable_bitField0_ |= 0x00000004;
              }
              while (input.getBytesUntilLimit() > 0) {
                failedTasks_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 33: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                succeededTasks_ = newDoubleList();
                mutable_bitField0_ |= 0x00000008;
              }
              succeededTasks_.addDouble(input.readDouble());
              break;
            }
            case 34: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000008) != 0) && input.getBytesUntilLimit() > 0) {
                succeededTasks_ = newDoubleList();
                mutable_bitField0_ |= 0x00000008;
              }
              while (input.getBytesUntilLimit() > 0) {
                succeededTasks_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 41: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                killedTasks_ = newDoubleList();
                mutable_bitField0_ |= 0x00000010;
              }
              killedTasks_.addDouble(input.readDouble());
              break;
            }
            case 42: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000010) != 0) && input.getBytesUntilLimit() > 0) {
                killedTasks_ = newDoubleList();
                mutable_bitField0_ |= 0x00000010;
              }
              while (input.getBytesUntilLimit() > 0) {
                killedTasks_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 49: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                inputBytes_ = newDoubleList();
                mutable_bitField0_ |= 0x00000020;
              }
              inputBytes_.addDouble(input.readDouble());
              break;
            }
            case 50: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000020) != 0) && input.getBytesUntilLimit() > 0) {
                inputBytes_ = newDoubleList();
                mutable_bitField0_ |= 0x00000020;
              }
              while (input.getBytesUntilLimit() > 0) {
                inputBytes_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 57: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                inputRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000040;
              }
              inputRecords_.addDouble(input.readDouble());
              break;
            }
            case 58: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000040) != 0) && input.getBytesUntilLimit() > 0) {
                inputRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000040;
              }
              while (input.getBytesUntilLimit() > 0) {
                inputRecords_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 65: {
              if (!((mutable_bitField0_ & 0x00000080) != 0)) {
                outputBytes_ = newDoubleList();
                mutable_bitField0_ |= 0x00000080;
              }
              outputBytes_.addDouble(input.readDouble());
              break;
            }
            case 66: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000080) != 0) && input.getBytesUntilLimit() > 0) {
                outputBytes_ = newDoubleList();
                mutable_bitField0_ |= 0x00000080;
              }
              while (input.getBytesUntilLimit() > 0) {
                outputBytes_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 73: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                outputRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000100;
              }
              outputRecords_.addDouble(input.readDouble());
              break;
            }
            case 74: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000100) != 0) && input.getBytesUntilLimit() > 0) {
                outputRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000100;
              }
              while (input.getBytesUntilLimit() > 0) {
                outputRecords_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 81: {
              if (!((mutable_bitField0_ & 0x00000200) != 0)) {
                shuffleRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000200;
              }
              shuffleRead_.addDouble(input.readDouble());
              break;
            }
            case 82: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000200) != 0) && input.getBytesUntilLimit() > 0) {
                shuffleRead_ = newDoubleList();
                mutable_bitField0_ |= 0x00000200;
              }
              while (input.getBytesUntilLimit() > 0) {
                shuffleRead_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 89: {
              if (!((mutable_bitField0_ & 0x00000400) != 0)) {
                shuffleReadRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000400;
              }
              shuffleReadRecords_.addDouble(input.readDouble());
              break;
            }
            case 90: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000400) != 0) && input.getBytesUntilLimit() > 0) {
                shuffleReadRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00000400;
              }
              while (input.getBytesUntilLimit() > 0) {
                shuffleReadRecords_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 97: {
              if (!((mutable_bitField0_ & 0x00000800) != 0)) {
                shuffleWrite_ = newDoubleList();
                mutable_bitField0_ |= 0x00000800;
              }
              shuffleWrite_.addDouble(input.readDouble());
              break;
            }
            case 98: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000800) != 0) && input.getBytesUntilLimit() > 0) {
                shuffleWrite_ = newDoubleList();
                mutable_bitField0_ |= 0x00000800;
              }
              while (input.getBytesUntilLimit() > 0) {
                shuffleWrite_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 105: {
              if (!((mutable_bitField0_ & 0x00001000) != 0)) {
                shuffleWriteRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00001000;
              }
              shuffleWriteRecords_.addDouble(input.readDouble());
              break;
            }
            case 106: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00001000) != 0) && input.getBytesUntilLimit() > 0) {
                shuffleWriteRecords_ = newDoubleList();
                mutable_bitField0_ |= 0x00001000;
              }
              while (input.getBytesUntilLimit() > 0) {
                shuffleWriteRecords_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 113: {
              if (!((mutable_bitField0_ & 0x00002000) != 0)) {
                memoryBytesSpilled_ = newDoubleList();
                mutable_bitField0_ |= 0x00002000;
              }
              memoryBytesSpilled_.addDouble(input.readDouble());
              break;
            }
            case 114: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00002000) != 0) && input.getBytesUntilLimit() > 0) {
                memoryBytesSpilled_ = newDoubleList();
                mutable_bitField0_ |= 0x00002000;
              }
              while (input.getBytesUntilLimit() > 0) {
                memoryBytesSpilled_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 121: {
              if (!((mutable_bitField0_ & 0x00004000) != 0)) {
                diskBytesSpilled_ = newDoubleList();
                mutable_bitField0_ |= 0x00004000;
              }
              diskBytesSpilled_.addDouble(input.readDouble());
              break;
            }
            case 122: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00004000) != 0) && input.getBytesUntilLimit() > 0) {
                diskBytesSpilled_ = newDoubleList();
                mutable_bitField0_ |= 0x00004000;
              }
              while (input.getBytesUntilLimit() > 0) {
                diskBytesSpilled_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 130: {
              org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.Builder subBuilder = null;
              if (peakMemoryMetrics_ != null) {
                subBuilder = peakMemoryMetrics_.toBuilder();
              }
              peakMemoryMetrics_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(peakMemoryMetrics_);
                peakMemoryMetrics_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          quantiles_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          taskTime_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          failedTasks_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          succeededTasks_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          killedTasks_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          inputBytes_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          inputRecords_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000080) != 0)) {
          outputBytes_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          outputRecords_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000200) != 0)) {
          shuffleRead_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000400) != 0)) {
          shuffleReadRecords_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000800) != 0)) {
          shuffleWrite_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00001000) != 0)) {
          shuffleWriteRecords_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00002000) != 0)) {
          memoryBytesSpilled_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00004000) != 0)) {
          diskBytesSpilled_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.Builder.class);
    }

    public static final int QUANTILES_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.DoubleList quantiles_;
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return A list containing the quantiles.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getQuantilesList() {
      return quantiles_;
    }
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return The count of quantiles.
     */
    public int getQuantilesCount() {
      return quantiles_.size();
    }
    /**
     * <code>repeated double quantiles = 1;</code>
     * @param index The index of the element to return.
     * @return The quantiles at the given index.
     */
    public double getQuantiles(int index) {
      return quantiles_.getDouble(index);
    }
    private int quantilesMemoizedSerializedSize = -1;

    public static final int TASK_TIME_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.DoubleList taskTime_;
    /**
     * <code>repeated double task_time = 2;</code>
     * @return A list containing the taskTime.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getTaskTimeList() {
      return taskTime_;
    }
    /**
     * <code>repeated double task_time = 2;</code>
     * @return The count of taskTime.
     */
    public int getTaskTimeCount() {
      return taskTime_.size();
    }
    /**
     * <code>repeated double task_time = 2;</code>
     * @param index The index of the element to return.
     * @return The taskTime at the given index.
     */
    public double getTaskTime(int index) {
      return taskTime_.getDouble(index);
    }
    private int taskTimeMemoizedSerializedSize = -1;

    public static final int FAILED_TASKS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.DoubleList failedTasks_;
    /**
     * <code>repeated double failed_tasks = 3;</code>
     * @return A list containing the failedTasks.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getFailedTasksList() {
      return failedTasks_;
    }
    /**
     * <code>repeated double failed_tasks = 3;</code>
     * @return The count of failedTasks.
     */
    public int getFailedTasksCount() {
      return failedTasks_.size();
    }
    /**
     * <code>repeated double failed_tasks = 3;</code>
     * @param index The index of the element to return.
     * @return The failedTasks at the given index.
     */
    public double getFailedTasks(int index) {
      return failedTasks_.getDouble(index);
    }
    private int failedTasksMemoizedSerializedSize = -1;

    public static final int SUCCEEDED_TASKS_FIELD_NUMBER = 4;
    private com.google.protobuf.Internal.DoubleList succeededTasks_;
    /**
     * <code>repeated double succeeded_tasks = 4;</code>
     * @return A list containing the succeededTasks.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getSucceededTasksList() {
      return succeededTasks_;
    }
    /**
     * <code>repeated double succeeded_tasks = 4;</code>
     * @return The count of succeededTasks.
     */
    public int getSucceededTasksCount() {
      return succeededTasks_.size();
    }
    /**
     * <code>repeated double succeeded_tasks = 4;</code>
     * @param index The index of the element to return.
     * @return The succeededTasks at the given index.
     */
    public double getSucceededTasks(int index) {
      return succeededTasks_.getDouble(index);
    }
    private int succeededTasksMemoizedSerializedSize = -1;

    public static final int KILLED_TASKS_FIELD_NUMBER = 5;
    private com.google.protobuf.Internal.DoubleList killedTasks_;
    /**
     * <code>repeated double killed_tasks = 5;</code>
     * @return A list containing the killedTasks.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getKilledTasksList() {
      return killedTasks_;
    }
    /**
     * <code>repeated double killed_tasks = 5;</code>
     * @return The count of killedTasks.
     */
    public int getKilledTasksCount() {
      return killedTasks_.size();
    }
    /**
     * <code>repeated double killed_tasks = 5;</code>
     * @param index The index of the element to return.
     * @return The killedTasks at the given index.
     */
    public double getKilledTasks(int index) {
      return killedTasks_.getDouble(index);
    }
    private int killedTasksMemoizedSerializedSize = -1;

    public static final int INPUT_BYTES_FIELD_NUMBER = 6;
    private com.google.protobuf.Internal.DoubleList inputBytes_;
    /**
     * <code>repeated double input_bytes = 6;</code>
     * @return A list containing the inputBytes.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getInputBytesList() {
      return inputBytes_;
    }
    /**
     * <code>repeated double input_bytes = 6;</code>
     * @return The count of inputBytes.
     */
    public int getInputBytesCount() {
      return inputBytes_.size();
    }
    /**
     * <code>repeated double input_bytes = 6;</code>
     * @param index The index of the element to return.
     * @return The inputBytes at the given index.
     */
    public double getInputBytes(int index) {
      return inputBytes_.getDouble(index);
    }
    private int inputBytesMemoizedSerializedSize = -1;

    public static final int INPUT_RECORDS_FIELD_NUMBER = 7;
    private com.google.protobuf.Internal.DoubleList inputRecords_;
    /**
     * <code>repeated double input_records = 7;</code>
     * @return A list containing the inputRecords.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getInputRecordsList() {
      return inputRecords_;
    }
    /**
     * <code>repeated double input_records = 7;</code>
     * @return The count of inputRecords.
     */
    public int getInputRecordsCount() {
      return inputRecords_.size();
    }
    /**
     * <code>repeated double input_records = 7;</code>
     * @param index The index of the element to return.
     * @return The inputRecords at the given index.
     */
    public double getInputRecords(int index) {
      return inputRecords_.getDouble(index);
    }
    private int inputRecordsMemoizedSerializedSize = -1;

    public static final int OUTPUT_BYTES_FIELD_NUMBER = 8;
    private com.google.protobuf.Internal.DoubleList outputBytes_;
    /**
     * <code>repeated double output_bytes = 8;</code>
     * @return A list containing the outputBytes.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getOutputBytesList() {
      return outputBytes_;
    }
    /**
     * <code>repeated double output_bytes = 8;</code>
     * @return The count of outputBytes.
     */
    public int getOutputBytesCount() {
      return outputBytes_.size();
    }
    /**
     * <code>repeated double output_bytes = 8;</code>
     * @param index The index of the element to return.
     * @return The outputBytes at the given index.
     */
    public double getOutputBytes(int index) {
      return outputBytes_.getDouble(index);
    }
    private int outputBytesMemoizedSerializedSize = -1;

    public static final int OUTPUT_RECORDS_FIELD_NUMBER = 9;
    private com.google.protobuf.Internal.DoubleList outputRecords_;
    /**
     * <code>repeated double output_records = 9;</code>
     * @return A list containing the outputRecords.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getOutputRecordsList() {
      return outputRecords_;
    }
    /**
     * <code>repeated double output_records = 9;</code>
     * @return The count of outputRecords.
     */
    public int getOutputRecordsCount() {
      return outputRecords_.size();
    }
    /**
     * <code>repeated double output_records = 9;</code>
     * @param index The index of the element to return.
     * @return The outputRecords at the given index.
     */
    public double getOutputRecords(int index) {
      return outputRecords_.getDouble(index);
    }
    private int outputRecordsMemoizedSerializedSize = -1;

    public static final int SHUFFLE_READ_FIELD_NUMBER = 10;
    private com.google.protobuf.Internal.DoubleList shuffleRead_;
    /**
     * <code>repeated double shuffle_read = 10;</code>
     * @return A list containing the shuffleRead.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getShuffleReadList() {
      return shuffleRead_;
    }
    /**
     * <code>repeated double shuffle_read = 10;</code>
     * @return The count of shuffleRead.
     */
    public int getShuffleReadCount() {
      return shuffleRead_.size();
    }
    /**
     * <code>repeated double shuffle_read = 10;</code>
     * @param index The index of the element to return.
     * @return The shuffleRead at the given index.
     */
    public double getShuffleRead(int index) {
      return shuffleRead_.getDouble(index);
    }
    private int shuffleReadMemoizedSerializedSize = -1;

    public static final int SHUFFLE_READ_RECORDS_FIELD_NUMBER = 11;
    private com.google.protobuf.Internal.DoubleList shuffleReadRecords_;
    /**
     * <code>repeated double shuffle_read_records = 11;</code>
     * @return A list containing the shuffleReadRecords.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getShuffleReadRecordsList() {
      return shuffleReadRecords_;
    }
    /**
     * <code>repeated double shuffle_read_records = 11;</code>
     * @return The count of shuffleReadRecords.
     */
    public int getShuffleReadRecordsCount() {
      return shuffleReadRecords_.size();
    }
    /**
     * <code>repeated double shuffle_read_records = 11;</code>
     * @param index The index of the element to return.
     * @return The shuffleReadRecords at the given index.
     */
    public double getShuffleReadRecords(int index) {
      return shuffleReadRecords_.getDouble(index);
    }
    private int shuffleReadRecordsMemoizedSerializedSize = -1;

    public static final int SHUFFLE_WRITE_FIELD_NUMBER = 12;
    private com.google.protobuf.Internal.DoubleList shuffleWrite_;
    /**
     * <code>repeated double shuffle_write = 12;</code>
     * @return A list containing the shuffleWrite.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getShuffleWriteList() {
      return shuffleWrite_;
    }
    /**
     * <code>repeated double shuffle_write = 12;</code>
     * @return The count of shuffleWrite.
     */
    public int getShuffleWriteCount() {
      return shuffleWrite_.size();
    }
    /**
     * <code>repeated double shuffle_write = 12;</code>
     * @param index The index of the element to return.
     * @return The shuffleWrite at the given index.
     */
    public double getShuffleWrite(int index) {
      return shuffleWrite_.getDouble(index);
    }
    private int shuffleWriteMemoizedSerializedSize = -1;

    public static final int SHUFFLE_WRITE_RECORDS_FIELD_NUMBER = 13;
    private com.google.protobuf.Internal.DoubleList shuffleWriteRecords_;
    /**
     * <code>repeated double shuffle_write_records = 13;</code>
     * @return A list containing the shuffleWriteRecords.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getShuffleWriteRecordsList() {
      return shuffleWriteRecords_;
    }
    /**
     * <code>repeated double shuffle_write_records = 13;</code>
     * @return The count of shuffleWriteRecords.
     */
    public int getShuffleWriteRecordsCount() {
      return shuffleWriteRecords_.size();
    }
    /**
     * <code>repeated double shuffle_write_records = 13;</code>
     * @param index The index of the element to return.
     * @return The shuffleWriteRecords at the given index.
     */
    public double getShuffleWriteRecords(int index) {
      return shuffleWriteRecords_.getDouble(index);
    }
    private int shuffleWriteRecordsMemoizedSerializedSize = -1;

    public static final int MEMORY_BYTES_SPILLED_FIELD_NUMBER = 14;
    private com.google.protobuf.Internal.DoubleList memoryBytesSpilled_;
    /**
     * <code>repeated double memory_bytes_spilled = 14;</code>
     * @return A list containing the memoryBytesSpilled.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getMemoryBytesSpilledList() {
      return memoryBytesSpilled_;
    }
    /**
     * <code>repeated double memory_bytes_spilled = 14;</code>
     * @return The count of memoryBytesSpilled.
     */
    public int getMemoryBytesSpilledCount() {
      return memoryBytesSpilled_.size();
    }
    /**
     * <code>repeated double memory_bytes_spilled = 14;</code>
     * @param index The index of the element to return.
     * @return The memoryBytesSpilled at the given index.
     */
    public double getMemoryBytesSpilled(int index) {
      return memoryBytesSpilled_.getDouble(index);
    }
    private int memoryBytesSpilledMemoizedSerializedSize = -1;

    public static final int DISK_BYTES_SPILLED_FIELD_NUMBER = 15;
    private com.google.protobuf.Internal.DoubleList diskBytesSpilled_;
    /**
     * <code>repeated double disk_bytes_spilled = 15;</code>
     * @return A list containing the diskBytesSpilled.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getDiskBytesSpilledList() {
      return diskBytesSpilled_;
    }
    /**
     * <code>repeated double disk_bytes_spilled = 15;</code>
     * @return The count of diskBytesSpilled.
     */
    public int getDiskBytesSpilledCount() {
      return diskBytesSpilled_.size();
    }
    /**
     * <code>repeated double disk_bytes_spilled = 15;</code>
     * @param index The index of the element to return.
     * @return The diskBytesSpilled at the given index.
     */
    public double getDiskBytesSpilled(int index) {
      return diskBytesSpilled_.getDouble(index);
    }
    private int diskBytesSpilledMemoizedSerializedSize = -1;

    public static final int PEAK_MEMORY_METRICS_FIELD_NUMBER = 16;
    private org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions peakMemoryMetrics_;
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
     * @return Whether the peakMemoryMetrics field is set.
     */
    @java.lang.Override
    public boolean hasPeakMemoryMetrics() {
      return peakMemoryMetrics_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
     * @return The peakMemoryMetrics.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions getPeakMemoryMetrics() {
      return peakMemoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.getDefaultInstance() : peakMemoryMetrics_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributionsOrBuilder getPeakMemoryMetricsOrBuilder() {
      return getPeakMemoryMetrics();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getQuantilesList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(quantilesMemoizedSerializedSize);
      }
      for (int i = 0; i < quantiles_.size(); i++) {
        output.writeDoubleNoTag(quantiles_.getDouble(i));
      }
      if (getTaskTimeList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(taskTimeMemoizedSerializedSize);
      }
      for (int i = 0; i < taskTime_.size(); i++) {
        output.writeDoubleNoTag(taskTime_.getDouble(i));
      }
      if (getFailedTasksList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(failedTasksMemoizedSerializedSize);
      }
      for (int i = 0; i < failedTasks_.size(); i++) {
        output.writeDoubleNoTag(failedTasks_.getDouble(i));
      }
      if (getSucceededTasksList().size() > 0) {
        output.writeUInt32NoTag(34);
        output.writeUInt32NoTag(succeededTasksMemoizedSerializedSize);
      }
      for (int i = 0; i < succeededTasks_.size(); i++) {
        output.writeDoubleNoTag(succeededTasks_.getDouble(i));
      }
      if (getKilledTasksList().size() > 0) {
        output.writeUInt32NoTag(42);
        output.writeUInt32NoTag(killedTasksMemoizedSerializedSize);
      }
      for (int i = 0; i < killedTasks_.size(); i++) {
        output.writeDoubleNoTag(killedTasks_.getDouble(i));
      }
      if (getInputBytesList().size() > 0) {
        output.writeUInt32NoTag(50);
        output.writeUInt32NoTag(inputBytesMemoizedSerializedSize);
      }
      for (int i = 0; i < inputBytes_.size(); i++) {
        output.writeDoubleNoTag(inputBytes_.getDouble(i));
      }
      if (getInputRecordsList().size() > 0) {
        output.writeUInt32NoTag(58);
        output.writeUInt32NoTag(inputRecordsMemoizedSerializedSize);
      }
      for (int i = 0; i < inputRecords_.size(); i++) {
        output.writeDoubleNoTag(inputRecords_.getDouble(i));
      }
      if (getOutputBytesList().size() > 0) {
        output.writeUInt32NoTag(66);
        output.writeUInt32NoTag(outputBytesMemoizedSerializedSize);
      }
      for (int i = 0; i < outputBytes_.size(); i++) {
        output.writeDoubleNoTag(outputBytes_.getDouble(i));
      }
      if (getOutputRecordsList().size() > 0) {
        output.writeUInt32NoTag(74);
        output.writeUInt32NoTag(outputRecordsMemoizedSerializedSize);
      }
      for (int i = 0; i < outputRecords_.size(); i++) {
        output.writeDoubleNoTag(outputRecords_.getDouble(i));
      }
      if (getShuffleReadList().size() > 0) {
        output.writeUInt32NoTag(82);
        output.writeUInt32NoTag(shuffleReadMemoizedSerializedSize);
      }
      for (int i = 0; i < shuffleRead_.size(); i++) {
        output.writeDoubleNoTag(shuffleRead_.getDouble(i));
      }
      if (getShuffleReadRecordsList().size() > 0) {
        output.writeUInt32NoTag(90);
        output.writeUInt32NoTag(shuffleReadRecordsMemoizedSerializedSize);
      }
      for (int i = 0; i < shuffleReadRecords_.size(); i++) {
        output.writeDoubleNoTag(shuffleReadRecords_.getDouble(i));
      }
      if (getShuffleWriteList().size() > 0) {
        output.writeUInt32NoTag(98);
        output.writeUInt32NoTag(shuffleWriteMemoizedSerializedSize);
      }
      for (int i = 0; i < shuffleWrite_.size(); i++) {
        output.writeDoubleNoTag(shuffleWrite_.getDouble(i));
      }
      if (getShuffleWriteRecordsList().size() > 0) {
        output.writeUInt32NoTag(106);
        output.writeUInt32NoTag(shuffleWriteRecordsMemoizedSerializedSize);
      }
      for (int i = 0; i < shuffleWriteRecords_.size(); i++) {
        output.writeDoubleNoTag(shuffleWriteRecords_.getDouble(i));
      }
      if (getMemoryBytesSpilledList().size() > 0) {
        output.writeUInt32NoTag(114);
        output.writeUInt32NoTag(memoryBytesSpilledMemoizedSerializedSize);
      }
      for (int i = 0; i < memoryBytesSpilled_.size(); i++) {
        output.writeDoubleNoTag(memoryBytesSpilled_.getDouble(i));
      }
      if (getDiskBytesSpilledList().size() > 0) {
        output.writeUInt32NoTag(122);
        output.writeUInt32NoTag(diskBytesSpilledMemoizedSerializedSize);
      }
      for (int i = 0; i < diskBytesSpilled_.size(); i++) {
        output.writeDoubleNoTag(diskBytesSpilled_.getDouble(i));
      }
      if (peakMemoryMetrics_ != null) {
        output.writeMessage(16, getPeakMemoryMetrics());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        dataSize = 8 * getQuantilesList().size();
        size += dataSize;
        if (!getQuantilesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        quantilesMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getTaskTimeList().size();
        size += dataSize;
        if (!getTaskTimeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        taskTimeMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getFailedTasksList().size();
        size += dataSize;
        if (!getFailedTasksList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        failedTasksMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getSucceededTasksList().size();
        size += dataSize;
        if (!getSucceededTasksList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        succeededTasksMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getKilledTasksList().size();
        size += dataSize;
        if (!getKilledTasksList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        killedTasksMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getInputBytesList().size();
        size += dataSize;
        if (!getInputBytesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        inputBytesMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getInputRecordsList().size();
        size += dataSize;
        if (!getInputRecordsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        inputRecordsMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getOutputBytesList().size();
        size += dataSize;
        if (!getOutputBytesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        outputBytesMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getOutputRecordsList().size();
        size += dataSize;
        if (!getOutputRecordsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        outputRecordsMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getShuffleReadList().size();
        size += dataSize;
        if (!getShuffleReadList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        shuffleReadMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getShuffleReadRecordsList().size();
        size += dataSize;
        if (!getShuffleReadRecordsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        shuffleReadRecordsMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getShuffleWriteList().size();
        size += dataSize;
        if (!getShuffleWriteList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        shuffleWriteMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getShuffleWriteRecordsList().size();
        size += dataSize;
        if (!getShuffleWriteRecordsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        shuffleWriteRecordsMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getMemoryBytesSpilledList().size();
        size += dataSize;
        if (!getMemoryBytesSpilledList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        memoryBytesSpilledMemoizedSerializedSize = dataSize;
      }
      {
        int dataSize = 0;
        dataSize = 8 * getDiskBytesSpilledList().size();
        size += dataSize;
        if (!getDiskBytesSpilledList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        diskBytesSpilledMemoizedSerializedSize = dataSize;
      }
      if (peakMemoryMetrics_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(16, getPeakMemoryMetrics());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions other = (org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions) obj;

      if (!getQuantilesList()
          .equals(other.getQuantilesList())) return false;
      if (!getTaskTimeList()
          .equals(other.getTaskTimeList())) return false;
      if (!getFailedTasksList()
          .equals(other.getFailedTasksList())) return false;
      if (!getSucceededTasksList()
          .equals(other.getSucceededTasksList())) return false;
      if (!getKilledTasksList()
          .equals(other.getKilledTasksList())) return false;
      if (!getInputBytesList()
          .equals(other.getInputBytesList())) return false;
      if (!getInputRecordsList()
          .equals(other.getInputRecordsList())) return false;
      if (!getOutputBytesList()
          .equals(other.getOutputBytesList())) return false;
      if (!getOutputRecordsList()
          .equals(other.getOutputRecordsList())) return false;
      if (!getShuffleReadList()
          .equals(other.getShuffleReadList())) return false;
      if (!getShuffleReadRecordsList()
          .equals(other.getShuffleReadRecordsList())) return false;
      if (!getShuffleWriteList()
          .equals(other.getShuffleWriteList())) return false;
      if (!getShuffleWriteRecordsList()
          .equals(other.getShuffleWriteRecordsList())) return false;
      if (!getMemoryBytesSpilledList()
          .equals(other.getMemoryBytesSpilledList())) return false;
      if (!getDiskBytesSpilledList()
          .equals(other.getDiskBytesSpilledList())) return false;
      if (hasPeakMemoryMetrics() != other.hasPeakMemoryMetrics()) return false;
      if (hasPeakMemoryMetrics()) {
        if (!getPeakMemoryMetrics()
            .equals(other.getPeakMemoryMetrics())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getQuantilesCount() > 0) {
        hash = (37 * hash) + QUANTILES_FIELD_NUMBER;
        hash = (53 * hash) + getQuantilesList().hashCode();
      }
      if (getTaskTimeCount() > 0) {
        hash = (37 * hash) + TASK_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getTaskTimeList().hashCode();
      }
      if (getFailedTasksCount() > 0) {
        hash = (37 * hash) + FAILED_TASKS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedTasksList().hashCode();
      }
      if (getSucceededTasksCount() > 0) {
        hash = (37 * hash) + SUCCEEDED_TASKS_FIELD_NUMBER;
        hash = (53 * hash) + getSucceededTasksList().hashCode();
      }
      if (getKilledTasksCount() > 0) {
        hash = (37 * hash) + KILLED_TASKS_FIELD_NUMBER;
        hash = (53 * hash) + getKilledTasksList().hashCode();
      }
      if (getInputBytesCount() > 0) {
        hash = (37 * hash) + INPUT_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + getInputBytesList().hashCode();
      }
      if (getInputRecordsCount() > 0) {
        hash = (37 * hash) + INPUT_RECORDS_FIELD_NUMBER;
        hash = (53 * hash) + getInputRecordsList().hashCode();
      }
      if (getOutputBytesCount() > 0) {
        hash = (37 * hash) + OUTPUT_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + getOutputBytesList().hashCode();
      }
      if (getOutputRecordsCount() > 0) {
        hash = (37 * hash) + OUTPUT_RECORDS_FIELD_NUMBER;
        hash = (53 * hash) + getOutputRecordsList().hashCode();
      }
      if (getShuffleReadCount() > 0) {
        hash = (37 * hash) + SHUFFLE_READ_FIELD_NUMBER;
        hash = (53 * hash) + getShuffleReadList().hashCode();
      }
      if (getShuffleReadRecordsCount() > 0) {
        hash = (37 * hash) + SHUFFLE_READ_RECORDS_FIELD_NUMBER;
        hash = (53 * hash) + getShuffleReadRecordsList().hashCode();
      }
      if (getShuffleWriteCount() > 0) {
        hash = (37 * hash) + SHUFFLE_WRITE_FIELD_NUMBER;
        hash = (53 * hash) + getShuffleWriteList().hashCode();
      }
      if (getShuffleWriteRecordsCount() > 0) {
        hash = (37 * hash) + SHUFFLE_WRITE_RECORDS_FIELD_NUMBER;
        hash = (53 * hash) + getShuffleWriteRecordsList().hashCode();
      }
      if (getMemoryBytesSpilledCount() > 0) {
        hash = (37 * hash) + MEMORY_BYTES_SPILLED_FIELD_NUMBER;
        hash = (53 * hash) + getMemoryBytesSpilledList().hashCode();
      }
      if (getDiskBytesSpilledCount() > 0) {
        hash = (37 * hash) + DISK_BYTES_SPILLED_FIELD_NUMBER;
        hash = (53 * hash) + getDiskBytesSpilledList().hashCode();
      }
      if (hasPeakMemoryMetrics()) {
        hash = (37 * hash) + PEAK_MEMORY_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getPeakMemoryMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorMetricsDistributions}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ExecutorMetricsDistributions)
        org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        quantiles_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        taskTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        failedTasks_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        succeededTasks_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000008);
        killedTasks_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000010);
        inputBytes_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000020);
        inputRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000040);
        outputBytes_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000080);
        outputRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000100);
        shuffleRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000200);
        shuffleReadRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000400);
        shuffleWrite_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000800);
        shuffleWriteRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00001000);
        memoryBytesSpilled_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00002000);
        diskBytesSpilled_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00004000);
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = null;
        } else {
          peakMemoryMetrics_ = null;
          peakMemoryMetricsBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions build() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions result = new org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          quantiles_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.quantiles_ = quantiles_;
        if (((bitField0_ & 0x00000002) != 0)) {
          taskTime_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.taskTime_ = taskTime_;
        if (((bitField0_ & 0x00000004) != 0)) {
          failedTasks_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.failedTasks_ = failedTasks_;
        if (((bitField0_ & 0x00000008) != 0)) {
          succeededTasks_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.succeededTasks_ = succeededTasks_;
        if (((bitField0_ & 0x00000010) != 0)) {
          killedTasks_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.killedTasks_ = killedTasks_;
        if (((bitField0_ & 0x00000020) != 0)) {
          inputBytes_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000020);
        }
        result.inputBytes_ = inputBytes_;
        if (((bitField0_ & 0x00000040) != 0)) {
          inputRecords_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000040);
        }
        result.inputRecords_ = inputRecords_;
        if (((bitField0_ & 0x00000080) != 0)) {
          outputBytes_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000080);
        }
        result.outputBytes_ = outputBytes_;
        if (((bitField0_ & 0x00000100) != 0)) {
          outputRecords_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000100);
        }
        result.outputRecords_ = outputRecords_;
        if (((bitField0_ & 0x00000200) != 0)) {
          shuffleRead_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000200);
        }
        result.shuffleRead_ = shuffleRead_;
        if (((bitField0_ & 0x00000400) != 0)) {
          shuffleReadRecords_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000400);
        }
        result.shuffleReadRecords_ = shuffleReadRecords_;
        if (((bitField0_ & 0x00000800) != 0)) {
          shuffleWrite_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000800);
        }
        result.shuffleWrite_ = shuffleWrite_;
        if (((bitField0_ & 0x00001000) != 0)) {
          shuffleWriteRecords_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00001000);
        }
        result.shuffleWriteRecords_ = shuffleWriteRecords_;
        if (((bitField0_ & 0x00002000) != 0)) {
          memoryBytesSpilled_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00002000);
        }
        result.memoryBytesSpilled_ = memoryBytesSpilled_;
        if (((bitField0_ & 0x00004000) != 0)) {
          diskBytesSpilled_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00004000);
        }
        result.diskBytesSpilled_ = diskBytesSpilled_;
        if (peakMemoryMetricsBuilder_ == null) {
          result.peakMemoryMetrics_ = peakMemoryMetrics_;
        } else {
          result.peakMemoryMetrics_ = peakMemoryMetricsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions.getDefaultInstance()) return this;
        if (!other.quantiles_.isEmpty()) {
          if (quantiles_.isEmpty()) {
            quantiles_ = other.quantiles_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureQuantilesIsMutable();
            quantiles_.addAll(other.quantiles_);
          }
          onChanged();
        }
        if (!other.taskTime_.isEmpty()) {
          if (taskTime_.isEmpty()) {
            taskTime_ = other.taskTime_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureTaskTimeIsMutable();
            taskTime_.addAll(other.taskTime_);
          }
          onChanged();
        }
        if (!other.failedTasks_.isEmpty()) {
          if (failedTasks_.isEmpty()) {
            failedTasks_ = other.failedTasks_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureFailedTasksIsMutable();
            failedTasks_.addAll(other.failedTasks_);
          }
          onChanged();
        }
        if (!other.succeededTasks_.isEmpty()) {
          if (succeededTasks_.isEmpty()) {
            succeededTasks_ = other.succeededTasks_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureSucceededTasksIsMutable();
            succeededTasks_.addAll(other.succeededTasks_);
          }
          onChanged();
        }
        if (!other.killedTasks_.isEmpty()) {
          if (killedTasks_.isEmpty()) {
            killedTasks_ = other.killedTasks_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureKilledTasksIsMutable();
            killedTasks_.addAll(other.killedTasks_);
          }
          onChanged();
        }
        if (!other.inputBytes_.isEmpty()) {
          if (inputBytes_.isEmpty()) {
            inputBytes_ = other.inputBytes_;
            bitField0_ = (bitField0_ & ~0x00000020);
          } else {
            ensureInputBytesIsMutable();
            inputBytes_.addAll(other.inputBytes_);
          }
          onChanged();
        }
        if (!other.inputRecords_.isEmpty()) {
          if (inputRecords_.isEmpty()) {
            inputRecords_ = other.inputRecords_;
            bitField0_ = (bitField0_ & ~0x00000040);
          } else {
            ensureInputRecordsIsMutable();
            inputRecords_.addAll(other.inputRecords_);
          }
          onChanged();
        }
        if (!other.outputBytes_.isEmpty()) {
          if (outputBytes_.isEmpty()) {
            outputBytes_ = other.outputBytes_;
            bitField0_ = (bitField0_ & ~0x00000080);
          } else {
            ensureOutputBytesIsMutable();
            outputBytes_.addAll(other.outputBytes_);
          }
          onChanged();
        }
        if (!other.outputRecords_.isEmpty()) {
          if (outputRecords_.isEmpty()) {
            outputRecords_ = other.outputRecords_;
            bitField0_ = (bitField0_ & ~0x00000100);
          } else {
            ensureOutputRecordsIsMutable();
            outputRecords_.addAll(other.outputRecords_);
          }
          onChanged();
        }
        if (!other.shuffleRead_.isEmpty()) {
          if (shuffleRead_.isEmpty()) {
            shuffleRead_ = other.shuffleRead_;
            bitField0_ = (bitField0_ & ~0x00000200);
          } else {
            ensureShuffleReadIsMutable();
            shuffleRead_.addAll(other.shuffleRead_);
          }
          onChanged();
        }
        if (!other.shuffleReadRecords_.isEmpty()) {
          if (shuffleReadRecords_.isEmpty()) {
            shuffleReadRecords_ = other.shuffleReadRecords_;
            bitField0_ = (bitField0_ & ~0x00000400);
          } else {
            ensureShuffleReadRecordsIsMutable();
            shuffleReadRecords_.addAll(other.shuffleReadRecords_);
          }
          onChanged();
        }
        if (!other.shuffleWrite_.isEmpty()) {
          if (shuffleWrite_.isEmpty()) {
            shuffleWrite_ = other.shuffleWrite_;
            bitField0_ = (bitField0_ & ~0x00000800);
          } else {
            ensureShuffleWriteIsMutable();
            shuffleWrite_.addAll(other.shuffleWrite_);
          }
          onChanged();
        }
        if (!other.shuffleWriteRecords_.isEmpty()) {
          if (shuffleWriteRecords_.isEmpty()) {
            shuffleWriteRecords_ = other.shuffleWriteRecords_;
            bitField0_ = (bitField0_ & ~0x00001000);
          } else {
            ensureShuffleWriteRecordsIsMutable();
            shuffleWriteRecords_.addAll(other.shuffleWriteRecords_);
          }
          onChanged();
        }
        if (!other.memoryBytesSpilled_.isEmpty()) {
          if (memoryBytesSpilled_.isEmpty()) {
            memoryBytesSpilled_ = other.memoryBytesSpilled_;
            bitField0_ = (bitField0_ & ~0x00002000);
          } else {
            ensureMemoryBytesSpilledIsMutable();
            memoryBytesSpilled_.addAll(other.memoryBytesSpilled_);
          }
          onChanged();
        }
        if (!other.diskBytesSpilled_.isEmpty()) {
          if (diskBytesSpilled_.isEmpty()) {
            diskBytesSpilled_ = other.diskBytesSpilled_;
            bitField0_ = (bitField0_ & ~0x00004000);
          } else {
            ensureDiskBytesSpilledIsMutable();
            diskBytesSpilled_.addAll(other.diskBytesSpilled_);
          }
          onChanged();
        }
        if (other.hasPeakMemoryMetrics()) {
          mergePeakMemoryMetrics(other.getPeakMemoryMetrics());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.DoubleList quantiles_ = emptyDoubleList();
      private void ensureQuantilesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          quantiles_ = mutableCopy(quantiles_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return A list containing the quantiles.
       */
      public java.util.List<java.lang.Double>
          getQuantilesList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(quantiles_) : quantiles_;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return The count of quantiles.
       */
      public int getQuantilesCount() {
        return quantiles_.size();
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param index The index of the element to return.
       * @return The quantiles at the given index.
       */
      public double getQuantiles(int index) {
        return quantiles_.getDouble(index);
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param index The index to set the value at.
       * @param value The quantiles to set.
       * @return This builder for chaining.
       */
      public Builder setQuantiles(
          int index, double value) {
        ensureQuantilesIsMutable();
        quantiles_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param value The quantiles to add.
       * @return This builder for chaining.
       */
      public Builder addQuantiles(double value) {
        ensureQuantilesIsMutable();
        quantiles_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param values The quantiles to add.
       * @return This builder for chaining.
       */
      public Builder addAllQuantiles(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureQuantilesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, quantiles_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearQuantiles() {
        quantiles_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList taskTime_ = emptyDoubleList();
      private void ensureTaskTimeIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          taskTime_ = mutableCopy(taskTime_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated double task_time = 2;</code>
       * @return A list containing the taskTime.
       */
      public java.util.List<java.lang.Double>
          getTaskTimeList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(taskTime_) : taskTime_;
      }
      /**
       * <code>repeated double task_time = 2;</code>
       * @return The count of taskTime.
       */
      public int getTaskTimeCount() {
        return taskTime_.size();
      }
      /**
       * <code>repeated double task_time = 2;</code>
       * @param index The index of the element to return.
       * @return The taskTime at the given index.
       */
      public double getTaskTime(int index) {
        return taskTime_.getDouble(index);
      }
      /**
       * <code>repeated double task_time = 2;</code>
       * @param index The index to set the value at.
       * @param value The taskTime to set.
       * @return This builder for chaining.
       */
      public Builder setTaskTime(
          int index, double value) {
        ensureTaskTimeIsMutable();
        taskTime_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double task_time = 2;</code>
       * @param value The taskTime to add.
       * @return This builder for chaining.
       */
      public Builder addTaskTime(double value) {
        ensureTaskTimeIsMutable();
        taskTime_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double task_time = 2;</code>
       * @param values The taskTime to add.
       * @return This builder for chaining.
       */
      public Builder addAllTaskTime(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureTaskTimeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, taskTime_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double task_time = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearTaskTime() {
        taskTime_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList failedTasks_ = emptyDoubleList();
      private void ensureFailedTasksIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          failedTasks_ = mutableCopy(failedTasks_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated double failed_tasks = 3;</code>
       * @return A list containing the failedTasks.
       */
      public java.util.List<java.lang.Double>
          getFailedTasksList() {
        return ((bitField0_ & 0x00000004) != 0) ?
                 java.util.Collections.unmodifiableList(failedTasks_) : failedTasks_;
      }
      /**
       * <code>repeated double failed_tasks = 3;</code>
       * @return The count of failedTasks.
       */
      public int getFailedTasksCount() {
        return failedTasks_.size();
      }
      /**
       * <code>repeated double failed_tasks = 3;</code>
       * @param index The index of the element to return.
       * @return The failedTasks at the given index.
       */
      public double getFailedTasks(int index) {
        return failedTasks_.getDouble(index);
      }
      /**
       * <code>repeated double failed_tasks = 3;</code>
       * @param index The index to set the value at.
       * @param value The failedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setFailedTasks(
          int index, double value) {
        ensureFailedTasksIsMutable();
        failedTasks_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double failed_tasks = 3;</code>
       * @param value The failedTasks to add.
       * @return This builder for chaining.
       */
      public Builder addFailedTasks(double value) {
        ensureFailedTasksIsMutable();
        failedTasks_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double failed_tasks = 3;</code>
       * @param values The failedTasks to add.
       * @return This builder for chaining.
       */
      public Builder addAllFailedTasks(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureFailedTasksIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, failedTasks_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double failed_tasks = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearFailedTasks() {
        failedTasks_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList succeededTasks_ = emptyDoubleList();
      private void ensureSucceededTasksIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          succeededTasks_ = mutableCopy(succeededTasks_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated double succeeded_tasks = 4;</code>
       * @return A list containing the succeededTasks.
       */
      public java.util.List<java.lang.Double>
          getSucceededTasksList() {
        return ((bitField0_ & 0x00000008) != 0) ?
                 java.util.Collections.unmodifiableList(succeededTasks_) : succeededTasks_;
      }
      /**
       * <code>repeated double succeeded_tasks = 4;</code>
       * @return The count of succeededTasks.
       */
      public int getSucceededTasksCount() {
        return succeededTasks_.size();
      }
      /**
       * <code>repeated double succeeded_tasks = 4;</code>
       * @param index The index of the element to return.
       * @return The succeededTasks at the given index.
       */
      public double getSucceededTasks(int index) {
        return succeededTasks_.getDouble(index);
      }
      /**
       * <code>repeated double succeeded_tasks = 4;</code>
       * @param index The index to set the value at.
       * @param value The succeededTasks to set.
       * @return This builder for chaining.
       */
      public Builder setSucceededTasks(
          int index, double value) {
        ensureSucceededTasksIsMutable();
        succeededTasks_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double succeeded_tasks = 4;</code>
       * @param value The succeededTasks to add.
       * @return This builder for chaining.
       */
      public Builder addSucceededTasks(double value) {
        ensureSucceededTasksIsMutable();
        succeededTasks_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double succeeded_tasks = 4;</code>
       * @param values The succeededTasks to add.
       * @return This builder for chaining.
       */
      public Builder addAllSucceededTasks(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureSucceededTasksIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, succeededTasks_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double succeeded_tasks = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearSucceededTasks() {
        succeededTasks_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList killedTasks_ = emptyDoubleList();
      private void ensureKilledTasksIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          killedTasks_ = mutableCopy(killedTasks_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <code>repeated double killed_tasks = 5;</code>
       * @return A list containing the killedTasks.
       */
      public java.util.List<java.lang.Double>
          getKilledTasksList() {
        return ((bitField0_ & 0x00000010) != 0) ?
                 java.util.Collections.unmodifiableList(killedTasks_) : killedTasks_;
      }
      /**
       * <code>repeated double killed_tasks = 5;</code>
       * @return The count of killedTasks.
       */
      public int getKilledTasksCount() {
        return killedTasks_.size();
      }
      /**
       * <code>repeated double killed_tasks = 5;</code>
       * @param index The index of the element to return.
       * @return The killedTasks at the given index.
       */
      public double getKilledTasks(int index) {
        return killedTasks_.getDouble(index);
      }
      /**
       * <code>repeated double killed_tasks = 5;</code>
       * @param index The index to set the value at.
       * @param value The killedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setKilledTasks(
          int index, double value) {
        ensureKilledTasksIsMutable();
        killedTasks_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double killed_tasks = 5;</code>
       * @param value The killedTasks to add.
       * @return This builder for chaining.
       */
      public Builder addKilledTasks(double value) {
        ensureKilledTasksIsMutable();
        killedTasks_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double killed_tasks = 5;</code>
       * @param values The killedTasks to add.
       * @return This builder for chaining.
       */
      public Builder addAllKilledTasks(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureKilledTasksIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, killedTasks_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double killed_tasks = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearKilledTasks() {
        killedTasks_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList inputBytes_ = emptyDoubleList();
      private void ensureInputBytesIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          inputBytes_ = mutableCopy(inputBytes_);
          bitField0_ |= 0x00000020;
         }
      }
      /**
       * <code>repeated double input_bytes = 6;</code>
       * @return A list containing the inputBytes.
       */
      public java.util.List<java.lang.Double>
          getInputBytesList() {
        return ((bitField0_ & 0x00000020) != 0) ?
                 java.util.Collections.unmodifiableList(inputBytes_) : inputBytes_;
      }
      /**
       * <code>repeated double input_bytes = 6;</code>
       * @return The count of inputBytes.
       */
      public int getInputBytesCount() {
        return inputBytes_.size();
      }
      /**
       * <code>repeated double input_bytes = 6;</code>
       * @param index The index of the element to return.
       * @return The inputBytes at the given index.
       */
      public double getInputBytes(int index) {
        return inputBytes_.getDouble(index);
      }
      /**
       * <code>repeated double input_bytes = 6;</code>
       * @param index The index to set the value at.
       * @param value The inputBytes to set.
       * @return This builder for chaining.
       */
      public Builder setInputBytes(
          int index, double value) {
        ensureInputBytesIsMutable();
        inputBytes_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double input_bytes = 6;</code>
       * @param value The inputBytes to add.
       * @return This builder for chaining.
       */
      public Builder addInputBytes(double value) {
        ensureInputBytesIsMutable();
        inputBytes_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double input_bytes = 6;</code>
       * @param values The inputBytes to add.
       * @return This builder for chaining.
       */
      public Builder addAllInputBytes(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureInputBytesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, inputBytes_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double input_bytes = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputBytes() {
        inputBytes_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000020);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList inputRecords_ = emptyDoubleList();
      private void ensureInputRecordsIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          inputRecords_ = mutableCopy(inputRecords_);
          bitField0_ |= 0x00000040;
         }
      }
      /**
       * <code>repeated double input_records = 7;</code>
       * @return A list containing the inputRecords.
       */
      public java.util.List<java.lang.Double>
          getInputRecordsList() {
        return ((bitField0_ & 0x00000040) != 0) ?
                 java.util.Collections.unmodifiableList(inputRecords_) : inputRecords_;
      }
      /**
       * <code>repeated double input_records = 7;</code>
       * @return The count of inputRecords.
       */
      public int getInputRecordsCount() {
        return inputRecords_.size();
      }
      /**
       * <code>repeated double input_records = 7;</code>
       * @param index The index of the element to return.
       * @return The inputRecords at the given index.
       */
      public double getInputRecords(int index) {
        return inputRecords_.getDouble(index);
      }
      /**
       * <code>repeated double input_records = 7;</code>
       * @param index The index to set the value at.
       * @param value The inputRecords to set.
       * @return This builder for chaining.
       */
      public Builder setInputRecords(
          int index, double value) {
        ensureInputRecordsIsMutable();
        inputRecords_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double input_records = 7;</code>
       * @param value The inputRecords to add.
       * @return This builder for chaining.
       */
      public Builder addInputRecords(double value) {
        ensureInputRecordsIsMutable();
        inputRecords_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double input_records = 7;</code>
       * @param values The inputRecords to add.
       * @return This builder for chaining.
       */
      public Builder addAllInputRecords(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureInputRecordsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, inputRecords_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double input_records = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputRecords() {
        inputRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000040);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList outputBytes_ = emptyDoubleList();
      private void ensureOutputBytesIsMutable() {
        if (!((bitField0_ & 0x00000080) != 0)) {
          outputBytes_ = mutableCopy(outputBytes_);
          bitField0_ |= 0x00000080;
         }
      }
      /**
       * <code>repeated double output_bytes = 8;</code>
       * @return A list containing the outputBytes.
       */
      public java.util.List<java.lang.Double>
          getOutputBytesList() {
        return ((bitField0_ & 0x00000080) != 0) ?
                 java.util.Collections.unmodifiableList(outputBytes_) : outputBytes_;
      }
      /**
       * <code>repeated double output_bytes = 8;</code>
       * @return The count of outputBytes.
       */
      public int getOutputBytesCount() {
        return outputBytes_.size();
      }
      /**
       * <code>repeated double output_bytes = 8;</code>
       * @param index The index of the element to return.
       * @return The outputBytes at the given index.
       */
      public double getOutputBytes(int index) {
        return outputBytes_.getDouble(index);
      }
      /**
       * <code>repeated double output_bytes = 8;</code>
       * @param index The index to set the value at.
       * @param value The outputBytes to set.
       * @return This builder for chaining.
       */
      public Builder setOutputBytes(
          int index, double value) {
        ensureOutputBytesIsMutable();
        outputBytes_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double output_bytes = 8;</code>
       * @param value The outputBytes to add.
       * @return This builder for chaining.
       */
      public Builder addOutputBytes(double value) {
        ensureOutputBytesIsMutable();
        outputBytes_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double output_bytes = 8;</code>
       * @param values The outputBytes to add.
       * @return This builder for chaining.
       */
      public Builder addAllOutputBytes(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureOutputBytesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, outputBytes_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double output_bytes = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputBytes() {
        outputBytes_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000080);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList outputRecords_ = emptyDoubleList();
      private void ensureOutputRecordsIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          outputRecords_ = mutableCopy(outputRecords_);
          bitField0_ |= 0x00000100;
         }
      }
      /**
       * <code>repeated double output_records = 9;</code>
       * @return A list containing the outputRecords.
       */
      public java.util.List<java.lang.Double>
          getOutputRecordsList() {
        return ((bitField0_ & 0x00000100) != 0) ?
                 java.util.Collections.unmodifiableList(outputRecords_) : outputRecords_;
      }
      /**
       * <code>repeated double output_records = 9;</code>
       * @return The count of outputRecords.
       */
      public int getOutputRecordsCount() {
        return outputRecords_.size();
      }
      /**
       * <code>repeated double output_records = 9;</code>
       * @param index The index of the element to return.
       * @return The outputRecords at the given index.
       */
      public double getOutputRecords(int index) {
        return outputRecords_.getDouble(index);
      }
      /**
       * <code>repeated double output_records = 9;</code>
       * @param index The index to set the value at.
       * @param value The outputRecords to set.
       * @return This builder for chaining.
       */
      public Builder setOutputRecords(
          int index, double value) {
        ensureOutputRecordsIsMutable();
        outputRecords_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double output_records = 9;</code>
       * @param value The outputRecords to add.
       * @return This builder for chaining.
       */
      public Builder addOutputRecords(double value) {
        ensureOutputRecordsIsMutable();
        outputRecords_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double output_records = 9;</code>
       * @param values The outputRecords to add.
       * @return This builder for chaining.
       */
      public Builder addAllOutputRecords(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureOutputRecordsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, outputRecords_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double output_records = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearOutputRecords() {
        outputRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000100);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList shuffleRead_ = emptyDoubleList();
      private void ensureShuffleReadIsMutable() {
        if (!((bitField0_ & 0x00000200) != 0)) {
          shuffleRead_ = mutableCopy(shuffleRead_);
          bitField0_ |= 0x00000200;
         }
      }
      /**
       * <code>repeated double shuffle_read = 10;</code>
       * @return A list containing the shuffleRead.
       */
      public java.util.List<java.lang.Double>
          getShuffleReadList() {
        return ((bitField0_ & 0x00000200) != 0) ?
                 java.util.Collections.unmodifiableList(shuffleRead_) : shuffleRead_;
      }
      /**
       * <code>repeated double shuffle_read = 10;</code>
       * @return The count of shuffleRead.
       */
      public int getShuffleReadCount() {
        return shuffleRead_.size();
      }
      /**
       * <code>repeated double shuffle_read = 10;</code>
       * @param index The index of the element to return.
       * @return The shuffleRead at the given index.
       */
      public double getShuffleRead(int index) {
        return shuffleRead_.getDouble(index);
      }
      /**
       * <code>repeated double shuffle_read = 10;</code>
       * @param index The index to set the value at.
       * @param value The shuffleRead to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleRead(
          int index, double value) {
        ensureShuffleReadIsMutable();
        shuffleRead_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_read = 10;</code>
       * @param value The shuffleRead to add.
       * @return This builder for chaining.
       */
      public Builder addShuffleRead(double value) {
        ensureShuffleReadIsMutable();
        shuffleRead_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_read = 10;</code>
       * @param values The shuffleRead to add.
       * @return This builder for chaining.
       */
      public Builder addAllShuffleRead(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureShuffleReadIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, shuffleRead_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_read = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleRead() {
        shuffleRead_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000200);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList shuffleReadRecords_ = emptyDoubleList();
      private void ensureShuffleReadRecordsIsMutable() {
        if (!((bitField0_ & 0x00000400) != 0)) {
          shuffleReadRecords_ = mutableCopy(shuffleReadRecords_);
          bitField0_ |= 0x00000400;
         }
      }
      /**
       * <code>repeated double shuffle_read_records = 11;</code>
       * @return A list containing the shuffleReadRecords.
       */
      public java.util.List<java.lang.Double>
          getShuffleReadRecordsList() {
        return ((bitField0_ & 0x00000400) != 0) ?
                 java.util.Collections.unmodifiableList(shuffleReadRecords_) : shuffleReadRecords_;
      }
      /**
       * <code>repeated double shuffle_read_records = 11;</code>
       * @return The count of shuffleReadRecords.
       */
      public int getShuffleReadRecordsCount() {
        return shuffleReadRecords_.size();
      }
      /**
       * <code>repeated double shuffle_read_records = 11;</code>
       * @param index The index of the element to return.
       * @return The shuffleReadRecords at the given index.
       */
      public double getShuffleReadRecords(int index) {
        return shuffleReadRecords_.getDouble(index);
      }
      /**
       * <code>repeated double shuffle_read_records = 11;</code>
       * @param index The index to set the value at.
       * @param value The shuffleReadRecords to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleReadRecords(
          int index, double value) {
        ensureShuffleReadRecordsIsMutable();
        shuffleReadRecords_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_read_records = 11;</code>
       * @param value The shuffleReadRecords to add.
       * @return This builder for chaining.
       */
      public Builder addShuffleReadRecords(double value) {
        ensureShuffleReadRecordsIsMutable();
        shuffleReadRecords_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_read_records = 11;</code>
       * @param values The shuffleReadRecords to add.
       * @return This builder for chaining.
       */
      public Builder addAllShuffleReadRecords(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureShuffleReadRecordsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, shuffleReadRecords_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_read_records = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleReadRecords() {
        shuffleReadRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000400);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList shuffleWrite_ = emptyDoubleList();
      private void ensureShuffleWriteIsMutable() {
        if (!((bitField0_ & 0x00000800) != 0)) {
          shuffleWrite_ = mutableCopy(shuffleWrite_);
          bitField0_ |= 0x00000800;
         }
      }
      /**
       * <code>repeated double shuffle_write = 12;</code>
       * @return A list containing the shuffleWrite.
       */
      public java.util.List<java.lang.Double>
          getShuffleWriteList() {
        return ((bitField0_ & 0x00000800) != 0) ?
                 java.util.Collections.unmodifiableList(shuffleWrite_) : shuffleWrite_;
      }
      /**
       * <code>repeated double shuffle_write = 12;</code>
       * @return The count of shuffleWrite.
       */
      public int getShuffleWriteCount() {
        return shuffleWrite_.size();
      }
      /**
       * <code>repeated double shuffle_write = 12;</code>
       * @param index The index of the element to return.
       * @return The shuffleWrite at the given index.
       */
      public double getShuffleWrite(int index) {
        return shuffleWrite_.getDouble(index);
      }
      /**
       * <code>repeated double shuffle_write = 12;</code>
       * @param index The index to set the value at.
       * @param value The shuffleWrite to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWrite(
          int index, double value) {
        ensureShuffleWriteIsMutable();
        shuffleWrite_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_write = 12;</code>
       * @param value The shuffleWrite to add.
       * @return This builder for chaining.
       */
      public Builder addShuffleWrite(double value) {
        ensureShuffleWriteIsMutable();
        shuffleWrite_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_write = 12;</code>
       * @param values The shuffleWrite to add.
       * @return This builder for chaining.
       */
      public Builder addAllShuffleWrite(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureShuffleWriteIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, shuffleWrite_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_write = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWrite() {
        shuffleWrite_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000800);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList shuffleWriteRecords_ = emptyDoubleList();
      private void ensureShuffleWriteRecordsIsMutable() {
        if (!((bitField0_ & 0x00001000) != 0)) {
          shuffleWriteRecords_ = mutableCopy(shuffleWriteRecords_);
          bitField0_ |= 0x00001000;
         }
      }
      /**
       * <code>repeated double shuffle_write_records = 13;</code>
       * @return A list containing the shuffleWriteRecords.
       */
      public java.util.List<java.lang.Double>
          getShuffleWriteRecordsList() {
        return ((bitField0_ & 0x00001000) != 0) ?
                 java.util.Collections.unmodifiableList(shuffleWriteRecords_) : shuffleWriteRecords_;
      }
      /**
       * <code>repeated double shuffle_write_records = 13;</code>
       * @return The count of shuffleWriteRecords.
       */
      public int getShuffleWriteRecordsCount() {
        return shuffleWriteRecords_.size();
      }
      /**
       * <code>repeated double shuffle_write_records = 13;</code>
       * @param index The index of the element to return.
       * @return The shuffleWriteRecords at the given index.
       */
      public double getShuffleWriteRecords(int index) {
        return shuffleWriteRecords_.getDouble(index);
      }
      /**
       * <code>repeated double shuffle_write_records = 13;</code>
       * @param index The index to set the value at.
       * @param value The shuffleWriteRecords to set.
       * @return This builder for chaining.
       */
      public Builder setShuffleWriteRecords(
          int index, double value) {
        ensureShuffleWriteRecordsIsMutable();
        shuffleWriteRecords_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_write_records = 13;</code>
       * @param value The shuffleWriteRecords to add.
       * @return This builder for chaining.
       */
      public Builder addShuffleWriteRecords(double value) {
        ensureShuffleWriteRecordsIsMutable();
        shuffleWriteRecords_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_write_records = 13;</code>
       * @param values The shuffleWriteRecords to add.
       * @return This builder for chaining.
       */
      public Builder addAllShuffleWriteRecords(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureShuffleWriteRecordsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, shuffleWriteRecords_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double shuffle_write_records = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearShuffleWriteRecords() {
        shuffleWriteRecords_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00001000);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList memoryBytesSpilled_ = emptyDoubleList();
      private void ensureMemoryBytesSpilledIsMutable() {
        if (!((bitField0_ & 0x00002000) != 0)) {
          memoryBytesSpilled_ = mutableCopy(memoryBytesSpilled_);
          bitField0_ |= 0x00002000;
         }
      }
      /**
       * <code>repeated double memory_bytes_spilled = 14;</code>
       * @return A list containing the memoryBytesSpilled.
       */
      public java.util.List<java.lang.Double>
          getMemoryBytesSpilledList() {
        return ((bitField0_ & 0x00002000) != 0) ?
                 java.util.Collections.unmodifiableList(memoryBytesSpilled_) : memoryBytesSpilled_;
      }
      /**
       * <code>repeated double memory_bytes_spilled = 14;</code>
       * @return The count of memoryBytesSpilled.
       */
      public int getMemoryBytesSpilledCount() {
        return memoryBytesSpilled_.size();
      }
      /**
       * <code>repeated double memory_bytes_spilled = 14;</code>
       * @param index The index of the element to return.
       * @return The memoryBytesSpilled at the given index.
       */
      public double getMemoryBytesSpilled(int index) {
        return memoryBytesSpilled_.getDouble(index);
      }
      /**
       * <code>repeated double memory_bytes_spilled = 14;</code>
       * @param index The index to set the value at.
       * @param value The memoryBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryBytesSpilled(
          int index, double value) {
        ensureMemoryBytesSpilledIsMutable();
        memoryBytesSpilled_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double memory_bytes_spilled = 14;</code>
       * @param value The memoryBytesSpilled to add.
       * @return This builder for chaining.
       */
      public Builder addMemoryBytesSpilled(double value) {
        ensureMemoryBytesSpilledIsMutable();
        memoryBytesSpilled_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double memory_bytes_spilled = 14;</code>
       * @param values The memoryBytesSpilled to add.
       * @return This builder for chaining.
       */
      public Builder addAllMemoryBytesSpilled(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureMemoryBytesSpilledIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, memoryBytesSpilled_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double memory_bytes_spilled = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryBytesSpilled() {
        memoryBytesSpilled_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00002000);
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.DoubleList diskBytesSpilled_ = emptyDoubleList();
      private void ensureDiskBytesSpilledIsMutable() {
        if (!((bitField0_ & 0x00004000) != 0)) {
          diskBytesSpilled_ = mutableCopy(diskBytesSpilled_);
          bitField0_ |= 0x00004000;
         }
      }
      /**
       * <code>repeated double disk_bytes_spilled = 15;</code>
       * @return A list containing the diskBytesSpilled.
       */
      public java.util.List<java.lang.Double>
          getDiskBytesSpilledList() {
        return ((bitField0_ & 0x00004000) != 0) ?
                 java.util.Collections.unmodifiableList(diskBytesSpilled_) : diskBytesSpilled_;
      }
      /**
       * <code>repeated double disk_bytes_spilled = 15;</code>
       * @return The count of diskBytesSpilled.
       */
      public int getDiskBytesSpilledCount() {
        return diskBytesSpilled_.size();
      }
      /**
       * <code>repeated double disk_bytes_spilled = 15;</code>
       * @param index The index of the element to return.
       * @return The diskBytesSpilled at the given index.
       */
      public double getDiskBytesSpilled(int index) {
        return diskBytesSpilled_.getDouble(index);
      }
      /**
       * <code>repeated double disk_bytes_spilled = 15;</code>
       * @param index The index to set the value at.
       * @param value The diskBytesSpilled to set.
       * @return This builder for chaining.
       */
      public Builder setDiskBytesSpilled(
          int index, double value) {
        ensureDiskBytesSpilledIsMutable();
        diskBytesSpilled_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double disk_bytes_spilled = 15;</code>
       * @param value The diskBytesSpilled to add.
       * @return This builder for chaining.
       */
      public Builder addDiskBytesSpilled(double value) {
        ensureDiskBytesSpilledIsMutable();
        diskBytesSpilled_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double disk_bytes_spilled = 15;</code>
       * @param values The diskBytesSpilled to add.
       * @return This builder for chaining.
       */
      public Builder addAllDiskBytesSpilled(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureDiskBytesSpilledIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, diskBytesSpilled_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double disk_bytes_spilled = 15;</code>
       * @return This builder for chaining.
       */
      public Builder clearDiskBytesSpilled() {
        diskBytesSpilled_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00004000);
        onChanged();
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions peakMemoryMetrics_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions, org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributionsOrBuilder> peakMemoryMetricsBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       * @return Whether the peakMemoryMetrics field is set.
       */
      public boolean hasPeakMemoryMetrics() {
        return peakMemoryMetricsBuilder_ != null || peakMemoryMetrics_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       * @return The peakMemoryMetrics.
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions getPeakMemoryMetrics() {
        if (peakMemoryMetricsBuilder_ == null) {
          return peakMemoryMetrics_ == null ? org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.getDefaultInstance() : peakMemoryMetrics_;
        } else {
          return peakMemoryMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       */
      public Builder setPeakMemoryMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions value) {
        if (peakMemoryMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          peakMemoryMetrics_ = value;
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       */
      public Builder setPeakMemoryMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.Builder builderForValue) {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = builderForValue.build();
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       */
      public Builder mergePeakMemoryMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions value) {
        if (peakMemoryMetricsBuilder_ == null) {
          if (peakMemoryMetrics_ != null) {
            peakMemoryMetrics_ =
              org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.newBuilder(peakMemoryMetrics_).mergeFrom(value).buildPartial();
          } else {
            peakMemoryMetrics_ = value;
          }
          onChanged();
        } else {
          peakMemoryMetricsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       */
      public Builder clearPeakMemoryMetrics() {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetrics_ = null;
          onChanged();
        } else {
          peakMemoryMetrics_ = null;
          peakMemoryMetricsBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.Builder getPeakMemoryMetricsBuilder() {
        
        onChanged();
        return getPeakMemoryMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributionsOrBuilder getPeakMemoryMetricsOrBuilder() {
        if (peakMemoryMetricsBuilder_ != null) {
          return peakMemoryMetricsBuilder_.getMessageOrBuilder();
        } else {
          return peakMemoryMetrics_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.getDefaultInstance() : peakMemoryMetrics_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions peak_memory_metrics = 16;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions, org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributionsOrBuilder> 
          getPeakMemoryMetricsFieldBuilder() {
        if (peakMemoryMetricsBuilder_ == null) {
          peakMemoryMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions, org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributionsOrBuilder>(
                  getPeakMemoryMetrics(),
                  getParentForChildren(),
                  isClean());
          peakMemoryMetrics_ = null;
        }
        return peakMemoryMetricsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ExecutorMetricsDistributions)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ExecutorMetricsDistributions)
    private static final org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ExecutorMetricsDistributions>
        PARSER = new com.google.protobuf.AbstractParser<ExecutorMetricsDistributions>() {
      @java.lang.Override
      public ExecutorMetricsDistributions parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ExecutorMetricsDistributions(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ExecutorMetricsDistributions> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ExecutorMetricsDistributions> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsDistributions getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExecutorPeakMetricsDistributionsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated double quantiles = 1;</code>
     * @return A list containing the quantiles.
     */
    java.util.List<java.lang.Double> getQuantilesList();
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return The count of quantiles.
     */
    int getQuantilesCount();
    /**
     * <code>repeated double quantiles = 1;</code>
     * @param index The index of the element to return.
     * @return The quantiles at the given index.
     */
    double getQuantiles(int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics> 
        getExecutorMetricsList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getExecutorMetrics(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    int getExecutorMetricsCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> 
        getExecutorMetricsOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getExecutorMetricsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions}
   */
  public static final class ExecutorPeakMetricsDistributions extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions)
      ExecutorPeakMetricsDistributionsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ExecutorPeakMetricsDistributions.newBuilder() to construct.
    private ExecutorPeakMetricsDistributions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ExecutorPeakMetricsDistributions() {
      quantiles_ = emptyDoubleList();
      executorMetrics_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ExecutorPeakMetricsDistributions();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ExecutorPeakMetricsDistributions(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                quantiles_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              quantiles_.addDouble(input.readDouble());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                quantiles_ = newDoubleList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                quantiles_.addDouble(input.readDouble());
              }
              input.popLimit(limit);
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                executorMetrics_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics>();
                mutable_bitField0_ |= 0x00000002;
              }
              executorMetrics_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          quantiles_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          executorMetrics_ = java.util.Collections.unmodifiableList(executorMetrics_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.Builder.class);
    }

    public static final int QUANTILES_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.DoubleList quantiles_;
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return A list containing the quantiles.
     */
    @java.lang.Override
    public java.util.List<java.lang.Double>
        getQuantilesList() {
      return quantiles_;
    }
    /**
     * <code>repeated double quantiles = 1;</code>
     * @return The count of quantiles.
     */
    public int getQuantilesCount() {
      return quantiles_.size();
    }
    /**
     * <code>repeated double quantiles = 1;</code>
     * @param index The index of the element to return.
     * @return The quantiles at the given index.
     */
    public double getQuantiles(int index) {
      return quantiles_.getDouble(index);
    }
    private int quantilesMemoizedSerializedSize = -1;

    public static final int EXECUTOR_METRICS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics> executorMetrics_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics> getExecutorMetricsList() {
      return executorMetrics_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> 
        getExecutorMetricsOrBuilderList() {
      return executorMetrics_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    @java.lang.Override
    public int getExecutorMetricsCount() {
      return executorMetrics_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getExecutorMetrics(int index) {
      return executorMetrics_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getExecutorMetricsOrBuilder(
        int index) {
      return executorMetrics_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getQuantilesList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(quantilesMemoizedSerializedSize);
      }
      for (int i = 0; i < quantiles_.size(); i++) {
        output.writeDoubleNoTag(quantiles_.getDouble(i));
      }
      for (int i = 0; i < executorMetrics_.size(); i++) {
        output.writeMessage(2, executorMetrics_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        dataSize = 8 * getQuantilesList().size();
        size += dataSize;
        if (!getQuantilesList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        quantilesMemoizedSerializedSize = dataSize;
      }
      for (int i = 0; i < executorMetrics_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, executorMetrics_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions other = (org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions) obj;

      if (!getQuantilesList()
          .equals(other.getQuantilesList())) return false;
      if (!getExecutorMetricsList()
          .equals(other.getExecutorMetricsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getQuantilesCount() > 0) {
        hash = (37 * hash) + QUANTILES_FIELD_NUMBER;
        hash = (53 * hash) + getQuantilesList().hashCode();
      }
      if (getExecutorMetricsCount() > 0) {
        hash = (37 * hash) + EXECUTOR_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorMetricsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions)
        org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.class, org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getExecutorMetricsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        quantiles_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        if (executorMetricsBuilder_ == null) {
          executorMetrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          executorMetricsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions build() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions result = new org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          quantiles_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.quantiles_ = quantiles_;
        if (executorMetricsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            executorMetrics_ = java.util.Collections.unmodifiableList(executorMetrics_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.executorMetrics_ = executorMetrics_;
        } else {
          result.executorMetrics_ = executorMetricsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions.getDefaultInstance()) return this;
        if (!other.quantiles_.isEmpty()) {
          if (quantiles_.isEmpty()) {
            quantiles_ = other.quantiles_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureQuantilesIsMutable();
            quantiles_.addAll(other.quantiles_);
          }
          onChanged();
        }
        if (executorMetricsBuilder_ == null) {
          if (!other.executorMetrics_.isEmpty()) {
            if (executorMetrics_.isEmpty()) {
              executorMetrics_ = other.executorMetrics_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureExecutorMetricsIsMutable();
              executorMetrics_.addAll(other.executorMetrics_);
            }
            onChanged();
          }
        } else {
          if (!other.executorMetrics_.isEmpty()) {
            if (executorMetricsBuilder_.isEmpty()) {
              executorMetricsBuilder_.dispose();
              executorMetricsBuilder_ = null;
              executorMetrics_ = other.executorMetrics_;
              bitField0_ = (bitField0_ & ~0x00000002);
              executorMetricsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getExecutorMetricsFieldBuilder() : null;
            } else {
              executorMetricsBuilder_.addAllMessages(other.executorMetrics_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.DoubleList quantiles_ = emptyDoubleList();
      private void ensureQuantilesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          quantiles_ = mutableCopy(quantiles_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return A list containing the quantiles.
       */
      public java.util.List<java.lang.Double>
          getQuantilesList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(quantiles_) : quantiles_;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return The count of quantiles.
       */
      public int getQuantilesCount() {
        return quantiles_.size();
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param index The index of the element to return.
       * @return The quantiles at the given index.
       */
      public double getQuantiles(int index) {
        return quantiles_.getDouble(index);
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param index The index to set the value at.
       * @param value The quantiles to set.
       * @return This builder for chaining.
       */
      public Builder setQuantiles(
          int index, double value) {
        ensureQuantilesIsMutable();
        quantiles_.setDouble(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param value The quantiles to add.
       * @return This builder for chaining.
       */
      public Builder addQuantiles(double value) {
        ensureQuantilesIsMutable();
        quantiles_.addDouble(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @param values The quantiles to add.
       * @return This builder for chaining.
       */
      public Builder addAllQuantiles(
          java.lang.Iterable<? extends java.lang.Double> values) {
        ensureQuantilesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, quantiles_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated double quantiles = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearQuantiles() {
        quantiles_ = emptyDoubleList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics> executorMetrics_ =
        java.util.Collections.emptyList();
      private void ensureExecutorMetricsIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          executorMetrics_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics>(executorMetrics_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> executorMetricsBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics> getExecutorMetricsList() {
        if (executorMetricsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(executorMetrics_);
        } else {
          return executorMetricsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public int getExecutorMetricsCount() {
        if (executorMetricsBuilder_ == null) {
          return executorMetrics_.size();
        } else {
          return executorMetricsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics getExecutorMetrics(int index) {
        if (executorMetricsBuilder_ == null) {
          return executorMetrics_.get(index);
        } else {
          return executorMetricsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder setExecutorMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (executorMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureExecutorMetricsIsMutable();
          executorMetrics_.set(index, value);
          onChanged();
        } else {
          executorMetricsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder setExecutorMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder builderForValue) {
        if (executorMetricsBuilder_ == null) {
          ensureExecutorMetricsIsMutable();
          executorMetrics_.set(index, builderForValue.build());
          onChanged();
        } else {
          executorMetricsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder addExecutorMetrics(org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (executorMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureExecutorMetricsIsMutable();
          executorMetrics_.add(value);
          onChanged();
        } else {
          executorMetricsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder addExecutorMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics value) {
        if (executorMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureExecutorMetricsIsMutable();
          executorMetrics_.add(index, value);
          onChanged();
        } else {
          executorMetricsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder addExecutorMetrics(
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder builderForValue) {
        if (executorMetricsBuilder_ == null) {
          ensureExecutorMetricsIsMutable();
          executorMetrics_.add(builderForValue.build());
          onChanged();
        } else {
          executorMetricsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder addExecutorMetrics(
          int index, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder builderForValue) {
        if (executorMetricsBuilder_ == null) {
          ensureExecutorMetricsIsMutable();
          executorMetrics_.add(index, builderForValue.build());
          onChanged();
        } else {
          executorMetricsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder addAllExecutorMetrics(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics> values) {
        if (executorMetricsBuilder_ == null) {
          ensureExecutorMetricsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, executorMetrics_);
          onChanged();
        } else {
          executorMetricsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder clearExecutorMetrics() {
        if (executorMetricsBuilder_ == null) {
          executorMetrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          executorMetricsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public Builder removeExecutorMetrics(int index) {
        if (executorMetricsBuilder_ == null) {
          ensureExecutorMetricsIsMutable();
          executorMetrics_.remove(index);
          onChanged();
        } else {
          executorMetricsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder getExecutorMetricsBuilder(
          int index) {
        return getExecutorMetricsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder getExecutorMetricsOrBuilder(
          int index) {
        if (executorMetricsBuilder_ == null) {
          return executorMetrics_.get(index);  } else {
          return executorMetricsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> 
           getExecutorMetricsOrBuilderList() {
        if (executorMetricsBuilder_ != null) {
          return executorMetricsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(executorMetrics_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder addExecutorMetricsBuilder() {
        return getExecutorMetricsFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder addExecutorMetricsBuilder(
          int index) {
        return getExecutorMetricsFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.ExecutorMetrics executor_metrics = 2;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder> 
           getExecutorMetricsBuilderList() {
        return getExecutorMetricsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder> 
          getExecutorMetricsFieldBuilder() {
        if (executorMetricsBuilder_ == null) {
          executorMetricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetrics.Builder, org.apache.spark.status.protobuf.StoreTypes.ExecutorMetricsOrBuilder>(
                  executorMetrics_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          executorMetrics_ = null;
        }
        return executorMetricsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.ExecutorPeakMetricsDistributions)
    private static final org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ExecutorPeakMetricsDistributions>
        PARSER = new com.google.protobuf.AbstractParser<ExecutorPeakMetricsDistributions>() {
      @java.lang.Override
      public ExecutorPeakMetricsDistributions parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ExecutorPeakMetricsDistributions(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ExecutorPeakMetricsDistributions> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ExecutorPeakMetricsDistributions> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.ExecutorPeakMetricsDistributions getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface AppSummaryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.AppSummary)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int32 num_completed_jobs = 1;</code>
     * @return The numCompletedJobs.
     */
    int getNumCompletedJobs();

    /**
     * <code>int32 num_completed_stages = 2;</code>
     * @return The numCompletedStages.
     */
    int getNumCompletedStages();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.AppSummary}
   */
  public static final class AppSummary extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.AppSummary)
      AppSummaryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use AppSummary.newBuilder() to construct.
    private AppSummary(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private AppSummary() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new AppSummary();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private AppSummary(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              numCompletedJobs_ = input.readInt32();
              break;
            }
            case 16: {

              numCompletedStages_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AppSummary_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AppSummary_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.AppSummary.class, org.apache.spark.status.protobuf.StoreTypes.AppSummary.Builder.class);
    }

    public static final int NUM_COMPLETED_JOBS_FIELD_NUMBER = 1;
    private int numCompletedJobs_;
    /**
     * <code>int32 num_completed_jobs = 1;</code>
     * @return The numCompletedJobs.
     */
    @java.lang.Override
    public int getNumCompletedJobs() {
      return numCompletedJobs_;
    }

    public static final int NUM_COMPLETED_STAGES_FIELD_NUMBER = 2;
    private int numCompletedStages_;
    /**
     * <code>int32 num_completed_stages = 2;</code>
     * @return The numCompletedStages.
     */
    @java.lang.Override
    public int getNumCompletedStages() {
      return numCompletedStages_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (numCompletedJobs_ != 0) {
        output.writeInt32(1, numCompletedJobs_);
      }
      if (numCompletedStages_ != 0) {
        output.writeInt32(2, numCompletedStages_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (numCompletedJobs_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, numCompletedJobs_);
      }
      if (numCompletedStages_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, numCompletedStages_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.AppSummary)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.AppSummary other = (org.apache.spark.status.protobuf.StoreTypes.AppSummary) obj;

      if (getNumCompletedJobs()
          != other.getNumCompletedJobs()) return false;
      if (getNumCompletedStages()
          != other.getNumCompletedStages()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NUM_COMPLETED_JOBS_FIELD_NUMBER;
      hash = (53 * hash) + getNumCompletedJobs();
      hash = (37 * hash) + NUM_COMPLETED_STAGES_FIELD_NUMBER;
      hash = (53 * hash) + getNumCompletedStages();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.AppSummary prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.AppSummary}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.AppSummary)
        org.apache.spark.status.protobuf.StoreTypes.AppSummaryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AppSummary_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AppSummary_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.AppSummary.class, org.apache.spark.status.protobuf.StoreTypes.AppSummary.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.AppSummary.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        numCompletedJobs_ = 0;

        numCompletedStages_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_AppSummary_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.AppSummary getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.AppSummary.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.AppSummary build() {
        org.apache.spark.status.protobuf.StoreTypes.AppSummary result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.AppSummary buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.AppSummary result = new org.apache.spark.status.protobuf.StoreTypes.AppSummary(this);
        result.numCompletedJobs_ = numCompletedJobs_;
        result.numCompletedStages_ = numCompletedStages_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.AppSummary) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.AppSummary)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.AppSummary other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.AppSummary.getDefaultInstance()) return this;
        if (other.getNumCompletedJobs() != 0) {
          setNumCompletedJobs(other.getNumCompletedJobs());
        }
        if (other.getNumCompletedStages() != 0) {
          setNumCompletedStages(other.getNumCompletedStages());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.AppSummary parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.AppSummary) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private int numCompletedJobs_ ;
      /**
       * <code>int32 num_completed_jobs = 1;</code>
       * @return The numCompletedJobs.
       */
      @java.lang.Override
      public int getNumCompletedJobs() {
        return numCompletedJobs_;
      }
      /**
       * <code>int32 num_completed_jobs = 1;</code>
       * @param value The numCompletedJobs to set.
       * @return This builder for chaining.
       */
      public Builder setNumCompletedJobs(int value) {
        
        numCompletedJobs_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_completed_jobs = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCompletedJobs() {
        
        numCompletedJobs_ = 0;
        onChanged();
        return this;
      }

      private int numCompletedStages_ ;
      /**
       * <code>int32 num_completed_stages = 2;</code>
       * @return The numCompletedStages.
       */
      @java.lang.Override
      public int getNumCompletedStages() {
        return numCompletedStages_;
      }
      /**
       * <code>int32 num_completed_stages = 2;</code>
       * @param value The numCompletedStages to set.
       * @return This builder for chaining.
       */
      public Builder setNumCompletedStages(int value) {
        
        numCompletedStages_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 num_completed_stages = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumCompletedStages() {
        
        numCompletedStages_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.AppSummary)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.AppSummary)
    private static final org.apache.spark.status.protobuf.StoreTypes.AppSummary DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.AppSummary();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.AppSummary getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<AppSummary>
        PARSER = new com.google.protobuf.AbstractParser<AppSummary>() {
      @java.lang.Override
      public AppSummary parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AppSummary(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<AppSummary> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<AppSummary> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.AppSummary getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface PoolDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.PoolData)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>repeated int64 stage_ids = 2;</code>
     * @return A list containing the stageIds.
     */
    java.util.List<java.lang.Long> getStageIdsList();
    /**
     * <code>repeated int64 stage_ids = 2;</code>
     * @return The count of stageIds.
     */
    int getStageIdsCount();
    /**
     * <code>repeated int64 stage_ids = 2;</code>
     * @param index The index of the element to return.
     * @return The stageIds at the given index.
     */
    long getStageIds(int index);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.PoolData}
   */
  public static final class PoolData extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.PoolData)
      PoolDataOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use PoolData.newBuilder() to construct.
    private PoolData(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private PoolData() {
      name_ = "";
      stageIds_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new PoolData();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private PoolData(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              name_ = s;
              break;
            }
            case 16: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                stageIds_ = newLongList();
                mutable_bitField0_ |= 0x00000002;
              }
              stageIds_.addLong(input.readInt64());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                stageIds_ = newLongList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                stageIds_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          stageIds_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PoolData_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PoolData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.PoolData.class, org.apache.spark.status.protobuf.StoreTypes.PoolData.Builder.class);
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 1;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int STAGE_IDS_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.LongList stageIds_;
    /**
     * <code>repeated int64 stage_ids = 2;</code>
     * @return A list containing the stageIds.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getStageIdsList() {
      return stageIds_;
    }
    /**
     * <code>repeated int64 stage_ids = 2;</code>
     * @return The count of stageIds.
     */
    public int getStageIdsCount() {
      return stageIds_.size();
    }
    /**
     * <code>repeated int64 stage_ids = 2;</code>
     * @param index The index of the element to return.
     * @return The stageIds at the given index.
     */
    public long getStageIds(int index) {
      return stageIds_.getLong(index);
    }
    private int stageIdsMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (getStageIdsList().size() > 0) {
        output.writeUInt32NoTag(18);
        output.writeUInt32NoTag(stageIdsMemoizedSerializedSize);
      }
      for (int i = 0; i < stageIds_.size(); i++) {
        output.writeInt64NoTag(stageIds_.getLong(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < stageIds_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(stageIds_.getLong(i));
        }
        size += dataSize;
        if (!getStageIdsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        stageIdsMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.PoolData)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.PoolData other = (org.apache.spark.status.protobuf.StoreTypes.PoolData) obj;

      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (!getStageIdsList()
          .equals(other.getStageIdsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (getStageIdsCount() > 0) {
        hash = (37 * hash) + STAGE_IDS_FIELD_NUMBER;
        hash = (53 * hash) + getStageIdsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.PoolData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.PoolData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.PoolData}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.PoolData)
        org.apache.spark.status.protobuf.StoreTypes.PoolDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PoolData_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PoolData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.PoolData.class, org.apache.spark.status.protobuf.StoreTypes.PoolData.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.PoolData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        stageIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_PoolData_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.PoolData getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.PoolData.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.PoolData build() {
        org.apache.spark.status.protobuf.StoreTypes.PoolData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.PoolData buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.PoolData result = new org.apache.spark.status.protobuf.StoreTypes.PoolData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((bitField0_ & 0x00000002) != 0)) {
          stageIds_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.stageIds_ = stageIds_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.PoolData) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.PoolData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.PoolData other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.PoolData.getDefaultInstance()) return this;
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (!other.stageIds_.isEmpty()) {
          if (stageIds_.isEmpty()) {
            stageIds_ = other.stageIds_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureStageIdsIsMutable();
            stageIds_.addAll(other.stageIds_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.PoolData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.PoolData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList stageIds_ = emptyLongList();
      private void ensureStageIdsIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          stageIds_ = mutableCopy(stageIds_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated int64 stage_ids = 2;</code>
       * @return A list containing the stageIds.
       */
      public java.util.List<java.lang.Long>
          getStageIdsList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(stageIds_) : stageIds_;
      }
      /**
       * <code>repeated int64 stage_ids = 2;</code>
       * @return The count of stageIds.
       */
      public int getStageIdsCount() {
        return stageIds_.size();
      }
      /**
       * <code>repeated int64 stage_ids = 2;</code>
       * @param index The index of the element to return.
       * @return The stageIds at the given index.
       */
      public long getStageIds(int index) {
        return stageIds_.getLong(index);
      }
      /**
       * <code>repeated int64 stage_ids = 2;</code>
       * @param index The index to set the value at.
       * @param value The stageIds to set.
       * @return This builder for chaining.
       */
      public Builder setStageIds(
          int index, long value) {
        ensureStageIdsIsMutable();
        stageIds_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stage_ids = 2;</code>
       * @param value The stageIds to add.
       * @return This builder for chaining.
       */
      public Builder addStageIds(long value) {
        ensureStageIdsIsMutable();
        stageIds_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stage_ids = 2;</code>
       * @param values The stageIds to add.
       * @return This builder for chaining.
       */
      public Builder addAllStageIds(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureStageIdsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, stageIds_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated int64 stage_ids = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearStageIds() {
        stageIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.PoolData)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.PoolData)
    private static final org.apache.spark.status.protobuf.StoreTypes.PoolData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.PoolData();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.PoolData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<PoolData>
        PARSER = new com.google.protobuf.AbstractParser<PoolData>() {
      @java.lang.Override
      public PoolData parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new PoolData(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<PoolData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<PoolData> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.PoolData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StateOperatorProgressOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.StateOperatorProgress)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string operator_name = 1;</code>
     * @return Whether the operatorName field is set.
     */
    boolean hasOperatorName();
    /**
     * <code>string operator_name = 1;</code>
     * @return The operatorName.
     */
    java.lang.String getOperatorName();
    /**
     * <code>string operator_name = 1;</code>
     * @return The bytes for operatorName.
     */
    com.google.protobuf.ByteString
        getOperatorNameBytes();

    /**
     * <code>int64 num_rows_total = 2;</code>
     * @return The numRowsTotal.
     */
    long getNumRowsTotal();

    /**
     * <code>int64 num_rows_updated = 3;</code>
     * @return The numRowsUpdated.
     */
    long getNumRowsUpdated();

    /**
     * <code>int64 all_updates_time_ms = 4;</code>
     * @return The allUpdatesTimeMs.
     */
    long getAllUpdatesTimeMs();

    /**
     * <code>int64 num_rows_removed = 5;</code>
     * @return The numRowsRemoved.
     */
    long getNumRowsRemoved();

    /**
     * <code>int64 all_removals_time_ms = 6;</code>
     * @return The allRemovalsTimeMs.
     */
    long getAllRemovalsTimeMs();

    /**
     * <code>int64 commit_time_ms = 7;</code>
     * @return The commitTimeMs.
     */
    long getCommitTimeMs();

    /**
     * <code>int64 memory_used_bytes = 8;</code>
     * @return The memoryUsedBytes.
     */
    long getMemoryUsedBytes();

    /**
     * <code>int64 num_rows_dropped_by_watermark = 9;</code>
     * @return The numRowsDroppedByWatermark.
     */
    long getNumRowsDroppedByWatermark();

    /**
     * <code>int64 num_shuffle_partitions = 10;</code>
     * @return The numShufflePartitions.
     */
    long getNumShufflePartitions();

    /**
     * <code>int64 num_state_store_instances = 11;</code>
     * @return The numStateStoreInstances.
     */
    long getNumStateStoreInstances();

    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */
    int getCustomMetricsCount();
    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */
    boolean containsCustomMetrics(
        java.lang.String key);
    /**
     * Use {@link #getCustomMetricsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.Long>
    getCustomMetrics();
    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */
    java.util.Map<java.lang.String, java.lang.Long>
    getCustomMetricsMap();
    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */

    long getCustomMetricsOrDefault(
        java.lang.String key,
        long defaultValue);
    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */

    long getCustomMetricsOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.StateOperatorProgress}
   */
  public static final class StateOperatorProgress extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.StateOperatorProgress)
      StateOperatorProgressOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StateOperatorProgress.newBuilder() to construct.
    private StateOperatorProgress(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StateOperatorProgress() {
      operatorName_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StateOperatorProgress();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StateOperatorProgress(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              operatorName_ = s;
              break;
            }
            case 16: {

              numRowsTotal_ = input.readInt64();
              break;
            }
            case 24: {

              numRowsUpdated_ = input.readInt64();
              break;
            }
            case 32: {

              allUpdatesTimeMs_ = input.readInt64();
              break;
            }
            case 40: {

              numRowsRemoved_ = input.readInt64();
              break;
            }
            case 48: {

              allRemovalsTimeMs_ = input.readInt64();
              break;
            }
            case 56: {

              commitTimeMs_ = input.readInt64();
              break;
            }
            case 64: {

              memoryUsedBytes_ = input.readInt64();
              break;
            }
            case 72: {

              numRowsDroppedByWatermark_ = input.readInt64();
              break;
            }
            case 80: {

              numShufflePartitions_ = input.readInt64();
              break;
            }
            case 88: {

              numStateStoreInstances_ = input.readInt64();
              break;
            }
            case 98: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                customMetrics_ = com.google.protobuf.MapField.newMapField(
                    CustomMetricsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000002;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.Long>
              customMetrics__ = input.readMessage(
                  CustomMetricsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              customMetrics_.getMutableMap().put(
                  customMetrics__.getKey(), customMetrics__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 12:
          return internalGetCustomMetrics();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.class, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder.class);
    }

    private int bitField0_;
    public static final int OPERATOR_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object operatorName_;
    /**
     * <code>string operator_name = 1;</code>
     * @return Whether the operatorName field is set.
     */
    @java.lang.Override
    public boolean hasOperatorName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string operator_name = 1;</code>
     * @return The operatorName.
     */
    @java.lang.Override
    public java.lang.String getOperatorName() {
      java.lang.Object ref = operatorName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        operatorName_ = s;
        return s;
      }
    }
    /**
     * <code>string operator_name = 1;</code>
     * @return The bytes for operatorName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getOperatorNameBytes() {
      java.lang.Object ref = operatorName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        operatorName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int NUM_ROWS_TOTAL_FIELD_NUMBER = 2;
    private long numRowsTotal_;
    /**
     * <code>int64 num_rows_total = 2;</code>
     * @return The numRowsTotal.
     */
    @java.lang.Override
    public long getNumRowsTotal() {
      return numRowsTotal_;
    }

    public static final int NUM_ROWS_UPDATED_FIELD_NUMBER = 3;
    private long numRowsUpdated_;
    /**
     * <code>int64 num_rows_updated = 3;</code>
     * @return The numRowsUpdated.
     */
    @java.lang.Override
    public long getNumRowsUpdated() {
      return numRowsUpdated_;
    }

    public static final int ALL_UPDATES_TIME_MS_FIELD_NUMBER = 4;
    private long allUpdatesTimeMs_;
    /**
     * <code>int64 all_updates_time_ms = 4;</code>
     * @return The allUpdatesTimeMs.
     */
    @java.lang.Override
    public long getAllUpdatesTimeMs() {
      return allUpdatesTimeMs_;
    }

    public static final int NUM_ROWS_REMOVED_FIELD_NUMBER = 5;
    private long numRowsRemoved_;
    /**
     * <code>int64 num_rows_removed = 5;</code>
     * @return The numRowsRemoved.
     */
    @java.lang.Override
    public long getNumRowsRemoved() {
      return numRowsRemoved_;
    }

    public static final int ALL_REMOVALS_TIME_MS_FIELD_NUMBER = 6;
    private long allRemovalsTimeMs_;
    /**
     * <code>int64 all_removals_time_ms = 6;</code>
     * @return The allRemovalsTimeMs.
     */
    @java.lang.Override
    public long getAllRemovalsTimeMs() {
      return allRemovalsTimeMs_;
    }

    public static final int COMMIT_TIME_MS_FIELD_NUMBER = 7;
    private long commitTimeMs_;
    /**
     * <code>int64 commit_time_ms = 7;</code>
     * @return The commitTimeMs.
     */
    @java.lang.Override
    public long getCommitTimeMs() {
      return commitTimeMs_;
    }

    public static final int MEMORY_USED_BYTES_FIELD_NUMBER = 8;
    private long memoryUsedBytes_;
    /**
     * <code>int64 memory_used_bytes = 8;</code>
     * @return The memoryUsedBytes.
     */
    @java.lang.Override
    public long getMemoryUsedBytes() {
      return memoryUsedBytes_;
    }

    public static final int NUM_ROWS_DROPPED_BY_WATERMARK_FIELD_NUMBER = 9;
    private long numRowsDroppedByWatermark_;
    /**
     * <code>int64 num_rows_dropped_by_watermark = 9;</code>
     * @return The numRowsDroppedByWatermark.
     */
    @java.lang.Override
    public long getNumRowsDroppedByWatermark() {
      return numRowsDroppedByWatermark_;
    }

    public static final int NUM_SHUFFLE_PARTITIONS_FIELD_NUMBER = 10;
    private long numShufflePartitions_;
    /**
     * <code>int64 num_shuffle_partitions = 10;</code>
     * @return The numShufflePartitions.
     */
    @java.lang.Override
    public long getNumShufflePartitions() {
      return numShufflePartitions_;
    }

    public static final int NUM_STATE_STORE_INSTANCES_FIELD_NUMBER = 11;
    private long numStateStoreInstances_;
    /**
     * <code>int64 num_state_store_instances = 11;</code>
     * @return The numStateStoreInstances.
     */
    @java.lang.Override
    public long getNumStateStoreInstances() {
      return numStateStoreInstances_;
    }

    public static final int CUSTOM_METRICS_FIELD_NUMBER = 12;
    private static final class CustomMetricsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.Long> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.Long>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_CustomMetricsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.INT64,
                  0L);
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.Long> customMetrics_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
    internalGetCustomMetrics() {
      if (customMetrics_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            CustomMetricsDefaultEntryHolder.defaultEntry);
      }
      return customMetrics_;
    }

    public int getCustomMetricsCount() {
      return internalGetCustomMetrics().getMap().size();
    }
    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */

    @java.lang.Override
    public boolean containsCustomMetrics(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetCustomMetrics().getMap().containsKey(key);
    }
    /**
     * Use {@link #getCustomMetricsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.Long> getCustomMetrics() {
      return getCustomMetricsMap();
    }
    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.Long> getCustomMetricsMap() {
      return internalGetCustomMetrics().getMap();
    }
    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */
    @java.lang.Override

    public long getCustomMetricsOrDefault(
        java.lang.String key,
        long defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Long> map =
          internalGetCustomMetrics().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
     */
    @java.lang.Override

    public long getCustomMetricsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Long> map =
          internalGetCustomMetrics().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, operatorName_);
      }
      if (numRowsTotal_ != 0L) {
        output.writeInt64(2, numRowsTotal_);
      }
      if (numRowsUpdated_ != 0L) {
        output.writeInt64(3, numRowsUpdated_);
      }
      if (allUpdatesTimeMs_ != 0L) {
        output.writeInt64(4, allUpdatesTimeMs_);
      }
      if (numRowsRemoved_ != 0L) {
        output.writeInt64(5, numRowsRemoved_);
      }
      if (allRemovalsTimeMs_ != 0L) {
        output.writeInt64(6, allRemovalsTimeMs_);
      }
      if (commitTimeMs_ != 0L) {
        output.writeInt64(7, commitTimeMs_);
      }
      if (memoryUsedBytes_ != 0L) {
        output.writeInt64(8, memoryUsedBytes_);
      }
      if (numRowsDroppedByWatermark_ != 0L) {
        output.writeInt64(9, numRowsDroppedByWatermark_);
      }
      if (numShufflePartitions_ != 0L) {
        output.writeInt64(10, numShufflePartitions_);
      }
      if (numStateStoreInstances_ != 0L) {
        output.writeInt64(11, numStateStoreInstances_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetCustomMetrics(),
          CustomMetricsDefaultEntryHolder.defaultEntry,
          12);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, operatorName_);
      }
      if (numRowsTotal_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, numRowsTotal_);
      }
      if (numRowsUpdated_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, numRowsUpdated_);
      }
      if (allUpdatesTimeMs_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, allUpdatesTimeMs_);
      }
      if (numRowsRemoved_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, numRowsRemoved_);
      }
      if (allRemovalsTimeMs_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, allRemovalsTimeMs_);
      }
      if (commitTimeMs_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, commitTimeMs_);
      }
      if (memoryUsedBytes_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, memoryUsedBytes_);
      }
      if (numRowsDroppedByWatermark_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(9, numRowsDroppedByWatermark_);
      }
      if (numShufflePartitions_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(10, numShufflePartitions_);
      }
      if (numStateStoreInstances_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(11, numStateStoreInstances_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.Long> entry
           : internalGetCustomMetrics().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.Long>
        customMetrics__ = CustomMetricsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(12, customMetrics__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress other = (org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress) obj;

      if (hasOperatorName() != other.hasOperatorName()) return false;
      if (hasOperatorName()) {
        if (!getOperatorName()
            .equals(other.getOperatorName())) return false;
      }
      if (getNumRowsTotal()
          != other.getNumRowsTotal()) return false;
      if (getNumRowsUpdated()
          != other.getNumRowsUpdated()) return false;
      if (getAllUpdatesTimeMs()
          != other.getAllUpdatesTimeMs()) return false;
      if (getNumRowsRemoved()
          != other.getNumRowsRemoved()) return false;
      if (getAllRemovalsTimeMs()
          != other.getAllRemovalsTimeMs()) return false;
      if (getCommitTimeMs()
          != other.getCommitTimeMs()) return false;
      if (getMemoryUsedBytes()
          != other.getMemoryUsedBytes()) return false;
      if (getNumRowsDroppedByWatermark()
          != other.getNumRowsDroppedByWatermark()) return false;
      if (getNumShufflePartitions()
          != other.getNumShufflePartitions()) return false;
      if (getNumStateStoreInstances()
          != other.getNumStateStoreInstances()) return false;
      if (!internalGetCustomMetrics().equals(
          other.internalGetCustomMetrics())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasOperatorName()) {
        hash = (37 * hash) + OPERATOR_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getOperatorName().hashCode();
      }
      hash = (37 * hash) + NUM_ROWS_TOTAL_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getNumRowsTotal());
      hash = (37 * hash) + NUM_ROWS_UPDATED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getNumRowsUpdated());
      hash = (37 * hash) + ALL_UPDATES_TIME_MS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getAllUpdatesTimeMs());
      hash = (37 * hash) + NUM_ROWS_REMOVED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getNumRowsRemoved());
      hash = (37 * hash) + ALL_REMOVALS_TIME_MS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getAllRemovalsTimeMs());
      hash = (37 * hash) + COMMIT_TIME_MS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getCommitTimeMs());
      hash = (37 * hash) + MEMORY_USED_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMemoryUsedBytes());
      hash = (37 * hash) + NUM_ROWS_DROPPED_BY_WATERMARK_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getNumRowsDroppedByWatermark());
      hash = (37 * hash) + NUM_SHUFFLE_PARTITIONS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getNumShufflePartitions());
      hash = (37 * hash) + NUM_STATE_STORE_INSTANCES_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getNumStateStoreInstances());
      if (!internalGetCustomMetrics().getMap().isEmpty()) {
        hash = (37 * hash) + CUSTOM_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetCustomMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.StateOperatorProgress}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.StateOperatorProgress)
        org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 12:
            return internalGetCustomMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 12:
            return internalGetMutableCustomMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.class, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        operatorName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        numRowsTotal_ = 0L;

        numRowsUpdated_ = 0L;

        allUpdatesTimeMs_ = 0L;

        numRowsRemoved_ = 0L;

        allRemovalsTimeMs_ = 0L;

        commitTimeMs_ = 0L;

        memoryUsedBytes_ = 0L;

        numRowsDroppedByWatermark_ = 0L;

        numShufflePartitions_ = 0L;

        numStateStoreInstances_ = 0L;

        internalGetMutableCustomMetrics().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress build() {
        org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress result = new org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.operatorName_ = operatorName_;
        result.numRowsTotal_ = numRowsTotal_;
        result.numRowsUpdated_ = numRowsUpdated_;
        result.allUpdatesTimeMs_ = allUpdatesTimeMs_;
        result.numRowsRemoved_ = numRowsRemoved_;
        result.allRemovalsTimeMs_ = allRemovalsTimeMs_;
        result.commitTimeMs_ = commitTimeMs_;
        result.memoryUsedBytes_ = memoryUsedBytes_;
        result.numRowsDroppedByWatermark_ = numRowsDroppedByWatermark_;
        result.numShufflePartitions_ = numShufflePartitions_;
        result.numStateStoreInstances_ = numStateStoreInstances_;
        result.customMetrics_ = internalGetCustomMetrics();
        result.customMetrics_.makeImmutable();
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.getDefaultInstance()) return this;
        if (other.hasOperatorName()) {
          bitField0_ |= 0x00000001;
          operatorName_ = other.operatorName_;
          onChanged();
        }
        if (other.getNumRowsTotal() != 0L) {
          setNumRowsTotal(other.getNumRowsTotal());
        }
        if (other.getNumRowsUpdated() != 0L) {
          setNumRowsUpdated(other.getNumRowsUpdated());
        }
        if (other.getAllUpdatesTimeMs() != 0L) {
          setAllUpdatesTimeMs(other.getAllUpdatesTimeMs());
        }
        if (other.getNumRowsRemoved() != 0L) {
          setNumRowsRemoved(other.getNumRowsRemoved());
        }
        if (other.getAllRemovalsTimeMs() != 0L) {
          setAllRemovalsTimeMs(other.getAllRemovalsTimeMs());
        }
        if (other.getCommitTimeMs() != 0L) {
          setCommitTimeMs(other.getCommitTimeMs());
        }
        if (other.getMemoryUsedBytes() != 0L) {
          setMemoryUsedBytes(other.getMemoryUsedBytes());
        }
        if (other.getNumRowsDroppedByWatermark() != 0L) {
          setNumRowsDroppedByWatermark(other.getNumRowsDroppedByWatermark());
        }
        if (other.getNumShufflePartitions() != 0L) {
          setNumShufflePartitions(other.getNumShufflePartitions());
        }
        if (other.getNumStateStoreInstances() != 0L) {
          setNumStateStoreInstances(other.getNumStateStoreInstances());
        }
        internalGetMutableCustomMetrics().mergeFrom(
            other.internalGetCustomMetrics());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object operatorName_ = "";
      /**
       * <code>string operator_name = 1;</code>
       * @return Whether the operatorName field is set.
       */
      public boolean hasOperatorName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string operator_name = 1;</code>
       * @return The operatorName.
       */
      public java.lang.String getOperatorName() {
        java.lang.Object ref = operatorName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          operatorName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string operator_name = 1;</code>
       * @return The bytes for operatorName.
       */
      public com.google.protobuf.ByteString
          getOperatorNameBytes() {
        java.lang.Object ref = operatorName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          operatorName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string operator_name = 1;</code>
       * @param value The operatorName to set.
       * @return This builder for chaining.
       */
      public Builder setOperatorName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        operatorName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string operator_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearOperatorName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        operatorName_ = getDefaultInstance().getOperatorName();
        onChanged();
        return this;
      }
      /**
       * <code>string operator_name = 1;</code>
       * @param value The bytes for operatorName to set.
       * @return This builder for chaining.
       */
      public Builder setOperatorNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        operatorName_ = value;
        onChanged();
        return this;
      }

      private long numRowsTotal_ ;
      /**
       * <code>int64 num_rows_total = 2;</code>
       * @return The numRowsTotal.
       */
      @java.lang.Override
      public long getNumRowsTotal() {
        return numRowsTotal_;
      }
      /**
       * <code>int64 num_rows_total = 2;</code>
       * @param value The numRowsTotal to set.
       * @return This builder for chaining.
       */
      public Builder setNumRowsTotal(long value) {
        
        numRowsTotal_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 num_rows_total = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumRowsTotal() {
        
        numRowsTotal_ = 0L;
        onChanged();
        return this;
      }

      private long numRowsUpdated_ ;
      /**
       * <code>int64 num_rows_updated = 3;</code>
       * @return The numRowsUpdated.
       */
      @java.lang.Override
      public long getNumRowsUpdated() {
        return numRowsUpdated_;
      }
      /**
       * <code>int64 num_rows_updated = 3;</code>
       * @param value The numRowsUpdated to set.
       * @return This builder for chaining.
       */
      public Builder setNumRowsUpdated(long value) {
        
        numRowsUpdated_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 num_rows_updated = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumRowsUpdated() {
        
        numRowsUpdated_ = 0L;
        onChanged();
        return this;
      }

      private long allUpdatesTimeMs_ ;
      /**
       * <code>int64 all_updates_time_ms = 4;</code>
       * @return The allUpdatesTimeMs.
       */
      @java.lang.Override
      public long getAllUpdatesTimeMs() {
        return allUpdatesTimeMs_;
      }
      /**
       * <code>int64 all_updates_time_ms = 4;</code>
       * @param value The allUpdatesTimeMs to set.
       * @return This builder for chaining.
       */
      public Builder setAllUpdatesTimeMs(long value) {
        
        allUpdatesTimeMs_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 all_updates_time_ms = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearAllUpdatesTimeMs() {
        
        allUpdatesTimeMs_ = 0L;
        onChanged();
        return this;
      }

      private long numRowsRemoved_ ;
      /**
       * <code>int64 num_rows_removed = 5;</code>
       * @return The numRowsRemoved.
       */
      @java.lang.Override
      public long getNumRowsRemoved() {
        return numRowsRemoved_;
      }
      /**
       * <code>int64 num_rows_removed = 5;</code>
       * @param value The numRowsRemoved to set.
       * @return This builder for chaining.
       */
      public Builder setNumRowsRemoved(long value) {
        
        numRowsRemoved_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 num_rows_removed = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumRowsRemoved() {
        
        numRowsRemoved_ = 0L;
        onChanged();
        return this;
      }

      private long allRemovalsTimeMs_ ;
      /**
       * <code>int64 all_removals_time_ms = 6;</code>
       * @return The allRemovalsTimeMs.
       */
      @java.lang.Override
      public long getAllRemovalsTimeMs() {
        return allRemovalsTimeMs_;
      }
      /**
       * <code>int64 all_removals_time_ms = 6;</code>
       * @param value The allRemovalsTimeMs to set.
       * @return This builder for chaining.
       */
      public Builder setAllRemovalsTimeMs(long value) {
        
        allRemovalsTimeMs_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 all_removals_time_ms = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearAllRemovalsTimeMs() {
        
        allRemovalsTimeMs_ = 0L;
        onChanged();
        return this;
      }

      private long commitTimeMs_ ;
      /**
       * <code>int64 commit_time_ms = 7;</code>
       * @return The commitTimeMs.
       */
      @java.lang.Override
      public long getCommitTimeMs() {
        return commitTimeMs_;
      }
      /**
       * <code>int64 commit_time_ms = 7;</code>
       * @param value The commitTimeMs to set.
       * @return This builder for chaining.
       */
      public Builder setCommitTimeMs(long value) {
        
        commitTimeMs_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 commit_time_ms = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearCommitTimeMs() {
        
        commitTimeMs_ = 0L;
        onChanged();
        return this;
      }

      private long memoryUsedBytes_ ;
      /**
       * <code>int64 memory_used_bytes = 8;</code>
       * @return The memoryUsedBytes.
       */
      @java.lang.Override
      public long getMemoryUsedBytes() {
        return memoryUsedBytes_;
      }
      /**
       * <code>int64 memory_used_bytes = 8;</code>
       * @param value The memoryUsedBytes to set.
       * @return This builder for chaining.
       */
      public Builder setMemoryUsedBytes(long value) {
        
        memoryUsedBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 memory_used_bytes = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearMemoryUsedBytes() {
        
        memoryUsedBytes_ = 0L;
        onChanged();
        return this;
      }

      private long numRowsDroppedByWatermark_ ;
      /**
       * <code>int64 num_rows_dropped_by_watermark = 9;</code>
       * @return The numRowsDroppedByWatermark.
       */
      @java.lang.Override
      public long getNumRowsDroppedByWatermark() {
        return numRowsDroppedByWatermark_;
      }
      /**
       * <code>int64 num_rows_dropped_by_watermark = 9;</code>
       * @param value The numRowsDroppedByWatermark to set.
       * @return This builder for chaining.
       */
      public Builder setNumRowsDroppedByWatermark(long value) {
        
        numRowsDroppedByWatermark_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 num_rows_dropped_by_watermark = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumRowsDroppedByWatermark() {
        
        numRowsDroppedByWatermark_ = 0L;
        onChanged();
        return this;
      }

      private long numShufflePartitions_ ;
      /**
       * <code>int64 num_shuffle_partitions = 10;</code>
       * @return The numShufflePartitions.
       */
      @java.lang.Override
      public long getNumShufflePartitions() {
        return numShufflePartitions_;
      }
      /**
       * <code>int64 num_shuffle_partitions = 10;</code>
       * @param value The numShufflePartitions to set.
       * @return This builder for chaining.
       */
      public Builder setNumShufflePartitions(long value) {
        
        numShufflePartitions_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 num_shuffle_partitions = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumShufflePartitions() {
        
        numShufflePartitions_ = 0L;
        onChanged();
        return this;
      }

      private long numStateStoreInstances_ ;
      /**
       * <code>int64 num_state_store_instances = 11;</code>
       * @return The numStateStoreInstances.
       */
      @java.lang.Override
      public long getNumStateStoreInstances() {
        return numStateStoreInstances_;
      }
      /**
       * <code>int64 num_state_store_instances = 11;</code>
       * @param value The numStateStoreInstances to set.
       * @return This builder for chaining.
       */
      public Builder setNumStateStoreInstances(long value) {
        
        numStateStoreInstances_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 num_state_store_instances = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumStateStoreInstances() {
        
        numStateStoreInstances_ = 0L;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.Long> customMetrics_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
      internalGetCustomMetrics() {
        if (customMetrics_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              CustomMetricsDefaultEntryHolder.defaultEntry);
        }
        return customMetrics_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
      internalGetMutableCustomMetrics() {
        onChanged();;
        if (customMetrics_ == null) {
          customMetrics_ = com.google.protobuf.MapField.newMapField(
              CustomMetricsDefaultEntryHolder.defaultEntry);
        }
        if (!customMetrics_.isMutable()) {
          customMetrics_ = customMetrics_.copy();
        }
        return customMetrics_;
      }

      public int getCustomMetricsCount() {
        return internalGetCustomMetrics().getMap().size();
      }
      /**
       * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
       */

      @java.lang.Override
      public boolean containsCustomMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetCustomMetrics().getMap().containsKey(key);
      }
      /**
       * Use {@link #getCustomMetricsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Long> getCustomMetrics() {
        return getCustomMetricsMap();
      }
      /**
       * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.Long> getCustomMetricsMap() {
        return internalGetCustomMetrics().getMap();
      }
      /**
       * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
       */
      @java.lang.Override

      public long getCustomMetricsOrDefault(
          java.lang.String key,
          long defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Long> map =
            internalGetCustomMetrics().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
       */
      @java.lang.Override

      public long getCustomMetricsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Long> map =
            internalGetCustomMetrics().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearCustomMetrics() {
        internalGetMutableCustomMetrics().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
       */

      public Builder removeCustomMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableCustomMetrics().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Long>
      getMutableCustomMetrics() {
        return internalGetMutableCustomMetrics().getMutableMap();
      }
      /**
       * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
       */
      public Builder putCustomMetrics(
          java.lang.String key,
          long value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        
        internalGetMutableCustomMetrics().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, int64&gt; custom_metrics = 12;</code>
       */

      public Builder putAllCustomMetrics(
          java.util.Map<java.lang.String, java.lang.Long> values) {
        internalGetMutableCustomMetrics().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.StateOperatorProgress)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.StateOperatorProgress)
    private static final org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StateOperatorProgress>
        PARSER = new com.google.protobuf.AbstractParser<StateOperatorProgress>() {
      @java.lang.Override
      public StateOperatorProgress parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StateOperatorProgress(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StateOperatorProgress> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StateOperatorProgress> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SourceProgressOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SourceProgress)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string description = 1;</code>
     * @return Whether the description field is set.
     */
    boolean hasDescription();
    /**
     * <code>string description = 1;</code>
     * @return The description.
     */
    java.lang.String getDescription();
    /**
     * <code>string description = 1;</code>
     * @return The bytes for description.
     */
    com.google.protobuf.ByteString
        getDescriptionBytes();

    /**
     * <code>string start_offset = 2;</code>
     * @return Whether the startOffset field is set.
     */
    boolean hasStartOffset();
    /**
     * <code>string start_offset = 2;</code>
     * @return The startOffset.
     */
    java.lang.String getStartOffset();
    /**
     * <code>string start_offset = 2;</code>
     * @return The bytes for startOffset.
     */
    com.google.protobuf.ByteString
        getStartOffsetBytes();

    /**
     * <code>string end_offset = 3;</code>
     * @return Whether the endOffset field is set.
     */
    boolean hasEndOffset();
    /**
     * <code>string end_offset = 3;</code>
     * @return The endOffset.
     */
    java.lang.String getEndOffset();
    /**
     * <code>string end_offset = 3;</code>
     * @return The bytes for endOffset.
     */
    com.google.protobuf.ByteString
        getEndOffsetBytes();

    /**
     * <code>string latest_offset = 4;</code>
     * @return Whether the latestOffset field is set.
     */
    boolean hasLatestOffset();
    /**
     * <code>string latest_offset = 4;</code>
     * @return The latestOffset.
     */
    java.lang.String getLatestOffset();
    /**
     * <code>string latest_offset = 4;</code>
     * @return The bytes for latestOffset.
     */
    com.google.protobuf.ByteString
        getLatestOffsetBytes();

    /**
     * <code>int64 num_input_rows = 5;</code>
     * @return The numInputRows.
     */
    long getNumInputRows();

    /**
     * <code>double input_rows_per_second = 6;</code>
     * @return The inputRowsPerSecond.
     */
    double getInputRowsPerSecond();

    /**
     * <code>double processed_rows_per_second = 7;</code>
     * @return The processedRowsPerSecond.
     */
    double getProcessedRowsPerSecond();

    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */
    int getMetricsCount();
    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */
    boolean containsMetrics(
        java.lang.String key);
    /**
     * Use {@link #getMetricsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getMetrics();
    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getMetricsMap();
    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */

    java.lang.String getMetricsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */

    java.lang.String getMetricsOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SourceProgress}
   */
  public static final class SourceProgress extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SourceProgress)
      SourceProgressOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SourceProgress.newBuilder() to construct.
    private SourceProgress(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SourceProgress() {
      description_ = "";
      startOffset_ = "";
      endOffset_ = "";
      latestOffset_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SourceProgress();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SourceProgress(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              description_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              startOffset_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              endOffset_ = s;
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000008;
              latestOffset_ = s;
              break;
            }
            case 40: {

              numInputRows_ = input.readInt64();
              break;
            }
            case 49: {

              inputRowsPerSecond_ = input.readDouble();
              break;
            }
            case 57: {

              processedRowsPerSecond_ = input.readDouble();
              break;
            }
            case 66: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                metrics_ = com.google.protobuf.MapField.newMapField(
                    MetricsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000010;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              metrics__ = input.readMessage(
                  MetricsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              metrics_.getMutableMap().put(
                  metrics__.getKey(), metrics__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SourceProgress_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 8:
          return internalGetMetrics();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SourceProgress_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SourceProgress.class, org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder.class);
    }

    private int bitField0_;
    public static final int DESCRIPTION_FIELD_NUMBER = 1;
    private volatile java.lang.Object description_;
    /**
     * <code>string description = 1;</code>
     * @return Whether the description field is set.
     */
    @java.lang.Override
    public boolean hasDescription() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string description = 1;</code>
     * @return The description.
     */
    @java.lang.Override
    public java.lang.String getDescription() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        description_ = s;
        return s;
      }
    }
    /**
     * <code>string description = 1;</code>
     * @return The bytes for description.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDescriptionBytes() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        description_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int START_OFFSET_FIELD_NUMBER = 2;
    private volatile java.lang.Object startOffset_;
    /**
     * <code>string start_offset = 2;</code>
     * @return Whether the startOffset field is set.
     */
    @java.lang.Override
    public boolean hasStartOffset() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string start_offset = 2;</code>
     * @return The startOffset.
     */
    @java.lang.Override
    public java.lang.String getStartOffset() {
      java.lang.Object ref = startOffset_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        startOffset_ = s;
        return s;
      }
    }
    /**
     * <code>string start_offset = 2;</code>
     * @return The bytes for startOffset.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStartOffsetBytes() {
      java.lang.Object ref = startOffset_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        startOffset_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int END_OFFSET_FIELD_NUMBER = 3;
    private volatile java.lang.Object endOffset_;
    /**
     * <code>string end_offset = 3;</code>
     * @return Whether the endOffset field is set.
     */
    @java.lang.Override
    public boolean hasEndOffset() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string end_offset = 3;</code>
     * @return The endOffset.
     */
    @java.lang.Override
    public java.lang.String getEndOffset() {
      java.lang.Object ref = endOffset_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        endOffset_ = s;
        return s;
      }
    }
    /**
     * <code>string end_offset = 3;</code>
     * @return The bytes for endOffset.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getEndOffsetBytes() {
      java.lang.Object ref = endOffset_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        endOffset_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int LATEST_OFFSET_FIELD_NUMBER = 4;
    private volatile java.lang.Object latestOffset_;
    /**
     * <code>string latest_offset = 4;</code>
     * @return Whether the latestOffset field is set.
     */
    @java.lang.Override
    public boolean hasLatestOffset() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>string latest_offset = 4;</code>
     * @return The latestOffset.
     */
    @java.lang.Override
    public java.lang.String getLatestOffset() {
      java.lang.Object ref = latestOffset_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        latestOffset_ = s;
        return s;
      }
    }
    /**
     * <code>string latest_offset = 4;</code>
     * @return The bytes for latestOffset.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getLatestOffsetBytes() {
      java.lang.Object ref = latestOffset_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        latestOffset_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int NUM_INPUT_ROWS_FIELD_NUMBER = 5;
    private long numInputRows_;
    /**
     * <code>int64 num_input_rows = 5;</code>
     * @return The numInputRows.
     */
    @java.lang.Override
    public long getNumInputRows() {
      return numInputRows_;
    }

    public static final int INPUT_ROWS_PER_SECOND_FIELD_NUMBER = 6;
    private double inputRowsPerSecond_;
    /**
     * <code>double input_rows_per_second = 6;</code>
     * @return The inputRowsPerSecond.
     */
    @java.lang.Override
    public double getInputRowsPerSecond() {
      return inputRowsPerSecond_;
    }

    public static final int PROCESSED_ROWS_PER_SECOND_FIELD_NUMBER = 7;
    private double processedRowsPerSecond_;
    /**
     * <code>double processed_rows_per_second = 7;</code>
     * @return The processedRowsPerSecond.
     */
    @java.lang.Override
    public double getProcessedRowsPerSecond() {
      return processedRowsPerSecond_;
    }

    public static final int METRICS_FIELD_NUMBER = 8;
    private static final class MetricsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SourceProgress_MetricsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> metrics_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetMetrics() {
      if (metrics_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            MetricsDefaultEntryHolder.defaultEntry);
      }
      return metrics_;
    }

    public int getMetricsCount() {
      return internalGetMetrics().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */

    @java.lang.Override
    public boolean containsMetrics(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetMetrics().getMap().containsKey(key);
    }
    /**
     * Use {@link #getMetricsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getMetrics() {
      return getMetricsMap();
    }
    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getMetricsMap() {
      return internalGetMetrics().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetrics().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; metrics = 8;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetrics().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, description_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, startOffset_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, endOffset_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, latestOffset_);
      }
      if (numInputRows_ != 0L) {
        output.writeInt64(5, numInputRows_);
      }
      if (inputRowsPerSecond_ != 0D) {
        output.writeDouble(6, inputRowsPerSecond_);
      }
      if (processedRowsPerSecond_ != 0D) {
        output.writeDouble(7, processedRowsPerSecond_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetMetrics(),
          MetricsDefaultEntryHolder.defaultEntry,
          8);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, description_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, startOffset_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, endOffset_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, latestOffset_);
      }
      if (numInputRows_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, numInputRows_);
      }
      if (inputRowsPerSecond_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(6, inputRowsPerSecond_);
      }
      if (processedRowsPerSecond_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(7, processedRowsPerSecond_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetMetrics().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        metrics__ = MetricsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(8, metrics__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SourceProgress)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SourceProgress other = (org.apache.spark.status.protobuf.StoreTypes.SourceProgress) obj;

      if (hasDescription() != other.hasDescription()) return false;
      if (hasDescription()) {
        if (!getDescription()
            .equals(other.getDescription())) return false;
      }
      if (hasStartOffset() != other.hasStartOffset()) return false;
      if (hasStartOffset()) {
        if (!getStartOffset()
            .equals(other.getStartOffset())) return false;
      }
      if (hasEndOffset() != other.hasEndOffset()) return false;
      if (hasEndOffset()) {
        if (!getEndOffset()
            .equals(other.getEndOffset())) return false;
      }
      if (hasLatestOffset() != other.hasLatestOffset()) return false;
      if (hasLatestOffset()) {
        if (!getLatestOffset()
            .equals(other.getLatestOffset())) return false;
      }
      if (getNumInputRows()
          != other.getNumInputRows()) return false;
      if (java.lang.Double.doubleToLongBits(getInputRowsPerSecond())
          != java.lang.Double.doubleToLongBits(
              other.getInputRowsPerSecond())) return false;
      if (java.lang.Double.doubleToLongBits(getProcessedRowsPerSecond())
          != java.lang.Double.doubleToLongBits(
              other.getProcessedRowsPerSecond())) return false;
      if (!internalGetMetrics().equals(
          other.internalGetMetrics())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasDescription()) {
        hash = (37 * hash) + DESCRIPTION_FIELD_NUMBER;
        hash = (53 * hash) + getDescription().hashCode();
      }
      if (hasStartOffset()) {
        hash = (37 * hash) + START_OFFSET_FIELD_NUMBER;
        hash = (53 * hash) + getStartOffset().hashCode();
      }
      if (hasEndOffset()) {
        hash = (37 * hash) + END_OFFSET_FIELD_NUMBER;
        hash = (53 * hash) + getEndOffset().hashCode();
      }
      if (hasLatestOffset()) {
        hash = (37 * hash) + LATEST_OFFSET_FIELD_NUMBER;
        hash = (53 * hash) + getLatestOffset().hashCode();
      }
      hash = (37 * hash) + NUM_INPUT_ROWS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getNumInputRows());
      hash = (37 * hash) + INPUT_ROWS_PER_SECOND_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getInputRowsPerSecond()));
      hash = (37 * hash) + PROCESSED_ROWS_PER_SECOND_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getProcessedRowsPerSecond()));
      if (!internalGetMetrics().getMap().isEmpty()) {
        hash = (37 * hash) + METRICS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SourceProgress prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SourceProgress}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SourceProgress)
        org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SourceProgress_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 8:
            return internalGetMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 8:
            return internalGetMutableMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SourceProgress_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SourceProgress.class, org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SourceProgress.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        description_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        startOffset_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        endOffset_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        latestOffset_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        numInputRows_ = 0L;

        inputRowsPerSecond_ = 0D;

        processedRowsPerSecond_ = 0D;

        internalGetMutableMetrics().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SourceProgress_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SourceProgress getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SourceProgress.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SourceProgress build() {
        org.apache.spark.status.protobuf.StoreTypes.SourceProgress result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SourceProgress buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SourceProgress result = new org.apache.spark.status.protobuf.StoreTypes.SourceProgress(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.description_ = description_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.startOffset_ = startOffset_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.endOffset_ = endOffset_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.latestOffset_ = latestOffset_;
        result.numInputRows_ = numInputRows_;
        result.inputRowsPerSecond_ = inputRowsPerSecond_;
        result.processedRowsPerSecond_ = processedRowsPerSecond_;
        result.metrics_ = internalGetMetrics();
        result.metrics_.makeImmutable();
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SourceProgress) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SourceProgress)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SourceProgress other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SourceProgress.getDefaultInstance()) return this;
        if (other.hasDescription()) {
          bitField0_ |= 0x00000001;
          description_ = other.description_;
          onChanged();
        }
        if (other.hasStartOffset()) {
          bitField0_ |= 0x00000002;
          startOffset_ = other.startOffset_;
          onChanged();
        }
        if (other.hasEndOffset()) {
          bitField0_ |= 0x00000004;
          endOffset_ = other.endOffset_;
          onChanged();
        }
        if (other.hasLatestOffset()) {
          bitField0_ |= 0x00000008;
          latestOffset_ = other.latestOffset_;
          onChanged();
        }
        if (other.getNumInputRows() != 0L) {
          setNumInputRows(other.getNumInputRows());
        }
        if (other.getInputRowsPerSecond() != 0D) {
          setInputRowsPerSecond(other.getInputRowsPerSecond());
        }
        if (other.getProcessedRowsPerSecond() != 0D) {
          setProcessedRowsPerSecond(other.getProcessedRowsPerSecond());
        }
        internalGetMutableMetrics().mergeFrom(
            other.internalGetMetrics());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SourceProgress parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SourceProgress) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object description_ = "";
      /**
       * <code>string description = 1;</code>
       * @return Whether the description field is set.
       */
      public boolean hasDescription() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string description = 1;</code>
       * @return The description.
       */
      public java.lang.String getDescription() {
        java.lang.Object ref = description_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          description_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string description = 1;</code>
       * @return The bytes for description.
       */
      public com.google.protobuf.ByteString
          getDescriptionBytes() {
        java.lang.Object ref = description_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          description_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string description = 1;</code>
       * @param value The description to set.
       * @return This builder for chaining.
       */
      public Builder setDescription(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        description_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string description = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearDescription() {
        bitField0_ = (bitField0_ & ~0x00000001);
        description_ = getDefaultInstance().getDescription();
        onChanged();
        return this;
      }
      /**
       * <code>string description = 1;</code>
       * @param value The bytes for description to set.
       * @return This builder for chaining.
       */
      public Builder setDescriptionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        description_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object startOffset_ = "";
      /**
       * <code>string start_offset = 2;</code>
       * @return Whether the startOffset field is set.
       */
      public boolean hasStartOffset() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string start_offset = 2;</code>
       * @return The startOffset.
       */
      public java.lang.String getStartOffset() {
        java.lang.Object ref = startOffset_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          startOffset_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string start_offset = 2;</code>
       * @return The bytes for startOffset.
       */
      public com.google.protobuf.ByteString
          getStartOffsetBytes() {
        java.lang.Object ref = startOffset_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          startOffset_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string start_offset = 2;</code>
       * @param value The startOffset to set.
       * @return This builder for chaining.
       */
      public Builder setStartOffset(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        startOffset_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string start_offset = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearStartOffset() {
        bitField0_ = (bitField0_ & ~0x00000002);
        startOffset_ = getDefaultInstance().getStartOffset();
        onChanged();
        return this;
      }
      /**
       * <code>string start_offset = 2;</code>
       * @param value The bytes for startOffset to set.
       * @return This builder for chaining.
       */
      public Builder setStartOffsetBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        startOffset_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object endOffset_ = "";
      /**
       * <code>string end_offset = 3;</code>
       * @return Whether the endOffset field is set.
       */
      public boolean hasEndOffset() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string end_offset = 3;</code>
       * @return The endOffset.
       */
      public java.lang.String getEndOffset() {
        java.lang.Object ref = endOffset_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          endOffset_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string end_offset = 3;</code>
       * @return The bytes for endOffset.
       */
      public com.google.protobuf.ByteString
          getEndOffsetBytes() {
        java.lang.Object ref = endOffset_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          endOffset_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string end_offset = 3;</code>
       * @param value The endOffset to set.
       * @return This builder for chaining.
       */
      public Builder setEndOffset(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        endOffset_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string end_offset = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearEndOffset() {
        bitField0_ = (bitField0_ & ~0x00000004);
        endOffset_ = getDefaultInstance().getEndOffset();
        onChanged();
        return this;
      }
      /**
       * <code>string end_offset = 3;</code>
       * @param value The bytes for endOffset to set.
       * @return This builder for chaining.
       */
      public Builder setEndOffsetBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        endOffset_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object latestOffset_ = "";
      /**
       * <code>string latest_offset = 4;</code>
       * @return Whether the latestOffset field is set.
       */
      public boolean hasLatestOffset() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>string latest_offset = 4;</code>
       * @return The latestOffset.
       */
      public java.lang.String getLatestOffset() {
        java.lang.Object ref = latestOffset_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          latestOffset_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string latest_offset = 4;</code>
       * @return The bytes for latestOffset.
       */
      public com.google.protobuf.ByteString
          getLatestOffsetBytes() {
        java.lang.Object ref = latestOffset_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          latestOffset_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string latest_offset = 4;</code>
       * @param value The latestOffset to set.
       * @return This builder for chaining.
       */
      public Builder setLatestOffset(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        latestOffset_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string latest_offset = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearLatestOffset() {
        bitField0_ = (bitField0_ & ~0x00000008);
        latestOffset_ = getDefaultInstance().getLatestOffset();
        onChanged();
        return this;
      }
      /**
       * <code>string latest_offset = 4;</code>
       * @param value The bytes for latestOffset to set.
       * @return This builder for chaining.
       */
      public Builder setLatestOffsetBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000008;
        latestOffset_ = value;
        onChanged();
        return this;
      }

      private long numInputRows_ ;
      /**
       * <code>int64 num_input_rows = 5;</code>
       * @return The numInputRows.
       */
      @java.lang.Override
      public long getNumInputRows() {
        return numInputRows_;
      }
      /**
       * <code>int64 num_input_rows = 5;</code>
       * @param value The numInputRows to set.
       * @return This builder for chaining.
       */
      public Builder setNumInputRows(long value) {
        
        numInputRows_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 num_input_rows = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumInputRows() {
        
        numInputRows_ = 0L;
        onChanged();
        return this;
      }

      private double inputRowsPerSecond_ ;
      /**
       * <code>double input_rows_per_second = 6;</code>
       * @return The inputRowsPerSecond.
       */
      @java.lang.Override
      public double getInputRowsPerSecond() {
        return inputRowsPerSecond_;
      }
      /**
       * <code>double input_rows_per_second = 6;</code>
       * @param value The inputRowsPerSecond to set.
       * @return This builder for chaining.
       */
      public Builder setInputRowsPerSecond(double value) {
        
        inputRowsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double input_rows_per_second = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputRowsPerSecond() {
        
        inputRowsPerSecond_ = 0D;
        onChanged();
        return this;
      }

      private double processedRowsPerSecond_ ;
      /**
       * <code>double processed_rows_per_second = 7;</code>
       * @return The processedRowsPerSecond.
       */
      @java.lang.Override
      public double getProcessedRowsPerSecond() {
        return processedRowsPerSecond_;
      }
      /**
       * <code>double processed_rows_per_second = 7;</code>
       * @param value The processedRowsPerSecond to set.
       * @return This builder for chaining.
       */
      public Builder setProcessedRowsPerSecond(double value) {
        
        processedRowsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double processed_rows_per_second = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearProcessedRowsPerSecond() {
        
        processedRowsPerSecond_ = 0D;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> metrics_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMetrics() {
        if (metrics_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              MetricsDefaultEntryHolder.defaultEntry);
        }
        return metrics_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableMetrics() {
        onChanged();;
        if (metrics_ == null) {
          metrics_ = com.google.protobuf.MapField.newMapField(
              MetricsDefaultEntryHolder.defaultEntry);
        }
        if (!metrics_.isMutable()) {
          metrics_ = metrics_.copy();
        }
        return metrics_;
      }

      public int getMetricsCount() {
        return internalGetMetrics().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 8;</code>
       */

      @java.lang.Override
      public boolean containsMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetMetrics().getMap().containsKey(key);
      }
      /**
       * Use {@link #getMetricsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getMetrics() {
        return getMetricsMap();
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 8;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getMetricsMap() {
        return internalGetMetrics().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 8;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetrics().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 8;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetrics().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearMetrics() {
        internalGetMutableMetrics().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 8;</code>
       */

      public Builder removeMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetrics().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableMetrics() {
        return internalGetMutableMetrics().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 8;</code>
       */
      public Builder putMetrics(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetrics().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 8;</code>
       */

      public Builder putAllMetrics(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableMetrics().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SourceProgress)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SourceProgress)
    private static final org.apache.spark.status.protobuf.StoreTypes.SourceProgress DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SourceProgress();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SourceProgress getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SourceProgress>
        PARSER = new com.google.protobuf.AbstractParser<SourceProgress>() {
      @java.lang.Override
      public SourceProgress parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SourceProgress(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SourceProgress> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SourceProgress> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SourceProgress getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SinkProgressOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.SinkProgress)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string description = 1;</code>
     * @return Whether the description field is set.
     */
    boolean hasDescription();
    /**
     * <code>string description = 1;</code>
     * @return The description.
     */
    java.lang.String getDescription();
    /**
     * <code>string description = 1;</code>
     * @return The bytes for description.
     */
    com.google.protobuf.ByteString
        getDescriptionBytes();

    /**
     * <code>int64 num_output_rows = 2;</code>
     * @return The numOutputRows.
     */
    long getNumOutputRows();

    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */
    int getMetricsCount();
    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */
    boolean containsMetrics(
        java.lang.String key);
    /**
     * Use {@link #getMetricsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getMetrics();
    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getMetricsMap();
    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */

    java.lang.String getMetricsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */

    java.lang.String getMetricsOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.SinkProgress}
   */
  public static final class SinkProgress extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.SinkProgress)
      SinkProgressOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SinkProgress.newBuilder() to construct.
    private SinkProgress(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SinkProgress() {
      description_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SinkProgress();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SinkProgress(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              description_ = s;
              break;
            }
            case 16: {

              numOutputRows_ = input.readInt64();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                metrics_ = com.google.protobuf.MapField.newMapField(
                    MetricsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000002;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              metrics__ = input.readMessage(
                  MetricsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              metrics_.getMutableMap().put(
                  metrics__.getKey(), metrics__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SinkProgress_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 3:
          return internalGetMetrics();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SinkProgress_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.SinkProgress.class, org.apache.spark.status.protobuf.StoreTypes.SinkProgress.Builder.class);
    }

    private int bitField0_;
    public static final int DESCRIPTION_FIELD_NUMBER = 1;
    private volatile java.lang.Object description_;
    /**
     * <code>string description = 1;</code>
     * @return Whether the description field is set.
     */
    @java.lang.Override
    public boolean hasDescription() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string description = 1;</code>
     * @return The description.
     */
    @java.lang.Override
    public java.lang.String getDescription() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        description_ = s;
        return s;
      }
    }
    /**
     * <code>string description = 1;</code>
     * @return The bytes for description.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDescriptionBytes() {
      java.lang.Object ref = description_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        description_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int NUM_OUTPUT_ROWS_FIELD_NUMBER = 2;
    private long numOutputRows_;
    /**
     * <code>int64 num_output_rows = 2;</code>
     * @return The numOutputRows.
     */
    @java.lang.Override
    public long getNumOutputRows() {
      return numOutputRows_;
    }

    public static final int METRICS_FIELD_NUMBER = 3;
    private static final class MetricsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SinkProgress_MetricsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> metrics_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetMetrics() {
      if (metrics_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            MetricsDefaultEntryHolder.defaultEntry);
      }
      return metrics_;
    }

    public int getMetricsCount() {
      return internalGetMetrics().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */

    @java.lang.Override
    public boolean containsMetrics(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetMetrics().getMap().containsKey(key);
    }
    /**
     * Use {@link #getMetricsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getMetrics() {
      return getMetricsMap();
    }
    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getMetricsMap() {
      return internalGetMetrics().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetrics().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; metrics = 3;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetrics().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, description_);
      }
      if (numOutputRows_ != 0L) {
        output.writeInt64(2, numOutputRows_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetMetrics(),
          MetricsDefaultEntryHolder.defaultEntry,
          3);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, description_);
      }
      if (numOutputRows_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, numOutputRows_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetMetrics().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        metrics__ = MetricsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(3, metrics__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.SinkProgress)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.SinkProgress other = (org.apache.spark.status.protobuf.StoreTypes.SinkProgress) obj;

      if (hasDescription() != other.hasDescription()) return false;
      if (hasDescription()) {
        if (!getDescription()
            .equals(other.getDescription())) return false;
      }
      if (getNumOutputRows()
          != other.getNumOutputRows()) return false;
      if (!internalGetMetrics().equals(
          other.internalGetMetrics())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasDescription()) {
        hash = (37 * hash) + DESCRIPTION_FIELD_NUMBER;
        hash = (53 * hash) + getDescription().hashCode();
      }
      hash = (37 * hash) + NUM_OUTPUT_ROWS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getNumOutputRows());
      if (!internalGetMetrics().getMap().isEmpty()) {
        hash = (37 * hash) + METRICS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.SinkProgress prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.SinkProgress}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.SinkProgress)
        org.apache.spark.status.protobuf.StoreTypes.SinkProgressOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SinkProgress_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetMutableMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SinkProgress_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.SinkProgress.class, org.apache.spark.status.protobuf.StoreTypes.SinkProgress.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.SinkProgress.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        description_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        numOutputRows_ = 0L;

        internalGetMutableMetrics().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_SinkProgress_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SinkProgress getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.SinkProgress.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SinkProgress build() {
        org.apache.spark.status.protobuf.StoreTypes.SinkProgress result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.SinkProgress buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.SinkProgress result = new org.apache.spark.status.protobuf.StoreTypes.SinkProgress(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.description_ = description_;
        result.numOutputRows_ = numOutputRows_;
        result.metrics_ = internalGetMetrics();
        result.metrics_.makeImmutable();
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.SinkProgress) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.SinkProgress)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.SinkProgress other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.SinkProgress.getDefaultInstance()) return this;
        if (other.hasDescription()) {
          bitField0_ |= 0x00000001;
          description_ = other.description_;
          onChanged();
        }
        if (other.getNumOutputRows() != 0L) {
          setNumOutputRows(other.getNumOutputRows());
        }
        internalGetMutableMetrics().mergeFrom(
            other.internalGetMetrics());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.SinkProgress parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.SinkProgress) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object description_ = "";
      /**
       * <code>string description = 1;</code>
       * @return Whether the description field is set.
       */
      public boolean hasDescription() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string description = 1;</code>
       * @return The description.
       */
      public java.lang.String getDescription() {
        java.lang.Object ref = description_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          description_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string description = 1;</code>
       * @return The bytes for description.
       */
      public com.google.protobuf.ByteString
          getDescriptionBytes() {
        java.lang.Object ref = description_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          description_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string description = 1;</code>
       * @param value The description to set.
       * @return This builder for chaining.
       */
      public Builder setDescription(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        description_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string description = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearDescription() {
        bitField0_ = (bitField0_ & ~0x00000001);
        description_ = getDefaultInstance().getDescription();
        onChanged();
        return this;
      }
      /**
       * <code>string description = 1;</code>
       * @param value The bytes for description to set.
       * @return This builder for chaining.
       */
      public Builder setDescriptionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        description_ = value;
        onChanged();
        return this;
      }

      private long numOutputRows_ ;
      /**
       * <code>int64 num_output_rows = 2;</code>
       * @return The numOutputRows.
       */
      @java.lang.Override
      public long getNumOutputRows() {
        return numOutputRows_;
      }
      /**
       * <code>int64 num_output_rows = 2;</code>
       * @param value The numOutputRows to set.
       * @return This builder for chaining.
       */
      public Builder setNumOutputRows(long value) {
        
        numOutputRows_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 num_output_rows = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumOutputRows() {
        
        numOutputRows_ = 0L;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> metrics_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMetrics() {
        if (metrics_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              MetricsDefaultEntryHolder.defaultEntry);
        }
        return metrics_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableMetrics() {
        onChanged();;
        if (metrics_ == null) {
          metrics_ = com.google.protobuf.MapField.newMapField(
              MetricsDefaultEntryHolder.defaultEntry);
        }
        if (!metrics_.isMutable()) {
          metrics_ = metrics_.copy();
        }
        return metrics_;
      }

      public int getMetricsCount() {
        return internalGetMetrics().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 3;</code>
       */

      @java.lang.Override
      public boolean containsMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetMetrics().getMap().containsKey(key);
      }
      /**
       * Use {@link #getMetricsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getMetrics() {
        return getMetricsMap();
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 3;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getMetricsMap() {
        return internalGetMetrics().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 3;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetrics().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 3;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetrics().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearMetrics() {
        internalGetMutableMetrics().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 3;</code>
       */

      public Builder removeMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetrics().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableMetrics() {
        return internalGetMutableMetrics().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 3;</code>
       */
      public Builder putMetrics(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetrics().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; metrics = 3;</code>
       */

      public Builder putAllMetrics(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableMetrics().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.SinkProgress)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.SinkProgress)
    private static final org.apache.spark.status.protobuf.StoreTypes.SinkProgress DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.SinkProgress();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.SinkProgress getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SinkProgress>
        PARSER = new com.google.protobuf.AbstractParser<SinkProgress>() {
      @java.lang.Override
      public SinkProgress parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SinkProgress(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SinkProgress> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SinkProgress> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SinkProgress getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StreamingQueryProgressOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.StreamingQueryProgress)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    boolean hasId();
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    java.lang.String getId();
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    com.google.protobuf.ByteString
        getIdBytes();

    /**
     * <code>string run_id = 2;</code>
     * @return Whether the runId field is set.
     */
    boolean hasRunId();
    /**
     * <code>string run_id = 2;</code>
     * @return The runId.
     */
    java.lang.String getRunId();
    /**
     * <code>string run_id = 2;</code>
     * @return The bytes for runId.
     */
    com.google.protobuf.ByteString
        getRunIdBytes();

    /**
     * <code>string name = 3;</code>
     * @return Whether the name field is set.
     */
    boolean hasName();
    /**
     * <code>string name = 3;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 3;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>string timestamp = 4;</code>
     * @return Whether the timestamp field is set.
     */
    boolean hasTimestamp();
    /**
     * <code>string timestamp = 4;</code>
     * @return The timestamp.
     */
    java.lang.String getTimestamp();
    /**
     * <code>string timestamp = 4;</code>
     * @return The bytes for timestamp.
     */
    com.google.protobuf.ByteString
        getTimestampBytes();

    /**
     * <code>int64 batch_id = 5;</code>
     * @return The batchId.
     */
    long getBatchId();

    /**
     * <code>int64 batch_duration = 6;</code>
     * @return The batchDuration.
     */
    long getBatchDuration();

    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */
    int getDurationMsCount();
    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */
    boolean containsDurationMs(
        java.lang.String key);
    /**
     * Use {@link #getDurationMsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.Long>
    getDurationMs();
    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */
    java.util.Map<java.lang.String, java.lang.Long>
    getDurationMsMap();
    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */

    long getDurationMsOrDefault(
        java.lang.String key,
        long defaultValue);
    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */

    long getDurationMsOrThrow(
        java.lang.String key);

    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */
    int getEventTimeCount();
    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */
    boolean containsEventTime(
        java.lang.String key);
    /**
     * Use {@link #getEventTimeMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getEventTime();
    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getEventTimeMap();
    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */

    java.lang.String getEventTimeOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */

    java.lang.String getEventTimeOrThrow(
        java.lang.String key);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress> 
        getStateOperatorsList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress getStateOperators(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    int getStateOperatorsCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder> 
        getStateOperatorsOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder getStateOperatorsOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    java.util.List<org.apache.spark.status.protobuf.StoreTypes.SourceProgress> 
        getSourcesList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SourceProgress getSources(int index);
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    int getSourcesCount();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder> 
        getSourcesOrBuilderList();
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder getSourcesOrBuilder(
        int index);

    /**
     * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
     * @return Whether the sink field is set.
     */
    boolean hasSink();
    /**
     * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
     * @return The sink.
     */
    org.apache.spark.status.protobuf.StoreTypes.SinkProgress getSink();
    /**
     * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.SinkProgressOrBuilder getSinkOrBuilder();

    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */
    int getObservedMetricsCount();
    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */
    boolean containsObservedMetrics(
        java.lang.String key);
    /**
     * Use {@link #getObservedMetricsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getObservedMetrics();
    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getObservedMetricsMap();
    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */

    java.lang.String getObservedMetricsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */

    java.lang.String getObservedMetricsOrThrow(
        java.lang.String key);
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.StreamingQueryProgress}
   */
  public static final class StreamingQueryProgress extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.StreamingQueryProgress)
      StreamingQueryProgressOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StreamingQueryProgress.newBuilder() to construct.
    private StreamingQueryProgress(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StreamingQueryProgress() {
      id_ = "";
      runId_ = "";
      name_ = "";
      timestamp_ = "";
      stateOperators_ = java.util.Collections.emptyList();
      sources_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StreamingQueryProgress();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StreamingQueryProgress(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              id_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              runId_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              name_ = s;
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();
              bitField0_ |= 0x00000008;
              timestamp_ = s;
              break;
            }
            case 40: {

              batchId_ = input.readInt64();
              break;
            }
            case 48: {

              batchDuration_ = input.readInt64();
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                durationMs_ = com.google.protobuf.MapField.newMapField(
                    DurationMsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000010;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.Long>
              durationMs__ = input.readMessage(
                  DurationMsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              durationMs_.getMutableMap().put(
                  durationMs__.getKey(), durationMs__.getValue());
              break;
            }
            case 66: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                eventTime_ = com.google.protobuf.MapField.newMapField(
                    EventTimeDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000020;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              eventTime__ = input.readMessage(
                  EventTimeDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              eventTime_.getMutableMap().put(
                  eventTime__.getKey(), eventTime__.getValue());
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                stateOperators_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress>();
                mutable_bitField0_ |= 0x00000040;
              }
              stateOperators_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.parser(), extensionRegistry));
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000080) != 0)) {
                sources_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SourceProgress>();
                mutable_bitField0_ |= 0x00000080;
              }
              sources_.add(
                  input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SourceProgress.parser(), extensionRegistry));
              break;
            }
            case 90: {
              org.apache.spark.status.protobuf.StoreTypes.SinkProgress.Builder subBuilder = null;
              if (sink_ != null) {
                subBuilder = sink_.toBuilder();
              }
              sink_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.SinkProgress.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(sink_);
                sink_ = subBuilder.buildPartial();
              }

              break;
            }
            case 98: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                observedMetrics_ = com.google.protobuf.MapField.newMapField(
                    ObservedMetricsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000100;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              observedMetrics__ = input.readMessage(
                  ObservedMetricsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              observedMetrics_.getMutableMap().put(
                  observedMetrics__.getKey(), observedMetrics__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          stateOperators_ = java.util.Collections.unmodifiableList(stateOperators_);
        }
        if (((mutable_bitField0_ & 0x00000080) != 0)) {
          sources_ = java.util.Collections.unmodifiableList(sources_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 7:
          return internalGetDurationMs();
        case 8:
          return internalGetEventTime();
        case 12:
          return internalGetObservedMetrics();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.class, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object id_;
    /**
     * <code>string id = 1;</code>
     * @return Whether the id field is set.
     */
    @java.lang.Override
    public boolean hasId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>string id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public java.lang.String getId() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        id_ = s;
        return s;
      }
    }
    /**
     * <code>string id = 1;</code>
     * @return The bytes for id.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getIdBytes() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        id_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int RUN_ID_FIELD_NUMBER = 2;
    private volatile java.lang.Object runId_;
    /**
     * <code>string run_id = 2;</code>
     * @return Whether the runId field is set.
     */
    @java.lang.Override
    public boolean hasRunId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>string run_id = 2;</code>
     * @return The runId.
     */
    @java.lang.Override
    public java.lang.String getRunId() {
      java.lang.Object ref = runId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        runId_ = s;
        return s;
      }
    }
    /**
     * <code>string run_id = 2;</code>
     * @return The bytes for runId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getRunIdBytes() {
      java.lang.Object ref = runId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        runId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int NAME_FIELD_NUMBER = 3;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 3;</code>
     * @return Whether the name field is set.
     */
    @java.lang.Override
    public boolean hasName() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>string name = 3;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 3;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TIMESTAMP_FIELD_NUMBER = 4;
    private volatile java.lang.Object timestamp_;
    /**
     * <code>string timestamp = 4;</code>
     * @return Whether the timestamp field is set.
     */
    @java.lang.Override
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>string timestamp = 4;</code>
     * @return The timestamp.
     */
    @java.lang.Override
    public java.lang.String getTimestamp() {
      java.lang.Object ref = timestamp_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        timestamp_ = s;
        return s;
      }
    }
    /**
     * <code>string timestamp = 4;</code>
     * @return The bytes for timestamp.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getTimestampBytes() {
      java.lang.Object ref = timestamp_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        timestamp_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BATCH_ID_FIELD_NUMBER = 5;
    private long batchId_;
    /**
     * <code>int64 batch_id = 5;</code>
     * @return The batchId.
     */
    @java.lang.Override
    public long getBatchId() {
      return batchId_;
    }

    public static final int BATCH_DURATION_FIELD_NUMBER = 6;
    private long batchDuration_;
    /**
     * <code>int64 batch_duration = 6;</code>
     * @return The batchDuration.
     */
    @java.lang.Override
    public long getBatchDuration() {
      return batchDuration_;
    }

    public static final int DURATION_MS_FIELD_NUMBER = 7;
    private static final class DurationMsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.Long> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.Long>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_DurationMsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.INT64,
                  0L);
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.Long> durationMs_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
    internalGetDurationMs() {
      if (durationMs_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            DurationMsDefaultEntryHolder.defaultEntry);
      }
      return durationMs_;
    }

    public int getDurationMsCount() {
      return internalGetDurationMs().getMap().size();
    }
    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */

    @java.lang.Override
    public boolean containsDurationMs(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetDurationMs().getMap().containsKey(key);
    }
    /**
     * Use {@link #getDurationMsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.Long> getDurationMs() {
      return getDurationMsMap();
    }
    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.Long> getDurationMsMap() {
      return internalGetDurationMs().getMap();
    }
    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */
    @java.lang.Override

    public long getDurationMsOrDefault(
        java.lang.String key,
        long defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Long> map =
          internalGetDurationMs().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
     */
    @java.lang.Override

    public long getDurationMsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Long> map =
          internalGetDurationMs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int EVENT_TIME_FIELD_NUMBER = 8;
    private static final class EventTimeDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_EventTimeEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> eventTime_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetEventTime() {
      if (eventTime_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            EventTimeDefaultEntryHolder.defaultEntry);
      }
      return eventTime_;
    }

    public int getEventTimeCount() {
      return internalGetEventTime().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */

    @java.lang.Override
    public boolean containsEventTime(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetEventTime().getMap().containsKey(key);
    }
    /**
     * Use {@link #getEventTimeMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getEventTime() {
      return getEventTimeMap();
    }
    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getEventTimeMap() {
      return internalGetEventTime().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */
    @java.lang.Override

    public java.lang.String getEventTimeOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetEventTime().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; event_time = 8;</code>
     */
    @java.lang.Override

    public java.lang.String getEventTimeOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetEventTime().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int STATE_OPERATORS_FIELD_NUMBER = 9;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress> stateOperators_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress> getStateOperatorsList() {
      return stateOperators_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder> 
        getStateOperatorsOrBuilderList() {
      return stateOperators_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    @java.lang.Override
    public int getStateOperatorsCount() {
      return stateOperators_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress getStateOperators(int index) {
      return stateOperators_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder getStateOperatorsOrBuilder(
        int index) {
      return stateOperators_.get(index);
    }

    public static final int SOURCES_FIELD_NUMBER = 10;
    private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SourceProgress> sources_;
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SourceProgress> getSourcesList() {
      return sources_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder> 
        getSourcesOrBuilderList() {
      return sources_;
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    @java.lang.Override
    public int getSourcesCount() {
      return sources_.size();
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SourceProgress getSources(int index) {
      return sources_.get(index);
    }
    /**
     * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder getSourcesOrBuilder(
        int index) {
      return sources_.get(index);
    }

    public static final int SINK_FIELD_NUMBER = 11;
    private org.apache.spark.status.protobuf.StoreTypes.SinkProgress sink_;
    /**
     * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
     * @return Whether the sink field is set.
     */
    @java.lang.Override
    public boolean hasSink() {
      return sink_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
     * @return The sink.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SinkProgress getSink() {
      return sink_ == null ? org.apache.spark.status.protobuf.StoreTypes.SinkProgress.getDefaultInstance() : sink_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.SinkProgressOrBuilder getSinkOrBuilder() {
      return getSink();
    }

    public static final int OBSERVED_METRICS_FIELD_NUMBER = 12;
    private static final class ObservedMetricsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_ObservedMetricsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> observedMetrics_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetObservedMetrics() {
      if (observedMetrics_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ObservedMetricsDefaultEntryHolder.defaultEntry);
      }
      return observedMetrics_;
    }

    public int getObservedMetricsCount() {
      return internalGetObservedMetrics().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */

    @java.lang.Override
    public boolean containsObservedMetrics(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetObservedMetrics().getMap().containsKey(key);
    }
    /**
     * Use {@link #getObservedMetricsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getObservedMetrics() {
      return getObservedMetricsMap();
    }
    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getObservedMetricsMap() {
      return internalGetObservedMetrics().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */
    @java.lang.Override

    public java.lang.String getObservedMetricsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetObservedMetrics().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
     */
    @java.lang.Override

    public java.lang.String getObservedMetricsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetObservedMetrics().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, runId_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, name_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, timestamp_);
      }
      if (batchId_ != 0L) {
        output.writeInt64(5, batchId_);
      }
      if (batchDuration_ != 0L) {
        output.writeInt64(6, batchDuration_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetDurationMs(),
          DurationMsDefaultEntryHolder.defaultEntry,
          7);
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetEventTime(),
          EventTimeDefaultEntryHolder.defaultEntry,
          8);
      for (int i = 0; i < stateOperators_.size(); i++) {
        output.writeMessage(9, stateOperators_.get(i));
      }
      for (int i = 0; i < sources_.size(); i++) {
        output.writeMessage(10, sources_.get(i));
      }
      if (sink_ != null) {
        output.writeMessage(11, getSink());
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetObservedMetrics(),
          ObservedMetricsDefaultEntryHolder.defaultEntry,
          12);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, id_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, runId_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, name_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, timestamp_);
      }
      if (batchId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, batchId_);
      }
      if (batchDuration_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, batchDuration_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.Long> entry
           : internalGetDurationMs().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.Long>
        durationMs__ = DurationMsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(7, durationMs__);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetEventTime().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        eventTime__ = EventTimeDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(8, eventTime__);
      }
      for (int i = 0; i < stateOperators_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, stateOperators_.get(i));
      }
      for (int i = 0; i < sources_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, sources_.get(i));
      }
      if (sink_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, getSink());
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetObservedMetrics().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        observedMetrics__ = ObservedMetricsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(12, observedMetrics__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress other = (org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress) obj;

      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (!getId()
            .equals(other.getId())) return false;
      }
      if (hasRunId() != other.hasRunId()) return false;
      if (hasRunId()) {
        if (!getRunId()
            .equals(other.getRunId())) return false;
      }
      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasTimestamp() != other.hasTimestamp()) return false;
      if (hasTimestamp()) {
        if (!getTimestamp()
            .equals(other.getTimestamp())) return false;
      }
      if (getBatchId()
          != other.getBatchId()) return false;
      if (getBatchDuration()
          != other.getBatchDuration()) return false;
      if (!internalGetDurationMs().equals(
          other.internalGetDurationMs())) return false;
      if (!internalGetEventTime().equals(
          other.internalGetEventTime())) return false;
      if (!getStateOperatorsList()
          .equals(other.getStateOperatorsList())) return false;
      if (!getSourcesList()
          .equals(other.getSourcesList())) return false;
      if (hasSink() != other.hasSink()) return false;
      if (hasSink()) {
        if (!getSink()
            .equals(other.getSink())) return false;
      }
      if (!internalGetObservedMetrics().equals(
          other.internalGetObservedMetrics())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId().hashCode();
      }
      if (hasRunId()) {
        hash = (37 * hash) + RUN_ID_FIELD_NUMBER;
        hash = (53 * hash) + getRunId().hashCode();
      }
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + getTimestamp().hashCode();
      }
      hash = (37 * hash) + BATCH_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getBatchId());
      hash = (37 * hash) + BATCH_DURATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getBatchDuration());
      if (!internalGetDurationMs().getMap().isEmpty()) {
        hash = (37 * hash) + DURATION_MS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetDurationMs().hashCode();
      }
      if (!internalGetEventTime().getMap().isEmpty()) {
        hash = (37 * hash) + EVENT_TIME_FIELD_NUMBER;
        hash = (53 * hash) + internalGetEventTime().hashCode();
      }
      if (getStateOperatorsCount() > 0) {
        hash = (37 * hash) + STATE_OPERATORS_FIELD_NUMBER;
        hash = (53 * hash) + getStateOperatorsList().hashCode();
      }
      if (getSourcesCount() > 0) {
        hash = (37 * hash) + SOURCES_FIELD_NUMBER;
        hash = (53 * hash) + getSourcesList().hashCode();
      }
      if (hasSink()) {
        hash = (37 * hash) + SINK_FIELD_NUMBER;
        hash = (53 * hash) + getSink().hashCode();
      }
      if (!internalGetObservedMetrics().getMap().isEmpty()) {
        hash = (37 * hash) + OBSERVED_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetObservedMetrics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.StreamingQueryProgress}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.StreamingQueryProgress)
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 7:
            return internalGetDurationMs();
          case 8:
            return internalGetEventTime();
          case 12:
            return internalGetObservedMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 7:
            return internalGetMutableDurationMs();
          case 8:
            return internalGetMutableEventTime();
          case 12:
            return internalGetMutableObservedMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.class, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getStateOperatorsFieldBuilder();
          getSourcesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        runId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        timestamp_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        batchId_ = 0L;

        batchDuration_ = 0L;

        internalGetMutableDurationMs().clear();
        internalGetMutableEventTime().clear();
        if (stateOperatorsBuilder_ == null) {
          stateOperators_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
        } else {
          stateOperatorsBuilder_.clear();
        }
        if (sourcesBuilder_ == null) {
          sources_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
        } else {
          sourcesBuilder_.clear();
        }
        if (sinkBuilder_ == null) {
          sink_ = null;
        } else {
          sink_ = null;
          sinkBuilder_ = null;
        }
        internalGetMutableObservedMetrics().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress build() {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress result = new org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.id_ = id_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.runId_ = runId_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.timestamp_ = timestamp_;
        result.batchId_ = batchId_;
        result.batchDuration_ = batchDuration_;
        result.durationMs_ = internalGetDurationMs();
        result.durationMs_.makeImmutable();
        result.eventTime_ = internalGetEventTime();
        result.eventTime_.makeImmutable();
        if (stateOperatorsBuilder_ == null) {
          if (((bitField0_ & 0x00000040) != 0)) {
            stateOperators_ = java.util.Collections.unmodifiableList(stateOperators_);
            bitField0_ = (bitField0_ & ~0x00000040);
          }
          result.stateOperators_ = stateOperators_;
        } else {
          result.stateOperators_ = stateOperatorsBuilder_.build();
        }
        if (sourcesBuilder_ == null) {
          if (((bitField0_ & 0x00000080) != 0)) {
            sources_ = java.util.Collections.unmodifiableList(sources_);
            bitField0_ = (bitField0_ & ~0x00000080);
          }
          result.sources_ = sources_;
        } else {
          result.sources_ = sourcesBuilder_.build();
        }
        if (sinkBuilder_ == null) {
          result.sink_ = sink_;
        } else {
          result.sink_ = sinkBuilder_.build();
        }
        result.observedMetrics_ = internalGetObservedMetrics();
        result.observedMetrics_.makeImmutable();
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.getDefaultInstance()) return this;
        if (other.hasId()) {
          bitField0_ |= 0x00000001;
          id_ = other.id_;
          onChanged();
        }
        if (other.hasRunId()) {
          bitField0_ |= 0x00000002;
          runId_ = other.runId_;
          onChanged();
        }
        if (other.hasName()) {
          bitField0_ |= 0x00000004;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasTimestamp()) {
          bitField0_ |= 0x00000008;
          timestamp_ = other.timestamp_;
          onChanged();
        }
        if (other.getBatchId() != 0L) {
          setBatchId(other.getBatchId());
        }
        if (other.getBatchDuration() != 0L) {
          setBatchDuration(other.getBatchDuration());
        }
        internalGetMutableDurationMs().mergeFrom(
            other.internalGetDurationMs());
        internalGetMutableEventTime().mergeFrom(
            other.internalGetEventTime());
        if (stateOperatorsBuilder_ == null) {
          if (!other.stateOperators_.isEmpty()) {
            if (stateOperators_.isEmpty()) {
              stateOperators_ = other.stateOperators_;
              bitField0_ = (bitField0_ & ~0x00000040);
            } else {
              ensureStateOperatorsIsMutable();
              stateOperators_.addAll(other.stateOperators_);
            }
            onChanged();
          }
        } else {
          if (!other.stateOperators_.isEmpty()) {
            if (stateOperatorsBuilder_.isEmpty()) {
              stateOperatorsBuilder_.dispose();
              stateOperatorsBuilder_ = null;
              stateOperators_ = other.stateOperators_;
              bitField0_ = (bitField0_ & ~0x00000040);
              stateOperatorsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getStateOperatorsFieldBuilder() : null;
            } else {
              stateOperatorsBuilder_.addAllMessages(other.stateOperators_);
            }
          }
        }
        if (sourcesBuilder_ == null) {
          if (!other.sources_.isEmpty()) {
            if (sources_.isEmpty()) {
              sources_ = other.sources_;
              bitField0_ = (bitField0_ & ~0x00000080);
            } else {
              ensureSourcesIsMutable();
              sources_.addAll(other.sources_);
            }
            onChanged();
          }
        } else {
          if (!other.sources_.isEmpty()) {
            if (sourcesBuilder_.isEmpty()) {
              sourcesBuilder_.dispose();
              sourcesBuilder_ = null;
              sources_ = other.sources_;
              bitField0_ = (bitField0_ & ~0x00000080);
              sourcesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getSourcesFieldBuilder() : null;
            } else {
              sourcesBuilder_.addAllMessages(other.sources_);
            }
          }
        }
        if (other.hasSink()) {
          mergeSink(other.getSink());
        }
        internalGetMutableObservedMetrics().mergeFrom(
            other.internalGetObservedMetrics());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object id_ = "";
      /**
       * <code>string id = 1;</code>
       * @return Whether the id field is set.
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>string id = 1;</code>
       * @return The id.
       */
      public java.lang.String getId() {
        java.lang.Object ref = id_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          id_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @return The bytes for id.
       */
      public com.google.protobuf.ByteString
          getIdBytes() {
        java.lang.Object ref = id_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          id_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = getDefaultInstance().getId();
        onChanged();
        return this;
      }
      /**
       * <code>string id = 1;</code>
       * @param value The bytes for id to set.
       * @return This builder for chaining.
       */
      public Builder setIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object runId_ = "";
      /**
       * <code>string run_id = 2;</code>
       * @return Whether the runId field is set.
       */
      public boolean hasRunId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>string run_id = 2;</code>
       * @return The runId.
       */
      public java.lang.String getRunId() {
        java.lang.Object ref = runId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          runId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string run_id = 2;</code>
       * @return The bytes for runId.
       */
      public com.google.protobuf.ByteString
          getRunIdBytes() {
        java.lang.Object ref = runId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          runId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string run_id = 2;</code>
       * @param value The runId to set.
       * @return This builder for chaining.
       */
      public Builder setRunId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        runId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string run_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearRunId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        runId_ = getDefaultInstance().getRunId();
        onChanged();
        return this;
      }
      /**
       * <code>string run_id = 2;</code>
       * @param value The bytes for runId to set.
       * @return This builder for chaining.
       */
      public Builder setRunIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000002;
        runId_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 3;</code>
       * @return Whether the name field is set.
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>string name = 3;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 3;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 3;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000004);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 3;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000004;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object timestamp_ = "";
      /**
       * <code>string timestamp = 4;</code>
       * @return Whether the timestamp field is set.
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>string timestamp = 4;</code>
       * @return The timestamp.
       */
      public java.lang.String getTimestamp() {
        java.lang.Object ref = timestamp_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          timestamp_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string timestamp = 4;</code>
       * @return The bytes for timestamp.
       */
      public com.google.protobuf.ByteString
          getTimestampBytes() {
        java.lang.Object ref = timestamp_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          timestamp_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string timestamp = 4;</code>
       * @param value The timestamp to set.
       * @return This builder for chaining.
       */
      public Builder setTimestamp(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string timestamp = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000008);
        timestamp_ = getDefaultInstance().getTimestamp();
        onChanged();
        return this;
      }
      /**
       * <code>string timestamp = 4;</code>
       * @param value The bytes for timestamp to set.
       * @return This builder for chaining.
       */
      public Builder setTimestampBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        bitField0_ |= 0x00000008;
        timestamp_ = value;
        onChanged();
        return this;
      }

      private long batchId_ ;
      /**
       * <code>int64 batch_id = 5;</code>
       * @return The batchId.
       */
      @java.lang.Override
      public long getBatchId() {
        return batchId_;
      }
      /**
       * <code>int64 batch_id = 5;</code>
       * @param value The batchId to set.
       * @return This builder for chaining.
       */
      public Builder setBatchId(long value) {
        
        batchId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 batch_id = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearBatchId() {
        
        batchId_ = 0L;
        onChanged();
        return this;
      }

      private long batchDuration_ ;
      /**
       * <code>int64 batch_duration = 6;</code>
       * @return The batchDuration.
       */
      @java.lang.Override
      public long getBatchDuration() {
        return batchDuration_;
      }
      /**
       * <code>int64 batch_duration = 6;</code>
       * @param value The batchDuration to set.
       * @return This builder for chaining.
       */
      public Builder setBatchDuration(long value) {
        
        batchDuration_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 batch_duration = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearBatchDuration() {
        
        batchDuration_ = 0L;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.Long> durationMs_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
      internalGetDurationMs() {
        if (durationMs_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              DurationMsDefaultEntryHolder.defaultEntry);
        }
        return durationMs_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.Long>
      internalGetMutableDurationMs() {
        onChanged();;
        if (durationMs_ == null) {
          durationMs_ = com.google.protobuf.MapField.newMapField(
              DurationMsDefaultEntryHolder.defaultEntry);
        }
        if (!durationMs_.isMutable()) {
          durationMs_ = durationMs_.copy();
        }
        return durationMs_;
      }

      public int getDurationMsCount() {
        return internalGetDurationMs().getMap().size();
      }
      /**
       * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
       */

      @java.lang.Override
      public boolean containsDurationMs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetDurationMs().getMap().containsKey(key);
      }
      /**
       * Use {@link #getDurationMsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Long> getDurationMs() {
        return getDurationMsMap();
      }
      /**
       * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.Long> getDurationMsMap() {
        return internalGetDurationMs().getMap();
      }
      /**
       * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
       */
      @java.lang.Override

      public long getDurationMsOrDefault(
          java.lang.String key,
          long defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Long> map =
            internalGetDurationMs().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
       */
      @java.lang.Override

      public long getDurationMsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.Long> map =
            internalGetDurationMs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearDurationMs() {
        internalGetMutableDurationMs().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
       */

      public Builder removeDurationMs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableDurationMs().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.Long>
      getMutableDurationMs() {
        return internalGetMutableDurationMs().getMutableMap();
      }
      /**
       * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
       */
      public Builder putDurationMs(
          java.lang.String key,
          long value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        
        internalGetMutableDurationMs().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, int64&gt; duration_ms = 7;</code>
       */

      public Builder putAllDurationMs(
          java.util.Map<java.lang.String, java.lang.Long> values) {
        internalGetMutableDurationMs().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> eventTime_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetEventTime() {
        if (eventTime_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              EventTimeDefaultEntryHolder.defaultEntry);
        }
        return eventTime_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableEventTime() {
        onChanged();;
        if (eventTime_ == null) {
          eventTime_ = com.google.protobuf.MapField.newMapField(
              EventTimeDefaultEntryHolder.defaultEntry);
        }
        if (!eventTime_.isMutable()) {
          eventTime_ = eventTime_.copy();
        }
        return eventTime_;
      }

      public int getEventTimeCount() {
        return internalGetEventTime().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; event_time = 8;</code>
       */

      @java.lang.Override
      public boolean containsEventTime(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetEventTime().getMap().containsKey(key);
      }
      /**
       * Use {@link #getEventTimeMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getEventTime() {
        return getEventTimeMap();
      }
      /**
       * <code>map&lt;string, string&gt; event_time = 8;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getEventTimeMap() {
        return internalGetEventTime().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; event_time = 8;</code>
       */
      @java.lang.Override

      public java.lang.String getEventTimeOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetEventTime().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; event_time = 8;</code>
       */
      @java.lang.Override

      public java.lang.String getEventTimeOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetEventTime().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearEventTime() {
        internalGetMutableEventTime().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; event_time = 8;</code>
       */

      public Builder removeEventTime(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableEventTime().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableEventTime() {
        return internalGetMutableEventTime().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; event_time = 8;</code>
       */
      public Builder putEventTime(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableEventTime().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; event_time = 8;</code>
       */

      public Builder putAllEventTime(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableEventTime().getMutableMap()
            .putAll(values);
        return this;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress> stateOperators_ =
        java.util.Collections.emptyList();
      private void ensureStateOperatorsIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          stateOperators_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress>(stateOperators_);
          bitField0_ |= 0x00000040;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder> stateOperatorsBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress> getStateOperatorsList() {
        if (stateOperatorsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(stateOperators_);
        } else {
          return stateOperatorsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public int getStateOperatorsCount() {
        if (stateOperatorsBuilder_ == null) {
          return stateOperators_.size();
        } else {
          return stateOperatorsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress getStateOperators(int index) {
        if (stateOperatorsBuilder_ == null) {
          return stateOperators_.get(index);
        } else {
          return stateOperatorsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder setStateOperators(
          int index, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress value) {
        if (stateOperatorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStateOperatorsIsMutable();
          stateOperators_.set(index, value);
          onChanged();
        } else {
          stateOperatorsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder setStateOperators(
          int index, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder builderForValue) {
        if (stateOperatorsBuilder_ == null) {
          ensureStateOperatorsIsMutable();
          stateOperators_.set(index, builderForValue.build());
          onChanged();
        } else {
          stateOperatorsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder addStateOperators(org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress value) {
        if (stateOperatorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStateOperatorsIsMutable();
          stateOperators_.add(value);
          onChanged();
        } else {
          stateOperatorsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder addStateOperators(
          int index, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress value) {
        if (stateOperatorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStateOperatorsIsMutable();
          stateOperators_.add(index, value);
          onChanged();
        } else {
          stateOperatorsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder addStateOperators(
          org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder builderForValue) {
        if (stateOperatorsBuilder_ == null) {
          ensureStateOperatorsIsMutable();
          stateOperators_.add(builderForValue.build());
          onChanged();
        } else {
          stateOperatorsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder addStateOperators(
          int index, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder builderForValue) {
        if (stateOperatorsBuilder_ == null) {
          ensureStateOperatorsIsMutable();
          stateOperators_.add(index, builderForValue.build());
          onChanged();
        } else {
          stateOperatorsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder addAllStateOperators(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress> values) {
        if (stateOperatorsBuilder_ == null) {
          ensureStateOperatorsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, stateOperators_);
          onChanged();
        } else {
          stateOperatorsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder clearStateOperators() {
        if (stateOperatorsBuilder_ == null) {
          stateOperators_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
          onChanged();
        } else {
          stateOperatorsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public Builder removeStateOperators(int index) {
        if (stateOperatorsBuilder_ == null) {
          ensureStateOperatorsIsMutable();
          stateOperators_.remove(index);
          onChanged();
        } else {
          stateOperatorsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder getStateOperatorsBuilder(
          int index) {
        return getStateOperatorsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder getStateOperatorsOrBuilder(
          int index) {
        if (stateOperatorsBuilder_ == null) {
          return stateOperators_.get(index);  } else {
          return stateOperatorsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder> 
           getStateOperatorsOrBuilderList() {
        if (stateOperatorsBuilder_ != null) {
          return stateOperatorsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(stateOperators_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder addStateOperatorsBuilder() {
        return getStateOperatorsFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder addStateOperatorsBuilder(
          int index) {
        return getStateOperatorsFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.StateOperatorProgress state_operators = 9;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder> 
           getStateOperatorsBuilderList() {
        return getStateOperatorsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder> 
          getStateOperatorsFieldBuilder() {
        if (stateOperatorsBuilder_ == null) {
          stateOperatorsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.StateOperatorProgressOrBuilder>(
                  stateOperators_,
                  ((bitField0_ & 0x00000040) != 0),
                  getParentForChildren(),
                  isClean());
          stateOperators_ = null;
        }
        return stateOperatorsBuilder_;
      }

      private java.util.List<org.apache.spark.status.protobuf.StoreTypes.SourceProgress> sources_ =
        java.util.Collections.emptyList();
      private void ensureSourcesIsMutable() {
        if (!((bitField0_ & 0x00000080) != 0)) {
          sources_ = new java.util.ArrayList<org.apache.spark.status.protobuf.StoreTypes.SourceProgress>(sources_);
          bitField0_ |= 0x00000080;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SourceProgress, org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder> sourcesBuilder_;

      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SourceProgress> getSourcesList() {
        if (sourcesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(sources_);
        } else {
          return sourcesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public int getSourcesCount() {
        if (sourcesBuilder_ == null) {
          return sources_.size();
        } else {
          return sourcesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SourceProgress getSources(int index) {
        if (sourcesBuilder_ == null) {
          return sources_.get(index);
        } else {
          return sourcesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder setSources(
          int index, org.apache.spark.status.protobuf.StoreTypes.SourceProgress value) {
        if (sourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSourcesIsMutable();
          sources_.set(index, value);
          onChanged();
        } else {
          sourcesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder setSources(
          int index, org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder builderForValue) {
        if (sourcesBuilder_ == null) {
          ensureSourcesIsMutable();
          sources_.set(index, builderForValue.build());
          onChanged();
        } else {
          sourcesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder addSources(org.apache.spark.status.protobuf.StoreTypes.SourceProgress value) {
        if (sourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSourcesIsMutable();
          sources_.add(value);
          onChanged();
        } else {
          sourcesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder addSources(
          int index, org.apache.spark.status.protobuf.StoreTypes.SourceProgress value) {
        if (sourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSourcesIsMutable();
          sources_.add(index, value);
          onChanged();
        } else {
          sourcesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder addSources(
          org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder builderForValue) {
        if (sourcesBuilder_ == null) {
          ensureSourcesIsMutable();
          sources_.add(builderForValue.build());
          onChanged();
        } else {
          sourcesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder addSources(
          int index, org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder builderForValue) {
        if (sourcesBuilder_ == null) {
          ensureSourcesIsMutable();
          sources_.add(index, builderForValue.build());
          onChanged();
        } else {
          sourcesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder addAllSources(
          java.lang.Iterable<? extends org.apache.spark.status.protobuf.StoreTypes.SourceProgress> values) {
        if (sourcesBuilder_ == null) {
          ensureSourcesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, sources_);
          onChanged();
        } else {
          sourcesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder clearSources() {
        if (sourcesBuilder_ == null) {
          sources_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
          onChanged();
        } else {
          sourcesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public Builder removeSources(int index) {
        if (sourcesBuilder_ == null) {
          ensureSourcesIsMutable();
          sources_.remove(index);
          onChanged();
        } else {
          sourcesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder getSourcesBuilder(
          int index) {
        return getSourcesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder getSourcesOrBuilder(
          int index) {
        if (sourcesBuilder_ == null) {
          return sources_.get(index);  } else {
          return sourcesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public java.util.List<? extends org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder> 
           getSourcesOrBuilderList() {
        if (sourcesBuilder_ != null) {
          return sourcesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(sources_);
        }
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder addSourcesBuilder() {
        return getSourcesFieldBuilder().addBuilder(
            org.apache.spark.status.protobuf.StoreTypes.SourceProgress.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder addSourcesBuilder(
          int index) {
        return getSourcesFieldBuilder().addBuilder(
            index, org.apache.spark.status.protobuf.StoreTypes.SourceProgress.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.spark.status.protobuf.SourceProgress sources = 10;</code>
       */
      public java.util.List<org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder> 
           getSourcesBuilderList() {
        return getSourcesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SourceProgress, org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder> 
          getSourcesFieldBuilder() {
        if (sourcesBuilder_ == null) {
          sourcesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SourceProgress, org.apache.spark.status.protobuf.StoreTypes.SourceProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.SourceProgressOrBuilder>(
                  sources_,
                  ((bitField0_ & 0x00000080) != 0),
                  getParentForChildren(),
                  isClean());
          sources_ = null;
        }
        return sourcesBuilder_;
      }

      private org.apache.spark.status.protobuf.StoreTypes.SinkProgress sink_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SinkProgress, org.apache.spark.status.protobuf.StoreTypes.SinkProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.SinkProgressOrBuilder> sinkBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       * @return Whether the sink field is set.
       */
      public boolean hasSink() {
        return sinkBuilder_ != null || sink_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       * @return The sink.
       */
      public org.apache.spark.status.protobuf.StoreTypes.SinkProgress getSink() {
        if (sinkBuilder_ == null) {
          return sink_ == null ? org.apache.spark.status.protobuf.StoreTypes.SinkProgress.getDefaultInstance() : sink_;
        } else {
          return sinkBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       */
      public Builder setSink(org.apache.spark.status.protobuf.StoreTypes.SinkProgress value) {
        if (sinkBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          sink_ = value;
          onChanged();
        } else {
          sinkBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       */
      public Builder setSink(
          org.apache.spark.status.protobuf.StoreTypes.SinkProgress.Builder builderForValue) {
        if (sinkBuilder_ == null) {
          sink_ = builderForValue.build();
          onChanged();
        } else {
          sinkBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       */
      public Builder mergeSink(org.apache.spark.status.protobuf.StoreTypes.SinkProgress value) {
        if (sinkBuilder_ == null) {
          if (sink_ != null) {
            sink_ =
              org.apache.spark.status.protobuf.StoreTypes.SinkProgress.newBuilder(sink_).mergeFrom(value).buildPartial();
          } else {
            sink_ = value;
          }
          onChanged();
        } else {
          sinkBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       */
      public Builder clearSink() {
        if (sinkBuilder_ == null) {
          sink_ = null;
          onChanged();
        } else {
          sink_ = null;
          sinkBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SinkProgress.Builder getSinkBuilder() {
        
        onChanged();
        return getSinkFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.SinkProgressOrBuilder getSinkOrBuilder() {
        if (sinkBuilder_ != null) {
          return sinkBuilder_.getMessageOrBuilder();
        } else {
          return sink_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.SinkProgress.getDefaultInstance() : sink_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.SinkProgress sink = 11;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.SinkProgress, org.apache.spark.status.protobuf.StoreTypes.SinkProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.SinkProgressOrBuilder> 
          getSinkFieldBuilder() {
        if (sinkBuilder_ == null) {
          sinkBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.SinkProgress, org.apache.spark.status.protobuf.StoreTypes.SinkProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.SinkProgressOrBuilder>(
                  getSink(),
                  getParentForChildren(),
                  isClean());
          sink_ = null;
        }
        return sinkBuilder_;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> observedMetrics_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetObservedMetrics() {
        if (observedMetrics_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ObservedMetricsDefaultEntryHolder.defaultEntry);
        }
        return observedMetrics_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableObservedMetrics() {
        onChanged();;
        if (observedMetrics_ == null) {
          observedMetrics_ = com.google.protobuf.MapField.newMapField(
              ObservedMetricsDefaultEntryHolder.defaultEntry);
        }
        if (!observedMetrics_.isMutable()) {
          observedMetrics_ = observedMetrics_.copy();
        }
        return observedMetrics_;
      }

      public int getObservedMetricsCount() {
        return internalGetObservedMetrics().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
       */

      @java.lang.Override
      public boolean containsObservedMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetObservedMetrics().getMap().containsKey(key);
      }
      /**
       * Use {@link #getObservedMetricsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getObservedMetrics() {
        return getObservedMetricsMap();
      }
      /**
       * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getObservedMetricsMap() {
        return internalGetObservedMetrics().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
       */
      @java.lang.Override

      public java.lang.String getObservedMetricsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetObservedMetrics().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
       */
      @java.lang.Override

      public java.lang.String getObservedMetricsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetObservedMetrics().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearObservedMetrics() {
        internalGetMutableObservedMetrics().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
       */

      public Builder removeObservedMetrics(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableObservedMetrics().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableObservedMetrics() {
        return internalGetMutableObservedMetrics().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
       */
      public Builder putObservedMetrics(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableObservedMetrics().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; observed_metrics = 12;</code>
       */

      public Builder putAllObservedMetrics(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableObservedMetrics().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.StreamingQueryProgress)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.StreamingQueryProgress)
    private static final org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StreamingQueryProgress>
        PARSER = new com.google.protobuf.AbstractParser<StreamingQueryProgress>() {
      @java.lang.Override
      public StreamingQueryProgress parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StreamingQueryProgress(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StreamingQueryProgress> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StreamingQueryProgress> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StreamingQueryProgressWrapperOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.status.protobuf.StreamingQueryProgressWrapper)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
     * @return Whether the progress field is set.
     */
    boolean hasProgress();
    /**
     * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
     * @return The progress.
     */
    org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress getProgress();
    /**
     * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
     */
    org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressOrBuilder getProgressOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.status.protobuf.StreamingQueryProgressWrapper}
   */
  public static final class StreamingQueryProgressWrapper extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.status.protobuf.StreamingQueryProgressWrapper)
      StreamingQueryProgressWrapperOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StreamingQueryProgressWrapper.newBuilder() to construct.
    private StreamingQueryProgressWrapper(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StreamingQueryProgressWrapper() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StreamingQueryProgressWrapper();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StreamingQueryProgressWrapper(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.Builder subBuilder = null;
              if (progress_ != null) {
                subBuilder = progress_.toBuilder();
              }
              progress_ = input.readMessage(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(progress_);
                progress_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper.class, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper.Builder.class);
    }

    public static final int PROGRESS_FIELD_NUMBER = 1;
    private org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress progress_;
    /**
     * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
     * @return Whether the progress field is set.
     */
    @java.lang.Override
    public boolean hasProgress() {
      return progress_ != null;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
     * @return The progress.
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress getProgress() {
      return progress_ == null ? org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.getDefaultInstance() : progress_;
    }
    /**
     * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressOrBuilder getProgressOrBuilder() {
      return getProgress();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (progress_ != null) {
        output.writeMessage(1, getProgress());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (progress_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getProgress());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper)) {
        return super.equals(obj);
      }
      org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper other = (org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper) obj;

      if (hasProgress() != other.hasProgress()) return false;
      if (hasProgress()) {
        if (!getProgress()
            .equals(other.getProgress())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasProgress()) {
        hash = (37 * hash) + PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + getProgress().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.status.protobuf.StreamingQueryProgressWrapper}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.status.protobuf.StreamingQueryProgressWrapper)
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapperOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper.class, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper.Builder.class);
      }

      // Construct using org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (progressBuilder_ == null) {
          progress_ = null;
        } else {
          progress_ = null;
          progressBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.status.protobuf.StoreTypes.internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper getDefaultInstanceForType() {
        return org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper build() {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper buildPartial() {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper result = new org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper(this);
        if (progressBuilder_ == null) {
          result.progress_ = progress_;
        } else {
          result.progress_ = progressBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper) {
          return mergeFrom((org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper other) {
        if (other == org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper.getDefaultInstance()) return this;
        if (other.hasProgress()) {
          mergeProgress(other.getProgress());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress progress_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressOrBuilder> progressBuilder_;
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       * @return Whether the progress field is set.
       */
      public boolean hasProgress() {
        return progressBuilder_ != null || progress_ != null;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       * @return The progress.
       */
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress getProgress() {
        if (progressBuilder_ == null) {
          return progress_ == null ? org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.getDefaultInstance() : progress_;
        } else {
          return progressBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       */
      public Builder setProgress(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress value) {
        if (progressBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          progress_ = value;
          onChanged();
        } else {
          progressBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       */
      public Builder setProgress(
          org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.Builder builderForValue) {
        if (progressBuilder_ == null) {
          progress_ = builderForValue.build();
          onChanged();
        } else {
          progressBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       */
      public Builder mergeProgress(org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress value) {
        if (progressBuilder_ == null) {
          if (progress_ != null) {
            progress_ =
              org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.newBuilder(progress_).mergeFrom(value).buildPartial();
          } else {
            progress_ = value;
          }
          onChanged();
        } else {
          progressBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       */
      public Builder clearProgress() {
        if (progressBuilder_ == null) {
          progress_ = null;
          onChanged();
        } else {
          progress_ = null;
          progressBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.Builder getProgressBuilder() {
        
        onChanged();
        return getProgressFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       */
      public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressOrBuilder getProgressOrBuilder() {
        if (progressBuilder_ != null) {
          return progressBuilder_.getMessageOrBuilder();
        } else {
          return progress_ == null ?
              org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.getDefaultInstance() : progress_;
        }
      }
      /**
       * <code>.org.apache.spark.status.protobuf.StreamingQueryProgress progress = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressOrBuilder> 
          getProgressFieldBuilder() {
        if (progressBuilder_ == null) {
          progressBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgress.Builder, org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressOrBuilder>(
                  getProgress(),
                  getParentForChildren(),
                  isClean());
          progress_ = null;
        }
        return progressBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.status.protobuf.StreamingQueryProgressWrapper)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.status.protobuf.StreamingQueryProgressWrapper)
    private static final org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper();
    }

    public static org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StreamingQueryProgressWrapper>
        PARSER = new com.google.protobuf.AbstractParser<StreamingQueryProgressWrapper>() {
      @java.lang.Override
      public StreamingQueryProgressWrapper parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StreamingQueryProgressWrapper(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StreamingQueryProgressWrapper> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StreamingQueryProgressWrapper> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.status.protobuf.StoreTypes.StreamingQueryProgressWrapper getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_JobData_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_JobData_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_JobData_KillTasksSummaryEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_JobData_KillTasksSummaryEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_JobDataWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_JobDataWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_AccumulableInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_AccumulableInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_MetricsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_MetricsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_ExecutorResourcesEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_ExecutorResourcesEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_TaskResourcesEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_TaskResourcesEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RuntimeInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RuntimeInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_PairStrings_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_PairStrings_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ApplicationInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ApplicationInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StreamBlockData_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StreamBlockData_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_CachedQuantile_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_CachedQuantile_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ProcessSummary_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ProcessSummary_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ProcessSummary_ProcessLogsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ProcessSummary_ProcessLogsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_MemoryMetrics_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_MemoryMetrics_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ResourceInformation_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ResourceInformation_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorSummary_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ExecutorLogsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ExecutorLogsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_AttributesEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorSummary_AttributesEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ResourcesEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ResourcesEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_ModifiedConfigsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_ModifiedConfigsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_JobsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_JobsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_MetricValuesEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_MetricValuesEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RDDOperationNode_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RDDOperationNode_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StreamingQueryData_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StreamingQueryData_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StageDataWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StageDataWrapper_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StageDataWrapper_LocalityEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StageDataWrapper_LocalityEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_TaskData_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_TaskData_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_TaskData_ExecutorLogsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_TaskData_ExecutorLogsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StageData_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StageData_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StageData_TasksEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StageData_TasksEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StageData_ExecutorSummaryEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StageData_ExecutorSummaryEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StageData_KilledTasksSummaryEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StageData_KilledTasksSummaryEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_TaskMetrics_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_TaskMetrics_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_InputMetrics_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_InputMetrics_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_OutputMetrics_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_OutputMetrics_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_AppSummary_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_AppSummary_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_PoolData_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_PoolData_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_CustomMetricsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_CustomMetricsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SourceProgress_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SourceProgress_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SourceProgress_MetricsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SourceProgress_MetricsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SinkProgress_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SinkProgress_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_SinkProgress_MetricsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_SinkProgress_MetricsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_DurationMsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_DurationMsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_EventTimeEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_EventTimeEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_ObservedMetricsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_ObservedMetricsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\021store_types.proto\022 org.apache.spark.st" +
      "atus.protobuf\"\200\006\n\007JobData\022\016\n\006job_id\030\001 \001(" +
      "\003\022\021\n\004name\030\002 \001(\tH\000\210\001\001\022\030\n\013description\030\003 \001(" +
      "\tH\001\210\001\001\022\034\n\017submission_time\030\004 \001(\003H\002\210\001\001\022\034\n\017" +
      "completion_time\030\005 \001(\003H\003\210\001\001\022\021\n\tstage_ids\030" +
      "\006 \003(\003\022\026\n\tjob_group\030\007 \001(\tH\004\210\001\001\022D\n\006status\030" +
      "\010 \001(\01624.org.apache.spark.status.protobuf" +
      ".JobExecutionStatus\022\021\n\tnum_tasks\030\t \001(\005\022\030" +
      "\n\020num_active_tasks\030\n \001(\005\022\033\n\023num_complete" +
      "d_tasks\030\013 \001(\005\022\031\n\021num_skipped_tasks\030\014 \001(\005" +
      "\022\030\n\020num_failed_tasks\030\r \001(\005\022\030\n\020num_killed" +
      "_tasks\030\016 \001(\005\022\035\n\025num_completed_indices\030\017 " +
      "\001(\005\022\031\n\021num_active_stages\030\020 \001(\005\022\034\n\024num_co" +
      "mpleted_stages\030\021 \001(\005\022\032\n\022num_skipped_stag" +
      "es\030\022 \001(\005\022\031\n\021num_failed_stages\030\023 \001(\005\022[\n\022k" +
      "ill_tasks_summary\030\024 \003(\0132?.org.apache.spa" +
      "rk.status.protobuf.JobData.KillTasksSumm" +
      "aryEntry\0327\n\025KillTasksSummaryEntry\022\013\n\003key" +
      "\030\001 \001(\t\022\r\n\005value\030\002 \001(\005:\0028\001B\007\n\005_nameB\016\n\014_d" +
      "escriptionB\022\n\020_submission_timeB\022\n\020_compl" +
      "etion_timeB\014\n\n_job_group\"\225\001\n\016JobDataWrap" +
      "per\0227\n\004info\030\001 \001(\0132).org.apache.spark.sta" +
      "tus.protobuf.JobData\022\026\n\016skipped_stages\030\002" +
      " \003(\005\022\035\n\020sql_execution_id\030\003 \001(\003H\000\210\001\001B\023\n\021_" +
      "sql_execution_id\"w\n\017AccumulableInfo\022\n\n\002i" +
      "d\030\001 \001(\003\022\021\n\004name\030\002 \001(\tH\000\210\001\001\022\023\n\006update\030\003 \001" +
      "(\tH\001\210\001\001\022\022\n\005value\030\004 \001(\tH\002\210\001\001B\007\n\005_nameB\t\n\007" +
      "_updateB\010\n\006_value\"\252\r\n\017TaskDataWrapper\022\017\n" +
      "\007task_id\030\001 \001(\003\022\r\n\005index\030\002 \001(\005\022\017\n\007attempt" +
      "\030\003 \001(\005\022\024\n\014partition_id\030\004 \001(\005\022\023\n\013launch_t" +
      "ime\030\005 \001(\003\022\032\n\022result_fetch_start\030\006 \001(\003\022\020\n" +
      "\010duration\030\007 \001(\003\022\030\n\013executor_id\030\010 \001(\tH\000\210\001" +
      "\001\022\021\n\004host\030\t \001(\tH\001\210\001\001\022\023\n\006status\030\n \001(\tH\002\210\001" +
      "\001\022\032\n\rtask_locality\030\013 \001(\tH\003\210\001\001\022\023\n\013specula" +
      "tive\030\014 \001(\010\022N\n\023accumulator_updates\030\r \003(\0132" +
      "1.org.apache.spark.status.protobuf.Accum" +
      "ulableInfo\022\032\n\rerror_message\030\016 \001(\tH\004\210\001\001\022\023" +
      "\n\013has_metrics\030\017 \001(\010\022!\n\031executor_deserial" +
      "ize_time\030\020 \001(\003\022%\n\035executor_deserialize_c" +
      "pu_time\030\021 \001(\003\022\031\n\021executor_run_time\030\022 \001(\003" +
      "\022\031\n\021executor_cpu_time\030\023 \001(\003\022\023\n\013result_si" +
      "ze\030\024 \001(\003\022\023\n\013jvm_gc_time\030\025 \001(\003\022!\n\031result_" +
      "serialization_time\030\026 \001(\003\022\034\n\024memory_bytes" +
      "_spilled\030\027 \001(\003\022\032\n\022disk_bytes_spilled\030\030 \001" +
      "(\003\022\035\n\025peak_execution_memory\030\031 \001(\003\022\030\n\020inp" +
      "ut_bytes_read\030\032 \001(\003\022\032\n\022input_records_rea" +
      "d\030\033 \001(\003\022\034\n\024output_bytes_written\030\034 \001(\003\022\036\n" +
      "\026output_records_written\030\035 \001(\003\022%\n\035shuffle" +
      "_remote_blocks_fetched\030\036 \001(\003\022$\n\034shuffle_" +
      "local_blocks_fetched\030\037 \001(\003\022\037\n\027shuffle_fe" +
      "tch_wait_time\030  \001(\003\022!\n\031shuffle_remote_by" +
      "tes_read\030! \001(\003\022)\n!shuffle_remote_bytes_r" +
      "ead_to_disk\030\" \001(\003\022 \n\030shuffle_local_bytes" +
      "_read\030# \001(\003\022\034\n\024shuffle_records_read\030$ \001(" +
      "\003\022\035\n\025shuffle_bytes_written\030% \001(\003\022\032\n\022shuf" +
      "fle_write_time\030& \001(\003\022\037\n\027shuffle_records_" +
      "written\030\' \001(\003\022\020\n\010stage_id\030( \001(\003\022\030\n\020stage" +
      "_attempt_id\030) \001(\005\022+\n#shuffle_corrupt_mer" +
      "ged_block_chunks\030* \001(\003\022+\n#shuffle_merged" +
      "_fetch_fallback_count\030+ \001(\003\022,\n$shuffle_m" +
      "erged_remote_blocks_fetched\030, \001(\003\022+\n#shu" +
      "ffle_merged_local_blocks_fetched\030- \001(\003\022," +
      "\n$shuffle_merged_remote_chunks_fetched\030." +
      " \001(\003\022+\n#shuffle_merged_local_chunks_fetc" +
      "hed\030/ \001(\003\022(\n shuffle_merged_remote_bytes" +
      "_read\0300 \001(\003\022\'\n\037shuffle_merged_local_byte" +
      "s_read\0301 \001(\003\022$\n\034shuffle_remote_reqs_dura" +
      "tion\0302 \001(\003\022*\n\"shuffle_merged_remote_req_" +
      "duration\0303 \001(\003B\016\n\014_executor_idB\007\n\005_hostB" +
      "\t\n\007_statusB\020\n\016_task_localityB\020\n\016_error_m" +
      "essage\"\222\001\n\017ExecutorMetrics\022O\n\007metrics\030\001 " +
      "\003(\0132>.org.apache.spark.status.protobuf.E" +
      "xecutorMetrics.MetricsEntry\032.\n\014MetricsEn" +
      "try\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\003:\0028\001\"\232\004\n\024" +
      "ExecutorStageSummary\022\021\n\ttask_time\030\001 \001(\003\022" +
      "\024\n\014failed_tasks\030\002 \001(\005\022\027\n\017succeeded_tasks" +
      "\030\003 \001(\005\022\024\n\014killed_tasks\030\004 \001(\005\022\023\n\013input_by" +
      "tes\030\005 \001(\003\022\025\n\rinput_records\030\006 \001(\003\022\024\n\014outp" +
      "ut_bytes\030\007 \001(\003\022\026\n\016output_records\030\010 \001(\003\022\024" +
      "\n\014shuffle_read\030\t \001(\003\022\034\n\024shuffle_read_rec" +
      "ords\030\n \001(\003\022\025\n\rshuffle_write\030\013 \001(\003\022\035\n\025shu" +
      "ffle_write_records\030\014 \001(\003\022\034\n\024memory_bytes" +
      "_spilled\030\r \001(\003\022\032\n\022disk_bytes_spilled\030\016 \001" +
      "(\003\022 \n\030is_blacklisted_for_stage\030\017 \001(\010\022S\n\023" +
      "peak_memory_metrics\030\020 \001(\01321.org.apache.s" +
      "park.status.protobuf.ExecutorMetricsH\000\210\001" +
      "\001\022\035\n\025is_excluded_for_stage\030\021 \001(\010B\026\n\024_pea" +
      "k_memory_metrics\"\271\001\n\033ExecutorStageSummar" +
      "yWrapper\022\020\n\010stage_id\030\001 \001(\003\022\030\n\020stage_atte" +
      "mpt_id\030\002 \001(\005\022\030\n\013executor_id\030\003 \001(\tH\000\210\001\001\022D" +
      "\n\004info\030\004 \001(\01326.org.apache.spark.status.p" +
      "rotobuf.ExecutorStageSummaryB\016\n\014_executo" +
      "r_id\"\251\001\n\027ExecutorResourceRequest\022\032\n\rreso" +
      "urce_name\030\001 \001(\tH\000\210\001\001\022\016\n\006amount\030\002 \001(\003\022\034\n\017" +
      "discoveryScript\030\003 \001(\tH\001\210\001\001\022\023\n\006vendor\030\004 \001" +
      "(\tH\002\210\001\001B\020\n\016_resource_nameB\022\n\020_discoveryS" +
      "criptB\t\n\007_vendor\"S\n\023TaskResourceRequest\022" +
      "\032\n\rresource_name\030\001 \001(\tH\000\210\001\001\022\016\n\006amount\030\002 " +
      "\001(\001B\020\n\016_resource_name\"\317\003\n\023ResourceProfil" +
      "eInfo\022\n\n\002id\030\001 \001(\005\022h\n\022executor_resources\030" +
      "\002 \003(\0132L.org.apache.spark.status.protobuf" +
      ".ResourceProfileInfo.ExecutorResourcesEn" +
      "try\022`\n\016task_resources\030\003 \003(\0132H.org.apache" +
      ".spark.status.protobuf.ResourceProfileIn" +
      "fo.TaskResourcesEntry\032s\n\026ExecutorResourc" +
      "esEntry\022\013\n\003key\030\001 \001(\t\022H\n\005value\030\002 \001(\01329.or" +
      "g.apache.spark.status.protobuf.ExecutorR" +
      "esourceRequest:\0028\001\032k\n\022TaskResourcesEntry" +
      "\022\013\n\003key\030\001 \001(\t\022D\n\005value\030\002 \001(\01325.org.apach" +
      "e.spark.status.protobuf.TaskResourceRequ" +
      "est:\0028\001\"\215\001\n\013RuntimeInfo\022\031\n\014java_version\030" +
      "\001 \001(\tH\000\210\001\001\022\026\n\tjava_home\030\002 \001(\tH\001\210\001\001\022\032\n\rsc" +
      "ala_version\030\003 \001(\tH\002\210\001\001B\017\n\r_java_versionB" +
      "\014\n\n_java_homeB\020\n\016_scala_version\"M\n\013PairS" +
      "trings\022\023\n\006value1\030\001 \001(\tH\000\210\001\001\022\023\n\006value2\030\002 " +
      "\001(\tH\001\210\001\001B\t\n\007_value1B\t\n\007_value2\"\240\004\n\032Appli" +
      "cationEnvironmentInfo\022>\n\007runtime\030\001 \001(\0132-" +
      ".org.apache.spark.status.protobuf.Runtim" +
      "eInfo\022G\n\020spark_properties\030\002 \003(\0132-.org.ap" +
      "ache.spark.status.protobuf.PairStrings\022H" +
      "\n\021hadoop_properties\030\003 \003(\0132-.org.apache.s" +
      "park.status.protobuf.PairStrings\022H\n\021syst" +
      "em_properties\030\004 \003(\0132-.org.apache.spark.s" +
      "tatus.protobuf.PairStrings\022I\n\022metrics_pr" +
      "operties\030\005 \003(\0132-.org.apache.spark.status" +
      ".protobuf.PairStrings\022H\n\021classpath_entri" +
      "es\030\006 \003(\0132-.org.apache.spark.status.proto" +
      "buf.PairStrings\022P\n\021resource_profiles\030\007 \003" +
      "(\01325.org.apache.spark.status.protobuf.Re" +
      "sourceProfileInfo\"o\n!ApplicationEnvironm" +
      "entInfoWrapper\022J\n\004info\030\001 \001(\0132<.org.apach" +
      "e.spark.status.protobuf.ApplicationEnvir" +
      "onmentInfo\"\377\001\n\026ApplicationAttemptInfo\022\027\n" +
      "\nattempt_id\030\001 \001(\tH\000\210\001\001\022\022\n\nstart_time\030\002 \001" +
      "(\003\022\020\n\010end_time\030\003 \001(\003\022\024\n\014last_updated\030\004 \001" +
      "(\003\022\020\n\010duration\030\005 \001(\003\022\027\n\nspark_user\030\006 \001(\t" +
      "H\001\210\001\001\022\021\n\tcompleted\030\007 \001(\010\022\036\n\021app_spark_ve" +
      "rsion\030\010 \001(\tH\002\210\001\001B\r\n\013_attempt_idB\r\n\013_spar" +
      "k_userB\024\n\022_app_spark_version\"\335\002\n\017Applica" +
      "tionInfo\022\017\n\002id\030\001 \001(\tH\000\210\001\001\022\021\n\004name\030\002 \001(\tH" +
      "\001\210\001\001\022\032\n\rcores_granted\030\003 \001(\005H\002\210\001\001\022\026\n\tmax_" +
      "cores\030\004 \001(\005H\003\210\001\001\022\037\n\022cores_per_executor\030\005" +
      " \001(\005H\004\210\001\001\022#\n\026memory_per_executor_mb\030\006 \001(" +
      "\005H\005\210\001\001\022J\n\010attempts\030\007 \003(\01328.org.apache.sp" +
      "ark.status.protobuf.ApplicationAttemptIn" +
      "foB\005\n\003_idB\007\n\005_nameB\020\n\016_cores_grantedB\014\n\n" +
      "_max_coresB\025\n\023_cores_per_executorB\031\n\027_me" +
      "mory_per_executor_mb\"Y\n\026ApplicationInfoW" +
      "rapper\022?\n\004info\030\001 \001(\01321.org.apache.spark." +
      "status.protobuf.ApplicationInfo\"\214\002\n\017Stre" +
      "amBlockData\022\021\n\004name\030\001 \001(\tH\000\210\001\001\022\030\n\013execut" +
      "or_id\030\002 \001(\tH\001\210\001\001\022\026\n\thost_port\030\003 \001(\tH\002\210\001\001" +
      "\022\032\n\rstorage_level\030\004 \001(\tH\003\210\001\001\022\022\n\nuse_memo" +
      "ry\030\005 \001(\010\022\020\n\010use_disk\030\006 \001(\010\022\024\n\014deserializ" +
      "ed\030\007 \001(\010\022\020\n\010mem_size\030\010 \001(\003\022\021\n\tdisk_size\030" +
      "\t \001(\003B\007\n\005_nameB\016\n\014_executor_idB\014\n\n_host_" +
      "portB\020\n\016_storage_level\"\371\002\n\023RDDDataDistri" +
      "bution\022\024\n\007address\030\001 \001(\tH\000\210\001\001\022\023\n\013memory_u" +
      "sed\030\002 \001(\003\022\030\n\020memory_remaining\030\003 \001(\003\022\021\n\td" +
      "isk_used\030\004 \001(\003\022 \n\023on_heap_memory_used\030\005 " +
      "\001(\003H\001\210\001\001\022!\n\024off_heap_memory_used\030\006 \001(\003H\002" +
      "\210\001\001\022%\n\030on_heap_memory_remaining\030\007 \001(\003H\003\210" +
      "\001\001\022&\n\031off_heap_memory_remaining\030\010 \001(\003H\004\210" +
      "\001\001B\n\n\010_addressB\026\n\024_on_heap_memory_usedB\027" +
      "\n\025_off_heap_memory_usedB\033\n\031_on_heap_memo" +
      "ry_remainingB\034\n\032_off_heap_memory_remaini" +
      "ng\"\243\001\n\020RDDPartitionInfo\022\027\n\nblock_name\030\001 " +
      "\001(\tH\000\210\001\001\022\032\n\rstorage_level\030\002 \001(\tH\001\210\001\001\022\023\n\013" +
      "memory_used\030\003 \001(\003\022\021\n\tdisk_used\030\004 \001(\003\022\021\n\t" +
      "executors\030\005 \003(\tB\r\n\013_block_nameB\020\n\016_stora" +
      "ge_level\"\337\002\n\016RDDStorageInfo\022\n\n\002id\030\001 \001(\005\022" +
      "\021\n\004name\030\002 \001(\tH\000\210\001\001\022\026\n\016num_partitions\030\003 \001" +
      "(\005\022\035\n\025num_cached_partitions\030\004 \001(\005\022\032\n\rsto" +
      "rage_level\030\005 \001(\tH\001\210\001\001\022\023\n\013memory_used\030\006 \001" +
      "(\003\022\021\n\tdisk_used\030\007 \001(\003\022P\n\021data_distributi" +
      "on\030\010 \003(\01325.org.apache.spark.status.proto" +
      "buf.RDDDataDistribution\022F\n\npartitions\030\t " +
      "\003(\01322.org.apache.spark.status.protobuf.R" +
      "DDPartitionInfoB\007\n\005_nameB\020\n\016_storage_lev" +
      "el\"W\n\025RDDStorageInfoWrapper\022>\n\004info\030\001 \001(" +
      "\01320.org.apache.spark.status.protobuf.RDD" +
      "StorageInfo\"_\n\026ResourceProfileWrapper\022E\n" +
      "\006rpInfo\030\001 \001(\01325.org.apache.spark.status." +
      "protobuf.ResourceProfileInfo\"\346\n\n\016CachedQ" +
      "uantile\022\020\n\010stage_id\030\001 \001(\003\022\030\n\020stage_attem" +
      "pt_id\030\002 \001(\005\022\025\n\010quantile\030\003 \001(\tH\000\210\001\001\022\022\n\nta" +
      "sk_count\030\004 \001(\003\022\020\n\010duration\030\005 \001(\001\022!\n\031exec" +
      "utor_deserialize_time\030\006 \001(\001\022%\n\035executor_" +
      "deserialize_cpu_time\030\007 \001(\001\022\031\n\021executor_r" +
      "un_time\030\010 \001(\001\022\031\n\021executor_cpu_time\030\t \001(\001" +
      "\022\023\n\013result_size\030\n \001(\001\022\023\n\013jvm_gc_time\030\013 \001" +
      "(\001\022!\n\031result_serialization_time\030\014 \001(\001\022\033\n" +
      "\023getting_result_time\030\r \001(\001\022\027\n\017scheduler_" +
      "delay\030\016 \001(\001\022\035\n\025peak_execution_memory\030\017 \001" +
      "(\001\022\034\n\024memory_bytes_spilled\030\020 \001(\001\022\032\n\022disk" +
      "_bytes_spilled\030\021 \001(\001\022\022\n\nbytes_read\030\022 \001(\001" +
      "\022\024\n\014records_read\030\023 \001(\001\022\025\n\rbytes_written\030" +
      "\024 \001(\001\022\027\n\017records_written\030\025 \001(\001\022\032\n\022shuffl" +
      "e_read_bytes\030\026 \001(\001\022\034\n\024shuffle_records_re" +
      "ad\030\027 \001(\001\022%\n\035shuffle_remote_blocks_fetche" +
      "d\030\030 \001(\001\022$\n\034shuffle_local_blocks_fetched\030" +
      "\031 \001(\001\022\037\n\027shuffle_fetch_wait_time\030\032 \001(\001\022!" +
      "\n\031shuffle_remote_bytes_read\030\033 \001(\001\022)\n!shu" +
      "ffle_remote_bytes_read_to_disk\030\034 \001(\001\022$\n\034" +
      "shuffle_total_blocks_fetched\030\035 \001(\001\022\033\n\023sh" +
      "uffle_write_bytes\030\036 \001(\001\022\035\n\025shuffle_write" +
      "_records\030\037 \001(\001\022\032\n\022shuffle_write_time\030  \001" +
      "(\001\022+\n#shuffle_corrupt_merged_block_chunk" +
      "s\030! \001(\001\022+\n#shuffle_merged_fetch_fallback" +
      "_count\030\" \001(\001\022,\n$shuffle_merged_remote_bl" +
      "ocks_fetched\030# \001(\001\022+\n#shuffle_merged_loc" +
      "al_blocks_fetched\030$ \001(\001\022,\n$shuffle_merge" +
      "d_remote_chunks_fetched\030% \001(\001\022+\n#shuffle" +
      "_merged_local_chunks_fetched\030& \001(\001\022(\n sh" +
      "uffle_merged_remote_bytes_read\030\' \001(\001\022\'\n\037" +
      "shuffle_merged_local_bytes_read\030( \001(\001\022$\n" +
      "\034shuffle_remote_reqs_duration\030) \001(\001\022+\n#s" +
      "huffle_merged_remote_reqs_duration\030* \001(\001" +
      "B\013\n\t_quantile\"\227\001\n\027SpeculationStageSummar" +
      "y\022\021\n\tnum_tasks\030\001 \001(\005\022\030\n\020num_active_tasks" +
      "\030\002 \001(\005\022\033\n\023num_completed_tasks\030\003 \001(\005\022\030\n\020n" +
      "um_failed_tasks\030\004 \001(\005\022\030\n\020num_killed_task" +
      "s\030\005 \001(\005\"\225\001\n\036SpeculationStageSummaryWrapp" +
      "er\022\020\n\010stage_id\030\001 \001(\003\022\030\n\020stage_attempt_id" +
      "\030\002 \001(\005\022G\n\004info\030\003 \001(\01329.org.apache.spark." +
      "status.protobuf.SpeculationStageSummary\"" +
      "\277\002\n\016ProcessSummary\022\017\n\002id\030\001 \001(\tH\000\210\001\001\022\026\n\th" +
      "ost_port\030\002 \001(\tH\001\210\001\001\022\021\n\tis_active\030\003 \001(\010\022\023" +
      "\n\013total_cores\030\004 \001(\005\022\020\n\010add_time\030\005 \001(\003\022\030\n" +
      "\013remove_time\030\006 \001(\003H\002\210\001\001\022W\n\014process_logs\030" +
      "\007 \003(\0132A.org.apache.spark.status.protobuf" +
      ".ProcessSummary.ProcessLogsEntry\0322\n\020Proc" +
      "essLogsEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t" +
      ":\0028\001B\005\n\003_idB\014\n\n_host_portB\016\n\014_remove_tim" +
      "e\"W\n\025ProcessSummaryWrapper\022>\n\004info\030\001 \001(\013" +
      "20.org.apache.spark.status.protobuf.Proc" +
      "essSummary\"\247\001\n\rMemoryMetrics\022#\n\033used_on_" +
      "heap_storage_memory\030\001 \001(\003\022$\n\034used_off_he" +
      "ap_storage_memory\030\002 \001(\003\022$\n\034total_on_heap" +
      "_storage_memory\030\003 \001(\003\022%\n\035total_off_heap_" +
      "storage_memory\030\004 \001(\003\"D\n\023ResourceInformat" +
      "ion\022\021\n\004name\030\001 \001(\tH\000\210\001\001\022\021\n\taddresses\030\002 \003(" +
      "\tB\007\n\005_name\"\315\n\n\017ExecutorSummary\022\017\n\002id\030\001 \001" +
      "(\tH\000\210\001\001\022\026\n\thost_port\030\002 \001(\tH\001\210\001\001\022\021\n\tis_ac" +
      "tive\030\003 \001(\010\022\022\n\nrdd_blocks\030\004 \001(\005\022\023\n\013memory" +
      "_used\030\005 \001(\003\022\021\n\tdisk_used\030\006 \001(\003\022\023\n\013total_" +
      "cores\030\007 \001(\005\022\021\n\tmax_tasks\030\010 \001(\005\022\024\n\014active" +
      "_tasks\030\t \001(\005\022\024\n\014failed_tasks\030\n \001(\005\022\027\n\017co" +
      "mpleted_tasks\030\013 \001(\005\022\023\n\013total_tasks\030\014 \001(\005" +
      "\022\026\n\016total_duration\030\r \001(\003\022\025\n\rtotal_gc_tim" +
      "e\030\016 \001(\003\022\031\n\021total_input_bytes\030\017 \001(\003\022\032\n\022to" +
      "tal_shuffle_read\030\020 \001(\003\022\033\n\023total_shuffle_" +
      "write\030\021 \001(\003\022\026\n\016is_blacklisted\030\022 \001(\010\022\022\n\nm" +
      "ax_memory\030\023 \001(\003\022\020\n\010add_time\030\024 \001(\003\022\030\n\013rem" +
      "ove_time\030\025 \001(\003H\002\210\001\001\022\032\n\rremove_reason\030\026 \001" +
      "(\tH\003\210\001\001\022Z\n\rexecutor_logs\030\027 \003(\0132C.org.apa" +
      "che.spark.status.protobuf.ExecutorSummar" +
      "y.ExecutorLogsEntry\022L\n\016memory_metrics\030\030 " +
      "\001(\0132/.org.apache.spark.status.protobuf.M" +
      "emoryMetricsH\004\210\001\001\022\035\n\025blacklisted_in_stag" +
      "es\030\031 \003(\003\022S\n\023peak_memory_metrics\030\032 \001(\01321." +
      "org.apache.spark.status.protobuf.Executo" +
      "rMetricsH\005\210\001\001\022U\n\nattributes\030\033 \003(\0132A.org." +
      "apache.spark.status.protobuf.ExecutorSum" +
      "mary.AttributesEntry\022S\n\tresources\030\034 \003(\0132" +
      "@.org.apache.spark.status.protobuf.Execu" +
      "torSummary.ResourcesEntry\022\033\n\023resource_pr" +
      "ofile_id\030\035 \001(\005\022\023\n\013is_excluded\030\036 \001(\010\022\032\n\022e" +
      "xcluded_in_stages\030\037 \003(\003\0323\n\021ExecutorLogsE" +
      "ntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001\0321\n\017" +
      "AttributesEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 " +
      "\001(\t:\0028\001\032g\n\016ResourcesEntry\022\013\n\003key\030\001 \001(\t\022D" +
      "\n\005value\030\002 \001(\01325.org.apache.spark.status." +
      "protobuf.ResourceInformation:\0028\001B\005\n\003_idB" +
      "\014\n\n_host_portB\016\n\014_remove_timeB\020\n\016_remove" +
      "_reasonB\021\n\017_memory_metricsB\026\n\024_peak_memo" +
      "ry_metrics\"Y\n\026ExecutorSummaryWrapper\022?\n\004" +
      "info\030\001 \001(\01321.org.apache.spark.status.pro" +
      "tobuf.ExecutorSummary\"m\n\rSQLPlanMetric\022\021" +
      "\n\004name\030\001 \001(\tH\000\210\001\001\022\026\n\016accumulator_id\030\002 \001(" +
      "\003\022\030\n\013metric_type\030\003 \001(\tH\001\210\001\001B\007\n\005_nameB\016\n\014" +
      "_metric_type\"\243\007\n\022SQLExecutionUIData\022\024\n\014e" +
      "xecution_id\030\001 \001(\003\022\031\n\021root_execution_id\030\002" +
      " \001(\003\022\030\n\013description\030\003 \001(\tH\000\210\001\001\022\024\n\007detail" +
      "s\030\004 \001(\tH\001\210\001\001\022&\n\031physical_plan_descriptio" +
      "n\030\005 \001(\tH\002\210\001\001\022c\n\020modified_configs\030\006 \003(\0132I" +
      ".org.apache.spark.status.protobuf.SQLExe" +
      "cutionUIData.ModifiedConfigsEntry\022@\n\007met" +
      "rics\030\007 \003(\0132/.org.apache.spark.status.pro" +
      "tobuf.SQLPlanMetric\022\027\n\017submission_time\030\010" +
      " \001(\003\022\034\n\017completion_time\030\t \001(\003H\003\210\001\001\022\032\n\rer" +
      "ror_message\030\n \001(\tH\004\210\001\001\022L\n\004jobs\030\013 \003(\0132>.o" +
      "rg.apache.spark.status.protobuf.SQLExecu" +
      "tionUIData.JobsEntry\022\016\n\006stages\030\014 \003(\003\022\035\n\025" +
      "metric_values_is_null\030\r \001(\010\022]\n\rmetric_va" +
      "lues\030\016 \003(\0132F.org.apache.spark.status.pro" +
      "tobuf.SQLExecutionUIData.MetricValuesEnt" +
      "ry\0326\n\024ModifiedConfigsEntry\022\013\n\003key\030\001 \001(\t\022" +
      "\r\n\005value\030\002 \001(\t:\0028\001\032a\n\tJobsEntry\022\013\n\003key\030\001" +
      " \001(\003\022C\n\005value\030\002 \001(\01624.org.apache.spark.s" +
      "tatus.protobuf.JobExecutionStatus:\0028\001\0323\n" +
      "\021MetricValuesEntry\022\013\n\003key\030\001 \001(\003\022\r\n\005value" +
      "\030\002 \001(\t:\0028\001B\016\n\014_descriptionB\n\n\010_detailsB\034" +
      "\n\032_physical_plan_descriptionB\022\n\020_complet" +
      "ion_timeB\020\n\016_error_message\"\232\001\n\022SparkPlan" +
      "GraphNode\022\n\n\002id\030\001 \001(\003\022\021\n\004name\030\002 \001(\tH\000\210\001\001" +
      "\022\021\n\004desc\030\003 \001(\tH\001\210\001\001\022@\n\007metrics\030\004 \003(\0132/.o" +
      "rg.apache.spark.status.protobuf.SQLPlanM" +
      "etricB\007\n\005_nameB\007\n\005_desc\"\360\001\n\034SparkPlanGra" +
      "phClusterWrapper\022\n\n\002id\030\001 \001(\003\022\021\n\004name\030\002 \001" +
      "(\tH\000\210\001\001\022\021\n\004desc\030\003 \001(\tH\001\210\001\001\022J\n\005nodes\030\004 \003(" +
      "\0132;.org.apache.spark.status.protobuf.Spa" +
      "rkPlanGraphNodeWrapper\022@\n\007metrics\030\005 \003(\0132" +
      "/.org.apache.spark.status.protobuf.SQLPl" +
      "anMetricB\007\n\005_nameB\007\n\005_desc\"\277\001\n\031SparkPlan" +
      "GraphNodeWrapper\022D\n\004node\030\001 \001(\01324.org.apa" +
      "che.spark.status.protobuf.SparkPlanGraph" +
      "NodeH\000\022Q\n\007cluster\030\002 \001(\0132>.org.apache.spa" +
      "rk.status.protobuf.SparkPlanGraphCluster" +
      "WrapperH\000B\t\n\007wrapper\"4\n\022SparkPlanGraphEd" +
      "ge\022\017\n\007from_id\030\001 \001(\003\022\r\n\005to_id\030\002 \001(\003\"\276\001\n\025S" +
      "parkPlanGraphWrapper\022\024\n\014execution_id\030\001 \001" +
      "(\003\022J\n\005nodes\030\002 \003(\0132;.org.apache.spark.sta" +
      "tus.protobuf.SparkPlanGraphNodeWrapper\022C" +
      "\n\005edges\030\003 \003(\01324.org.apache.spark.status." +
      "protobuf.SparkPlanGraphEdge\"2\n\020RDDOperat" +
      "ionEdge\022\017\n\007from_id\030\001 \001(\005\022\r\n\005to_id\030\002 \001(\005\"" +
      "\331\001\n\020RDDOperationNode\022\n\n\002id\030\001 \001(\005\022\021\n\004name" +
      "\030\002 \001(\tH\000\210\001\001\022\016\n\006cached\030\003 \001(\010\022\017\n\007barrier\030\004" +
      " \001(\010\022\025\n\010callsite\030\005 \001(\tH\001\210\001\001\022X\n\032output_de" +
      "terministic_level\030\006 \001(\01624.org.apache.spa" +
      "rk.status.protobuf.DeterministicLevelB\007\n" +
      "\005_nameB\013\n\t_callsite\"\357\001\n\032RDDOperationClus" +
      "terWrapper\022\017\n\002id\030\001 \001(\tH\000\210\001\001\022\021\n\004name\030\002 \001(" +
      "\tH\001\210\001\001\022G\n\013child_nodes\030\003 \003(\01322.org.apache" +
      ".spark.status.protobuf.RDDOperationNode\022" +
      "T\n\016child_clusters\030\004 \003(\0132<.org.apache.spa" +
      "rk.status.protobuf.RDDOperationClusterWr" +
      "apperB\005\n\003_idB\007\n\005_name\"\333\002\n\030RDDOperationGr" +
      "aphWrapper\022\020\n\010stage_id\030\001 \001(\003\022A\n\005edges\030\002 " +
      "\003(\01322.org.apache.spark.status.protobuf.R" +
      "DDOperationEdge\022J\n\016outgoing_edges\030\003 \003(\0132" +
      "2.org.apache.spark.status.protobuf.RDDOp" +
      "erationEdge\022J\n\016incoming_edges\030\004 \003(\01322.or" +
      "g.apache.spark.status.protobuf.RDDOperat" +
      "ionEdge\022R\n\014root_cluster\030\005 \001(\0132<.org.apac" +
      "he.spark.status.protobuf.RDDOperationClu" +
      "sterWrapper\"\350\001\n\022StreamingQueryData\022\021\n\004na" +
      "me\030\001 \001(\tH\000\210\001\001\022\017\n\002id\030\002 \001(\tH\001\210\001\001\022\023\n\006run_id" +
      "\030\003 \001(\tH\002\210\001\001\022\021\n\tis_active\030\004 \001(\010\022\026\n\texcept" +
      "ion\030\005 \001(\tH\003\210\001\001\022\027\n\017start_timestamp\030\006 \001(\003\022" +
      "\032\n\rend_timestamp\030\007 \001(\003H\004\210\001\001B\007\n\005_nameB\005\n\003" +
      "_idB\t\n\007_run_idB\014\n\n_exceptionB\020\n\016_end_tim" +
      "estamp\"\343\001\n\020StageDataWrapper\0229\n\004info\030\001 \001(" +
      "\0132+.org.apache.spark.status.protobuf.Sta" +
      "geData\022\017\n\007job_ids\030\002 \003(\003\022R\n\010locality\030\003 \003(" +
      "\0132@.org.apache.spark.status.protobuf.Sta" +
      "geDataWrapper.LocalityEntry\032/\n\rLocalityE" +
      "ntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\003:\0028\001\"\204\006\n" +
      "\010TaskData\022\017\n\007task_id\030\001 \001(\003\022\r\n\005index\030\002 \001(" +
      "\005\022\017\n\007attempt\030\003 \001(\005\022\024\n\014partition_id\030\004 \001(\005" +
      "\022\023\n\013launch_time\030\005 \001(\003\022\037\n\022result_fetch_st" +
      "art\030\006 \001(\003H\000\210\001\001\022\025\n\010duration\030\007 \001(\003H\001\210\001\001\022\030\n" +
      "\013executor_id\030\010 \001(\tH\002\210\001\001\022\021\n\004host\030\t \001(\tH\003\210" +
      "\001\001\022\023\n\006status\030\n \001(\tH\004\210\001\001\022\032\n\rtask_locality" +
      "\030\013 \001(\tH\005\210\001\001\022\023\n\013speculative\030\014 \001(\010\022N\n\023accu" +
      "mulator_updates\030\r \003(\01321.org.apache.spark" +
      ".status.protobuf.AccumulableInfo\022\032\n\rerro" +
      "r_message\030\016 \001(\tH\006\210\001\001\022H\n\014task_metrics\030\017 \001" +
      "(\0132-.org.apache.spark.status.protobuf.Ta" +
      "skMetricsH\007\210\001\001\022S\n\rexecutor_logs\030\020 \003(\0132<." +
      "org.apache.spark.status.protobuf.TaskDat" +
      "a.ExecutorLogsEntry\022\027\n\017scheduler_delay\030\021" +
      " \001(\003\022\033\n\023getting_result_time\030\022 \001(\003\0323\n\021Exe" +
      "cutorLogsEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001" +
      "(\t:\0028\001B\025\n\023_result_fetch_startB\013\n\t_durati" +
      "onB\016\n\014_executor_idB\007\n\005_hostB\t\n\007_statusB\020" +
      "\n\016_task_localityB\020\n\016_error_messageB\017\n\r_t" +
      "ask_metrics\"\346\027\n\tStageData\022=\n\006status\030\001 \001(" +
      "\0162-.org.apache.spark.status.protobuf.Sta" +
      "geStatus\022\020\n\010stage_id\030\002 \001(\003\022\022\n\nattempt_id" +
      "\030\003 \001(\005\022\021\n\tnum_tasks\030\004 \001(\005\022\030\n\020num_active_" +
      "tasks\030\005 \001(\005\022\032\n\022num_complete_tasks\030\006 \001(\005\022" +
      "\030\n\020num_failed_tasks\030\007 \001(\005\022\030\n\020num_killed_" +
      "tasks\030\010 \001(\005\022\035\n\025num_completed_indices\030\t \001" +
      "(\005\022\034\n\017submission_time\030\n \001(\003H\000\210\001\001\022%\n\030firs" +
      "t_task_launched_time\030\013 \001(\003H\001\210\001\001\022\034\n\017compl" +
      "etion_time\030\014 \001(\003H\002\210\001\001\022\033\n\016failure_reason\030" +
      "\r \001(\tH\003\210\001\001\022!\n\031executor_deserialize_time\030" +
      "\016 \001(\003\022%\n\035executor_deserialize_cpu_time\030\017" +
      " \001(\003\022\031\n\021executor_run_time\030\020 \001(\003\022\031\n\021execu" +
      "tor_cpu_time\030\021 \001(\003\022\023\n\013result_size\030\022 \001(\003\022" +
      "\023\n\013jvm_gc_time\030\023 \001(\003\022!\n\031result_serializa" +
      "tion_time\030\024 \001(\003\022\034\n\024memory_bytes_spilled\030" +
      "\025 \001(\003\022\032\n\022disk_bytes_spilled\030\026 \001(\003\022\035\n\025pea" +
      "k_execution_memory\030\027 \001(\003\022\023\n\013input_bytes\030" +
      "\030 \001(\003\022\025\n\rinput_records\030\031 \001(\003\022\024\n\014output_b" +
      "ytes\030\032 \001(\003\022\026\n\016output_records\030\033 \001(\003\022%\n\035sh" +
      "uffle_remote_blocks_fetched\030\034 \001(\003\022$\n\034shu" +
      "ffle_local_blocks_fetched\030\035 \001(\003\022\037\n\027shuff",
      "le_fetch_wait_time\030\036 \001(\003\022!\n\031shuffle_remo" +
      "te_bytes_read\030\037 \001(\003\022)\n!shuffle_remote_by" +
      "tes_read_to_disk\030  \001(\003\022 \n\030shuffle_local_" +
      "bytes_read\030! \001(\003\022\032\n\022shuffle_read_bytes\030\"" +
      " \001(\003\022\034\n\024shuffle_read_records\030# \001(\003\022\033\n\023sh" +
      "uffle_write_bytes\030$ \001(\003\022\032\n\022shuffle_write" +
      "_time\030% \001(\003\022\035\n\025shuffle_write_records\030& \001" +
      "(\003\022\021\n\004name\030\' \001(\tH\004\210\001\001\022\030\n\013description\030( \001" +
      "(\tH\005\210\001\001\022\024\n\007details\030) \001(\tH\006\210\001\001\022\034\n\017schedul" +
      "ing_pool\030* \001(\tH\007\210\001\001\022\017\n\007rdd_ids\030+ \003(\003\022N\n\023" +
      "accumulator_updates\030, \003(\01321.org.apache.s" +
      "park.status.protobuf.AccumulableInfo\022E\n\005" +
      "tasks\030- \003(\01326.org.apache.spark.status.pr" +
      "otobuf.StageData.TasksEntry\022Z\n\020executor_" +
      "summary\030. \003(\0132@.org.apache.spark.status." +
      "protobuf.StageData.ExecutorSummaryEntry\022" +
      "[\n\023speculation_summary\030/ \001(\01329.org.apach" +
      "e.spark.status.protobuf.SpeculationStage" +
      "SummaryH\010\210\001\001\022a\n\024killed_tasks_summary\0300 \003" +
      "(\0132C.org.apache.spark.status.protobuf.St" +
      "ageData.KilledTasksSummaryEntry\022\033\n\023resou" +
      "rce_profile_id\0301 \001(\005\022U\n\025peak_executor_me" +
      "trics\0302 \001(\01321.org.apache.spark.status.pr" +
      "otobuf.ExecutorMetricsH\t\210\001\001\022b\n\032task_metr" +
      "ics_distributions\0303 \001(\01329.org.apache.spa" +
      "rk.status.protobuf.TaskMetricDistributio" +
      "nsH\n\210\001\001\022k\n\036executor_metrics_distribution" +
      "s\0304 \001(\0132>.org.apache.spark.status.protob" +
      "uf.ExecutorMetricsDistributionsH\013\210\001\001\022+\n#" +
      "shuffle_corrupt_merged_block_chunks\0305 \001(" +
      "\003\022+\n#shuffle_merged_fetch_fallback_count" +
      "\0306 \001(\003\022,\n$shuffle_merged_remote_blocks_f" +
      "etched\0307 \001(\003\022+\n#shuffle_merged_local_blo" +
      "cks_fetched\0308 \001(\003\022,\n$shuffle_merged_remo" +
      "te_chunks_fetched\0309 \001(\003\022+\n#shuffle_merge" +
      "d_local_chunks_fetched\030: \001(\003\022(\n shuffle_" +
      "merged_remote_bytes_read\030; \001(\003\022\'\n\037shuffl" +
      "e_merged_local_bytes_read\030< \001(\003\022$\n\034shuff" +
      "le_remote_reqs_duration\030= \001(\003\022+\n#shuffle" +
      "_merged_remote_reqs_duration\030> \001(\003\022\037\n\027is" +
      "_shuffle_push_enabled\030? \001(\010\022\035\n\025shuffle_m" +
      "ergers_count\030@ \001(\005\032X\n\nTasksEntry\022\013\n\003key\030" +
      "\001 \001(\003\0229\n\005value\030\002 \001(\0132*.org.apache.spark." +
      "status.protobuf.TaskData:\0028\001\032n\n\024Executor" +
      "SummaryEntry\022\013\n\003key\030\001 \001(\t\022E\n\005value\030\002 \001(\013" +
      "26.org.apache.spark.status.protobuf.Exec" +
      "utorStageSummary:\0028\001\0329\n\027KilledTasksSumma" +
      "ryEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\005:\0028\001B" +
      "\022\n\020_submission_timeB\033\n\031_first_task_launc" +
      "hed_timeB\022\n\020_completion_timeB\021\n\017_failure" +
      "_reasonB\007\n\005_nameB\016\n\014_descriptionB\n\n\010_det" +
      "ailsB\022\n\020_scheduling_poolB\026\n\024_speculation" +
      "_summaryB\030\n\026_peak_executor_metricsB\035\n\033_t" +
      "ask_metrics_distributionsB!\n\037_executor_m" +
      "etrics_distributions\"\355\004\n\013TaskMetrics\022!\n\031" +
      "executor_deserialize_time\030\001 \001(\003\022%\n\035execu" +
      "tor_deserialize_cpu_time\030\002 \001(\003\022\031\n\021execut" +
      "or_run_time\030\003 \001(\003\022\031\n\021executor_cpu_time\030\004" +
      " \001(\003\022\023\n\013result_size\030\005 \001(\003\022\023\n\013jvm_gc_time" +
      "\030\006 \001(\003\022!\n\031result_serialization_time\030\007 \001(" +
      "\003\022\034\n\024memory_bytes_spilled\030\010 \001(\003\022\032\n\022disk_" +
      "bytes_spilled\030\t \001(\003\022\035\n\025peak_execution_me" +
      "mory\030\n \001(\003\022E\n\rinput_metrics\030\013 \001(\0132..org." +
      "apache.spark.status.protobuf.InputMetric" +
      "s\022G\n\016output_metrics\030\014 \001(\0132/.org.apache.s" +
      "park.status.protobuf.OutputMetrics\022R\n\024sh" +
      "uffle_read_metrics\030\r \001(\01324.org.apache.sp" +
      "ark.status.protobuf.ShuffleReadMetrics\022T" +
      "\n\025shuffle_write_metrics\030\016 \001(\01325.org.apac" +
      "he.spark.status.protobuf.ShuffleWriteMet" +
      "rics\"8\n\014InputMetrics\022\022\n\nbytes_read\030\001 \001(\003" +
      "\022\024\n\014records_read\030\002 \001(\003\"?\n\rOutputMetrics\022" +
      "\025\n\rbytes_written\030\001 \001(\003\022\027\n\017records_writte" +
      "n\030\002 \001(\003\"\323\002\n\022ShuffleReadMetrics\022\035\n\025remote" +
      "_blocks_fetched\030\001 \001(\003\022\034\n\024local_blocks_fe" +
      "tched\030\002 \001(\003\022\027\n\017fetch_wait_time\030\003 \001(\003\022\031\n\021" +
      "remote_bytes_read\030\004 \001(\003\022!\n\031remote_bytes_" +
      "read_to_disk\030\005 \001(\003\022\030\n\020local_bytes_read\030\006" +
      " \001(\003\022\024\n\014records_read\030\007 \001(\003\022\034\n\024remote_req" +
      "s_duration\030\010 \001(\003\022[\n\031shuffle_push_read_me" +
      "trics\030\t \001(\01328.org.apache.spark.status.pr" +
      "otobuf.ShufflePushReadMetrics\"\340\002\n\026Shuffl" +
      "ePushReadMetrics\022#\n\033corrupt_merged_block" +
      "_chunks\030\001 \001(\003\022#\n\033merged_fetch_fallback_c" +
      "ount\030\002 \001(\003\022$\n\034remote_merged_blocks_fetch" +
      "ed\030\003 \001(\003\022#\n\033local_merged_blocks_fetched\030" +
      "\004 \001(\003\022$\n\034remote_merged_chunks_fetched\030\005 " +
      "\001(\003\022#\n\033local_merged_chunks_fetched\030\006 \001(\003" +
      "\022 \n\030remote_merged_bytes_read\030\007 \001(\003\022\037\n\027lo" +
      "cal_merged_bytes_read\030\010 \001(\003\022#\n\033remote_me" +
      "rged_reqs_duration\030\t \001(\003\"Y\n\023ShuffleWrite" +
      "Metrics\022\025\n\rbytes_written\030\001 \001(\003\022\022\n\nwrite_" +
      "time\030\002 \001(\003\022\027\n\017records_written\030\003 \001(\003\"\204\006\n\027" +
      "TaskMetricDistributions\022\021\n\tquantiles\030\001 \003" +
      "(\001\022\020\n\010duration\030\002 \003(\001\022!\n\031executor_deseria" +
      "lize_time\030\003 \003(\001\022%\n\035executor_deserialize_" +
      "cpu_time\030\004 \003(\001\022\031\n\021executor_run_time\030\005 \003(" +
      "\001\022\031\n\021executor_cpu_time\030\006 \003(\001\022\023\n\013result_s" +
      "ize\030\007 \003(\001\022\023\n\013jvm_gc_time\030\010 \003(\001\022!\n\031result" +
      "_serialization_time\030\t \003(\001\022\033\n\023getting_res" +
      "ult_time\030\n \003(\001\022\027\n\017scheduler_delay\030\013 \003(\001\022" +
      "\035\n\025peak_execution_memory\030\014 \003(\001\022\034\n\024memory" +
      "_bytes_spilled\030\r \003(\001\022\032\n\022disk_bytes_spill" +
      "ed\030\016 \003(\001\022Q\n\rinput_metrics\030\017 \001(\0132:.org.ap" +
      "ache.spark.status.protobuf.InputMetricDi" +
      "stributions\022S\n\016output_metrics\030\020 \001(\0132;.or" +
      "g.apache.spark.status.protobuf.OutputMet" +
      "ricDistributions\022^\n\024shuffle_read_metrics" +
      "\030\021 \001(\0132@.org.apache.spark.status.protobu" +
      "f.ShuffleReadMetricDistributions\022`\n\025shuf" +
      "fle_write_metrics\030\022 \001(\0132A.org.apache.spa" +
      "rk.status.protobuf.ShuffleWriteMetricDis" +
      "tributions\"D\n\030InputMetricDistributions\022\022" +
      "\n\nbytes_read\030\001 \003(\001\022\024\n\014records_read\030\002 \003(\001" +
      "\"K\n\031OutputMetricDistributions\022\025\n\rbytes_w" +
      "ritten\030\001 \003(\001\022\027\n\017records_written\030\002 \003(\001\"\210\003" +
      "\n\036ShuffleReadMetricDistributions\022\022\n\nread" +
      "_bytes\030\001 \003(\001\022\024\n\014read_records\030\002 \003(\001\022\035\n\025re" +
      "mote_blocks_fetched\030\003 \003(\001\022\034\n\024local_block" +
      "s_fetched\030\004 \003(\001\022\027\n\017fetch_wait_time\030\005 \003(\001" +
      "\022\031\n\021remote_bytes_read\030\006 \003(\001\022!\n\031remote_by" +
      "tes_read_to_disk\030\007 \003(\001\022\034\n\024total_blocks_f" +
      "etched\030\010 \003(\001\022\034\n\024remote_reqs_duration\030\t \003" +
      "(\001\022l\n\036shuffle_push_read_metrics_dist\030\n \001" +
      "(\0132D.org.apache.spark.status.protobuf.Sh" +
      "ufflePushReadMetricDistributions\"\354\002\n\"Shu" +
      "fflePushReadMetricDistributions\022#\n\033corru" +
      "pt_merged_block_chunks\030\001 \003(\001\022#\n\033merged_f" +
      "etch_fallback_count\030\002 \003(\001\022$\n\034remote_merg" +
      "ed_blocks_fetched\030\003 \003(\001\022#\n\033local_merged_" +
      "blocks_fetched\030\004 \003(\001\022$\n\034remote_merged_ch" +
      "unks_fetched\030\005 \003(\001\022#\n\033local_merged_chunk" +
      "s_fetched\030\006 \003(\001\022 \n\030remote_merged_bytes_r" +
      "ead\030\007 \003(\001\022\037\n\027local_merged_bytes_read\030\010 \003" +
      "(\001\022#\n\033remote_merged_reqs_duration\030\t \003(\001\"" +
      "a\n\037ShuffleWriteMetricDistributions\022\023\n\013wr" +
      "ite_bytes\030\001 \003(\001\022\025\n\rwrite_records\030\002 \003(\001\022\022" +
      "\n\nwrite_time\030\003 \003(\001\"\350\003\n\034ExecutorMetricsDi" +
      "stributions\022\021\n\tquantiles\030\001 \003(\001\022\021\n\ttask_t" +
      "ime\030\002 \003(\001\022\024\n\014failed_tasks\030\003 \003(\001\022\027\n\017succe" +
      "eded_tasks\030\004 \003(\001\022\024\n\014killed_tasks\030\005 \003(\001\022\023" +
      "\n\013input_bytes\030\006 \003(\001\022\025\n\rinput_records\030\007 \003" +
      "(\001\022\024\n\014output_bytes\030\010 \003(\001\022\026\n\016output_recor" +
      "ds\030\t \003(\001\022\024\n\014shuffle_read\030\n \003(\001\022\034\n\024shuffl" +
      "e_read_records\030\013 \003(\001\022\025\n\rshuffle_write\030\014 " +
      "\003(\001\022\035\n\025shuffle_write_records\030\r \003(\001\022\034\n\024me" +
      "mory_bytes_spilled\030\016 \003(\001\022\032\n\022disk_bytes_s" +
      "pilled\030\017 \003(\001\022_\n\023peak_memory_metrics\030\020 \001(" +
      "\0132B.org.apache.spark.status.protobuf.Exe" +
      "cutorPeakMetricsDistributions\"\202\001\n Execut" +
      "orPeakMetricsDistributions\022\021\n\tquantiles\030" +
      "\001 \003(\001\022K\n\020executor_metrics\030\002 \003(\01321.org.ap" +
      "ache.spark.status.protobuf.ExecutorMetri" +
      "cs\"F\n\nAppSummary\022\032\n\022num_completed_jobs\030\001" +
      " \001(\005\022\034\n\024num_completed_stages\030\002 \001(\005\"9\n\010Po" +
      "olData\022\021\n\004name\030\001 \001(\tH\000\210\001\001\022\021\n\tstage_ids\030\002" +
      " \003(\003B\007\n\005_name\"\203\004\n\025StateOperatorProgress\022" +
      "\032\n\roperator_name\030\001 \001(\tH\000\210\001\001\022\026\n\016num_rows_" +
      "total\030\002 \001(\003\022\030\n\020num_rows_updated\030\003 \001(\003\022\033\n" +
      "\023all_updates_time_ms\030\004 \001(\003\022\030\n\020num_rows_r" +
      "emoved\030\005 \001(\003\022\034\n\024all_removals_time_ms\030\006 \001" +
      "(\003\022\026\n\016commit_time_ms\030\007 \001(\003\022\031\n\021memory_use" +
      "d_bytes\030\010 \001(\003\022%\n\035num_rows_dropped_by_wat" +
      "ermark\030\t \001(\003\022\036\n\026num_shuffle_partitions\030\n" +
      " \001(\003\022!\n\031num_state_store_instances\030\013 \001(\003\022" +
      "b\n\016custom_metrics\030\014 \003(\0132J.org.apache.spa" +
      "rk.status.protobuf.StateOperatorProgress" +
      ".CustomMetricsEntry\0324\n\022CustomMetricsEntr" +
      "y\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\003:\0028\001B\020\n\016_op" +
      "erator_name\"\226\003\n\016SourceProgress\022\030\n\013descri" +
      "ption\030\001 \001(\tH\000\210\001\001\022\031\n\014start_offset\030\002 \001(\tH\001" +
      "\210\001\001\022\027\n\nend_offset\030\003 \001(\tH\002\210\001\001\022\032\n\rlatest_o" +
      "ffset\030\004 \001(\tH\003\210\001\001\022\026\n\016num_input_rows\030\005 \001(\003" +
      "\022\035\n\025input_rows_per_second\030\006 \001(\001\022!\n\031proce" +
      "ssed_rows_per_second\030\007 \001(\001\022N\n\007metrics\030\010 " +
      "\003(\0132=.org.apache.spark.status.protobuf.S" +
      "ourceProgress.MetricsEntry\032.\n\014MetricsEnt" +
      "ry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001B\016\n\014_d" +
      "escriptionB\017\n\r_start_offsetB\r\n\013_end_offs" +
      "etB\020\n\016_latest_offset\"\317\001\n\014SinkProgress\022\030\n" +
      "\013description\030\001 \001(\tH\000\210\001\001\022\027\n\017num_output_ro" +
      "ws\030\002 \001(\003\022L\n\007metrics\030\003 \003(\0132;.org.apache.s" +
      "park.status.protobuf.SinkProgress.Metric" +
      "sEntry\032.\n\014MetricsEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005v" +
      "alue\030\002 \001(\t:\0028\001B\016\n\014_description\"\321\006\n\026Strea" +
      "mingQueryProgress\022\017\n\002id\030\001 \001(\tH\000\210\001\001\022\023\n\006ru" +
      "n_id\030\002 \001(\tH\001\210\001\001\022\021\n\004name\030\003 \001(\tH\002\210\001\001\022\026\n\tti" +
      "mestamp\030\004 \001(\tH\003\210\001\001\022\020\n\010batch_id\030\005 \001(\003\022\026\n\016" +
      "batch_duration\030\006 \001(\003\022]\n\013duration_ms\030\007 \003(" +
      "\0132H.org.apache.spark.status.protobuf.Str" +
      "eamingQueryProgress.DurationMsEntry\022[\n\ne" +
      "vent_time\030\010 \003(\0132G.org.apache.spark.statu" +
      "s.protobuf.StreamingQueryProgress.EventT" +
      "imeEntry\022P\n\017state_operators\030\t \003(\01327.org." +
      "apache.spark.status.protobuf.StateOperat" +
      "orProgress\022A\n\007sources\030\n \003(\01320.org.apache" +
      ".spark.status.protobuf.SourceProgress\022<\n" +
      "\004sink\030\013 \001(\0132..org.apache.spark.status.pr" +
      "otobuf.SinkProgress\022g\n\020observed_metrics\030" +
      "\014 \003(\0132M.org.apache.spark.status.protobuf" +
      ".StreamingQueryProgress.ObservedMetricsE" +
      "ntry\0321\n\017DurationMsEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005" +
      "value\030\002 \001(\003:\0028\001\0320\n\016EventTimeEntry\022\013\n\003key" +
      "\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001\0326\n\024ObservedMet" +
      "ricsEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028" +
      "\001B\005\n\003_idB\t\n\007_run_idB\007\n\005_nameB\014\n\n_timesta" +
      "mp\"k\n\035StreamingQueryProgressWrapper\022J\n\010p" +
      "rogress\030\001 \001(\01328.org.apache.spark.status." +
      "protobuf.StreamingQueryProgress*\303\001\n\022JobE" +
      "xecutionStatus\022$\n JOB_EXECUTION_STATUS_U" +
      "NSPECIFIED\020\000\022 \n\034JOB_EXECUTION_STATUS_RUN" +
      "NING\020\001\022\"\n\036JOB_EXECUTION_STATUS_SUCCEEDED" +
      "\020\002\022\037\n\033JOB_EXECUTION_STATUS_FAILED\020\003\022 \n\034J" +
      "OB_EXECUTION_STATUS_UNKNOWN\020\004*\250\001\n\022Determ" +
      "inisticLevel\022#\n\037DETERMINISTIC_LEVEL_UNSP" +
      "ECIFIED\020\000\022#\n\037DETERMINISTIC_LEVEL_DETERMI" +
      "NATE\020\001\022!\n\035DETERMINISTIC_LEVEL_UNORDERED\020" +
      "\002\022%\n!DETERMINISTIC_LEVEL_INDETERMINATE\020\003" +
      "*\254\001\n\013StageStatus\022\034\n\030STAGE_STATUS_UNSPECI" +
      "FIED\020\000\022\027\n\023STAGE_STATUS_ACTIVE\020\001\022\031\n\025STAGE" +
      "_STATUS_COMPLETE\020\002\022\027\n\023STAGE_STATUS_FAILE" +
      "D\020\003\022\030\n\024STAGE_STATUS_PENDING\020\004\022\030\n\024STAGE_S" +
      "TATUS_SKIPPED\020\005b\006proto3"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        });
    internal_static_org_apache_spark_status_protobuf_JobData_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_JobData_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_JobData_descriptor,
        new java.lang.String[] { "JobId", "Name", "Description", "SubmissionTime", "CompletionTime", "StageIds", "JobGroup", "Status", "NumTasks", "NumActiveTasks", "NumCompletedTasks", "NumSkippedTasks", "NumFailedTasks", "NumKilledTasks", "NumCompletedIndices", "NumActiveStages", "NumCompletedStages", "NumSkippedStages", "NumFailedStages", "KillTasksSummary", "Name", "Description", "SubmissionTime", "CompletionTime", "JobGroup", });
    internal_static_org_apache_spark_status_protobuf_JobData_KillTasksSummaryEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_JobData_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_JobData_KillTasksSummaryEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_JobData_KillTasksSummaryEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_JobDataWrapper_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_org_apache_spark_status_protobuf_JobDataWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_JobDataWrapper_descriptor,
        new java.lang.String[] { "Info", "SkippedStages", "SqlExecutionId", "SqlExecutionId", });
    internal_static_org_apache_spark_status_protobuf_AccumulableInfo_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_org_apache_spark_status_protobuf_AccumulableInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_AccumulableInfo_descriptor,
        new java.lang.String[] { "Id", "Name", "Update", "Value", "Name", "Update", "Value", });
    internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_TaskDataWrapper_descriptor,
        new java.lang.String[] { "TaskId", "Index", "Attempt", "PartitionId", "LaunchTime", "ResultFetchStart", "Duration", "ExecutorId", "Host", "Status", "TaskLocality", "Speculative", "AccumulatorUpdates", "ErrorMessage", "HasMetrics", "ExecutorDeserializeTime", "ExecutorDeserializeCpuTime", "ExecutorRunTime", "ExecutorCpuTime", "ResultSize", "JvmGcTime", "ResultSerializationTime", "MemoryBytesSpilled", "DiskBytesSpilled", "PeakExecutionMemory", "InputBytesRead", "InputRecordsRead", "OutputBytesWritten", "OutputRecordsWritten", "ShuffleRemoteBlocksFetched", "ShuffleLocalBlocksFetched", "ShuffleFetchWaitTime", "ShuffleRemoteBytesRead", "ShuffleRemoteBytesReadToDisk", "ShuffleLocalBytesRead", "ShuffleRecordsRead", "ShuffleBytesWritten", "ShuffleWriteTime", "ShuffleRecordsWritten", "StageId", "StageAttemptId", "ShuffleCorruptMergedBlockChunks", "ShuffleMergedFetchFallbackCount", "ShuffleMergedRemoteBlocksFetched", "ShuffleMergedLocalBlocksFetched", "ShuffleMergedRemoteChunksFetched", "ShuffleMergedLocalChunksFetched", "ShuffleMergedRemoteBytesRead", "ShuffleMergedLocalBytesRead", "ShuffleRemoteReqsDuration", "ShuffleMergedRemoteReqDuration", "ExecutorId", "Host", "Status", "TaskLocality", "ErrorMessage", });
    internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_descriptor,
        new java.lang.String[] { "Metrics", });
    internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_MetricsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_MetricsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorMetrics_MetricsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorStageSummary_descriptor,
        new java.lang.String[] { "TaskTime", "FailedTasks", "SucceededTasks", "KilledTasks", "InputBytes", "InputRecords", "OutputBytes", "OutputRecords", "ShuffleRead", "ShuffleReadRecords", "ShuffleWrite", "ShuffleWriteRecords", "MemoryBytesSpilled", "DiskBytesSpilled", "IsBlacklistedForStage", "PeakMemoryMetrics", "IsExcludedForStage", "PeakMemoryMetrics", });
    internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorStageSummaryWrapper_descriptor,
        new java.lang.String[] { "StageId", "StageAttemptId", "ExecutorId", "Info", "ExecutorId", });
    internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorResourceRequest_descriptor,
        new java.lang.String[] { "ResourceName", "Amount", "DiscoveryScript", "Vendor", "ResourceName", "DiscoveryScript", "Vendor", });
    internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_TaskResourceRequest_descriptor,
        new java.lang.String[] { "ResourceName", "Amount", "ResourceName", });
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_descriptor,
        new java.lang.String[] { "Id", "ExecutorResources", "TaskResources", });
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_ExecutorResourcesEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_ExecutorResourcesEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_ExecutorResourcesEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_TaskResourcesEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_descriptor.getNestedTypes().get(1);
    internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_TaskResourcesEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ResourceProfileInfo_TaskResourcesEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_RuntimeInfo_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_org_apache_spark_status_protobuf_RuntimeInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RuntimeInfo_descriptor,
        new java.lang.String[] { "JavaVersion", "JavaHome", "ScalaVersion", "JavaVersion", "JavaHome", "ScalaVersion", });
    internal_static_org_apache_spark_status_protobuf_PairStrings_descriptor =
      getDescriptor().getMessageTypes().get(11);
    internal_static_org_apache_spark_status_protobuf_PairStrings_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_PairStrings_descriptor,
        new java.lang.String[] { "Value1", "Value2", "Value1", "Value2", });
    internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_descriptor =
      getDescriptor().getMessageTypes().get(12);
    internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfo_descriptor,
        new java.lang.String[] { "Runtime", "SparkProperties", "HadoopProperties", "SystemProperties", "MetricsProperties", "ClasspathEntries", "ResourceProfiles", });
    internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_descriptor =
      getDescriptor().getMessageTypes().get(13);
    internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ApplicationEnvironmentInfoWrapper_descriptor,
        new java.lang.String[] { "Info", });
    internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_descriptor =
      getDescriptor().getMessageTypes().get(14);
    internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ApplicationAttemptInfo_descriptor,
        new java.lang.String[] { "AttemptId", "StartTime", "EndTime", "LastUpdated", "Duration", "SparkUser", "Completed", "AppSparkVersion", "AttemptId", "SparkUser", "AppSparkVersion", });
    internal_static_org_apache_spark_status_protobuf_ApplicationInfo_descriptor =
      getDescriptor().getMessageTypes().get(15);
    internal_static_org_apache_spark_status_protobuf_ApplicationInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ApplicationInfo_descriptor,
        new java.lang.String[] { "Id", "Name", "CoresGranted", "MaxCores", "CoresPerExecutor", "MemoryPerExecutorMb", "Attempts", "Id", "Name", "CoresGranted", "MaxCores", "CoresPerExecutor", "MemoryPerExecutorMb", });
    internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_descriptor =
      getDescriptor().getMessageTypes().get(16);
    internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ApplicationInfoWrapper_descriptor,
        new java.lang.String[] { "Info", });
    internal_static_org_apache_spark_status_protobuf_StreamBlockData_descriptor =
      getDescriptor().getMessageTypes().get(17);
    internal_static_org_apache_spark_status_protobuf_StreamBlockData_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StreamBlockData_descriptor,
        new java.lang.String[] { "Name", "ExecutorId", "HostPort", "StorageLevel", "UseMemory", "UseDisk", "Deserialized", "MemSize", "DiskSize", "Name", "ExecutorId", "HostPort", "StorageLevel", });
    internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_descriptor =
      getDescriptor().getMessageTypes().get(18);
    internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RDDDataDistribution_descriptor,
        new java.lang.String[] { "Address", "MemoryUsed", "MemoryRemaining", "DiskUsed", "OnHeapMemoryUsed", "OffHeapMemoryUsed", "OnHeapMemoryRemaining", "OffHeapMemoryRemaining", "Address", "OnHeapMemoryUsed", "OffHeapMemoryUsed", "OnHeapMemoryRemaining", "OffHeapMemoryRemaining", });
    internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_descriptor =
      getDescriptor().getMessageTypes().get(19);
    internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RDDPartitionInfo_descriptor,
        new java.lang.String[] { "BlockName", "StorageLevel", "MemoryUsed", "DiskUsed", "Executors", "BlockName", "StorageLevel", });
    internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_descriptor =
      getDescriptor().getMessageTypes().get(20);
    internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RDDStorageInfo_descriptor,
        new java.lang.String[] { "Id", "Name", "NumPartitions", "NumCachedPartitions", "StorageLevel", "MemoryUsed", "DiskUsed", "DataDistribution", "Partitions", "Name", "StorageLevel", });
    internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_descriptor =
      getDescriptor().getMessageTypes().get(21);
    internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RDDStorageInfoWrapper_descriptor,
        new java.lang.String[] { "Info", });
    internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_descriptor =
      getDescriptor().getMessageTypes().get(22);
    internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ResourceProfileWrapper_descriptor,
        new java.lang.String[] { "RpInfo", });
    internal_static_org_apache_spark_status_protobuf_CachedQuantile_descriptor =
      getDescriptor().getMessageTypes().get(23);
    internal_static_org_apache_spark_status_protobuf_CachedQuantile_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_CachedQuantile_descriptor,
        new java.lang.String[] { "StageId", "StageAttemptId", "Quantile", "TaskCount", "Duration", "ExecutorDeserializeTime", "ExecutorDeserializeCpuTime", "ExecutorRunTime", "ExecutorCpuTime", "ResultSize", "JvmGcTime", "ResultSerializationTime", "GettingResultTime", "SchedulerDelay", "PeakExecutionMemory", "MemoryBytesSpilled", "DiskBytesSpilled", "BytesRead", "RecordsRead", "BytesWritten", "RecordsWritten", "ShuffleReadBytes", "ShuffleRecordsRead", "ShuffleRemoteBlocksFetched", "ShuffleLocalBlocksFetched", "ShuffleFetchWaitTime", "ShuffleRemoteBytesRead", "ShuffleRemoteBytesReadToDisk", "ShuffleTotalBlocksFetched", "ShuffleWriteBytes", "ShuffleWriteRecords", "ShuffleWriteTime", "ShuffleCorruptMergedBlockChunks", "ShuffleMergedFetchFallbackCount", "ShuffleMergedRemoteBlocksFetched", "ShuffleMergedLocalBlocksFetched", "ShuffleMergedRemoteChunksFetched", "ShuffleMergedLocalChunksFetched", "ShuffleMergedRemoteBytesRead", "ShuffleMergedLocalBytesRead", "ShuffleRemoteReqsDuration", "ShuffleMergedRemoteReqsDuration", "Quantile", });
    internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_descriptor =
      getDescriptor().getMessageTypes().get(24);
    internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SpeculationStageSummary_descriptor,
        new java.lang.String[] { "NumTasks", "NumActiveTasks", "NumCompletedTasks", "NumFailedTasks", "NumKilledTasks", });
    internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_descriptor =
      getDescriptor().getMessageTypes().get(25);
    internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SpeculationStageSummaryWrapper_descriptor,
        new java.lang.String[] { "StageId", "StageAttemptId", "Info", });
    internal_static_org_apache_spark_status_protobuf_ProcessSummary_descriptor =
      getDescriptor().getMessageTypes().get(26);
    internal_static_org_apache_spark_status_protobuf_ProcessSummary_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ProcessSummary_descriptor,
        new java.lang.String[] { "Id", "HostPort", "IsActive", "TotalCores", "AddTime", "RemoveTime", "ProcessLogs", "Id", "HostPort", "RemoveTime", });
    internal_static_org_apache_spark_status_protobuf_ProcessSummary_ProcessLogsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_ProcessSummary_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_ProcessSummary_ProcessLogsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ProcessSummary_ProcessLogsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_descriptor =
      getDescriptor().getMessageTypes().get(27);
    internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ProcessSummaryWrapper_descriptor,
        new java.lang.String[] { "Info", });
    internal_static_org_apache_spark_status_protobuf_MemoryMetrics_descriptor =
      getDescriptor().getMessageTypes().get(28);
    internal_static_org_apache_spark_status_protobuf_MemoryMetrics_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_MemoryMetrics_descriptor,
        new java.lang.String[] { "UsedOnHeapStorageMemory", "UsedOffHeapStorageMemory", "TotalOnHeapStorageMemory", "TotalOffHeapStorageMemory", });
    internal_static_org_apache_spark_status_protobuf_ResourceInformation_descriptor =
      getDescriptor().getMessageTypes().get(29);
    internal_static_org_apache_spark_status_protobuf_ResourceInformation_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ResourceInformation_descriptor,
        new java.lang.String[] { "Name", "Addresses", "Name", });
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor =
      getDescriptor().getMessageTypes().get(30);
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor,
        new java.lang.String[] { "Id", "HostPort", "IsActive", "RddBlocks", "MemoryUsed", "DiskUsed", "TotalCores", "MaxTasks", "ActiveTasks", "FailedTasks", "CompletedTasks", "TotalTasks", "TotalDuration", "TotalGcTime", "TotalInputBytes", "TotalShuffleRead", "TotalShuffleWrite", "IsBlacklisted", "MaxMemory", "AddTime", "RemoveTime", "RemoveReason", "ExecutorLogs", "MemoryMetrics", "BlacklistedInStages", "PeakMemoryMetrics", "Attributes", "Resources", "ResourceProfileId", "IsExcluded", "ExcludedInStages", "Id", "HostPort", "RemoveTime", "RemoveReason", "MemoryMetrics", "PeakMemoryMetrics", });
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ExecutorLogsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ExecutorLogsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ExecutorLogsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_AttributesEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor.getNestedTypes().get(1);
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_AttributesEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorSummary_AttributesEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ResourcesEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_ExecutorSummary_descriptor.getNestedTypes().get(2);
    internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ResourcesEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorSummary_ResourcesEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_descriptor =
      getDescriptor().getMessageTypes().get(31);
    internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorSummaryWrapper_descriptor,
        new java.lang.String[] { "Info", });
    internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_descriptor =
      getDescriptor().getMessageTypes().get(32);
    internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SQLPlanMetric_descriptor,
        new java.lang.String[] { "Name", "AccumulatorId", "MetricType", "Name", "MetricType", });
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor =
      getDescriptor().getMessageTypes().get(33);
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor,
        new java.lang.String[] { "ExecutionId", "RootExecutionId", "Description", "Details", "PhysicalPlanDescription", "ModifiedConfigs", "Metrics", "SubmissionTime", "CompletionTime", "ErrorMessage", "Jobs", "Stages", "MetricValuesIsNull", "MetricValues", "Description", "Details", "PhysicalPlanDescription", "CompletionTime", "ErrorMessage", });
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_ModifiedConfigsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_ModifiedConfigsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_ModifiedConfigsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_JobsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor.getNestedTypes().get(1);
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_JobsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_JobsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_MetricValuesEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_descriptor.getNestedTypes().get(2);
    internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_MetricValuesEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SQLExecutionUIData_MetricValuesEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_descriptor =
      getDescriptor().getMessageTypes().get(34);
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNode_descriptor,
        new java.lang.String[] { "Id", "Name", "Desc", "Metrics", "Name", "Desc", });
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_descriptor =
      getDescriptor().getMessageTypes().get(35);
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SparkPlanGraphClusterWrapper_descriptor,
        new java.lang.String[] { "Id", "Name", "Desc", "Nodes", "Metrics", "Name", "Desc", });
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_descriptor =
      getDescriptor().getMessageTypes().get(36);
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SparkPlanGraphNodeWrapper_descriptor,
        new java.lang.String[] { "Node", "Cluster", "Wrapper", });
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_descriptor =
      getDescriptor().getMessageTypes().get(37);
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SparkPlanGraphEdge_descriptor,
        new java.lang.String[] { "FromId", "ToId", });
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_descriptor =
      getDescriptor().getMessageTypes().get(38);
    internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SparkPlanGraphWrapper_descriptor,
        new java.lang.String[] { "ExecutionId", "Nodes", "Edges", });
    internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_descriptor =
      getDescriptor().getMessageTypes().get(39);
    internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RDDOperationEdge_descriptor,
        new java.lang.String[] { "FromId", "ToId", });
    internal_static_org_apache_spark_status_protobuf_RDDOperationNode_descriptor =
      getDescriptor().getMessageTypes().get(40);
    internal_static_org_apache_spark_status_protobuf_RDDOperationNode_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RDDOperationNode_descriptor,
        new java.lang.String[] { "Id", "Name", "Cached", "Barrier", "Callsite", "OutputDeterministicLevel", "Name", "Callsite", });
    internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_descriptor =
      getDescriptor().getMessageTypes().get(41);
    internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RDDOperationClusterWrapper_descriptor,
        new java.lang.String[] { "Id", "Name", "ChildNodes", "ChildClusters", "Id", "Name", });
    internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_descriptor =
      getDescriptor().getMessageTypes().get(42);
    internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_RDDOperationGraphWrapper_descriptor,
        new java.lang.String[] { "StageId", "Edges", "OutgoingEdges", "IncomingEdges", "RootCluster", });
    internal_static_org_apache_spark_status_protobuf_StreamingQueryData_descriptor =
      getDescriptor().getMessageTypes().get(43);
    internal_static_org_apache_spark_status_protobuf_StreamingQueryData_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StreamingQueryData_descriptor,
        new java.lang.String[] { "Name", "Id", "RunId", "IsActive", "Exception", "StartTimestamp", "EndTimestamp", "Name", "Id", "RunId", "Exception", "EndTimestamp", });
    internal_static_org_apache_spark_status_protobuf_StageDataWrapper_descriptor =
      getDescriptor().getMessageTypes().get(44);
    internal_static_org_apache_spark_status_protobuf_StageDataWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StageDataWrapper_descriptor,
        new java.lang.String[] { "Info", "JobIds", "Locality", });
    internal_static_org_apache_spark_status_protobuf_StageDataWrapper_LocalityEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_StageDataWrapper_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_StageDataWrapper_LocalityEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StageDataWrapper_LocalityEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_TaskData_descriptor =
      getDescriptor().getMessageTypes().get(45);
    internal_static_org_apache_spark_status_protobuf_TaskData_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_TaskData_descriptor,
        new java.lang.String[] { "TaskId", "Index", "Attempt", "PartitionId", "LaunchTime", "ResultFetchStart", "Duration", "ExecutorId", "Host", "Status", "TaskLocality", "Speculative", "AccumulatorUpdates", "ErrorMessage", "TaskMetrics", "ExecutorLogs", "SchedulerDelay", "GettingResultTime", "ResultFetchStart", "Duration", "ExecutorId", "Host", "Status", "TaskLocality", "ErrorMessage", "TaskMetrics", });
    internal_static_org_apache_spark_status_protobuf_TaskData_ExecutorLogsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_TaskData_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_TaskData_ExecutorLogsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_TaskData_ExecutorLogsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_StageData_descriptor =
      getDescriptor().getMessageTypes().get(46);
    internal_static_org_apache_spark_status_protobuf_StageData_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StageData_descriptor,
        new java.lang.String[] { "Status", "StageId", "AttemptId", "NumTasks", "NumActiveTasks", "NumCompleteTasks", "NumFailedTasks", "NumKilledTasks", "NumCompletedIndices", "SubmissionTime", "FirstTaskLaunchedTime", "CompletionTime", "FailureReason", "ExecutorDeserializeTime", "ExecutorDeserializeCpuTime", "ExecutorRunTime", "ExecutorCpuTime", "ResultSize", "JvmGcTime", "ResultSerializationTime", "MemoryBytesSpilled", "DiskBytesSpilled", "PeakExecutionMemory", "InputBytes", "InputRecords", "OutputBytes", "OutputRecords", "ShuffleRemoteBlocksFetched", "ShuffleLocalBlocksFetched", "ShuffleFetchWaitTime", "ShuffleRemoteBytesRead", "ShuffleRemoteBytesReadToDisk", "ShuffleLocalBytesRead", "ShuffleReadBytes", "ShuffleReadRecords", "ShuffleWriteBytes", "ShuffleWriteTime", "ShuffleWriteRecords", "Name", "Description", "Details", "SchedulingPool", "RddIds", "AccumulatorUpdates", "Tasks", "ExecutorSummary", "SpeculationSummary", "KilledTasksSummary", "ResourceProfileId", "PeakExecutorMetrics", "TaskMetricsDistributions", "ExecutorMetricsDistributions", "ShuffleCorruptMergedBlockChunks", "ShuffleMergedFetchFallbackCount", "ShuffleMergedRemoteBlocksFetched", "ShuffleMergedLocalBlocksFetched", "ShuffleMergedRemoteChunksFetched", "ShuffleMergedLocalChunksFetched", "ShuffleMergedRemoteBytesRead", "ShuffleMergedLocalBytesRead", "ShuffleRemoteReqsDuration", "ShuffleMergedRemoteReqsDuration", "IsShufflePushEnabled", "ShuffleMergersCount", "SubmissionTime", "FirstTaskLaunchedTime", "CompletionTime", "FailureReason", "Name", "Description", "Details", "SchedulingPool", "SpeculationSummary", "PeakExecutorMetrics", "TaskMetricsDistributions", "ExecutorMetricsDistributions", });
    internal_static_org_apache_spark_status_protobuf_StageData_TasksEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_StageData_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_StageData_TasksEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StageData_TasksEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_StageData_ExecutorSummaryEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_StageData_descriptor.getNestedTypes().get(1);
    internal_static_org_apache_spark_status_protobuf_StageData_ExecutorSummaryEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StageData_ExecutorSummaryEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_StageData_KilledTasksSummaryEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_StageData_descriptor.getNestedTypes().get(2);
    internal_static_org_apache_spark_status_protobuf_StageData_KilledTasksSummaryEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StageData_KilledTasksSummaryEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_TaskMetrics_descriptor =
      getDescriptor().getMessageTypes().get(47);
    internal_static_org_apache_spark_status_protobuf_TaskMetrics_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_TaskMetrics_descriptor,
        new java.lang.String[] { "ExecutorDeserializeTime", "ExecutorDeserializeCpuTime", "ExecutorRunTime", "ExecutorCpuTime", "ResultSize", "JvmGcTime", "ResultSerializationTime", "MemoryBytesSpilled", "DiskBytesSpilled", "PeakExecutionMemory", "InputMetrics", "OutputMetrics", "ShuffleReadMetrics", "ShuffleWriteMetrics", });
    internal_static_org_apache_spark_status_protobuf_InputMetrics_descriptor =
      getDescriptor().getMessageTypes().get(48);
    internal_static_org_apache_spark_status_protobuf_InputMetrics_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_InputMetrics_descriptor,
        new java.lang.String[] { "BytesRead", "RecordsRead", });
    internal_static_org_apache_spark_status_protobuf_OutputMetrics_descriptor =
      getDescriptor().getMessageTypes().get(49);
    internal_static_org_apache_spark_status_protobuf_OutputMetrics_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_OutputMetrics_descriptor,
        new java.lang.String[] { "BytesWritten", "RecordsWritten", });
    internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_descriptor =
      getDescriptor().getMessageTypes().get(50);
    internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ShuffleReadMetrics_descriptor,
        new java.lang.String[] { "RemoteBlocksFetched", "LocalBlocksFetched", "FetchWaitTime", "RemoteBytesRead", "RemoteBytesReadToDisk", "LocalBytesRead", "RecordsRead", "RemoteReqsDuration", "ShufflePushReadMetrics", });
    internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_descriptor =
      getDescriptor().getMessageTypes().get(51);
    internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetrics_descriptor,
        new java.lang.String[] { "CorruptMergedBlockChunks", "MergedFetchFallbackCount", "RemoteMergedBlocksFetched", "LocalMergedBlocksFetched", "RemoteMergedChunksFetched", "LocalMergedChunksFetched", "RemoteMergedBytesRead", "LocalMergedBytesRead", "RemoteMergedReqsDuration", });
    internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_descriptor =
      getDescriptor().getMessageTypes().get(52);
    internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetrics_descriptor,
        new java.lang.String[] { "BytesWritten", "WriteTime", "RecordsWritten", });
    internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_descriptor =
      getDescriptor().getMessageTypes().get(53);
    internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_TaskMetricDistributions_descriptor,
        new java.lang.String[] { "Quantiles", "Duration", "ExecutorDeserializeTime", "ExecutorDeserializeCpuTime", "ExecutorRunTime", "ExecutorCpuTime", "ResultSize", "JvmGcTime", "ResultSerializationTime", "GettingResultTime", "SchedulerDelay", "PeakExecutionMemory", "MemoryBytesSpilled", "DiskBytesSpilled", "InputMetrics", "OutputMetrics", "ShuffleReadMetrics", "ShuffleWriteMetrics", });
    internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_descriptor =
      getDescriptor().getMessageTypes().get(54);
    internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_InputMetricDistributions_descriptor,
        new java.lang.String[] { "BytesRead", "RecordsRead", });
    internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_descriptor =
      getDescriptor().getMessageTypes().get(55);
    internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_OutputMetricDistributions_descriptor,
        new java.lang.String[] { "BytesWritten", "RecordsWritten", });
    internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_descriptor =
      getDescriptor().getMessageTypes().get(56);
    internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ShuffleReadMetricDistributions_descriptor,
        new java.lang.String[] { "ReadBytes", "ReadRecords", "RemoteBlocksFetched", "LocalBlocksFetched", "FetchWaitTime", "RemoteBytesRead", "RemoteBytesReadToDisk", "TotalBlocksFetched", "RemoteReqsDuration", "ShufflePushReadMetricsDist", });
    internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_descriptor =
      getDescriptor().getMessageTypes().get(57);
    internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ShufflePushReadMetricDistributions_descriptor,
        new java.lang.String[] { "CorruptMergedBlockChunks", "MergedFetchFallbackCount", "RemoteMergedBlocksFetched", "LocalMergedBlocksFetched", "RemoteMergedChunksFetched", "LocalMergedChunksFetched", "RemoteMergedBytesRead", "LocalMergedBytesRead", "RemoteMergedReqsDuration", });
    internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_descriptor =
      getDescriptor().getMessageTypes().get(58);
    internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ShuffleWriteMetricDistributions_descriptor,
        new java.lang.String[] { "WriteBytes", "WriteRecords", "WriteTime", });
    internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_descriptor =
      getDescriptor().getMessageTypes().get(59);
    internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorMetricsDistributions_descriptor,
        new java.lang.String[] { "Quantiles", "TaskTime", "FailedTasks", "SucceededTasks", "KilledTasks", "InputBytes", "InputRecords", "OutputBytes", "OutputRecords", "ShuffleRead", "ShuffleReadRecords", "ShuffleWrite", "ShuffleWriteRecords", "MemoryBytesSpilled", "DiskBytesSpilled", "PeakMemoryMetrics", });
    internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_descriptor =
      getDescriptor().getMessageTypes().get(60);
    internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_ExecutorPeakMetricsDistributions_descriptor,
        new java.lang.String[] { "Quantiles", "ExecutorMetrics", });
    internal_static_org_apache_spark_status_protobuf_AppSummary_descriptor =
      getDescriptor().getMessageTypes().get(61);
    internal_static_org_apache_spark_status_protobuf_AppSummary_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_AppSummary_descriptor,
        new java.lang.String[] { "NumCompletedJobs", "NumCompletedStages", });
    internal_static_org_apache_spark_status_protobuf_PoolData_descriptor =
      getDescriptor().getMessageTypes().get(62);
    internal_static_org_apache_spark_status_protobuf_PoolData_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_PoolData_descriptor,
        new java.lang.String[] { "Name", "StageIds", "Name", });
    internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_descriptor =
      getDescriptor().getMessageTypes().get(63);
    internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_descriptor,
        new java.lang.String[] { "OperatorName", "NumRowsTotal", "NumRowsUpdated", "AllUpdatesTimeMs", "NumRowsRemoved", "AllRemovalsTimeMs", "CommitTimeMs", "MemoryUsedBytes", "NumRowsDroppedByWatermark", "NumShufflePartitions", "NumStateStoreInstances", "CustomMetrics", "OperatorName", });
    internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_CustomMetricsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_CustomMetricsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StateOperatorProgress_CustomMetricsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_SourceProgress_descriptor =
      getDescriptor().getMessageTypes().get(64);
    internal_static_org_apache_spark_status_protobuf_SourceProgress_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SourceProgress_descriptor,
        new java.lang.String[] { "Description", "StartOffset", "EndOffset", "LatestOffset", "NumInputRows", "InputRowsPerSecond", "ProcessedRowsPerSecond", "Metrics", "Description", "StartOffset", "EndOffset", "LatestOffset", });
    internal_static_org_apache_spark_status_protobuf_SourceProgress_MetricsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_SourceProgress_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_SourceProgress_MetricsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SourceProgress_MetricsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_SinkProgress_descriptor =
      getDescriptor().getMessageTypes().get(65);
    internal_static_org_apache_spark_status_protobuf_SinkProgress_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SinkProgress_descriptor,
        new java.lang.String[] { "Description", "NumOutputRows", "Metrics", "Description", });
    internal_static_org_apache_spark_status_protobuf_SinkProgress_MetricsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_SinkProgress_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_SinkProgress_MetricsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_SinkProgress_MetricsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor =
      getDescriptor().getMessageTypes().get(66);
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor,
        new java.lang.String[] { "Id", "RunId", "Name", "Timestamp", "BatchId", "BatchDuration", "DurationMs", "EventTime", "StateOperators", "Sources", "Sink", "ObservedMetrics", "Id", "RunId", "Name", "Timestamp", });
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_DurationMsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor.getNestedTypes().get(0);
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_DurationMsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_DurationMsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_EventTimeEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor.getNestedTypes().get(1);
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_EventTimeEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_EventTimeEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_ObservedMetricsEntry_descriptor =
      internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_descriptor.getNestedTypes().get(2);
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_ObservedMetricsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StreamingQueryProgress_ObservedMetricsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_descriptor =
      getDescriptor().getMessageTypes().get(67);
    internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_status_protobuf_StreamingQueryProgressWrapper_descriptor,
        new java.lang.String[] { "Progress", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
